import jpype
import re
import pickle
import platform
import pandas as pd
import os
import wget
import tarfile
import shutil
import subprocess
import gzip
import zipfile  # for windows
import glob
from lxml import etree
import io  # debugging only
from datetime import datetime, timedelta

from nlp_modules.configuration import (
    XML_ATTRIB_REFDATE,
    HEIDELTIME_STANDALONE,
    TREETAGGER_LINUX,
    TREETAGGER_EXEC,
    TREETAGGER_SCRIPTS,
    TREETAGGER_PARAMETER_FILES_BNC,
    TREETAGGER_PARAMETER_FILES_PENN,
    TREETAGGER_CHUNKER,
    TREETAGGER_MACOSX,
    TREETAGGER_WINDOWS,
)

from nlp_modules.base import NLPModule, PipelineDep, NLPDependencyException
from datetime import datetime

pd.options.mode.chained_assignment = None


class HeidelTimeWrapper:

    """
    Wrapper around java-based HeidelTime - uses jpype to manage JVM via python
    Thanks is due to : https://github.com/amineabdaoui/python-heideltime.git
    This speeds up per-file processing in the JVM by about 5 times versus an os.process based call
    (roughly 1 minute for the full GUM corpus v6.0)

    JPype in 64-bit windows needs 64-bit java for 64-bit python
    need to add the 64-bit java / JRE folder to the JAVA_HOME variable in Windows
    64-bit java is generally in C:\\Program Files\\Java\\<your jre folder>>

    """

    def __init__(self, lang, config=None, doc=None, output=None, jarpath=None):

        if not jpype.isJVMStarted():
            jpype.startJVM(
                jpype.getDefaultJVMPath(),
                "-ea",
                "-Djava.class.path=%s" % jarpath,
                convertStrings=True,
            )

        # get the Java classes we want to use
        heideltime_resources = jpype.JPackage(
            "de.unihd.dbs.uima.annotator.heideltime.resources"
        )
        heideltime_standalone = jpype.JPackage("de.unihd.dbs.heideltime.standalone")

        # constants
        LANGUAGES = {
            "english": heideltime_resources.Language.ENGLISH
            #'german': heideltime_resources.Language.GERMAN,
            #'dutch': heideltime_resources.Language.DUTCH,
            #'italian': heideltime_resources.Language.ITALIAN,
            #'spanish': heideltime_resources.Language.SPANISH,
            #'arabic': heideltime_resources.Language.ARABIC,
            #'french': heideltime_resources.Language.FRENCH,
            #'chinese': heideltime_resources.Language.CHINESE,
            #'russian': heideltime_resources.Language.RUSSIAN,
            #'portuguese': heideltime_resources.Language.PORTUGUESE
        }

        DOCUMENTS = {
            "narratives": heideltime_standalone.DocumentType.NARRATIVES,
            "news": heideltime_standalone.DocumentType.NEWS,
            "colloquial": heideltime_standalone.DocumentType.COLLOQUIAL,
            "scientific": heideltime_standalone.DocumentType.SCIENTIFIC,
        }

        OUTPUTS = {
            "timeml": heideltime_standalone.OutputType.TIMEML,
            "xmi": heideltime_standalone.OutputType.XMI,
        }

        self.language = LANGUAGES[lang]
        if doc is None:
            self.doc_type = DOCUMENTS["news"]
        else:
            self.doc_type = DOCUMENTS[doc]
        if output is None:
            self.output_type = OUTPUTS["timeml"]
        else:
            self.output_type = OUTPUTS[output]

        self.heideltime = heideltime_standalone.HeidelTimeStandalone(
            self.language, self.doc_type, self.output_type, config
        )

    def convert_date(self, day, month, year):
        sdf = jpype.java.text.SimpleDateFormat("dd-M-yyyy hh:mm:ss")
        str_date = str(day) + "-" + str(month) + "-" + str(year) + " 00:00:00"
        return sdf.parse(str_date)

    def parse(self, text, date_ref=None):
        """
        date_ref format must be YYYY-MM-DD or exception
        this may be guaranteed if dateCreated in the xml is YYYY-MM-DD
        """
        if date_ref is None:
            document_creation_date = jpype.java.util.Date()
        else:
            # convert to Java.util.Date
            document_creation_date = self.convert_date(
                date_ref.split("-")[2], date_ref.split("-")[1], date_ref.split("-")[0]
            )

        # the main juice...ensure that convertString param is set to True when starting the JVM or you'll get a java String here
        return self.heideltime.process(text, document_creation_date)


class DateTimeFilterModel:

    """
    A model that removes FPs generated by Heideltime - including TIMEX3 SETS, concepts like 'now','currently' ,
    and most dates/durations without an anchor date or in adjectival form e.g the 21-year itch or tomorrow / today
    """

    def __init__(self, modelfile, datafile=None):
        """
        modelfile - path to the pickle file used as the filter
        datafile - path to the training data file. #TODO
        """

        self.articles = ["a", "an"]

        # load the random forest model
        with open(modelfile, "rb") as f:
            self.rf = pickle.load(f)

        # this is just a template and not the real feature set; this is used to build the real featureset
        # the postags are taken from the label encoder model that encodes the result of the pos tag ensembler
        self.featuredict = {
            "obl": 0,
            "obl:npmod": 0,
            "obl:tmod": 0,
            "nsubj": 0,
            "nsubj:pass": 0,
            "obj": 0,
            "iobj": 0,
            "csubj": 0,
            "csubj:pass": 0,
            "ccomp": 0,
            "xcomp": 0,
            "nummod": 0,
            "acl": 0,
            "amod": 0,
            "appos": 0,
            "acl:relcl": 0,
            "det": 0,
            "det:predet": 0,
            "neg": 0,
            "nmod": 0,
            "case": 0,
            "nmod:npmod": 0,
            "nmod:tmod": 0,
            "nmod:poss": 0,
            "advcl": 0,
            "advmod": 0,
            "compound": 0,
            "compound:prt": 0,
            "flat": 0,
            "fixed": 0,
            "foreign": 0,
            "goeswith": 0,
            "list": 0,
            "dislocated": 0,
            "parataxis": 0,
            "orphan": 0,
            "reparandum": 0,
            "vocative": 0,
            "discourse": 0,
            "expl": 0,
            "aux": 0,
            "aux:pass": 0,
            "cop": 0,
            "mark": 0,
            "punct": 0,
            "conj": 0,
            "cc": 0,
            "cc:preconj": 0,
            "root": 0,
            "dep": 0,
            "$": 0,
            "''": 0,
            ", ": 0,
            "-LRB-": 0,
            "-LSB-": 0,
            "-RRB-": 0,
            "-RSB-": 0,
            ".": 0,
            ":": 0,
            "ADD": 0,
            "ADJ": 0,
            "NP": 0,
            "ADP": 0,
            "ADV": 0,
            "AFX": 0,
            "AUX": 0,
            "CC": 0,
            "CCONJ": 0,
            "CD": 0,
            "DET": 0,
            "DT": 0,
            "EX": 0,
            "FW": 0,
            "GW": 0,
            "HYPH": 0,
            "IN": 0,
            "INTJ": 0,
            "JJ": 0,
            "JJR": 0,
            "JJS": 0,
            "LS": 0,
            "MD": 0,
            "NFP": 0,
            "NN": 0,
            "NNP": 0,
            "NNPS": 0,
            "NNS": 0,
            "NOUN": 0,
            "NUM": 0,
            "PART": 0,
            "PDT": 0,
            "POS": 0,
            "PRON": 0,
            "PROPN": 0,
            "PRP": 0,
            "PRP$": 0,
            "PUNCT": 0,
            "RB": 0,
            "RBR": 0,
            "RBS": 0,
            "RP": 0,
            "SYM": 0,
            "TO": 0,
            "UH": 0,
            "VB": 0,
            "VBD": 0,
            "VBG": 0,
            "VBN": 0,
            "VBP": 0,
            "VBZ": 0,
            "VERB": 0,
            "WDT": 0,
            "WP": 0,
            "WP$": 0,
            "WRB": 0,
            "X": 0,
            "``": 0,
            "january": 0,
            "february": 0,
            "march": 0,
            "april": 0,
            "may": 0,
            "june": 0,
            "july": 0,
            "august": 0,
            "september": 0,
            "october": 0,
            "november": 0,
            "december": 0,
            "summer": 0,
            "winter": 0,
            "autumn": 0,
            "spring": 0,
            "christmas": 0,
            "christmas_eve": 0,
            "easter": 0,
            "easter_sunday": 0,
            "monday": 0,
            "tuesday": 0,
            "wednesday": 0,
            "thursday": 0,
            "friday": 0,
            "saturday": 0,
            "sunday": 0,
            "start1_obl": 0,
            "start1_obl:npmod": 0,
            "start1_obl:tmod": 0,
            "start1_nsubj": 0,
            "start1_nsubj:pass": 0,
            "start1_obj": 0,
            "start1_iobj": 0,
            "start1_csubj": 0,
            "start1_csubj:pass": 0,
            "start1_ccomp": 0,
            "start1_xcomp": 0,
            "start1_nummod": 0,
            "start1_acl": 0,
            "start1_amod": 0,
            "start1_appos": 0,
            "start1_acl:relcl": 0,
            "start1_det": 0,
            "start1_det:predet": 0,
            "start1_neg": 0,
            "start1_nmod": 0,
            "start1_case": 0,
            "start1_nmod:npmod": 0,
            "start1_nmod:tmod": 0,
            "start1_nmod:poss": 0,
            "start1_advcl": 0,
            "start1_advmod": 0,
            "start1_compound": 0,
            "start1_compound:prt": 0,
            "start1_flat": 0,
            "start1_fixed": 0,
            "start1_foreign": 0,
            "start1_goeswith": 0,
            "start1_list": 0,
            "start1_dislocated": 0,
            "start1_parataxis": 0,
            "start1_orphan": 0,
            "start1_reparandum": 0,
            "start1_vocative": 0,
            "start1_discourse": 0,
            "start1_expl": 0,
            "start1_aux": 0,
            "start1_aux:pass": 0,
            "start1_cop": 0,
            "start1_mark": 0,
            "start1_conj": 0,
            "start1_cc": 0,
            "start1_cc:preconj": 0,
            "start1_punct": 0,
            "start1_root": 0,
            "start1_dep": 0,
            "start1_$": 0,
            "start1_''": 0,
            "start1_, ": 0,
            "start1_-LRB-": 0,
            "start1_-LSB-": 0,
            "start1_-RRB-": 0,
            "start1_-RSB-": 0,
            "start1_.": 0,
            "start1_:": 0,
            "start1_ADD": 0,
            "start1_ADJ": 0,
            "start1_NP": 0,
            "start1_ADP": 0,
            "start1_ADV": 0,
            "start1_AFX": 0,
            "start1_AUX": 0,
            "start1_CC": 0,
            "start1_CCONJ": 0,
            "start1_CD": 0,
            "start1_DET": 0,
            "start1_DT": 0,
            "start1_EX": 0,
            "start1_FW": 0,
            "start1_GW": 0,
            "start1_HYPH": 0,
            "start1_IN": 0,
            "start1_INTJ": 0,
            "start1_JJ": 0,
            "start1_JJR": 0,
            "start1_JJS": 0,
            "start1_LS": 0,
            "start1_MD": 0,
            "start1_NFP": 0,
            "start1_NN": 0,
            "start1_NNP": 0,
            "start1_NNPS": 0,
            "start1_NNS": 0,
            "start1_NOUN": 0,
            "start1_NUM": 0,
            "start1_PART": 0,
            "start1_PDT": 0,
            "start1_POS": 0,
            "start1_PRON": 0,
            "start1_PROPN": 0,
            "start1_PRP": 0,
            "start1_PRP$": 0,
            "start1_PUNCT": 0,
            "start1_RB": 0,
            "start1_RBR": 0,
            "start1_RBS": 0,
            "start1_RP": 0,
            "start1_SYM": 0,
            "start1_TO": 0,
            "start1_UH": 0,
            "start1_VB": 0,
            "start1_VBD": 0,
            "start1_VBG": 0,
            "start1_VBN": 0,
            "start1_VBP": 0,
            "start1_VBZ": 0,
            "start1_VERB": 0,
            "start1_WDT": 0,
            "start1_WP": 0,
            "start1_WP$": 0,
            "start1_WRB": 0,
            "start1_X": 0,
            "start1_``": 0,
            "end1_obl": 0,
            "end1_obl:npmod": 0,
            "end1_obl:tmod": 0,
            "end1_nsubj": 0,
            "end1_nsubj:pass": 0,
            "end1_obj": 0,
            "end1_iobj": 0,
            "end1_csubj": 0,
            "end1_csubj:pass": 0,
            "end1_ccomp": 0,
            "end1_xcomp": 0,
            "end1_nummod": 0,
            "end1_acl": 0,
            "end1_amod": 0,
            "end1_appos": 0,
            "end1_acl:relcl": 0,
            "end1_det": 0,
            "end1_det:predet": 0,
            "end1_neg": 0,
            "end1_nmod": 0,
            "end1_case": 0,
            "end1_nmod:npmod": 0,
            "end1_nmod:tmod": 0,
            "end1_nmod:poss": 0,
            "end1_advcl": 0,
            "end1_advmod": 0,
            "end1_compound": 0,
            "end1_compound:prt": 0,
            "end1_flat": 0,
            "end1_fixed": 0,
            "end1_foreign": 0,
            "end1_goeswith": 0,
            "end1_list": 0,
            "end1_dislocated": 0,
            "end1_parataxis": 0,
            "end1_orphan": 0,
            "end1_reparandum": 0,
            "end1_vocative": 0,
            "end1_discourse": 0,
            "end1_expl": 0,
            "end1_aux": 0,
            "end1_aux:pass": 0,
            "end1_cop": 0,
            "end1_mark": 0,
            "end1_punct": 0,
            "end1_conj": 0,
            "end1_cc": 0,
            "end1_cc:preconj": 0,
            "end1_root": 0,
            "end1_dep": 0,
            "end1_$": 0,
            "end1_''": 0,
            "end1_, ": 0,
            "end1_-LRB-": 0,
            "end1_-LSB-": 0,
            "end1_-RRB-": 0,
            "end1_-RSB-": 0,
            "end1_.": 0,
            "end1_:": 0,
            "end1_ADD": 0,
            "end1_ADJ": 0,
            "end1_NP": 0,
            "end1_ADP": 0,
            "end1_ADV": 0,
            "end1_AFX": 0,
            "end1_AUX": 0,
            "end1_CC": 0,
            "end1_CCONJ": 0,
            "end1_CD": 0,
            "end1_DET": 0,
            "end1_DT": 0,
            "end1_EX": 0,
            "end1_FW": 0,
            "end1_GW": 0,
            "end1_HYPH": 0,
            "end1_IN": 0,
            "end1_INTJ": 0,
            "end1_JJ": 0,
            "end1_JJR": 0,
            "end1_JJS": 0,
            "end1_LS": 0,
            "end1_MD": 0,
            "end1_NFP": 0,
            "end1_NN": 0,
            "end1_NNP": 0,
            "end1_NNPS": 0,
            "end1_NNS": 0,
            "end1_NOUN": 0,
            "end1_NUM": 0,
            "end1_PART": 0,
            "end1_PDT": 0,
            "end1_POS": 0,
            "end1_PRON": 0,
            "end1_PROPN": 0,
            "end1_PRP": 0,
            "end1_PRP$": 0,
            "end1_PUNCT": 0,
            "end1_RB": 0,
            "end1_RBR": 0,
            "end1_RBS": 0,
            "end1_RP": 0,
            "end1_SYM": 0,
            "end1_TO": 0,
            "end1_UH": 0,
            "end1_VB": 0,
            "end1_VBD": 0,
            "end1_VBG": 0,
            "end1_VBN": 0,
            "end1_VBP": 0,
            "end1_VBZ": 0,
            "end1_VERB": 0,
            "end1_WDT": 0,
            "end1_WP": 0,
            "end1_WP$": 0,
            "end1_WRB": 0,
            "end1_X": 0,
            "end1_``": 0,
            "end2_obl": 0,
            "end2_obl:npmod": 0,
            "end2_obl:tmod": 0,
            "end2_nsubj": 0,
            "end2_nsubj:pass": 0,
            "end2_obj": 0,
            "end2_iobj": 0,
            "end2_csubj": 0,
            "end2_csubj:pass": 0,
            "end2_ccomp": 0,
            "end2_xcomp": 0,
            "end2_nummod": 0,
            "end2_acl": 0,
            "end2_amod": 0,
            "end2_appos": 0,
            "end2_acl:relcl": 0,
            "end2_det": 0,
            "end2_det:predet": 0,
            "end2_neg": 0,
            "end2_nmod": 0,
            "end2_case": 0,
            "end2_nmod:npmod": 0,
            "end2_nmod:tmod": 0,
            "end2_nmod:poss": 0,
            "end2_advcl": 0,
            "end2_advmod": 0,
            "end2_compound": 0,
            "end2_compound:prt": 0,
            "end2_flat": 0,
            "end2_fixed": 0,
            "end2_foreign": 0,
            "end2_goeswith": 0,
            "end2_list": 0,
            "end2_dislocated": 0,
            "end2_parataxis": 0,
            "end2_orphan": 0,
            "end2_reparandum": 0,
            "end2_vocative": 0,
            "end2_discourse": 0,
            "end2_expl": 0,
            "end2_aux": 0,
            "end2_aux:pass": 0,
            "end2_cop": 0,
            "end2_mark": 0,
            "end2_punct": 0,
            "end2_conj": 0,
            "end2_cc": 0,
            "end2_cc:preconj": 0,
            "end2_root": 0,
            "end2_dep": 0,
            "end2_$": 0,
            "end2_''": 0,
            "end2_, ": 0,
            "end2_-LRB-": 0,
            "end2_-LSB-": 0,
            "end2_-RRB-": 0,
            "end2_-RSB-": 0,
            "end2_.": 0,
            "end2_:": 0,
            "end2_ADD": 0,
            "end2_ADJ": 0,
            "end2_NP": 0,
            "end2_ADP": 0,
            "end2_ADV": 0,
            "end2_AFX": 0,
            "end2_AUX": 0,
            "end2_CC": 0,
            "end2_CCONJ": 0,
            "end2_CD": 0,
            "end2_DET": 0,
            "end2_DT": 0,
            "end2_EX": 0,
            "end2_FW": 0,
            "end2_GW": 0,
            "end2_HYPH": 0,
            "end2_IN": 0,
            "end2_INTJ": 0,
            "end2_JJ": 0,
            "end2_JJR": 0,
            "end2_JJS": 0,
            "end2_LS": 0,
            "end2_MD": 0,
            "end2_NFP": 0,
            "end2_NN": 0,
            "end2_NNP": 0,
            "end2_NNPS": 0,
            "end2_NNS": 0,
            "end2_NOUN": 0,
            "end2_NUM": 0,
            "end2_PART": 0,
            "end2_PDT": 0,
            "end2_POS": 0,
            "end2_PRON": 0,
            "end2_PROPN": 0,
            "end2_PRP": 0,
            "end2_PP$": 0,
            "end1_PP$": 0,
            "start2_PP$": 0,
            "start1_PP$": 0,
            "end2_PP": 0,
            "end1_PP": 0,
            "start2_PP": 0,
            "start1_PP": 0,
            "end2_PRP$": 0,
            "end2_PUNCT": 0,
            "end2_RB": 0,
            "end2_RBR": 0,
            "end2_RBS": 0,
            "end2_RP": 0,
            "end2_SYM": 0,
            "end2_TO": 0,
            "end2_UH": 0,
            "end2_VB": 0,
            "end2_VBD": 0,
            "end2_VBG": 0,
            "end2_VBN": 0,
            "end2_VBP": 0,
            "end2_VBZ": 0,
            "end2_VERB": 0,
            "end2_WDT": 0,
            "end2_WP": 0,
            "end2_WP$": 0,
            "end2_WRB": 0,
            "end2_X": 0,
            "end2_``": 0,
            "start2_obl": 0,
            "start2_obl:npmod": 0,
            "start2_obl:tmod": 0,
            "start2_nsubj": 0,
            "start2_nsubj:pass": 0,
            "start2_obj": 0,
            "start2_iobj": 0,
            "start2_csubj": 0,
            "start2_csubj:pass": 0,
            "start2_ccomp": 0,
            "start2_xcomp": 0,
            "start2_nummod": 0,
            "start2_acl": 0,
            "start2_amod": 0,
            "start2_appos": 0,
            "start2_acl:relcl": 0,
            "start2_det": 0,
            "start2_det:predet": 0,
            "start2_neg": 0,
            "start2_nmod": 0,
            "start2_case": 0,
            "start2_nmod:npmod": 0,
            "start2_nmod:tmod": 0,
            "start2_nmod:poss": 0,
            "start2_advcl": 0,
            "start2_advmod": 0,
            "start2_compound": 0,
            "start2_compound:prt": 0,
            "start2_flat": 0,
            "start2_fixed": 0,
            "start2_foreign": 0,
            "start2_goeswith": 0,
            "start2_list": 0,
            "start2_dislocated": 0,
            "start2_parataxis": 0,
            "start2_orphan": 0,
            "start2_reparandum": 0,
            "start2_vocative": 0,
            "start2_discourse": 0,
            "start2_expl": 0,
            "start2_aux": 0,
            "start2_aux:pass": 0,
            "start2_cop": 0,
            "start2_mark": 0,
            "start2_punct": 0,
            "start2_conj": 0,
            "start2_cc": 0,
            "start2_cc:preconj": 0,
            "start2_root": 0,
            "start2_dep": 0,
            "start2_$": 0,
            "start2_''": 0,
            "start2_, ": 0,
            "start2_-LRB-": 0,
            "start2_-LSB-": 0,
            "start2_-RRB-": 0,
            "start2_-RSB-": 0,
            "start2_.": 0,
            "start2_:": 0,
            "start2_ADD": 0,
            "start2_ADJ": 0,
            "start2_NP": 0,
            "start2_ADP": 0,
            "start2_ADV": 0,
            "start2_AFX": 0,
            "start2_AUX": 0,
            "start2_CC": 0,
            "start2_CCONJ": 0,
            "start2_CD": 0,
            "start2_DET": 0,
            "start2_DT": 0,
            "start2_EX": 0,
            "start2_FW": 0,
            "start2_GW": 0,
            "start2_HYPH": 0,
            "start2_IN": 0,
            "start2_INTJ": 0,
            "start2_JJ": 0,
            "start2_JJR": 0,
            "start2_JJS": 0,
            "start2_LS": 0,
            "start2_MD": 0,
            "start2_NFP": 0,
            "start2_NN": 0,
            "start2_NNP": 0,
            "start2_NNPS": 0,
            "start2_NNS": 0,
            "start2_NOUN": 0,
            "start2_NUM": 0,
            "start2_PART": 0,
            "start2_PDT": 0,
            "start2_POS": 0,
            "start2_PRON": 0,
            "start2_PROPN": 0,
            "start2_PRP": 0,
            "start2_PRP$": 0,
            "start2_PUNCT": 0,
            "start2_RB": 0,
            "start2_RBR": 0,
            "start2_RBS": 0,
            "start2_RP": 0,
            "start2_SYM": 0,
            "start2_TO": 0,
            "start2_UH": 0,
            "start2_VB": 0,
            "start2_VBD": 0,
            "start2_VBG": 0,
            "start2_VBN": 0,
            "start2_VBP": 0,
            "start2_VBZ": 0,
            "start2_VERB": 0,
            "start2_WDT": 0,
            "start2_WP": 0,
            "start2_WP$": 0,
            "start2_WRB": 0,
            "start2_X": 0,
            "start2_``": 0,
            "day": 0,
            "century": 0,
            "millenia": 0,
            "hour": 0,
            "minute": 0,
            "year": 0,
            "second": 0,
            "month": 0,
            "start2_day": 0,
            "start2_century": 0,
            "start2_millenia": 0,
            "start2_hour": 0,
            "start2_minute": 0,
            "start2_year": 0,
            "start2_second": 0,
            "start2_month": 0,
            "start1_day": 0,
            "start1_century": 0,
            "start1_millenia": 0,
            "start1_hour": 0,
            "start1_minute": 0,
            "start1_year": 0,
            "start1_second": 0,
            "start1_month": 0,
            "end2_day": 0,
            "end2_century": 0,
            "end2_millenia": 0,
            "end2_hour": 0,
            "end2_minute": 0,
            "end2_year": 0,
            "end2_second": 0,
            "end2_month": 0,
            "end1_day": 0,
            "end1_century": 0,
            "end1_millenia": 0,
            "end1_hour": 0,
            "end1_minute": 0,
            "end1_year": 0,
            "end1_second": 0,
            "end1_month": 0,
            "start1_january": 0,
            "start1_february": 0,
            "start1_march": 0,
            "start1_april": 0,
            "start1_may": 0,
            "start1_june": 0,
            "start1_july": 0,
            "start1_august": 0,
            "start1_september": 0,
            "start1_october": 0,
            "start1_november": 0,
            "start1_december": 0,
            "start1_summer": 0,
            "start1_winter": 0,
            "start1_autumn": 0,
            "start1_spring": 0,
            "start1_christmas": 0,
            "start1_christmas_eve": 0,
            "start1_easter": 0,
            "start1_easter_sunday": 0,
            "start1_monday": 0,
            "start1_tuesday": 0,
            "start1_wednesday": 0,
            "start1_thursday": 0,
            "start1_friday": 0,
            "start1_saturday": 0,
            "start1_sunday": 0,
            "start2_january": 0,
            "start2_february": 0,
            "start2_march": 0,
            "start2_april": 0,
            "start2_may": 0,
            "start2_june": 0,
            "start2_july": 0,
            "start2_august": 0,
            "start2_september": 0,
            "start2_october": 0,
            "start2_november": 0,
            "start2_december": 0,
            "start2_summer": 0,
            "start2_winter": 0,
            "start2_autumn": 0,
            "start2_spring": 0,
            "start2_christmas": 0,
            "start2_christmas_eve": 0,
            "start2_easter": 0,
            "start2_easter_sunday": 0,
            "start2_monday": 0,
            "start2_tuesday": 0,
            "start2_wednesday": 0,
            "start2_thursday": 0,
            "start2_friday": 0,
            "start2_saturday": 0,
            "start2_sunday": 0,
            "end2_january": 0,
            "end2_february": 0,
            "end2_march": 0,
            "end2_april": 0,
            "end2_may": 0,
            "end2_june": 0,
            "end2_july": 0,
            "end2_august": 0,
            "end2_september": 0,
            "end2_october": 0,
            "end2_november": 0,
            "end2_december": 0,
            "end2_summer": 0,
            "end2_winter": 0,
            "end2_autumn": 0,
            "end2_spring": 0,
            "end2_christmas": 0,
            "end2_christmas_eve": 0,
            "end2_easter": 0,
            "end2_easter_sunday": 0,
            "end2_monday": 0,
            "end2_tuesday": 0,
            "end2_wednesday": 0,
            "end2_thursday": 0,
            "end2_friday": 0,
            "end2_saturday": 0,
            "end2_sunday": 0,
            "end1_january": 0,
            "end1_february": 0,
            "end1_march": 0,
            "end1_april": 0,
            "end1_may": 0,
            "end1_june": 0,
            "end1_july": 0,
            "end1_august": 0,
            "end1_september": 0,
            "end1_october": 0,
            "end1_november": 0,
            "end1_december": 0,
            "end1_summer": 0,
            "end1_winter": 0,
            "end1_autumn": 0,
            "end1_spring": 0,
            "end1_christmas": 0,
            "end1_christmas_eve": 0,
            "end1_easter": 0,
            "end1_easter_sunday": 0,
            "end1_monday": 0,
            "end1_tuesday": 0,
            "end1_wednesday": 0,
            "end1_thursday": 0,
            "end1_friday": 0,
            "end1_saturday": 0,
            "end1_sunday": 0,
        }

    def train(self):
        pass  # TODO

    def inference(self):
        pass

    def build_featureset(
        self, sfull, stok, phrase, index, timextype, timexvalue, timexmod
    ):
        """
        Builds a row of features for the date phrase from a template and adds some extra features
        sfull is the sentence with each token tagged with its corresponding feature
        stok is just the sentence made up of its english tokens
        phrase is the date phrase from HT that we are going to classify as a valid Time Expression (or no) in the context of GUM
        timex type, value, mod are TIMEX3 attributes that HT has created for this phrase
        """

        # Otherwise the split() method wont work well and the indexes will be off
        sfull = re.sub(" +", " ", sfull)
        stok = re.sub(" +", " ", stok)
        phrase = re.sub(" +", " ", phrase)

        features = sfull.split()
        search = re.search(
            re.escape(phrase.lower()), stok.lower()
        )  # this is not supposed to return None,  generally

        # Gets the start index of the phrase in the sentence
        if search.start() > 0:
            startindex = stok[0 : search.start()].count(" ")
        else:
            startindex = 0

        endindex = startindex + len(phrase.split()) - 1

        # start building the features now that we have the indices of the phrase in the sentence
        # Features are basically manually built count vectors of the defined featureset in the template
        fdict = self.featuredict.copy()  # need to do deep copy from the template
        featurelist = fdict.keys()

        # This will check features that are specific word tokens in the sentence,  e.g January,  Tuesday,
        for i in range(startindex, endindex + 1, 1):
            feats = features[i].split("/")  # get the token word
            if str(feats[1]).lower() in featurelist:
                fdict[
                    str(feats[1]).lower()
                ] += 1  # Increment if the word is in the feature list
            if str(feats[4]) in fdict.keys():
                fdict[str(feats[4])] += 1  # the Penn treebank POS tag
            if str(feats[7]) in fdict.keys():
                fdict[str(feats[7])] += 1  # the UD tag

        # Get the +2 and -2 token features
        if startindex > 0:
            feats = features[startindex - 1].split("/")  # get the token word
            if str(feats[1]).lower() in fdict.keys():
                fdict["start1_" + str(feats[1]).lower()] += 1
            if str(feats[4]) in fdict.keys():
                fdict["start1_" + str(feats[4])] += 1
            if str(feats[7]) in fdict.keys():
                fdict["start1_" + str(feats[7])] += 1

        if startindex > 1:
            feats = features[startindex - 2].split("/")  # get the token word
            if str(feats[1]).lower() in fdict.keys():
                fdict["start2_" + str(feats[1]).lower()] += 1
            if str(feats[4]) in fdict.keys():
                fdict["start2_" + str(feats[4])] += 1
            if str(feats[7]) in fdict.keys():
                fdict["start2_" + str(feats[7])] += 1

        if endindex + 1 < len(features):
            feats = features[endindex + 1].split("/")  # get the token word
            if str(feats[1]).lower() in fdict.keys():
                fdict["end1_" + str(feats[1]).lower()] += 1
            if str(feats[4]) in fdict.keys():
                fdict["end1_" + str(feats[4])] += 1
            if str(feats[7]) in fdict.keys():
                fdict["end1_" + str(feats[7])] += 1

        if endindex + 2 < len(features):
            feats = features[endindex + 2].split("/")  # get the token word
            if str(feats[1]).lower() in fdict.keys():
                fdict["end2_" + str(feats[1]).lower()] += 1
            if str(feats[4]) in fdict.keys():
                fdict["end2_" + str(feats[4])] += 1
            if str(feats[7]) in fdict.keys():
                fdict["end2_" + str(feats[7])] += 1

        # and finally,  dump out all the other interactions
        fdict["CD_nmod"] = int(fdict["CD"]) * int(fdict["nmod"])
        fdict["CD_nmodtmod"] = int(fdict["CD"]) * int(fdict["nmod:tmod"])
        fdict["CD_compound"] = int(fdict["CD"]) * int(fdict["compound"])
        fdict["CD_nummod"] = int(fdict["CD"]) * int(fdict["nummod"])

        fdict["countmods"] = (
            fdict["obl:npmod"]
            + fdict["obl:tmod"]
            + fdict["nummod"]
            + fdict["amod"]
            + fdict["nmod"]
            + fdict["nmod:npmod"]
            + fdict["nmod:tmod"]
            + fdict["nmod:poss"]
            + fdict["advmod"]
        )

        fdict["summer_compound"] = fdict["summer"] * fdict["compound"]
        fdict["winter_compound"] = fdict["winter"] * fdict["compound"]
        fdict["autumn_compound"] = fdict["autumn"] * fdict["compound"]
        fdict["spring_compound"] = fdict["spring"] * fdict["compound"]

        fdict["easter_compound"] = fdict["easter"] * fdict["compound"]
        fdict["eastersunday_compound"] = fdict["easter_sunday"] * fdict["compound"]
        fdict["christmas_compound"] = fdict["christmas"] * fdict["compound"]
        fdict["christmaseve_compound"] = fdict["christmas_eve"] * fdict["compound"]

        fdict["CD_obl"] = fdict["CD"] * fdict["obl"]
        fdict["CD_dep"] = fdict["CD"] * fdict["dep"]
        fdict["sunday_obl"] = fdict["sunday"] * fdict["obl"]
        fdict["monday_obl"] = fdict["monday"] * fdict["obl"]
        fdict["tuesday_obl"] = fdict["tuesday"] * fdict["obl"]
        fdict["wednesday_obl"] = fdict["wednesday"] * fdict["obl"]
        fdict["thursday_obl"] = fdict["thursday"] * fdict["obl"]
        fdict["friday_obl"] = fdict["friday"] * fdict["obl"]
        fdict["saturday_obl"] = fdict["saturday"] * fdict["obl"]

        fdict["NP_flat"] = fdict["NP"] * fdict["flat"]
        fdict["NP_obl"] = fdict["NP"] * fdict["obl"]
        fdict["NN_obl"] = fdict["NN"] * fdict["obl"]
        fdict["NP_nummod"] = fdict["NP"] * fdict["nummod"]
        fdict["NN_nummod"] = fdict["NN"] * fdict["nummod"]
        fdict["NP_nmod"] = fdict["NP"] * fdict["nmod"]
        fdict["NN_nmod"] = fdict["NN"] * fdict["nmod"]
        fdict["NP_compound"] = fdict["NP"] * fdict["compound"]
        fdict["NN_compound"] = fdict["NN"] * fdict["compound"]
        fdict["RB_amod"] = fdict["RB"] * fdict["amod"]
        fdict["JJ_amod"] = fdict["JJ"] * fdict["amod"]
        fdict["NN_obltmod"] = fdict["NN"] * fdict["obl:tmod"]

        # these become useful when adding the tags to the xml
        fdict["sentence_index"] = index + 1
        fdict["start_index"] = int(startindex)
        fdict["phrase"] = phrase
        fdict["timextype"] = timextype
        fdict["timexvalue"] = timexvalue
        fdict["timexmod"] = timexmod

        return fdict


class DateTimeRecognizer(NLPModule):
    def __init__(self, config):

        super().__init__(config=None)

        self.ostype = platform.system()

        # self.parser = etree.XMLParser(
        #    recover=True
        # )  # Unused

        self.decoding = "ascii"
        self.binpath = config["BIN_DIR"]
        self.htbin = (
            self.binpath + "datetime" + os.sep + "heideltime-standalone" + os.sep
        )
        self.ttgbin = self.binpath + config["TTG_PATH"]
        self.htfiltermodel = (
            self.binpath
            + "datetime"
            + os.sep
            + "datetimefilter_sklearn_rf_0_22_1.pickle"
        )  # stored in the github

        if os.path.exists(
            self.htbin
            + "heideltime-standalone"
            + os.sep
            + "de.unihd.dbs.heideltime.standalone.jar"
        ):
            try:
                self.htjarpath = (
                    self.htbin
                    + "heideltime-standalone"
                    + os.sep
                    + "de.unihd.dbs.heideltime.standalone.jar"
                )
                self.htconfigpropspath = (
                    self.htbin + "heideltime-standalone" + os.sep + "config.props"
                )
                self.hw = HeidelTimeWrapper(
                    "english",
                    config=self.htconfigpropspath,
                    doc="news",
                    jarpath=self.htjarpath,
                )
                self.hwnarr = HeidelTimeWrapper(
                    "english",
                    config=self.htconfigpropspath,
                    doc="narratives",
                    jarpath=self.htjarpath,
                )
            except Exception:
                raise NLPDependencyException(
                    "If your platform is Windows, please ensure you have a) installed a 64-bit JRE, and b) set the 64-bit JRE folder location in the JAVA_HOME environment variable as a higher priority. Alternatively, bypass the Date/Time recognizer module."
                )

        # object for refining HT dates to better align to GUM standards
        self.datefilter = DateTimeFilterModel(modelfile=self.htfiltermodel)

        # to extract TIMEX3 expressions from TimeML xml standard
        self.timex3regex = r"<TIMEX3.*?<\/TIMEX3>"
        self.timex3tagregex = r"(?<=\<)(.*?)(?=\>)"

        # Cleans xml file manually by removing unfriendly chars and replacing them
        self.regexxmltag = r"<.*>"

        # TEI normalization rules
        self.regexnonchars = (
            r"[^0-9-PYESUWIFASP:]"
        )  # anything not a number or hyphen,  or P or Y or E e.g P19Y or WE or seasons
        self.regexnonchars2 = (
            r"[^0-9\-]"
        )  # applied to certain types of expressions only
        self.regexyyyymmdd = r"\b[0-9]{4}-[0-9]{2}-[0-9]{2}\b"  # matches YYYY-MM-DD
        self.regexmmdd = (
            r"--[0-9]{2}-[0-9]{2}\b"
        )  # --mm-dd,  unlikely to be found in TimeML
        self.regexmm = r"--[0-9]{2}\b"  # matches --mm,  unlikely
        self.regexyyyymm = r"\b[0-9]{4}-[0-9]{2}\b"  # matches YYYY-MM,  likely!
        self.regexyyyy = r"[0-9]{4}"  # matches YYYY,  likely!
        self.regexyyyy_yyyy = (
            r"\b[0-9]{4}-[0-9]{4}\b"
        )  # matches year ranges e.g 2006-2007
        self.regexyyyy_d = (
            r"\b[0-9]{4}-[0-9]{1}\b"
        )  # a negative match,  yyyy-i which comes up in soccer match descriptions (the second half)
        self.regexyyyyss = r"\b[0-9]{4}-(SU|WI|FA|SP)\b"  # yyyy_seasoncode eg 2014-SU
        self.regexcentury = r"^(0|1|2)[0-9]\b"  # centuries e.g 18 for 18th century
        self.regexyearrange = (
            r"\b(|1|2)[0-9]{2}\b"
        )  # where the timex3 value is like 198 for 'the 1980s'
        self.phraseyearrange = (
            r"\b([0-9]{2}|[0-9]{4})s\b"
        )  # validates that the phrase itself is a year range 'the 1980s'
        self.regexduration = r"\bP([0-9]{2}|[0-9]{1})Y\b"  # captures duration values
        self.regextimeformat = r"\b[0-9]{2}:[0-9]{2}\b"  # matches time formats eg 20:00
        self.regexweekformat = (
            r"\b[0-9]{4}-W[0-9]{2}\b"
        )  # matches weeks, e.g '2004-W01' which will be resolved to ranges
        self.regexweekendweek = (
            r"\b[0-9]{4}-W[0-9]{2}-WE\b"
        )  # e.g 2014-W23-WE, weekend of week 23 of the year

        self.regextimerange = (
            r"--[0-9]{2}:[0-9]{2}---[0-9]{2}:[0-9]{2}\b"
        )  # time ranges in timex3 are like --hh:mm---hh:mm

        self.seasons = {
            "SU": ["--06", "--09"],
            "WI": ["--12", "--03"],
            "FA": ["--09", "--12"],
            "SP": ["--03", "--06"],
        }  # taken from the Farmer's Almanac
        self.monthslist = [
            "january",
            "february",
            "march",
            "april",
            "may",
            "june",
            "july",
            "august",
            "september",
            "october",
            "november",
            "december",
        ]
        self.gazettedates = {
            "spanish golden age": ["from:1556", "to:1659"],
            "easter": ["notBefore:--03", "notAfter:--05"],
            "easter sunday": ["notBefore:--03", "notAfter:--05"],
            "christmas": ["when:--12-25"],
            "christmas eve": ["when:--12-24"],
            "world war 2": ["from:1939-09-01", "to:1945-02-01"],
            "world war ii": ["from:1939-09-01", "to:1945-02-01"],
            "world war 1": ["from:1914", "to:1918"],
            "world war i": ["from:1914", "to:1918"],
            "the american revolution": ["notBefore:1775", "notAfter:1783"],
            "the american revolutionary war": ["notBefore:1775", "notAfter:1783"],
            "the civil war": ["notBefore:1861", "notAfter:1865"],
            "the american civil war": ["notBefore:1861", "notAfter:1865"],
            "the reconstruction era": ["notBefore:1863", "notAfter:1887"],
        }  # dates based on lookups of events

        # helps check against impossible years relative to the dct
        self.prevsentencedate = None

        # TODO: 'soft-wire' feature names from the label encoder for pos tagging instead of hard-wiring in the dictionary features
        # of the datetime filter model class
        # self.cd = '/'.join(os.path.abspath(__file__).split('/')[:-2])
        # with open(self.cd + postaglabelencoderobj,  "rb") as f:
        #    le = pickle.load(f)

    requires = (PipelineDep.POS_TAG, PipelineDep.PARSE)
    provides = (PipelineDep.DATETIME,)

    def timex_to_tei(self, timextype, timexvalue, timexmod, phrase, dct):
        """
        Params: the timex3 type,  the timex3 val,  timex3 mod attribute,  the actual date phrase,  and the document creation time

        Normalizes TIMEX3 values to TEI encodings
        Returns the tag type with attributes to build the date/time tag
        Also does some validations and makes some assumptions
        Add any fixes for normalization issues here

        To skip conversion and the date/time phrase,  return an empty dictionary
        """

        # Helper - cleans the timex3 val
        def striphyphens(text):
            if text[-1:] == "-":
                text = text[:-1]
            if text[:1] == "-" and text[:2] != "-":
                text = text[1:]
            return text

        # Helper - screens impossible years
        def check_year(text, phrase):
            """
            filters out impossible years in a date that have been marked years by mistake
            and if necessary rebuilds the timex3 val with just the month and day part
            """

            dctyear = dct.split("-")[0]

            if (
                self.prevsentencedate is not None and "--" not in self.prevsentencedate
            ):  # this means it has a year
                prevsentyear = self.prevsentencedate.split("-")[0]
                if abs(int(dctyear) - int(prevsentyear)) > 100:
                    prevsentyear = None  # revert to None
            else:
                prevsentyear = None

            # check the year four way.
            textyear = text.split("-")[0]
            if prevsentyear is not None and prevsentyear == dctyear:
                matches = re.findall(self.regexyyyy, phrase)
                for match in matches:
                    if match == textyear:
                        return text
                    else:
                        return prevsentyear + "-" + "-".join(text.split("-")[1:])

            elif (
                abs(int(dctyear) - int(textyear)) > 100
            ):  # cant really be 100+ year diff between dct and the timex date
                matches = re.findall(self.regexyyyy, phrase)
                for match in matches:
                    if match == textyear:
                        return text

                text = "--" + "-".join(
                    text.split("-")[1:]
                )  # get rid of text and dct year as they are impossible

                return text

            return text

        # init
        result = {}

        # Wrapping this whole function around a big try-catch , to return an empty dictionary if the normalization fails
        # so that the pipeline isnt broken but this date/time phrase is missed
        # FYI the pipeline without exception handling was tested on all 4708 files in the out folder with success
        try:

            if (
                timextype == "SET"
            ):  # dont handle SETs in TEI as they cant be mapped to the TEI attributes
                return result

            temp = re.sub(self.regexnonchars, "", timexvalue)

            # capture time ranges before stripping the dashes
            if re.match(self.regextimerange, temp):
                temp = temp.split("-")
                result["time"] = [
                    "from:" + temp[2].replace(":", ""),
                    "to:" + temp[5].replace(":", ""),
                ]
                return result

            temp = striphyphens(temp)

            # check for negative matches here and skip
            if re.match(self.regexyyyy_d, temp):
                return result

            if re.match(self.regexweekendweek, temp):
                val = temp.split("-")
                week = "-".join(val[:2])
                resolved = datetime.strptime(
                    week + "-1", "%Y-W%W-%w"
                )  # monday is the first day of the week
                fromdate = resolved + timedelta(days=5)
                todate = resolved + timedelta(days=6)
                result["date"] = [
                    "from:" + fromdate.strftime("%Y-%m-%d"),
                    "to:" + todate.strftime("%Y-%m-%d"),
                ]
                return result

            if re.match(self.regexweekformat, temp):
                resolved = datetime.strptime(
                    temp + "-1", "%Y-W%W-%w"
                )  # Monday is the first day of the week
                todate = resolved + timedelta(days=6)
                result["date"] = [
                    "from:" + str(resolved.strftime("%Y-%m-%d")),
                    "to:" + str(todate.strftime("%Y-%m-%d")),
                ]
                return result

            #  TODO: gazetted dates,  extract straight from the token as the date/time filter will remove these otherwise
            for key, value in self.gazettedates.items():
                if key in phrase.lower().strip():
                    result["date"] = value
                    return result

            # check for time TODO: revisit,  Heideltime cant detect time values in raw tokens like 20:00,  08:30,  etc
            if re.search(self.regextimeformat, temp):
                result["time"] = ["when:" + temp.replace(":", "").replace("-", "")]
                return result

            # mod MORE_THAN - will result in a date_notBefore:dct - timex3 duration val
            if timexmod == "MORE_THAN" and re.search(self.regexduration, temp):
                # e.g more than 20 years ago.
                result["date"] = [
                    "notBefore:"
                    + str(
                        int(dct.split("-")[0])
                        - int(temp.replace("P", "").replace("Y", ""))
                    )
                    + "-"
                    + "-".join(dct.split("-")[1:])
                ]
                return result

            # TIMEX3 centuries are two digit values. Convert to a date from_to
            if re.search(self.regexcentury, temp):
                for month in self.monthslist:
                    if month in phrase.lower():
                        return result

                if timexmod is None:
                    result["date"] = ["from:" + temp + "00", "to:" + temp + "99"]
                else:  # TIMEX3 has START,  MID,  END so split it 3 ways
                    if timexmod == "START":
                        result["date"] = ["from:" + temp + "00", "to:" + temp + "33"]
                    elif timexmod == "MID":
                        result["date"] = ["from:" + temp + "33", "to:" + temp + "66"]
                    elif timexmod == "END":
                        result["date"] = ["from:" + temp + "66", "to:" + temp + "99"]
                return result

            # check for year ranges,  e.g 1980s,  1990s
            if re.search(self.phraseyearrange, phrase) and re.match(
                self.regexyearrange, temp
            ):
                result["date"] = ["from:" + str(temp) + "0", "to:" + str(temp) + "9"]
                return result

            # Check for year range specified in the phrase as yyyy-yyyy
            if re.search(self.regexyyyy_yyyy, phrase) or re.search(
                self.regexyyyy_yyyy, temp
            ):
                if re.search(self.regexyyyy_yyyy, temp):
                    fromyear = temp.split("-")[0]
                    toyear = temp.split("-")[1]
                    result["date"] = ["from:" + fromyear, "to:" + toyear]
                elif re.search(self.regexyyyy_yyyy, phrase):
                    fromyear = phrase.split("-")[0]
                    toyear = phrase.split("-")[1]
                    result["date"] = ["from:" + fromyear, "to:" + toyear]

                return result

            # Check for -mm-dd,  or --mm. This rarely happens based on the target folder samples
            if re.match(self.regexmmdd, temp) or re.match(self.regexmm, temp):
                result["date"] = ["when:" + temp]
                return result

            # Check if yyyy-mm-dd or yyyy-mm matches
            if re.match(self.regexyyyymmdd, temp) or re.match(self.regexyyyymm, temp):

                # clean temp further to prevent certain expressions like :1994-09-01TNI 'the night of 1994 Sep 1st)
                temp = re.sub(self.regexnonchars2, "", temp)
                date = temp.split("-")
                if (
                    date[0] not in phrase and phrase in temp
                ):  # this is a cricket score,  likely
                    return result

                temp = check_year(temp, phrase)

                if re.match(self.regexmm, temp):  # too sparse,  skip this
                    return result

                # if the year turned out to be impossible,  dont reset the previous date marker
                if re.match(self.regexyyyymmdd, temp) or re.match(
                    self.regexyyyymm, temp
                ):
                    self.prevsentencedate = temp

                if int(date[1]) > 12:
                    return result
                if len(date) == 3:
                    if int(date[2]) > 31:
                        return result

                result["date"] = ["when:" + temp]
                return result

            # Seasons! the timex3 val is always YYYY-(SU|WI|FA|SP)
            if re.match(self.regexyyyyss, temp):
                season = check_year(temp, phrase)
                if re.match(self.regexyyyyss, season):
                    # require the season year to match dct year or something's off
                    seasonyear = temp.split("-")[0]
                    dctyear = dct.split("-")[0]
                    if seasonyear != dctyear:
                        return result

                    season = temp.split("-")[-1]
                    if season != "WI":
                        result["date"] = [
                            "from:"
                            + temp.split("-")[0]
                            + "-"
                            + self.seasons[season][0].replace("--", ""),
                            "to:"
                            + temp.split("-")[0]
                            + "-"
                            + self.seasons[season][1].replace("--", ""),
                        ]
                    else:
                        result["date"] = [
                            "from:"
                            + temp.split("-")[0]
                            + "-"
                            + self.seasons[season][0].replace("--", ""),
                            "to:"
                            + str(int(temp.split("-")[0]) + 1)
                            + "-"
                            + self.seasons[season][1].replace("--", ""),
                        ]

                return result

            # Normal years
            if re.match(self.regexyyyy, temp):
                # if the year is way too far into the future,  something's off
                # will affect sci-fi genres mostly without a  better method
                if int(temp) > 2300:  # 300 years
                    return result
                result["date"] = ["when:" + temp]
                return result

        except Exception as e:
            result = {}  # TODO log

        return result

    def test_dependencies(self):
        def set_bat_config():
            """
            Changes the root location batch file parameter in all the .bat files from C:\TreeTagger to the bin folder path
            Installing in C:\ directly is risky as it assumes Administrator privileges on the machine
            So its assumed that the user has Amalgum installed in a relative location where admin privileges are not needed
            Invoked for Windows machines only
            :return:
            """
            for file in glob.glob(
                self.ttgbin + "TreeTagger" + os.sep + "bin" + os.sep + "*.bat"
            ):
                with open(file, "r") as i:
                    lines = i.readlines()

                for x in range(0, len(lines)):
                    if "TAGDIR" in lines[x]:
                        lines[x] = "set TAGDIR=" + self.ttgbin + "TreeTagger"
                        break

                with open(file, "w") as o:
                    o.writelines(lines)

        def set_ht_config_props(ostype):
            """
            sets heideltime config to point to the tree tagger by manually editing the config.props
            """
            with open(
                self.htbin + "heideltime-standalone" + os.sep + "config.2.props", "w"
            ) as c2:
                with open(
                    self.htbin + "heideltime-standalone" + os.sep + "config.props", "r"
                ) as c:
                    for line in c:
                        if "treeTaggerHome" in str(line):
                            if ostype == "Windows":
                                c2.write(
                                    "treeTaggerHome = "
                                    + self.ttgbin.replace("\\", "\\\\")
                                    + "TreeTagger"
                                    + "\n"
                                )
                            else:
                                c2.write("treeTaggerHome = " + self.ttgbin[:-1] + "\n")
                        else:
                            c2.write(line)

            os.remove(self.htbin + "heideltime-standalone" + os.sep + "config.props")
            os.rename(
                self.htbin + "heideltime-standalone" + os.sep + "config.2.props",
                self.htbin + "heideltime-standalone" + os.sep + "config.props",
            )

        ostype = platform.system()

        if ostype == "Darwin":
            ttgbinary = TREETAGGER_MACOSX
        elif ostype == "Windows":
            # set the PATH
            os.environ["PATH"] += os.pathsep + self.ttgbin
            ttgbinary = TREETAGGER_WINDOWS
        else:
            ttgbinary = TREETAGGER_LINUX

        if not os.path.exists(self.htbin):
            try:
                os.mkdir(self.htbin)
            except Exception as e:
                raise NLPDependencyException(
                    "Errored while creating the HeidelTime standalone home directory"
                )
        if not os.path.exists(self.ttgbin):
            try:
                os.mkdir(self.ttgbin)
            except Exception as e:
                raise NLPDependencyException(
                    "Errored while creating the Tree Tagger home Directory"
                )

        hashtjar = False
        hasttbin = False

        for root, sub, files in os.walk(self.htbin):
            if hashtjar == True:
                break
            for filename in files:
                if filename == "de.unihd.dbs.heideltime.standalone.jar":
                    hashtjar = (
                        True
                    )  # we'll assume that all is present if jar is present
                    break

        for root, sub, files in os.walk(self.ttgbin):
            if hasttbin == True:
                break
            for filename in files:
                if filename == "install-tagger.sh":
                    hasttbin = True
                    break

        try:
            if hashtjar == False:
                # download the HT standalone
                filename = wget.download(HEIDELTIME_STANDALONE, out=self.htbin)
                tar = tarfile.open(filename, "r:gz")
                tar.extractall(path=self.htbin)
                tar.close()

                set_ht_config_props(ostype)

            if hasttbin == False:
                # Download the TreeTagger as a HT dependency

                # first the package
                filename = wget.download(ttgbinary, out=self.ttgbin)

                # install.sh isnt setup for Windows so we need to manually extract the config and script files
                if ostype == "Windows":
                    with zipfile.ZipFile(filename, "r") as z:
                        z.extractall(path=self.ttgbin)
                    set_bat_config()

                # then the scripts
                filename = wget.download(TREETAGGER_SCRIPTS, out=self.ttgbin)
                if ostype == "Windows":
                    tar = tarfile.open(filename, "r:gz")
                    tar.extractall(path=self.ttgbin + os.sep + "TreeTagger")
                    tar.close()

                # then the parameter files
                if ostype == "Windows":
                    filename = wget.download(
                        TREETAGGER_PARAMETER_FILES_PENN,
                        out=self.ttgbin + os.sep + "TreeTagger" + os.sep + "lib",
                    )
                    with gzip.open(filename, "rb") as gz:
                        filecontent = gz.read()
                        with open(filename.replace(".gz", ""), "wb") as gout:
                            gout.write(filecontent)
                else:
                    _ = wget.download(TREETAGGER_PARAMETER_FILES_PENN, out=self.ttgbin)

                if ostype == "Windows":
                    filename = wget.download(
                        TREETAGGER_PARAMETER_FILES_BNC,
                        out=self.ttgbin + os.sep + "TreeTagger" + os.sep + "lib",
                    )
                    with gzip.open(filename, "rb") as gz:
                        filecontent = gz.read()
                        with open(filename.replace(".gz", ""), "wb") as gout:
                            gout.write(filecontent)
                else:
                    _ = wget.download(TREETAGGER_PARAMETER_FILES_BNC, out=self.ttgbin)

                if ostype == "Windows":
                    filename = wget.download(
                        TREETAGGER_CHUNKER,
                        out=self.ttgbin + os.sep + "TreeTagger" + os.sep + "lib",
                    )
                    with gzip.open(filename, "rb") as gz:
                        filecontent = gz.read()
                        with open(filename.replace(".gz", ""), "wb") as gout:
                            gout.write(filecontent)
                else:
                    _ = wget.download(TREETAGGER_CHUNKER, out=self.ttgbin)

                # then the install file
                filename = wget.download(TREETAGGER_EXEC, out=self.ttgbin)
                filename = filename.split(os.sep)[-1]

                # does the install for non windows platforms only
                if ostype != "Windows":
                    currdir = os.getcwd()
                    os.chdir(self.ttgbin)
                    subprocess.call(["sh", filename])
                    os.chdir(currdir)

        except Exception as e:
            # in case this errors, scrap the folder so next run is clean
            shutil.rmtree(self.htbin)
            shutil.rmtree(self.ttgbin)
            raise NLPDependencyException(
                "Errored downloading HeidelTime and/or TreeTagger dependencies. Please check the logs."
            )

    def process_file(self, docdict):
        def replace_xml_chars(text):
            text = (
                text.replace("&", "&amp;")
                .replace("&amp;amp;", "&amp;")
                .replace(">", "&gt;")
                .replace("&amp;gt;", "&gt;")
                .replace("<", "&lt;")
                .replace("&amp;lt;", "&lt;")
                .replace('"', "&quot;")
                .replace("&amp;quot;", "&quot;")
                .replace("'", "&apos;")
                .replace("&amp;apos;", "&apos;")
            )
            return text

        def clean_xml_text(xmltext):
            """
            goes through xmltext line by line and replaces chars with XML-friendly equivalents
            """
            result = []
            for line in xmltext.split("\n"):
                if not re.match(self.regexxmltag, line):
                    result.append(replace_xml_chars(str(line)))
                else:
                    if line.startswith("<ref"):
                        line = line.replace("&", "&amp;").replace("&amp;amp;", "&amp;")
                    result.append(line)

            result = "\n".join(result)

            return result

        def add_datetime_tags(node, counter=0):
            """
            Recursively builds the xml file in memory in place,  and stamps the date xml on it
            the counter is an accumulator that keep tracks of how many sentences we have iterated over

            the basic idea is to loop through each sentence with all its elements top down,  and add the date/time elements
            sequentially from the top down. When a date/time element is added to the sentence,  all the elements of the sentence
            are rebuilt and re-ordered in place before the next element is added. Date elements are added either in an existing
            element's text,  or tail. In the event that the date phrase is not present in any element's text or tail,  it is added to the
            "s"'s text element.

            The key to the logic is that the sentence elements are rebuilt top to bottom for every new date element added to the sent
            the key variables 'elements' and 'processed'
            the list 'elements' stores the order of elements in a sentence every time a date is added
            and the bool processed flag ensures that the sentence is rebuilt top to bottom for every date addition
            """
            for item in node:
                if item.tag == "s":
                    counter += 1

                    if counter in sentenceindices:
                        df = indexphrases[(indexphrases.sentence_index == counter)]
                        if len(df) > 0:
                            # initialization
                            dateindex = 0

                            while dateindex < len(df):  # will loop top to bottom
                                elements = (
                                    []
                                )  # rebuild the sentence xml elements for every phrase
                                phrase = str(df["phrase"].iloc[dateindex])
                                timextype = df["timextype"].iloc[dateindex]
                                timexvalue = df["timexvalue"].iloc[dateindex]
                                timexmod = df["timexmod"].iloc[dateindex]
                                processed = False

                                for n in item:
                                    if processed == True:
                                        # the date has been processed.
                                        # add all the other elements in the sentence and rebuild the full sentence xml
                                        # then move to the next date in the sentence
                                        elements.append(n)
                                        continue

                                    fulltext = n.text.split("\n") + n.tail.split("\n")
                                    fulltext = [f for f in fulltext if f]
                                    fulltext = [f.split("\t")[0] for f in fulltext]
                                    fulltext = " ".join(fulltext)

                                    if (
                                        phrase not in fulltext
                                    ):  # will always be exact match,  not fuzzy
                                        elements.append(n)  # move to the next element
                                    else:
                                        # its in the text or the tail
                                        attributes = self.timex_to_tei(
                                            timextype,
                                            timexvalue,
                                            timexmod,
                                            phrase,
                                            dateCreated,
                                        )  # normalization happens here
                                        for (
                                            key,
                                            value,
                                        ) in attributes.items():  # loops only once

                                            text = n.text.split("\n")
                                            text = [f for f in text if f]
                                            text = [f.split("\t")[0] for f in text]
                                            text = " ".join(text)

                                            tail = n.tail.split("\n")
                                            tail = [f for f in tail if f]
                                            tail = [f.split("\t")[0] for f in tail]
                                            tail = " ".join(tail)

                                            if phrase in text:
                                                search = re.search(
                                                    re.escape(phrase), text
                                                )
                                                if (
                                                    search is not None
                                                    and search.start() > 0
                                                ):
                                                    startindex = text[
                                                        0 : search.start()
                                                    ].count(" ")
                                                else:
                                                    startindex = 0

                                                splittext = n.text.split("\n")

                                            else:
                                                search = re.search(
                                                    re.escape(phrase), tail
                                                )

                                                if (
                                                    search is not None
                                                    and search.start() > 0
                                                ):
                                                    startindex = tail[
                                                        0 : search.start()
                                                    ].count(" ")
                                                else:
                                                    startindex = 0

                                                splittext = n.tail.split("\n")

                                            endindex = startindex + len(phrase.split())

                                            splittext = [t for t in splittext if t]
                                            predatetext = splittext[0:startindex]
                                            datetext = splittext[startindex:endindex]
                                            postdatetext = splittext[
                                                endindex : len(splittext)
                                            ]

                                            if phrase in text:
                                                if (
                                                    str("\n".join(predatetext)).strip()
                                                    == ""
                                                ):
                                                    n.text = "\n"
                                                else:
                                                    n.text = (
                                                        "\n"
                                                        + "\n".join(predatetext)
                                                        + "\n"
                                                    )
                                            else:
                                                if (
                                                    str("\n".join(predatetext)).strip()
                                                    == ""
                                                ):
                                                    n.tail = "\n"
                                                else:
                                                    n.tail = (
                                                        "\n"
                                                        + "\n".join(predatetext)
                                                        + "\n"
                                                    )

                                            # build the date element
                                            date = etree.Element(key)
                                            for attribs in value:
                                                date.set(
                                                    attribs.split(":")[0],
                                                    attribs.split(":")[1],
                                                )

                                            date.text = (
                                                "\n" + "\n".join(datetext) + "\n"
                                            )

                                            if (
                                                str("\n".join(postdatetext)).strip()
                                                == ""
                                            ):
                                                date.tail = "\n"
                                            else:
                                                date.tail = (
                                                    "\n"
                                                    + "\n".join(postdatetext)
                                                    + "\n"
                                                )

                                            if (
                                                phrase in text
                                            ):  # the date is nested in the text of the element and a child of the elem
                                                n.append(date)
                                                elements.append(n)
                                            else:  # the date is a new child of the sentence,  immediately follows the element
                                                elements.append(n)
                                                elements.append(date)

                                        if (
                                            n not in elements
                                        ):  # if normalization rejects the phrase,  add the existing element
                                            elements.append(n)
                                        # mark processed
                                        processed = True
                                        dateindex += 1

                                if processed == False:

                                    # the date is in the item's ("s"'s) text. Not in any of the elements.
                                    text = item.text.split("\n")
                                    text = [f for f in text if f]
                                    text = [f.split("\t")[0] for f in text]
                                    text = " ".join(text)

                                    attributes = self.timex_to_tei(
                                        timextype,
                                        timexvalue,
                                        timexmod,
                                        phrase,
                                        dateCreated,
                                    )  # normalization happens here
                                    for (
                                        key,
                                        value,
                                    ) in attributes.items():  # loops only once

                                        search = re.search(re.escape(phrase), text)

                                        # TODO: find a way to remove this None check.
                                        # With this, some dates dont get tagged where there
                                        # are multiple dates within a Nested element in the sentence
                                        # these are pretty hard to add :),  skipping for now as their number should be minimal
                                        if search is not None:
                                            if search.start() > 0:
                                                startindex = text[
                                                    0 : search.start()
                                                ].count(" ")
                                            else:
                                                startindex = 0

                                            splittext = item.text.split("\n")
                                            splittext = [s for s in splittext if s]

                                            endindex = startindex + len(phrase.split())

                                            predatetext = splittext[0:startindex]
                                            datetext = splittext[startindex:endindex]
                                            postdatetext = splittext[
                                                endindex : len(splittext)
                                            ]

                                            if (
                                                str("\n".join(predatetext)).strip()
                                                == ""
                                            ):
                                                item.text = "\n"
                                            else:
                                                item.text = (
                                                    "\n" + "\n".join(predatetext) + "\n"
                                                )

                                            # build the date element
                                            date = etree.Element(key)
                                            for attribs in value:
                                                date.set(
                                                    attribs.split(":")[0],
                                                    attribs.split(":")[1],
                                                )

                                            date.text = (
                                                "\n" + "\n".join(datetext) + "\n"
                                            )

                                            if (
                                                str("\n".join(postdatetext)).strip()
                                                == ""
                                            ):
                                                date.tail = "\n"
                                            else:
                                                date.tail = (
                                                    "\n"
                                                    + "\n".join(postdatetext)
                                                    + "\n"
                                                )

                                            elements.insert(0, date)

                                    dateindex += 1

                                # now build the final sentence sequentially with all tags and date tags
                                text = item.text
                                tail = item.tail
                                for i in range(0, len(elements)):
                                    if elements[i] in item:
                                        item.remove(elements[i])

                                item.text = text
                                for i in range(0, len(elements)):
                                    item.append(elements[i])
                                item.tail = tail

                counter = add_datetime_tags(
                    item, counter
                )  # don't remove the accumulator pattern

            return counter

        xmltext = docdict["xml"]
        xmltext = clean_xml_text(
            xmltext
        )  # forcibly convert to XML-friendly chars and keep them in the pipeline..
        xmltree = etree.fromstring(
            xmltext  # , parser=self.parser
        )  # If this passes, the XML clean was successful
        root = xmltree

        if XML_ATTRIB_REFDATE in root.attrib:
            dateCreated = root.attrib[
                XML_ATTRIB_REFDATE
            ]  # assumed to be in default YYYY-MM-DD or the process breaks
        else:
            dateCreated = datetime.today().strftime("%Y-%m-%d")

        filename = root.attrib["id"]

        sentences = (
            []
        )  # to hold the list of sentences in the file built from everything
        sentencestokens = []  # holds sentences built from tokens only

        # build the sentences from the conllu file
        # sentence boundaries have multiple \n in between
        # with open(conllufile, 'r') as r:
        conlludata = docdict["dep"]
        sent = []
        senttok = []
        for line in conlludata.split("\n"):
            line = line.strip()
            if line == "":
                if len(sent) == 0:
                    continue  # second newline
                sentences.append(sent)
                sentencestokens.append(senttok)
                sent = []
                senttok = []
            elif "\t" in line:
                senttok.append(line.split("\t")[1])
                sent.append(
                    line.replace("\t", "/")
                )  # changes the delimiter to a cleaner one

        sentences.append(sent)
        sentencestokens.append(senttok)

        # rollup
        for i in range(0, len(sentencestokens)):
            sentencestokens[i] = str(" ".join(sentencestokens[i])).strip()
            sentences[i] = str(
                " ".join(sentences[i])
            ).strip()  # now inner delimited by '/

        # now call heideltime to process the whole file at once
        text = "\n".join(sentencestokens)

        # the main course...
        if "_news_" in filename or "_interview_" in filename:
            result = self.hw.parse(text, dateCreated)
        else:
            result = self.hwnarr.parse(text, dateCreated)

        # gets datetime expressions and timex3 attributes for said expression
        # for the whole document
        # uncomment this for debugging
        # with open(outputdir + '/xml/' + filename.split('/')[-1].replace('.xml', '_timex3.xml'), 'w') as p:
        #    p.writelines(result)
        dates, attribs = self.parse_timex3_xml(result)

        # build dataframe for second filter pass
        # custom random forest model that filters out all the False Positives generated by HT
        inferencedf = pd.DataFrame(columns=self.datefilter.featuredict.keys())
        for i in range(0, len(dates)):
            if len(dates[i]) == 0:
                continue  # nothing in the sentence

            for j in range(0, len(dates[i])):
                timextype = attribs[i][j][0][
                    1
                ]  # assumes that the order is always type then value then mod
                timexvalue = attribs[i][j][1][1]
                if len(attribs[i][j]) > 2:
                    timexmod = attribs[i][j][2][1]
                    timexmod = timexmod.replace('"', "")
                else:
                    timexmod = None

                f = self.datefilter.build_featureset(
                    sentences[i],
                    sentencestokens[i],
                    dates[i][j],
                    i,
                    timextype,
                    timexvalue,
                    timexmod,
                )
                inferencedf = inferencedf.append(f, ignore_index=True)

        if len(inferencedf) != 0:
            indexphrases = inferencedf[
                [
                    "sentence_index",
                    "start_index",
                    "phrase",
                    "timextype",
                    "timexvalue",
                    "timexmod",
                ]
            ]
            inferencedf.drop(
                columns=[
                    "sentence_index",
                    "start_index",
                    "phrase",
                    "timextype",
                    "timexvalue",
                    "timexmod",
                ],
                axis=1,
                inplace=True,
            )

            # Filter the dates that dont pass GUM annotated standards..
            tpprobs = self.datefilter.rf.predict(inferencedf)
            indexphrases["label"] = pd.Series(tpprobs)
            indexphrases = indexphrases.loc[indexphrases["label"] == 1]

            if (
                indexphrases is not None and len(indexphrases) != 0
            ):  # only if we have dates..
                indexphrases.sort_values(
                    ["sentence_index", "start_index"],
                    ascending=[True, True],
                    inplace=True,
                )

                # need to collapse the same phrase text in different places in the sentence
                indexphrases = indexphrases.groupby(
                    by=["sentence_index", "phrase"]
                ).head(1)
                indexphrases["phrase"] = indexphrases.groupby(
                    by=["sentence_index", "start_index"]
                )["phrase"].transform(lambda x: "-".join(x))
                indexphrases["timexvalue"] = indexphrases.groupby(
                    by=["sentence_index", "start_index"]
                )["timexvalue"].transform(lambda x: "-".join(x))
                indexphrases.drop_duplicates(inplace=True)
                sentenceindices = set(indexphrases["sentence_index"].tolist())

                # Build the xml document with the new date tags
                _ = add_datetime_tags(root)  # modify xml in place and add date tags

        xmlstring = etree.tostring(
            root, encoding="utf8", method="xml", xml_declaration=True
        ).decode()

        return {"dep": conlludata, "xml": xmlstring}

    def extract_datetimephrases(self, sent):
        """
        returns the timex3 captured expression and the timex attributes for the expression
        sentence by sentence
        """

        result = []
        attributes = []

        matches = re.findall(self.timex3regex, sent)
        for match in matches:

            attrib = re.findall(self.timex3tagregex, match)
            attrib = str(attrib[0]).replace(
                "TIMEX3", ""
            )  # there's only going to be one match ever
            attrib = attrib.split()
            attrib = [attr.split("=") for attr in attrib]
            # we only want the type and value and mod for now,  fetch more if needed
            attrib = [
                attr
                for attr in attrib
                if attr[0] == "type" or attr[0] == "value" or attr[0] == "mod"
            ]
            attributes.append(attrib)

            phrase = re.sub(self.timex3tagregex, "", match)
            phrase = phrase.replace(">", "").replace("<", "")
            result.append(phrase)

        return result, attributes

    def parse_timex3_xml(self, xmltext):
        """
        Parses the TimeX3 file and returns hypotheses
        maps the dates and attributes,  to each sentence as a list of lists
        :param xmltext: The xml file output with TIMEX3 tags created by the tool
        :return: list of list of dates and attributes for each sentence
        """

        xmltext = xmltext.split("\n")
        xmltext = xmltext[3:]  # discard first 3 tags
        xmltext = xmltext[:-2]  # and the last 2

        sorted_dates = []
        sorted_attribs = []

        for sent in xmltext:
            dates, attributes = self.extract_datetimephrases(sent)
            sorted_dates.append(dates)
            sorted_attribs.append(attributes)

        return sorted_dates, sorted_attribs

    def run(self, input_dir, output_dir):

        processing_function = self.process_file
        self.process_files_multiformat(
            input_dir, output_dir, processing_function, multithreaded=True
        )

    def run_debug(self, filename, input_dir, output_dir):
        """
        Use this for debugging a single file
        """
        depfile = input_dir + "/dep/" + filename.replace(".xml", ".conllu")
        xmlfile = input_dir + "/xml/" + filename

        content_dict = {}
        with io.open(depfile, "r", encoding="utf8") as f:
            content_dict["dep"] = f.read()
        with io.open(xmlfile, "r", encoding="utf8") as f:
            content_dict["xml"] = f.read()

        content_dict["filename"] = filename
        result = self.process_file(content_dict)
        with open(output_dir + filename, "w") as o:
            o.writelines(result["xml"])


def main():

    # Testing only

    TTG_PATH = "treetagger/bin"
    BIN_DIR = "<full path to>/amalgum/amalgum/bin/"
    config = {"TTG_PATH": TTG_PATH, "BIN_DIR": BIN_DIR}
    dtr = DateTimeRecognizer(config)
    dtr.test_dependencies()
    # filename = 'autogum_bio_doc165.xml' # has an interesting case, see "25 Mar-3 April 2002"
    # filename = 'autogum_bio_doc575.xml' #example of a week of year normalization from timex to tei
    filename = "autogum_reddit_doc010.xml"  # an example of a weekend normalization
    # filename = 'autogum_voyage_doc429.xml'

    input_dir = "<full path to>/amalgum/amalgum/target/04_DepParser"
    output_dir = "<full path to>/amalgum/amalgum/target/testdate/"
    dtr.run_debug(filename, input_dir, output_dir)


if __name__ == "__main__":
    main()
