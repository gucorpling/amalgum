#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=6. Results and Discussion
1-1	0-2	6.	abstract[1]|abstract[2]	new[1]|new[2]	coref|coref	2-1[4_2]|6-4[29_1]
1-2	3-10	Results	abstract[1]|abstract[2]	new[1]|new[2]	_	_
1-3	11-14	and	abstract[2]	new[2]	_	_
1-4	15-25	Discussion	abstract[2]|abstract	new[2]|new	_	_

#Text=The results of the evaluation experiments with our algorithm are presented in Table 5 .
2-1	26-29	The	abstract[4]	giv[4]	coref	3-12[15_4]
2-2	30-37	results	abstract[4]	giv[4]	_	_
2-3	38-40	of	abstract[4]	giv[4]	_	_
2-4	41-44	the	abstract[4]|event[6]	giv[4]|new[6]	coref	6-7[30_6]
2-5	45-55	evaluation	abstract[4]|abstract|event[6]	giv[4]|new|new[6]	_	_
2-6	56-67	experiments	abstract[4]|event[6]	giv[4]|new[6]	_	_
2-7	68-72	with	abstract[4]|event[6]	giv[4]|new[6]	_	_
2-8	73-76	our	abstract[4]|event[6]|person|abstract[8]	giv[4]|new[6]|acc|new[8]	ana	22-8
2-9	77-86	algorithm	abstract[4]|event[6]|abstract[8]	giv[4]|new[6]|new[8]	_	_
2-10	87-90	are	_	_	_	_
2-11	91-100	presented	_	_	_	_
2-12	101-103	in	_	_	_	_
2-13	104-109	Table	abstract[9]	new[9]	coref	13-15[0_9]
2-14	110-111	5	abstract[9]	new[9]	_	_
2-15	112-113	.	_	_	_	_

#Text=The variant without the limit of n-grams per input segment produces unbalanced results ( especially on SYOS ) , with relatively low Precision .
3-1	114-117	The	abstract[10]	new[10]	_	_
3-2	118-125	variant	abstract[10]	new[10]	_	_
3-3	126-133	without	abstract[10]	new[10]	_	_
3-4	134-137	the	abstract[10]|abstract[11]	new[10]|new[11]	coref	4-3[18_11]
3-5	138-143	limit	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-6	144-146	of	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-7	147-154	n-grams	abstract[10]|abstract[11]|quantity[12]	new[10]|new[11]|new[12]	coref	7-25[43_12]
3-8	155-158	per	abstract[10]|abstract[11]|quantity[12]	new[10]|new[11]|new[12]	_	_
3-9	159-164	input	abstract[10]|abstract[11]|quantity[12]|abstract|abstract[14]	new[10]|new[11]|new[12]|new|new[14]	coref|coref	7-27|7-27[45_14]
3-10	165-172	segment	abstract[10]|abstract[11]|quantity[12]|abstract[14]	new[10]|new[11]|new[12]|new[14]	_	_
3-11	173-181	produces	_	_	_	_
3-12	182-192	unbalanced	abstract[15]	giv[15]	appos	3-15[16_15]
3-13	193-200	results	abstract[15]	giv[15]	_	_
3-14	201-202	(	_	_	_	_
3-15	203-213	especially	abstract[16]	giv[16]	coref	19-14[142_16]
3-16	214-216	on	abstract[16]	giv[16]	_	_
3-17	217-221	SYOS	abstract[16]	giv[16]	_	_
3-18	222-223	)	_	_	_	_
3-19	224-225	,	_	_	_	_
3-20	226-230	with	_	_	_	_
3-21	231-241	relatively	abstract[17]	new[17]	coref	4-8[0_17]
3-22	242-245	low	abstract[17]	new[17]	_	_
3-23	246-255	Precision	abstract[17]	new[17]	_	_
3-24	256-257	.	_	_	_	_

#Text=After setting the limit to 2 , Precision improves at the cost of a drop in Recall .
4-1	258-263	After	_	_	_	_
4-2	264-271	setting	_	_	_	_
4-3	272-275	the	abstract[18]	giv[18]	_	_
4-4	276-281	limit	abstract[18]	giv[18]	_	_
4-5	282-284	to	_	_	_	_
4-6	285-286	2	quantity	new	_	_
4-7	287-288	,	_	_	_	_
4-8	289-298	Precision	abstract	giv	coref	8-14
4-9	299-307	improves	_	_	_	_
4-10	308-310	at	_	_	_	_
4-11	311-314	the	abstract[21]	new[21]	_	_
4-12	315-319	cost	abstract[21]	new[21]	_	_
4-13	320-322	of	abstract[21]	new[21]	_	_
4-14	323-324	a	abstract[21]|abstract[22]	new[21]|new[22]	coref	5-13[27_22]
4-15	325-329	drop	abstract[21]|abstract[22]	new[21]|new[22]	_	_
4-16	330-332	in	abstract[21]|abstract[22]	new[21]|new[22]	_	_
4-17	333-339	Recall	abstract[21]|abstract[22]|abstract	new[21]|new[22]|new	coref	8-26
4-18	340-341	.	_	_	_	_

#Text=The F-score is better for SYOS , while on AKJ there is a very slight drop .
5-1	342-345	The	abstract[24]	new[24]	coref	8-16[0_24]
5-2	346-353	F-score	abstract[24]	new[24]	_	_
5-3	354-356	is	_	_	_	_
5-4	357-363	better	_	_	_	_
5-5	364-367	for	_	_	_	_
5-6	368-372	SYOS	abstract	new	coref	26-9
5-7	373-374	,	_	_	_	_
5-8	375-380	while	_	_	_	_
5-9	381-383	on	_	_	_	_
5-10	384-387	AKJ	person	new	_	_
5-11	388-393	there	_	_	_	_
5-12	394-396	is	_	_	_	_
5-13	397-398	a	abstract[27]	giv[27]	coref	12-39[97_27]
5-14	399-403	very	abstract[27]	giv[27]	_	_
5-15	404-410	slight	abstract[27]	giv[27]	_	_
5-16	411-415	drop	abstract[27]	giv[27]	_	_
5-17	416-417	.	_	_	_	_

#Text=Table 6 shows the results of experiments with the Stupid Backoff model .
6-1	418-423	Table	object[28]	new[28]	_	_
6-2	424-425	6	object[28]	new[28]	_	_
6-3	426-431	shows	_	_	_	_
6-4	432-435	the	abstract[29]	giv[29]	coref	7-8[36_29]
6-5	436-443	results	abstract[29]	giv[29]	_	_
6-6	444-446	of	abstract[29]	giv[29]	_	_
6-7	447-458	experiments	abstract[29]|event[30]	giv[29]|giv[30]	_	_
6-8	459-463	with	abstract[29]|event[30]	giv[29]|giv[30]	_	_
6-9	464-467	the	abstract[29]|event[30]|abstract[33]	giv[29]|giv[30]|new[33]	coref	11-31[84_33]
6-10	468-474	Stupid	abstract[29]|event[30]|person|abstract[33]	giv[29]|giv[30]|new|new[33]	_	_
6-11	475-482	Backoff	abstract[29]|event[30]|event|abstract[33]	giv[29]|giv[30]|new|new[33]	coref	7-3
6-12	483-488	model	abstract[29]|event[30]|abstract[33]	giv[29]|giv[30]|new[33]	_	_
6-13	489-490	.	_	_	_	_

#Text=When no backoff factor is applied , results for both test sets are similar to those from the MiNgMatch Segmenter without the limit of n-grams per input segment .
7-1	491-495	When	_	_	_	_
7-2	496-498	no	abstract[35]	new[35]	ana	7-16[39_35]
7-3	499-506	backoff	abstract|abstract[35]	giv|new[35]	coref	8-3
7-4	507-513	factor	abstract[35]	new[35]	_	_
7-5	514-516	is	_	_	_	_
7-6	517-524	applied	_	_	_	_
7-7	525-526	,	_	_	_	_
7-8	527-534	results	abstract[36]	giv[36]	coref	11-20[81_36]
7-9	535-538	for	abstract[36]	giv[36]	_	_
7-10	539-543	both	abstract[36]|abstract[38]	giv[36]|new[38]	_	_
7-11	544-548	test	abstract[36]|abstract|abstract[38]	giv[36]|new|new[38]	coref	21-6
7-12	549-553	sets	abstract[36]|abstract[38]	giv[36]|new[38]	_	_
7-13	554-557	are	_	_	_	_
7-14	558-565	similar	_	_	_	_
7-15	566-568	to	_	_	_	_
7-16	569-574	those	abstract[39]	giv[39]	coref	8-2[47_39]
7-17	575-579	from	abstract[39]	giv[39]	_	_
7-18	580-583	the	abstract[39]|object[41]	giv[39]|new[41]	coref	16-5[113_41]
7-19	584-593	MiNgMatch	abstract[39]|object|object[41]	giv[39]|new|new[41]	coref	12-8
7-20	594-603	Segmenter	abstract[39]|object[41]	giv[39]|new[41]	_	_
7-21	604-611	without	_	_	_	_
7-22	612-615	the	abstract[42]	new[42]	coref	12-12[88_42]
7-23	616-621	limit	abstract[42]	new[42]	_	_
7-24	622-624	of	abstract[42]	new[42]	_	_
7-25	625-632	n-grams	abstract[42]|quantity[43]	new[42]|giv[43]	coref	11-28[0_43]
7-26	633-636	per	abstract[42]|quantity[43]	new[42]|giv[43]	_	_
7-27	637-642	input	abstract[42]|quantity[43]|abstract|abstract[45]	new[42]|giv[43]|giv|giv[45]	coref|coref	12-17|12-17[91_45]
7-28	643-650	segment	abstract[42]|quantity[43]|abstract[45]	new[42]|giv[43]|giv[45]	_	_
7-29	651-652	.	_	_	_	_

#Text=Setting the backoff factor to an appropriate value allows for significant improvement in Precision and F-score ( and in some cases also small improvements in Recall ) .
8-1	653-660	Setting	_	_	_	_
8-2	661-664	the	abstract[47]	giv[47]	coref	9-10[58_47]
8-3	665-672	backoff	abstract|abstract[47]	giv|giv[47]	coref	9-12
8-4	673-679	factor	abstract[47]	giv[47]	_	_
8-5	680-682	to	abstract[47]	giv[47]	_	_
8-6	683-685	an	abstract[47]|abstract[48]	giv[47]|new[48]	coref	9-29[61_48]
8-7	686-697	appropriate	abstract[47]|abstract[48]	giv[47]|new[48]	_	_
8-8	698-703	value	abstract[47]|abstract[48]	giv[47]|new[48]	_	_
8-9	704-710	allows	_	_	_	_
8-10	711-714	for	_	_	_	_
8-11	715-726	significant	abstract[49]	new[49]	coref	10-7[69_49]
8-12	727-738	improvement	abstract[49]	new[49]	_	_
8-13	739-741	in	abstract[49]	new[49]	_	_
8-14	742-751	Precision	abstract[49]|abstract	new[49]|giv	coref	10-10[70_0]
8-15	752-755	and	abstract[49]	new[49]	_	_
8-16	756-763	F-score	abstract[49]|abstract	new[49]|giv	coref	9-2[55_0]
8-17	764-765	(	_	_	_	_
8-18	766-769	and	_	_	_	_
8-19	770-772	in	_	_	_	_
8-20	773-777	some	abstract[52]|abstract[53]	new[52]|new[53]	coref	27-16[198_52]
8-21	778-783	cases	abstract[52]|abstract[53]	new[52]|new[53]	_	_
8-22	784-788	also	abstract[53]	new[53]	_	_
8-23	789-794	small	abstract[53]	new[53]	_	_
8-24	795-807	improvements	abstract[53]	new[53]	_	_
8-25	808-810	in	abstract[53]	new[53]	_	_
8-26	811-817	Recall	abstract[53]|abstract	new[53]|giv	ana	9-5
8-27	818-819	)	_	_	_	_
8-28	820-821	.	_	_	_	_

#Text=For the F-score , it is better to set a low backoff factor ( e. g. , 0.09 ) for 1-grams only , than to set it to a fixed value for all backoff steps ( e. g. , 0.4 , as Brants et al. did ) .
9-1	822-825	For	_	_	_	_
9-2	826-829	the	abstract[55]	giv[55]	coref	25-15[184_55]
9-3	830-837	F-score	abstract[55]	giv[55]	_	_
9-4	838-839	,	_	_	_	_
9-5	840-842	it	abstract	giv	ana	9-27
9-6	843-845	is	_	_	_	_
9-7	846-852	better	_	_	_	_
9-8	853-855	to	_	_	_	_
9-9	856-859	set	_	_	_	_
9-10	860-861	a	abstract[58]	giv[58]	coref	10-1[67_58]
9-11	862-865	low	abstract[58]	giv[58]	_	_
9-12	866-873	backoff	abstract|abstract[58]	giv|giv[58]	coref	9-34
9-13	874-880	factor	abstract[58]	giv[58]	_	_
9-14	881-882	(	_	_	_	_
9-15	883-885	e.	_	_	_	_
9-16	886-888	g.	_	_	_	_
9-17	889-890	,	_	_	_	_
9-18	891-895	0.09	_	_	_	_
9-19	896-897	)	_	_	_	_
9-20	898-901	for	_	_	_	_
9-21	902-909	1-grams	quantity[59]	new[59]	_	_
9-22	910-914	only	quantity[59]	new[59]	_	_
9-23	915-916	,	_	_	_	_
9-24	917-921	than	_	_	_	_
9-25	922-924	to	_	_	_	_
9-26	925-928	set	_	_	_	_
9-27	929-931	it	abstract	giv	coref	12-44
9-28	932-934	to	_	_	_	_
9-29	935-936	a	abstract[61]	giv[61]	_	_
9-30	937-942	fixed	abstract[61]	giv[61]	_	_
9-31	943-948	value	abstract[61]	giv[61]	_	_
9-32	949-952	for	abstract[61]	giv[61]	_	_
9-33	953-956	all	abstract[61]|person[63]	giv[61]|new[63]	appos	9-37[64_63]
9-34	957-964	backoff	abstract[61]|event|person[63]	giv[61]|giv|new[63]	coref	10-2
9-35	965-970	steps	abstract[61]|person[63]	giv[61]|new[63]	_	_
9-36	971-972	(	_	_	_	_
9-37	973-975	e.	abstract[64]	giv[64]	coref	36-31[0_64]
9-38	976-978	g.	abstract[64]	giv[64]	_	_
9-39	979-980	,	abstract[64]	giv[64]	_	_
9-40	981-984	0.4	abstract[64]|quantity	giv[64]|new	coref	10-5
9-41	985-986	,	abstract[64]	giv[64]	_	_
9-42	987-989	as	abstract[64]	giv[64]	_	_
9-43	990-996	Brants	abstract[64]	giv[64]	_	_
9-44	997-999	et	abstract[64]	giv[64]	_	_
9-45	1000-1003	al.	abstract[64]	giv[64]	_	_
9-46	1004-1007	did	_	_	_	_
9-47	1008-1009	)	_	_	_	_
9-48	1010-1011	.	_	_	_	_

#Text=A backoff factor of 0.4 gives significant improvement in Precision with higher order n-gram models , but at the same time Recall drops drastically and overall performance deteriorates .
10-1	1012-1013	A	abstract[67]	giv[67]	coref	11-12[79_67]
10-2	1014-1021	backoff	abstract|abstract[67]	giv|giv[67]	coref	11-13
10-3	1022-1028	factor	abstract[67]	giv[67]	_	_
10-4	1029-1031	of	abstract[67]	giv[67]	_	_
10-5	1032-1035	0.4	abstract[67]|quantity	giv[67]|giv	_	_
10-6	1036-1041	gives	_	_	_	_
10-7	1042-1053	significant	abstract[69]	giv[69]	coref	27-20[199_69]
10-8	1054-1065	improvement	abstract[69]	giv[69]	_	_
10-9	1066-1068	in	abstract[69]	giv[69]	_	_
10-10	1069-1078	Precision	abstract[69]|abstract[70]	giv[69]|giv[70]	coref	12-30[93_70]
10-11	1079-1083	with	abstract[69]|abstract[70]	giv[69]|giv[70]	_	_
10-12	1084-1090	higher	abstract[69]|abstract[70]|abstract[71]	giv[69]|giv[70]|new[71]	coref	11-4[76_71]
10-13	1091-1096	order	abstract[69]|abstract[70]|abstract[71]	giv[69]|giv[70]|new[71]	_	_
10-14	1097-1103	n-gram	abstract[69]|abstract[70]|abstract[72]	giv[69]|giv[70]|new[72]	coref	11-2[75_72]
10-15	1104-1110	models	abstract[69]|abstract[70]|abstract[72]	giv[69]|giv[70]|new[72]	_	_
10-16	1111-1112	,	_	_	_	_
10-17	1113-1116	but	_	_	_	_
10-18	1117-1119	at	_	_	_	_
10-19	1120-1123	the	time[73]	new[73]	_	_
10-20	1124-1128	same	time[73]	new[73]	_	_
10-21	1129-1133	time	time[73]	new[73]	_	_
10-22	1134-1140	Recall	_	_	_	_
10-23	1141-1146	drops	_	_	_	_
10-24	1147-1158	drastically	_	_	_	_
10-25	1159-1162	and	_	_	_	_
10-26	1163-1170	overall	abstract[74]	new[74]	coref	26-18[192_74]
10-27	1171-1182	performance	abstract[74]	new[74]	_	_
10-28	1183-1195	deteriorates	_	_	_	_
10-29	1196-1197	.	_	_	_	_

#Text=For models with an n-gram order of 3 or higher , the backoff factor has a bigger impact on the results than further increasing the order of n-grams included in the model .
11-1	1198-1201	For	_	_	_	_
11-2	1202-1208	models	abstract[75]	giv[75]	coref	13-7[0_75]
11-3	1209-1213	with	abstract[75]	giv[75]	_	_
11-4	1214-1216	an	abstract[75]|abstract[76]	giv[75]|giv[76]	_	_
11-5	1217-1223	n-gram	abstract[75]|abstract[76]	giv[75]|giv[76]	_	_
11-6	1224-1229	order	abstract[75]|abstract[76]	giv[75]|giv[76]	_	_
11-7	1230-1232	of	abstract[75]|abstract[76]	giv[75]|giv[76]	_	_
11-8	1233-1234	3	abstract[75]|abstract[76]|quantity	giv[75]|giv[76]|new	_	_
11-9	1235-1237	or	abstract[75]|abstract[76]	giv[75]|giv[76]	_	_
11-10	1238-1244	higher	abstract[75]|abstract[76]	giv[75]|giv[76]	_	_
11-11	1245-1246	,	_	_	_	_
11-12	1247-1250	the	abstract[79]	giv[79]	_	_
11-13	1251-1258	backoff	abstract|abstract[79]	giv|giv[79]	coref	12-23[92_0]
11-14	1259-1265	factor	abstract[79]	giv[79]	_	_
11-15	1266-1269	has	_	_	_	_
11-16	1270-1271	a	abstract[80]	new[80]	_	_
11-17	1272-1278	bigger	abstract[80]	new[80]	_	_
11-18	1279-1285	impact	abstract[80]	new[80]	_	_
11-19	1286-1288	on	abstract[80]	new[80]	_	_
11-20	1289-1292	the	abstract[80]|abstract[81]	new[80]|giv[81]	coref	12-4[86_81]
11-21	1293-1300	results	abstract[80]|abstract[81]	new[80]|giv[81]	_	_
11-22	1301-1305	than	_	_	_	_
11-23	1306-1313	further	_	_	_	_
11-24	1314-1324	increasing	_	_	_	_
11-25	1325-1328	the	abstract[82]	new[82]	_	_
11-26	1329-1334	order	abstract[82]	new[82]	_	_
11-27	1335-1337	of	abstract[82]	new[82]	_	_
11-28	1338-1345	n-grams	abstract[82]|quantity	new[82]|giv	coref	12-15[89_0]
11-29	1346-1354	included	_	_	_	_
11-30	1355-1357	in	_	_	_	_
11-31	1358-1361	the	abstract[84]	giv[84]	coref	17-1[115_84]
11-32	1362-1367	model	abstract[84]	giv[84]	_	_
11-33	1368-1369	.	_	_	_	_

#Text=A comparison with the results yielded by MiNgMatch shows that setting the limit of n-grams per input segment is more effective than Stupid Backoff as a method for improving precision of the segmentation process — it leads to a much smaller drop in Recall .
12-1	1370-1371	A	abstract[85]	new[85]	_	_
12-2	1372-1382	comparison	abstract[85]	new[85]	_	_
12-3	1383-1387	with	abstract[85]	new[85]	_	_
12-4	1388-1391	the	abstract[85]|abstract[86]	new[85]|giv[86]	_	_
12-5	1392-1399	results	abstract[85]|abstract[86]	new[85]|giv[86]	_	_
12-6	1400-1407	yielded	_	_	_	_
12-7	1408-1410	by	_	_	_	_
12-8	1411-1420	MiNgMatch	organization	giv	_	_
12-9	1421-1426	shows	_	_	_	_
12-10	1427-1431	that	_	_	_	_
12-11	1432-1439	setting	_	_	_	_
12-12	1440-1443	the	quantity[88]	giv[88]	_	_
12-13	1444-1449	limit	quantity[88]	giv[88]	_	_
12-14	1450-1452	of	quantity[88]	giv[88]	_	_
12-15	1453-1460	n-grams	quantity[88]|quantity[89]	giv[88]|giv[89]	coref	32-16[229_89]
12-16	1461-1464	per	quantity[88]|quantity[89]	giv[88]|giv[89]	_	_
12-17	1465-1470	input	quantity[88]|quantity[89]|abstract|abstract[91]	giv[88]|giv[89]|giv|giv[91]	coref|coref	17-56[131_91]|18-14[137_0]
12-18	1471-1478	segment	quantity[88]|quantity[89]|abstract[91]	giv[88]|giv[89]|giv[91]	_	_
12-19	1479-1481	is	_	_	_	_
12-20	1482-1486	more	_	_	_	_
12-21	1487-1496	effective	_	_	_	_
12-22	1497-1501	than	_	_	_	_
12-23	1502-1508	Stupid	event[92]	giv[92]	coref	34-3[0_92]
12-24	1509-1516	Backoff	event[92]	giv[92]	_	_
12-25	1517-1519	as	_	_	_	_
12-26	1520-1521	a	_	_	_	_
12-27	1522-1528	method	_	_	_	_
12-28	1529-1532	for	_	_	_	_
12-29	1533-1542	improving	_	_	_	_
12-30	1543-1552	precision	abstract[93]	giv[93]	coref	35-28[254_93]
12-31	1553-1555	of	abstract[93]	giv[93]	_	_
12-32	1556-1559	the	abstract[93]|abstract[95]	giv[93]|new[95]	ana	12-36[0_95]
12-33	1560-1572	segmentation	abstract[93]|abstract|abstract[95]	giv[93]|new|new[95]	coref	18-20[138_0]
12-34	1573-1580	process	abstract[93]|abstract[95]	giv[93]|new[95]	_	_
12-35	1581-1582	—	_	_	_	_
12-36	1583-1585	it	abstract	giv	coref	21-13[156_0]
12-37	1586-1591	leads	_	_	_	_
12-38	1592-1594	to	_	_	_	_
12-39	1595-1596	a	abstract[97]	giv[97]	_	_
12-40	1597-1601	much	abstract[97]	giv[97]	_	_
12-41	1602-1609	smaller	abstract[97]	giv[97]	_	_
12-42	1610-1614	drop	abstract[97]	giv[97]	_	_
12-43	1615-1617	in	abstract[97]	giv[97]	_	_
12-44	1618-1624	Recall	abstract[97]|abstract	giv[97]|giv	coref	15-5[110_0]
12-45	1625-1626	.	_	_	_	_

#Text=The results of the experiment with models employing modified Kneser-Ney smoothing are shown in Table 7 .
13-1	1627-1630	The	abstract[99]	new[99]	coref	15-9[111_99]
13-2	1631-1638	results	abstract[99]	new[99]	_	_
13-3	1639-1641	of	abstract[99]	new[99]	_	_
13-4	1642-1645	the	abstract[99]|event[100]	new[99]|new[100]	_	_
13-5	1646-1656	experiment	abstract[99]|event[100]	new[99]|new[100]	_	_
13-6	1657-1661	with	abstract[99]|event[100]	new[99]|new[100]	_	_
13-7	1662-1668	models	abstract[99]|event[100]|abstract	new[99]|new[100]|giv	ana	14-1
13-8	1669-1678	employing	_	_	_	_
13-9	1679-1687	modified	abstract[103]	new[103]	coref	24-13[179_103]
13-10	1688-1698	Kneser-Ney	person|abstract[103]	new|new[103]	coref	24-13
13-11	1699-1708	smoothing	abstract[103]	new[103]	_	_
13-12	1709-1712	are	_	_	_	_
13-13	1713-1718	shown	_	_	_	_
13-14	1719-1721	in	_	_	_	_
13-15	1722-1727	Table	abstract|abstract[105]	giv|new[105]	coref	22-4
13-16	1728-1729	7	abstract[105]	new[105]	_	_
13-17	1730-1731	.	_	_	_	_

#Text=They achieve higher Precision than both the other types of n-gram models .
14-1	1732-1736	They	abstract	giv	coref	14-11[109_0]
14-2	1737-1744	achieve	_	_	_	_
14-3	1745-1751	higher	abstract[107]	new[107]	coref	24-1[173_107]
14-4	1752-1761	Precision	abstract[107]	new[107]	_	_
14-5	1762-1766	than	abstract[107]	new[107]	_	_
14-6	1767-1771	both	abstract[107]|abstract[108]	new[107]|new[108]	_	_
14-7	1772-1775	the	abstract[107]|abstract[108]	new[107]|new[108]	_	_
14-8	1776-1781	other	abstract[107]|abstract[108]	new[107]|new[108]	_	_
14-9	1782-1787	types	abstract[107]|abstract[108]	new[107]|new[108]	_	_
14-10	1788-1790	of	abstract[107]|abstract[108]	new[107]|new[108]	_	_
14-11	1791-1797	n-gram	abstract[107]|abstract[108]|abstract[109]	new[107]|new[108]|giv[109]	coref	19-1[140_109]
14-12	1798-1804	models	abstract[107]|abstract[108]|abstract[109]	new[107]|new[108]|giv[109]	_	_
14-13	1805-1806	.	_	_	_	_

#Text=Nevertheless , due to very low Recall , the overall results are low .
15-1	1807-1819	Nevertheless	_	_	_	_
15-2	1820-1821	,	_	_	_	_
15-3	1822-1825	due	_	_	_	_
15-4	1826-1828	to	_	_	_	_
15-5	1829-1833	very	abstract[110]	giv[110]	_	_
15-6	1834-1837	low	abstract[110]	giv[110]	_	_
15-7	1838-1844	Recall	abstract[110]	giv[110]	_	_
15-8	1845-1846	,	_	_	_	_
15-9	1847-1850	the	abstract[111]	giv[111]	coref	16-1[112_111]
15-10	1851-1858	overall	abstract[111]	giv[111]	_	_
15-11	1859-1866	results	abstract[111]	giv[111]	_	_
15-12	1867-1870	are	_	_	_	_
15-13	1871-1874	low	_	_	_	_
15-14	1875-1876	.	_	_	_	_

#Text=The results obtained by the Universal Segmenter are presented in Table 8 .
16-1	1877-1880	The	abstract[112]	giv[112]	coref	23-6[172_112]
16-2	1881-1888	results	abstract[112]	giv[112]	_	_
16-3	1889-1897	obtained	_	_	_	_
16-4	1898-1900	by	_	_	_	_
16-5	1901-1904	the	object[113]	giv[113]	coref	24-10[177_113]
16-6	1905-1914	Universal	object[113]	giv[113]	_	_
16-7	1915-1924	Segmenter	object[113]	giv[113]	_	_
16-8	1925-1928	are	_	_	_	_
16-9	1929-1938	presented	_	_	_	_
16-10	1939-1941	in	_	_	_	_
16-11	1942-1947	Table	abstract[114]	new[114]	_	_
16-12	1948-1949	8	abstract[114]	new[114]	_	_
16-13	1950-1951	.	_	_	_	_

#Text=The default model ( regardless of what kind of character representations are used — conventional character embeddings or concatenated n-gram vectors ) learns from the training data that the first and the last character of a word ( corresponding to B , E and S tags ) are always adjacent either to the boundary of a space-delimited segment or to a punctuation mark .
17-1	1952-1955	The	abstract[115]	giv[115]	coref	18-5[134_115]
17-2	1956-1963	default	abstract[115]	giv[115]	_	_
17-3	1964-1969	model	abstract[115]	giv[115]	_	_
17-4	1970-1971	(	_	_	_	_
17-5	1972-1982	regardless	animal[116]	new[116]	_	_
17-6	1983-1985	of	animal[116]	new[116]	_	_
17-7	1986-1990	what	animal[116]	new[116]	_	_
17-8	1991-1995	kind	animal[116]|abstract[118]	new[116]|new[118]	_	_
17-9	1996-1998	of	animal[116]|abstract[118]	new[116]|new[118]	_	_
17-10	1999-2008	character	animal[116]|abstract|abstract[118]	new[116]|new|new[118]	coref	17-16
17-11	2009-2024	representations	animal[116]|abstract[118]	new[116]|new[118]	_	_
17-12	2025-2028	are	_	_	_	_
17-13	2029-2033	used	_	_	_	_
17-14	2034-2035	—	_	_	_	_
17-15	2036-2048	conventional	abstract[120]	new[120]	_	_
17-16	2049-2058	character	abstract|abstract[120]	giv|new[120]	coref	17-32[124_0]
17-17	2059-2069	embeddings	abstract[120]	new[120]	_	_
17-18	2070-2072	or	_	_	_	_
17-19	2073-2085	concatenated	object[121]	new[121]	coref	26-22[193_121]
17-20	2086-2092	n-gram	object[121]	new[121]	_	_
17-21	2093-2100	vectors	object[121]	new[121]	_	_
17-22	2101-2102	)	_	_	_	_
17-23	2103-2109	learns	_	_	_	_
17-24	2110-2114	from	_	_	_	_
17-25	2115-2118	the	abstract[123]	new[123]	coref	20-10[0_123]
17-26	2119-2127	training	abstract|abstract[123]	new|new[123]	coref	32-27
17-27	2128-2132	data	abstract[123]	new[123]	_	_
17-28	2133-2137	that	_	_	_	_
17-29	2138-2141	the	_	_	_	_
17-30	2142-2147	first	_	_	_	_
17-31	2148-2151	and	_	_	_	_
17-32	2152-2155	the	abstract[124]	giv[124]	coref	29-6[0_124]
17-33	2156-2160	last	abstract[124]	giv[124]	_	_
17-34	2161-2170	character	abstract[124]	giv[124]	_	_
17-35	2171-2173	of	abstract[124]	giv[124]	_	_
17-36	2174-2175	a	abstract[124]|abstract[125]	giv[124]|new[125]	coref	20-16[0_125]
17-37	2176-2180	word	abstract[124]|abstract[125]	giv[124]|new[125]	_	_
17-38	2181-2182	(	_	_	_	_
17-39	2183-2196	corresponding	_	_	_	_
17-40	2197-2199	to	_	_	_	_
17-41	2200-2201	B	object	new	_	_
17-42	2202-2203	,	_	_	_	_
17-43	2204-2205	E	person	new	_	_
17-44	2206-2209	and	_	_	_	_
17-45	2210-2211	S	abstract|abstract[129]	new|new[129]	_	_
17-46	2212-2216	tags	abstract[129]	new[129]	_	_
17-47	2217-2218	)	_	_	_	_
17-48	2219-2222	are	_	_	_	_
17-49	2223-2229	always	_	_	_	_
17-50	2230-2238	adjacent	_	_	_	_
17-51	2239-2245	either	place[130]	new[130]	_	_
17-52	2246-2248	to	place[130]	new[130]	_	_
17-53	2249-2252	the	place[130]	new[130]	_	_
17-54	2253-2261	boundary	place[130]	new[130]	_	_
17-55	2262-2264	of	place[130]	new[130]	_	_
17-56	2265-2266	a	place[130]|abstract[131]	new[130]|giv[131]	coref	36-81[278_131]
17-57	2267-2282	space-delimited	place[130]|abstract[131]	new[130]|giv[131]	_	_
17-58	2283-2290	segment	place[130]|abstract[131]	new[130]|giv[131]	_	_
17-59	2291-2293	or	_	_	_	_
17-60	2294-2296	to	_	_	_	_
17-61	2297-2298	a	object[133]	new[133]	_	_
17-62	2299-2310	punctuation	abstract|object[133]	new|new[133]	coref	18-8
17-63	2311-2315	mark	object[133]	new[133]	_	_
17-64	2316-2317	.	_	_	_	_

#Text=As a result , the model separates punctuation from alpha-numeric strings found in the input , but never applies further segmentation to them .
18-1	2318-2320	As	_	_	_	_
18-2	2321-2322	a	_	_	_	_
18-3	2323-2329	result	_	_	_	_
18-4	2330-2331	,	_	_	_	_
18-5	2332-2335	the	abstract[134]	giv[134]	coref	20-6[144_134]
18-6	2336-2341	model	abstract[134]	giv[134]	_	_
18-7	2342-2351	separates	_	_	_	_
18-8	2352-2363	punctuation	abstract	giv	_	_
18-9	2364-2368	from	_	_	_	_
18-10	2369-2382	alpha-numeric	abstract[136]	new[136]	ana	18-23[0_136]
18-11	2383-2390	strings	abstract[136]	new[136]	_	_
18-12	2391-2396	found	_	_	_	_
18-13	2397-2399	in	_	_	_	_
18-14	2400-2403	the	abstract[137]	giv[137]	_	_
18-15	2404-2409	input	abstract[137]	giv[137]	_	_
18-16	2410-2411	,	_	_	_	_
18-17	2412-2415	but	_	_	_	_
18-18	2416-2421	never	_	_	_	_
18-19	2422-2429	applies	_	_	_	_
18-20	2430-2437	further	abstract[138]	giv[138]	coref	21-14[0_138]
18-21	2438-2450	segmentation	abstract[138]	giv[138]	_	_
18-22	2451-2453	to	abstract[138]	giv[138]	_	_
18-23	2454-2458	them	abstract[138]|abstract	giv[138]|giv	coref	20-19[149_0]
18-24	2459-2460	.	_	_	_	_

#Text=US-ISP models are better but still notably worse than lexical n-gram models ( especially on SYOS ) .
19-1	2461-2467	US-ISP	abstract[140]	giv[140]	coref	19-10[141_140]
19-2	2468-2474	models	abstract[140]	giv[140]	_	_
19-3	2475-2478	are	_	_	_	_
19-4	2479-2485	better	_	_	_	_
19-5	2486-2489	but	_	_	_	_
19-6	2490-2495	still	_	_	_	_
19-7	2496-2503	notably	_	_	_	_
19-8	2504-2509	worse	_	_	_	_
19-9	2510-2514	than	_	_	_	_
19-10	2515-2522	lexical	abstract[141]	giv[141]	coref	29-24[0_141]
19-11	2523-2529	n-gram	abstract[141]	giv[141]	_	_
19-12	2530-2536	models	abstract[141]	giv[141]	_	_
19-13	2537-2538	(	abstract[141]	giv[141]	_	_
19-14	2539-2549	especially	abstract[141]|abstract[142]	giv[141]|giv[142]	_	_
19-15	2550-2552	on	abstract[141]|abstract[142]	giv[141]|giv[142]	_	_
19-16	2553-2557	SYOS	abstract[141]|abstract[142]	giv[141]|giv[142]	_	_
19-17	2558-2559	)	abstract[141]	giv[141]	_	_
19-18	2560-2561	.	_	_	_	_

#Text=Unlike with default settings , the model trained on data without whitespaces learns to predict word boundaries within strings of alpha-numeric characters .
20-1	2562-2568	Unlike	abstract[143]	new[143]	_	_
20-2	2569-2573	with	abstract[143]	new[143]	_	_
20-3	2574-2581	default	abstract[143]	new[143]	_	_
20-4	2582-2590	settings	abstract[143]	new[143]	_	_
20-5	2591-2592	,	_	_	_	_
20-6	2593-2596	the	abstract[144]	giv[144]	coref	22-25[166_144]
20-7	2597-2602	model	abstract[144]	giv[144]	_	_
20-8	2603-2610	trained	_	_	_	_
20-9	2611-2613	on	_	_	_	_
20-10	2614-2618	data	abstract	giv	coref	21-6[152_0]
20-11	2619-2626	without	_	_	_	_
20-12	2627-2638	whitespaces	abstract	new	coref	22-34[169_0]
20-13	2639-2645	learns	_	_	_	_
20-14	2646-2648	to	_	_	_	_
20-15	2649-2656	predict	_	_	_	_
20-16	2657-2661	word	abstract|place[148]	giv|new[148]	coref|coref	22-14|22-13[163_148]
20-17	2662-2672	boundaries	place[148]	new[148]	_	_
20-18	2673-2679	within	_	_	_	_
20-19	2680-2687	strings	abstract[149]	giv[149]	coref	34-25[249_149]
20-20	2688-2690	of	abstract[149]	giv[149]	_	_
20-21	2691-2704	alpha-numeric	abstract[149]|abstract[150]	giv[149]|new[150]	coref	28-20[207_150]
20-22	2705-2715	characters	abstract[149]|abstract[150]	giv[149]|new[150]	_	_
20-23	2716-2717	.	_	_	_	_

#Text=However , when presented with test data including spaces , they impede the segmentation process rather than supporting it .
21-1	2718-2725	However	_	_	_	_
21-2	2726-2727	,	_	_	_	_
21-3	2728-2732	when	_	_	_	_
21-4	2733-2742	presented	_	_	_	_
21-5	2743-2747	with	_	_	_	_
21-6	2748-2752	test	abstract|abstract[152]	giv|giv[152]	coref|coref	22-22|22-32[0_152]
21-7	2753-2757	data	abstract[152]	giv[152]	_	_
21-8	2758-2767	including	abstract[152]	giv[152]	_	_
21-9	2768-2774	spaces	abstract[152]|abstract	giv[152]|new	ana	21-11
21-10	2775-2776	,	_	_	_	_
21-11	2777-2781	they	abstract	giv	_	_
21-12	2782-2788	impede	_	_	_	_
21-13	2789-2792	the	abstract[156]	giv[156]	ana	21-19[0_156]
21-14	2793-2805	segmentation	abstract|abstract[156]	giv|giv[156]	coref	29-17[216_0]
21-15	2806-2813	process	abstract[156]	giv[156]	_	_
21-16	2814-2820	rather	_	_	_	_
21-17	2821-2825	than	_	_	_	_
21-18	2826-2836	supporting	_	_	_	_
21-19	2837-2839	it	abstract	giv	_	_
21-20	2840-2841	.	_	_	_	_

#Text=As shown in Table 9 , if we only take into account the word boundaries not already indicated in the raw test set , the model makes more correct predictions in data where the whitespaces have all been removed .
22-1	2842-2844	As	_	_	_	_
22-2	2845-2850	shown	_	_	_	_
22-3	2851-2853	in	_	_	_	_
22-4	2854-2859	Table	abstract|object[159]	giv|new[159]	_	_
22-5	2860-2861	9	object[159]	new[159]	_	_
22-6	2862-2863	,	_	_	_	_
22-7	2864-2866	if	_	_	_	_
22-8	2867-2869	we	person	giv	_	_
22-9	2870-2874	only	_	_	_	_
22-10	2875-2879	take	_	_	_	_
22-11	2880-2884	into	_	_	_	_
22-12	2885-2892	account	abstract	new	_	_
22-13	2893-2896	the	place[163]	giv[163]	_	_
22-14	2897-2901	word	abstract|place[163]	giv|giv[163]	coref	29-17
22-15	2902-2912	boundaries	place[163]	giv[163]	_	_
22-16	2913-2916	not	_	_	_	_
22-17	2917-2924	already	_	_	_	_
22-18	2925-2934	indicated	_	_	_	_
22-19	2935-2937	in	_	_	_	_
22-20	2938-2941	the	abstract[165]	new[165]	coref	29-7[0_165]
22-21	2942-2945	raw	abstract[165]	new[165]	_	_
22-22	2946-2950	test	abstract|abstract[165]	giv|new[165]	coref	32-10
22-23	2951-2954	set	abstract[165]	new[165]	_	_
22-24	2955-2956	,	_	_	_	_
22-25	2957-2960	the	abstract[166]	giv[166]	coref	24-3[175_166]
22-26	2961-2966	model	abstract[166]	giv[166]	_	_
22-27	2967-2972	makes	_	_	_	_
22-28	2973-2977	more	event[167]	new[167]	_	_
22-29	2978-2985	correct	event[167]	new[167]	_	_
22-30	2986-2997	predictions	event[167]	new[167]	_	_
22-31	2998-3000	in	event[167]	new[167]	_	_
22-32	3001-3005	data	event[167]|abstract	new[167]|giv	coref	32-3
22-33	3006-3011	where	_	_	_	_
22-34	3012-3015	the	abstract[169]	giv[169]	_	_
22-35	3016-3027	whitespaces	abstract[169]	giv[169]	_	_
22-36	3028-3032	have	_	_	_	_
22-37	3033-3036	all	_	_	_	_
22-38	3037-3041	been	_	_	_	_
22-39	3042-3049	removed	_	_	_	_
22-40	3050-3051	.	_	_	_	_

#Text=Models with multi-word tokens achieve significantly higher results .
23-1	3052-3058	Models	object[170]	new[170]	_	_
23-2	3059-3063	with	object[170]	new[170]	_	_
23-3	3064-3074	multi-word	object[170]|object[171]	new[170]|new[171]	coref	25-9[183_171]
23-4	3075-3081	tokens	object[170]|object[171]	new[170]|new[171]	_	_
23-5	3082-3089	achieve	_	_	_	_
23-6	3090-3103	significantly	abstract[172]	giv[172]	coref	27-4[195_172]
23-7	3104-3110	higher	abstract[172]	giv[172]	_	_
23-8	3111-3118	results	abstract[172]	giv[172]	_	_
23-9	3119-3120	.	_	_	_	_

#Text=Precision of the US-MWTs model is on par with the segmenter applying Kneser-Ney smoothing , while maintaining relatively high Recall .
24-1	3121-3130	Precision	abstract[173]	giv[173]	coref	25-21[185_173]
24-2	3131-3133	of	abstract[173]	giv[173]	_	_
24-3	3134-3137	the	abstract[173]|abstract[175]	giv[173]|giv[175]	coref	25-6[182_175]
24-4	3138-3145	US-MWTs	abstract[173]|object|abstract[175]	giv[173]|new|giv[175]	_	_
24-5	3146-3151	model	abstract[173]|abstract[175]	giv[173]|giv[175]	_	_
24-6	3152-3154	is	_	_	_	_
24-7	3155-3157	on	_	_	_	_
24-8	3158-3161	par	person[176]	new[176]	_	_
24-9	3162-3166	with	person[176]	new[176]	_	_
24-10	3167-3170	the	person[176]|object[177]	new[176]|giv[177]	coref	26-14[191_177]
24-11	3171-3180	segmenter	person[176]|object[177]	new[176]|giv[177]	_	_
24-12	3181-3189	applying	_	_	_	_
24-13	3190-3200	Kneser-Ney	person|abstract[179]	giv|giv[179]	coref|coref	36-42|36-41[267_179]
24-14	3201-3210	smoothing	abstract[179]	giv[179]	_	_
24-15	3211-3212	,	_	_	_	_
24-16	3213-3218	while	_	_	_	_
24-17	3219-3230	maintaining	_	_	_	_
24-18	3231-3241	relatively	abstract[180]	new[180]	ana	25-1[0_180]
24-19	3242-3246	high	abstract[180]	new[180]	_	_
24-20	3247-3253	Recall	abstract[180]	new[180]	_	_
24-21	3254-3255	.	_	_	_	_

#Text=It yields lower Recall than the model with randomly generated multi-word tokens , but the F-score is higher due to better Precision .
25-1	3256-3258	It	abstract	giv	_	_
25-2	3259-3265	yields	_	_	_	_
25-3	3266-3271	lower	_	_	_	_
25-4	3272-3278	Recall	_	_	_	_
25-5	3279-3283	than	_	_	_	_
25-6	3284-3287	the	abstract[182]	giv[182]	coref	26-5[188_182]
25-7	3288-3293	model	abstract[182]	giv[182]	_	_
25-8	3294-3298	with	abstract[182]	giv[182]	_	_
25-9	3299-3307	randomly	abstract[182]|object[183]	giv[182]|giv[183]	coref	33-4[236_183]
25-10	3308-3317	generated	abstract[182]|object[183]	giv[182]|giv[183]	_	_
25-11	3318-3328	multi-word	abstract[182]|object[183]	giv[182]|giv[183]	_	_
25-12	3329-3335	tokens	abstract[182]|object[183]	giv[182]|giv[183]	_	_
25-13	3336-3337	,	_	_	_	_
25-14	3338-3341	but	_	_	_	_
25-15	3342-3345	the	abstract[184]	giv[184]	_	_
25-16	3346-3353	F-score	abstract[184]	giv[184]	_	_
25-17	3354-3356	is	_	_	_	_
25-18	3357-3363	higher	_	_	_	_
25-19	3364-3367	due	_	_	_	_
25-20	3368-3370	to	_	_	_	_
25-21	3371-3377	better	abstract[185]	giv[185]	_	_
25-22	3378-3387	Precision	abstract[185]	giv[185]	_	_
25-23	3388-3389	.	_	_	_	_

#Text=With the exception of the US-ISP model on SYOS , all variants of the neural segmenter achieved the best performance with concatenated 9-gram vectors .
26-1	3390-3394	With	_	_	_	_
26-2	3395-3398	the	abstract[186]	new[186]	_	_
26-3	3399-3408	exception	abstract[186]	new[186]	_	_
26-4	3409-3411	of	abstract[186]	new[186]	_	_
26-5	3412-3415	the	abstract[186]|abstract[188]	new[186]|giv[188]	coref	34-1[243_188]
26-6	3416-3422	US-ISP	abstract[186]|place|abstract[188]	new[186]|new|giv[188]	_	_
26-7	3423-3428	model	abstract[186]|abstract[188]	new[186]|giv[188]	_	_
26-8	3429-3431	on	abstract[186]|abstract[188]	new[186]|giv[188]	_	_
26-9	3432-3436	SYOS	abstract[186]|abstract[188]|object	new[186]|giv[188]|giv	_	_
26-10	3437-3438	,	_	_	_	_
26-11	3439-3442	all	object[190]	new[190]	_	_
26-12	3443-3451	variants	object[190]	new[190]	_	_
26-13	3452-3454	of	object[190]	new[190]	_	_
26-14	3455-3458	the	object[190]|object[191]	new[190]|giv[191]	_	_
26-15	3459-3465	neural	object[190]|object[191]	new[190]|giv[191]	_	_
26-16	3466-3475	segmenter	object[190]|object[191]	new[190]|giv[191]	_	_
26-17	3476-3484	achieved	_	_	_	_
26-18	3485-3488	the	abstract[192]	giv[192]	ana	27-1[0_192]
26-19	3489-3493	best	abstract[192]	giv[192]	_	_
26-20	3494-3505	performance	abstract[192]	giv[192]	_	_
26-21	3506-3510	with	abstract[192]	giv[192]	_	_
26-22	3511-3523	concatenated	abstract[192]|object[193]	giv[192]|giv[193]	_	_
26-23	3524-3530	9-gram	abstract[192]|object[193]	giv[192]|giv[193]	_	_
26-24	3531-3538	vectors	abstract[192]|object[193]	giv[192]|giv[193]	_	_
26-25	3539-3540	.	_	_	_	_

#Text=This contrasts with the results reported by Shao et al. for Chinese , where in most cases there was no further improvement beyond 3-grams .
27-1	3541-3545	This	abstract	giv	_	_
27-2	3546-3555	contrasts	_	_	_	_
27-3	3556-3560	with	_	_	_	_
27-4	3561-3564	the	abstract[195]	giv[195]	_	_
27-5	3565-3572	results	abstract[195]	giv[195]	_	_
27-6	3573-3581	reported	_	_	_	_
27-7	3582-3584	by	_	_	_	_
27-8	3585-3589	Shao	person	new	_	_
27-9	3590-3592	et	_	_	_	_
27-10	3593-3596	al.	_	_	_	_
27-11	3597-3600	for	_	_	_	_
27-12	3601-3608	Chinese	abstract	new	coref	28-14
27-13	3609-3610	,	_	_	_	_
27-14	3611-3616	where	_	_	_	_
27-15	3617-3619	in	_	_	_	_
27-16	3620-3624	most	abstract[198]	giv[198]	coref	36-66[273_198]
27-17	3625-3630	cases	abstract[198]	giv[198]	_	_
27-18	3631-3636	there	_	_	_	_
27-19	3637-3640	was	_	_	_	_
27-20	3641-3643	no	abstract[199]	giv[199]	_	_
27-21	3644-3651	further	abstract[199]	giv[199]	_	_
27-22	3652-3663	improvement	abstract[199]	giv[199]	_	_
27-23	3664-3670	beyond	abstract[199]	giv[199]	_	_
27-24	3671-3678	3-grams	abstract[199]|substance	giv[199]|new	_	_
27-25	3679-3680	.	_	_	_	_

#Text=This behavior is a consequence of differences between writing systems : words in Chinese are on average composed of less characters than in languages using alphabetic scripts .
28-1	3681-3685	This	abstract[201]	new[201]	coref	28-4[202_201]
28-2	3686-3694	behavior	abstract[201]	new[201]	_	_
28-3	3695-3697	is	_	_	_	_
28-4	3698-3699	a	abstract[202]	giv[202]	_	_
28-5	3700-3711	consequence	abstract[202]	giv[202]	_	_
28-6	3712-3714	of	abstract[202]	giv[202]	_	_
28-7	3715-3726	differences	abstract[202]|abstract	giv[202]|new	_	_
28-8	3727-3734	between	_	_	_	_
28-9	3735-3742	writing	_	_	_	_
28-10	3743-3750	systems	abstract	new	_	_
28-11	3751-3752	:	_	_	_	_
28-12	3753-3758	words	abstract[205]	new[205]	coref	36-49[269_205]
28-13	3759-3761	in	abstract[205]	new[205]	_	_
28-14	3762-3769	Chinese	abstract[205]|abstract	new[205]|giv	_	_
28-15	3770-3773	are	_	_	_	_
28-16	3774-3776	on	_	_	_	_
28-17	3777-3784	average	_	_	_	_
28-18	3785-3793	composed	_	_	_	_
28-19	3794-3796	of	_	_	_	_
28-20	3797-3801	less	abstract[207]	giv[207]	coref	29-10[214_207]
28-21	3802-3812	characters	abstract[207]	giv[207]	_	_
28-22	3813-3817	than	abstract[207]	giv[207]	_	_
28-23	3818-3820	in	abstract[207]	giv[207]	_	_
28-24	3821-3830	languages	abstract[207]|abstract	giv[207]|new	_	_
28-25	3831-3836	using	_	_	_	_
28-26	3837-3847	alphabetic	abstract[209]	new[209]	_	_
28-27	3848-3855	scripts	abstract[209]	new[209]	_	_
28-28	3856-3857	.	_	_	_	_

#Text=Due to a much bigger character set size , hanzi characters are also more informative to word segmentation , hence better performance with models using shorter context .
29-1	3858-3861	Due	_	_	_	_
29-2	3862-3864	to	_	_	_	_
29-3	3865-3866	a	quantity[212]	new[212]	_	_
29-4	3867-3871	much	quantity[212]	new[212]	_	_
29-5	3872-3878	bigger	quantity[212]	new[212]	_	_
29-6	3879-3888	character	abstract|quantity[212]	giv|new[212]	_	_
29-7	3889-3892	set	abstract|quantity[212]	giv|new[212]	coref	32-9[227_0]
29-8	3893-3897	size	quantity[212]	new[212]	_	_
29-9	3898-3899	,	_	_	_	_
29-10	3900-3905	hanzi	person|abstract[214]	new|giv[214]	_	_
29-11	3906-3916	characters	abstract[214]	giv[214]	_	_
29-12	3917-3920	are	_	_	_	_
29-13	3921-3925	also	_	_	_	_
29-14	3926-3930	more	_	_	_	_
29-15	3931-3942	informative	_	_	_	_
29-16	3943-3945	to	_	_	_	_
29-17	3946-3950	word	abstract|abstract[216]	giv|giv[216]	appos	29-20[217_216]
29-18	3951-3963	segmentation	abstract[216]	giv[216]	_	_
29-19	3964-3965	,	_	_	_	_
29-20	3966-3971	hence	abstract[217]	giv[217]	_	_
29-21	3972-3978	better	abstract[217]	giv[217]	_	_
29-22	3979-3990	performance	abstract[217]	giv[217]	_	_
29-23	3991-3995	with	abstract[217]	giv[217]	_	_
29-24	3996-4002	models	abstract[217]|abstract	giv[217]|giv	coref	33-14[239_0]
29-25	4003-4008	using	_	_	_	_
29-26	4009-4016	shorter	abstract[219]	new[219]	_	_
29-27	4017-4024	context	abstract[219]	new[219]	_	_
29-28	4025-4026	.	_	_	_	_

#Text=6.1 .
30-1	4027-4030	6.1	abstract	new	_	_
30-2	4031-4032	.	_	_	_	_

#Text=General Observations
31-1	4033-4040	General	person|abstract[222]	new|new[222]	_	_
31-2	4041-4053	Observations	abstract[222]	new[222]	_	_

#Text=Due to data sparsity , n-gram coverage in the test set ( the fraction of n-grams in the test data that can be found in the training set ) is low ( see Table 10 ) .
32-1	4054-4057	Due	_	_	_	_
32-2	4058-4060	to	_	_	_	_
32-3	4061-4065	data	abstract|abstract[224]	giv|new[224]	coref	32-18[231_0]
32-4	4066-4074	sparsity	abstract[224]	new[224]	_	_
32-5	4075-4076	,	_	_	_	_
32-6	4077-4083	n-gram	quantity[225]	new[225]	_	_
32-7	4084-4092	coverage	quantity[225]	new[225]	_	_
32-8	4093-4095	in	quantity[225]	new[225]	_	_
32-9	4096-4099	the	quantity[225]|abstract[227]	new[225]|giv[227]	coref	32-26[233_227]
32-10	4100-4104	test	quantity[225]|abstract|abstract[227]	new[225]|giv|giv[227]	coref	32-19
32-11	4105-4108	set	quantity[225]|abstract[227]	new[225]|giv[227]	_	_
32-12	4109-4110	(	_	_	_	_
32-13	4111-4114	the	quantity[228]	new[228]	_	_
32-14	4115-4123	fraction	quantity[228]	new[228]	_	_
32-15	4124-4126	of	quantity[228]	new[228]	_	_
32-16	4127-4134	n-grams	quantity[228]|quantity[229]	new[228]|giv[229]	_	_
32-17	4135-4137	in	quantity[228]|quantity[229]	new[228]|giv[229]	_	_
32-18	4138-4141	the	quantity[228]|quantity[229]|abstract[231]	new[228]|giv[229]|giv[231]	_	_
32-19	4142-4146	test	quantity[228]|quantity[229]|abstract|abstract[231]	new[228]|giv[229]|giv|giv[231]	coref	33-9
32-20	4147-4151	data	quantity[228]|quantity[229]|abstract[231]	new[228]|giv[229]|giv[231]	_	_
32-21	4152-4156	that	_	_	_	_
32-22	4157-4160	can	_	_	_	_
32-23	4161-4163	be	_	_	_	_
32-24	4164-4169	found	_	_	_	_
32-25	4170-4172	in	_	_	_	_
32-26	4173-4176	the	abstract[233]	giv[233]	coref	33-8[238_233]
32-27	4177-4185	training	abstract|abstract[233]	giv|giv[233]	_	_
32-28	4186-4189	set	abstract[233]	giv[233]	_	_
32-29	4190-4191	)	_	_	_	_
32-30	4192-4194	is	_	_	_	_
32-31	4195-4198	low	_	_	_	_
32-32	4199-4200	(	_	_	_	_
32-33	4201-4204	see	_	_	_	_
32-34	4205-4210	Table	abstract[234]	new[234]	ana	33-1[0_234]
32-35	4211-4213	10	abstract[234]	new[234]	_	_
32-36	4214-4215	)	_	_	_	_
32-37	4216-4217	.	_	_	_	_

#Text=It means that many multi-word tokens from the test set are known to n-gram models as separate unigrams , but not in the form of a single n-gram .
33-1	4218-4220	It	abstract	giv	_	_
33-2	4221-4226	means	_	_	_	_
33-3	4227-4231	that	_	_	_	_
33-4	4232-4236	many	object[236]	giv[236]	coref	35-7[251_236]
33-5	4237-4247	multi-word	object[236]	giv[236]	_	_
33-6	4248-4254	tokens	object[236]	giv[236]	_	_
33-7	4255-4259	from	object[236]	giv[236]	_	_
33-8	4260-4263	the	object[236]|abstract[238]	giv[236]|giv[238]	_	_
33-9	4264-4268	test	object[236]|abstract|abstract[238]	giv[236]|giv|giv[238]	_	_
33-10	4269-4272	set	object[236]|abstract[238]	giv[236]|giv[238]	_	_
33-11	4273-4276	are	_	_	_	_
33-12	4277-4282	known	_	_	_	_
33-13	4283-4285	to	_	_	_	_
33-14	4286-4292	n-gram	abstract[239]	giv[239]	coref	36-6[0_239]
33-15	4293-4299	models	abstract[239]	giv[239]	_	_
33-16	4300-4302	as	abstract[239]	giv[239]	_	_
33-17	4303-4311	separate	abstract[239]	giv[239]	_	_
33-18	4312-4320	unigrams	abstract[239]	giv[239]	_	_
33-19	4321-4322	,	_	_	_	_
33-20	4323-4326	but	_	_	_	_
33-21	4327-4330	not	_	_	_	_
33-22	4331-4333	in	_	_	_	_
33-23	4334-4337	the	abstract[240]	new[240]	_	_
33-24	4338-4342	form	abstract[240]	new[240]	_	_
33-25	4343-4345	of	abstract[240]	new[240]	_	_
33-26	4346-4347	a	abstract[240]|object[241]	new[240]|new[241]	_	_
33-27	4348-4354	single	abstract[240]|object[241]	new[240]|new[241]	_	_
33-28	4355-4361	n-gram	abstract[240]|object[241]	new[240]|new[241]	_	_
33-29	4362-4363	.	_	_	_	_

#Text=The Stupid Backoff model with a backoff factor for unigrams set to a moderate value ( such as 0.09 ) is able to segment such strings correctly .
34-1	4364-4367	The	abstract[243]	giv[243]	coref	36-18[259_243]
34-2	4368-4374	Stupid	abstract[243]	giv[243]	_	_
34-3	4375-4382	Backoff	event|abstract[243]	giv|giv[243]	coref	34-7
34-4	4383-4388	model	abstract[243]	giv[243]	_	_
34-5	4389-4393	with	abstract[243]	giv[243]	_	_
34-6	4394-4395	a	abstract[243]|abstract[245]	giv[243]|new[245]	coref	36-26[0_245]
34-7	4396-4403	backoff	abstract[243]|abstract|abstract[245]	giv[243]|giv|new[245]	coref	36-21[260_0]
34-8	4404-4410	factor	abstract[243]|abstract[245]	giv[243]|new[245]	_	_
34-9	4411-4414	for	abstract[243]|abstract[245]	giv[243]|new[245]	_	_
34-10	4415-4423	unigrams	abstract[243]|abstract[245]|abstract	giv[243]|new[245]|new	coref	35-22[253_0]
34-11	4424-4427	set	_	_	_	_
34-12	4428-4430	to	_	_	_	_
34-13	4431-4432	a	abstract[247]	new[247]	ana	35-3[0_247]
34-14	4433-4441	moderate	abstract[247]	new[247]	_	_
34-15	4442-4447	value	abstract[247]	new[247]	_	_
34-16	4448-4449	(	abstract[247]	new[247]	_	_
34-17	4450-4454	such	abstract[247]	new[247]	_	_
34-18	4455-4457	as	abstract[247]	new[247]	_	_
34-19	4458-4462	0.09	abstract[247]|quantity	new[247]|new	_	_
34-20	4463-4464	)	abstract[247]	new[247]	_	_
34-21	4465-4467	is	_	_	_	_
34-22	4468-4472	able	_	_	_	_
34-23	4473-4475	to	_	_	_	_
34-24	4476-4483	segment	_	_	_	_
34-25	4484-4488	such	abstract[249]	giv[249]	_	_
34-26	4489-4496	strings	abstract[249]	giv[249]	_	_
34-27	4497-4506	correctly	_	_	_	_
34-28	4507-4508	.	_	_	_	_

#Text=However , it also erroneously segments some OoV single-word tokens whose surface forms happen to be interpretable as a sequence of concatenated in-vocabulary unigrams , resulting in lower Precision .
35-1	4509-4516	However	_	_	_	_
35-2	4517-4518	,	_	_	_	_
35-3	4519-4521	it	abstract	giv	_	_
35-4	4522-4526	also	_	_	_	_
35-5	4527-4538	erroneously	_	_	_	_
35-6	4539-4547	segments	_	_	_	_
35-7	4548-4552	some	object[251]	giv[251]	_	_
35-8	4553-4556	OoV	object[251]	giv[251]	_	_
35-9	4557-4568	single-word	object[251]	giv[251]	_	_
35-10	4569-4575	tokens	object[251]	giv[251]	_	_
35-11	4576-4581	whose	place[252]	new[252]	_	_
35-12	4582-4589	surface	place[252]	new[252]	_	_
35-13	4590-4595	forms	_	_	_	_
35-14	4596-4602	happen	_	_	_	_
35-15	4603-4605	to	_	_	_	_
35-16	4606-4608	be	_	_	_	_
35-17	4609-4622	interpretable	_	_	_	_
35-18	4623-4625	as	_	_	_	_
35-19	4626-4627	a	_	_	_	_
35-20	4628-4636	sequence	_	_	_	_
35-21	4637-4639	of	_	_	_	_
35-22	4640-4652	concatenated	abstract[253]	giv[253]	coref	36-11[257_253]
35-23	4653-4666	in-vocabulary	abstract[253]	giv[253]	_	_
35-24	4667-4675	unigrams	abstract[253]	giv[253]	_	_
35-25	4676-4677	,	_	_	_	_
35-26	4678-4687	resulting	_	_	_	_
35-27	4688-4690	in	_	_	_	_
35-28	4691-4696	lower	abstract[254]	giv[254]	_	_
35-29	4697-4706	Precision	abstract[254]	giv[254]	_	_
35-30	4707-4708	.	_	_	_	_

#Text=On the other hand , models assigning low scores to unigrams ( such as a 4- or 5-gram model with the Stupid Backoff and backoff factor set as suggested by Brants et al. , and in particular the model applying modified Kneser-Ney smoothing ) are better at handling OoV words ( see Table 11 ) , but as a result of probability multiplication , in many cases they score unseen multi-word segments higher than a sequence of unigrams into which the given segment should be divided , hence yielding lower Recall .
36-1	4709-4711	On	_	_	_	_
36-2	4712-4715	the	_	_	_	_
36-3	4716-4721	other	_	_	_	_
36-4	4722-4726	hand	_	_	_	_
36-5	4727-4728	,	_	_	_	_
36-6	4729-4735	models	abstract	giv	_	_
36-7	4736-4745	assigning	_	_	_	_
36-8	4746-4749	low	abstract[256]	new[256]	_	_
36-9	4750-4756	scores	abstract[256]	new[256]	_	_
36-10	4757-4759	to	_	_	_	_
36-11	4760-4768	unigrams	abstract[257]	giv[257]	coref	36-78[0_257]
36-12	4769-4770	(	abstract[257]	giv[257]	_	_
36-13	4771-4775	such	abstract[257]	giv[257]	_	_
36-14	4776-4778	as	abstract[257]	giv[257]	_	_
36-15	4779-4780	a	abstract[257]|abstract[258]	giv[257]|new[258]	_	_
36-16	4781-4783	4-	abstract[257]|abstract[258]	giv[257]|new[258]	_	_
36-17	4784-4786	or	abstract[257]	giv[257]	_	_
36-18	4787-4793	5-gram	abstract[257]|abstract[259]	giv[257]|giv[259]	coref	36-38[265_259]
36-19	4794-4799	model	abstract[257]|abstract[259]	giv[257]|giv[259]	_	_
36-20	4800-4804	with	abstract[257]|abstract[259]	giv[257]|giv[259]	_	_
36-21	4805-4808	the	abstract[257]|abstract[259]|event[260]|abstract[261]	giv[257]|giv[259]|giv[260]|new[261]	coref|ana	36-25[0_260]|36-68[0_261]
36-22	4809-4815	Stupid	abstract[257]|abstract[259]|event[260]|abstract[261]	giv[257]|giv[259]|giv[260]|new[261]	_	_
36-23	4816-4823	Backoff	abstract[257]|abstract[259]|event[260]|abstract[261]	giv[257]|giv[259]|giv[260]|new[261]	_	_
36-24	4824-4827	and	abstract[257]|abstract[259]|abstract[261]	giv[257]|giv[259]|new[261]	_	_
36-25	4828-4835	backoff	abstract[257]|abstract[259]|abstract[261]|event	giv[257]|giv[259]|new[261]|giv	_	_
36-26	4836-4842	factor	abstract[257]|abstract[259]|abstract[261]|abstract	giv[257]|giv[259]|new[261]|giv	_	_
36-27	4843-4846	set	_	_	_	_
36-28	4847-4849	as	_	_	_	_
36-29	4850-4859	suggested	_	_	_	_
36-30	4860-4862	by	_	_	_	_
36-31	4863-4869	Brants	person	giv	_	_
36-32	4870-4872	et	_	_	_	_
36-33	4873-4876	al.	_	_	_	_
36-34	4877-4878	,	_	_	_	_
36-35	4879-4882	and	_	_	_	_
36-36	4883-4885	in	_	_	_	_
36-37	4886-4896	particular	_	_	_	_
36-38	4897-4900	the	abstract[265]	giv[265]	_	_
36-39	4901-4906	model	abstract[265]	giv[265]	_	_
36-40	4907-4915	applying	_	_	_	_
36-41	4916-4924	modified	abstract[267]	giv[267]	_	_
36-42	4925-4935	Kneser-Ney	person|abstract[267]	giv|giv[267]	_	_
36-43	4936-4945	smoothing	abstract[267]	giv[267]	_	_
36-44	4946-4947	)	_	_	_	_
36-45	4948-4951	are	_	_	_	_
36-46	4952-4958	better	_	_	_	_
36-47	4959-4961	at	_	_	_	_
36-48	4962-4970	handling	_	_	_	_
36-49	4971-4974	OoV	abstract|abstract[269]	new|giv[269]	_	_
36-50	4975-4980	words	abstract[269]	giv[269]	_	_
36-51	4981-4982	(	_	_	_	_
36-52	4983-4986	see	_	_	_	_
36-53	4987-4992	Table	abstract[270]	new[270]	_	_
36-54	4993-4995	11	abstract[270]	new[270]	_	_
36-55	4996-4997	)	_	_	_	_
36-56	4998-4999	,	_	_	_	_
36-57	5000-5003	but	_	_	_	_
36-58	5004-5006	as	_	_	_	_
36-59	5007-5008	a	_	_	_	_
36-60	5009-5015	result	_	_	_	_
36-61	5016-5018	of	_	_	_	_
36-62	5019-5030	probability	abstract|abstract[272]	new|new[272]	_	_
36-63	5031-5045	multiplication	abstract[272]	new[272]	_	_
36-64	5046-5047	,	_	_	_	_
36-65	5048-5050	in	_	_	_	_
36-66	5051-5055	many	abstract[273]	giv[273]	_	_
36-67	5056-5061	cases	abstract[273]	giv[273]	_	_
36-68	5062-5066	they	abstract	giv	_	_
36-69	5067-5072	score	_	_	_	_
36-70	5073-5079	unseen	abstract[275]	new[275]	_	_
36-71	5080-5090	multi-word	abstract[275]	new[275]	_	_
36-72	5091-5099	segments	abstract[275]	new[275]	_	_
36-73	5100-5106	higher	abstract[275]	new[275]	_	_
36-74	5107-5111	than	_	_	_	_
36-75	5112-5113	a	abstract[276]	new[276]	_	_
36-76	5114-5122	sequence	abstract[276]	new[276]	_	_
36-77	5123-5125	of	abstract[276]	new[276]	_	_
36-78	5126-5134	unigrams	abstract[276]|abstract	new[276]|giv	_	_
36-79	5135-5139	into	_	_	_	_
36-80	5140-5145	which	_	_	_	_
36-81	5146-5149	the	abstract[278]	giv[278]	_	_
36-82	5150-5155	given	abstract[278]	giv[278]	_	_
36-83	5156-5163	segment	abstract[278]	giv[278]	_	_
36-84	5164-5170	should	_	_	_	_
36-85	5171-5173	be	_	_	_	_
36-86	5174-5181	divided	_	_	_	_
36-87	5182-5183	,	_	_	_	_
36-88	5184-5189	hence	_	_	_	_
36-89	5190-5198	yielding	_	_	_	_
36-90	5199-5204	lower	_	_	_	_
36-91	5205-5211	Recall	_	_	_	_
36-92	5212-5213	.	_	_	_	_
