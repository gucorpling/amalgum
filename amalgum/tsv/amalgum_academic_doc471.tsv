#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=3. Advantages of Deep Learning Methods
1-1	0-2	3.	_	_	_	_
1-2	3-13	Advantages	abstract[1]	new[1]	_	_
1-3	14-16	of	abstract[1]	new[1]	_	_
1-4	17-21	Deep	abstract[1]|abstract[3]	new[1]|new[3]	coref	2-11[7_3]
1-5	22-30	Learning	abstract[1]|person|abstract[3]	new[1]|new|new[3]	_	_
1-6	31-38	Methods	abstract[1]|abstract[3]	new[1]|new[3]	_	_

#Text=To overcome the aforementioned challenges , various image processing and machine learning methods have been proposed and have so far achieved great progress .
2-1	39-41	To	_	_	_	_
2-2	42-50	overcome	_	_	_	_
2-3	51-54	the	abstract[4]	new[4]	_	_
2-4	55-69	aforementioned	abstract[4]	new[4]	_	_
2-5	70-80	challenges	abstract[4]	new[4]	_	_
2-6	81-82	,	_	_	_	_
2-7	83-90	various	abstract[6]	new[6]	_	_
2-8	91-96	image	object|abstract[6]	new|new[6]	coref	4-18
2-9	97-107	processing	abstract[6]	new[6]	_	_
2-10	108-111	and	_	_	_	_
2-11	112-119	machine	abstract[7]	giv[7]	coref	3-11[13_7]
2-12	120-128	learning	abstract[7]	giv[7]	_	_
2-13	129-136	methods	abstract[7]	giv[7]	_	_
2-14	137-141	have	_	_	_	_
2-15	142-146	been	_	_	_	_
2-16	147-155	proposed	_	_	_	_
2-17	156-159	and	_	_	_	_
2-18	160-164	have	_	_	_	_
2-19	165-167	so	_	_	_	_
2-20	168-171	far	_	_	_	_
2-21	172-180	achieved	_	_	_	_
2-22	181-186	great	abstract[8]	new[8]	_	_
2-23	187-195	progress	abstract[8]	new[8]	_	_
2-24	196-197	.	_	_	_	_

#Text=However , it is important to note the advantages of deep learning methods over non-deep-learning methods ( also called shallow-learning methods ) .
3-1	198-205	However	_	_	_	_
3-2	206-207	,	_	_	_	_
3-3	208-210	it	abstract	new	cata	3-3[0_10]
3-4	211-213	is	_	_	_	_
3-5	214-223	important	_	_	_	_
3-6	224-226	to	abstract[10]	new[10]	_	_
3-7	227-231	note	abstract[10]	new[10]	_	_
3-8	232-235	the	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-9	236-246	advantages	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-10	247-249	of	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-11	250-254	deep	abstract[10]|abstract[11]|abstract[13]	new[10]|new[11]|giv[13]	coref	3-20[14_13]
3-12	255-263	learning	abstract[10]|abstract[11]|abstract|abstract[13]	new[10]|new[11]|new|giv[13]	coref	4-15
3-13	264-271	methods	abstract[10]|abstract[11]|abstract[13]	new[10]|new[11]|giv[13]	_	_
3-14	272-276	over	abstract[10]|abstract[11]|abstract[13]	new[10]|new[11]|giv[13]	_	_
3-15	277-294	non-deep-learning	abstract[10]|abstract[11]|abstract[13]	new[10]|new[11]|giv[13]	_	_
3-16	295-302	methods	abstract[10]|abstract[11]|abstract[13]	new[10]|new[11]|giv[13]	_	_
3-17	303-304	(	_	_	_	_
3-18	305-309	also	_	_	_	_
3-19	310-316	called	_	_	_	_
3-20	317-333	shallow-learning	abstract[14]	giv[14]	coref	21-4[145_14]
3-21	334-341	methods	abstract[14]	giv[14]	_	_
3-22	342-343	)	_	_	_	_
3-23	344-345	.	_	_	_	_

#Text=Currently , convolutional neural networks ( CNNs ) are the most frequently used deep learning model for image data classification , including tumor detection in pathology images of breast cancer , renal cell carcinoma , prostate cancer , and head and neck cancer .
4-1	346-355	Currently	_	_	_	_
4-2	356-357	,	_	_	_	_
4-3	358-371	convolutional	place[15]	new[15]	coref	4-10[18_15]
4-4	372-378	neural	place[15]	new[15]	_	_
4-5	379-387	networks	place[15]	new[15]	_	_
4-6	388-389	(	_	_	_	_
4-7	390-394	CNNs	organization	new	coref	5-10[36_0]
4-8	395-396	)	_	_	_	_
4-9	397-400	are	_	_	_	_
4-10	401-404	the	place[18]	giv[18]	coref	5-16[39_18]
4-11	405-409	most	place[18]	giv[18]	_	_
4-12	410-420	frequently	place[18]	giv[18]	_	_
4-13	421-425	used	place[18]	giv[18]	_	_
4-14	426-430	deep	place[18]	giv[18]	_	_
4-15	431-439	learning	abstract|place[18]	giv|giv[18]	coref	7-8
4-16	440-445	model	place[18]	giv[18]	_	_
4-17	446-449	for	place[18]	giv[18]	_	_
4-18	450-455	image	place[18]|object|abstract[21]	giv[18]|giv|new[21]	coref|coref	5-12|6-27[50_21]
4-19	456-460	data	place[18]|abstract|abstract[21]	giv[18]|new|new[21]	coref	19-25[131_0]
4-20	461-475	classification	place[18]|abstract[21]	giv[18]|new[21]	_	_
4-21	476-477	,	place[18]	giv[18]	_	_
4-22	478-487	including	place[18]	giv[18]	_	_
4-23	488-493	tumor	place[18]|abstract|abstract[23]	giv[18]|new|new[23]	coref	7-30[65_23]
4-24	494-503	detection	place[18]|abstract[23]	giv[18]|new[23]	_	_
4-25	504-506	in	place[18]|abstract[23]	giv[18]|new[23]	_	_
4-26	507-516	pathology	place[18]|abstract[23]|abstract|object[25]	giv[18]|new[23]|new|new[25]	coref|coref	7-24|7-24[61_25]
4-27	517-523	images	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-28	524-526	of	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-29	527-533	breast	place[18]|abstract[23]|object[25]|abstract[26]	giv[18]|new[23]|new[25]|new[26]	_	_
4-30	534-540	cancer	place[18]|abstract[23]|object[25]|abstract[26]	giv[18]|new[23]|new[25]|new[26]	_	_
4-31	541-542	,	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-32	543-548	renal	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-33	549-553	cell	place[18]|abstract[23]|object[25]|place|object[28]	giv[18]|new[23]|new[25]|new|new[28]	coref	7-30
4-34	554-563	carcinoma	place[18]|abstract[23]|object[25]|object[28]	giv[18]|new[23]|new[25]|new[28]	_	_
4-35	564-565	,	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-36	566-574	prostate	place[18]|abstract[23]|object[25]|place|abstract[30]	giv[18]|new[23]|new[25]|new|new[30]	_	_
4-37	575-581	cancer	place[18]|abstract[23]|object[25]|abstract[30]	giv[18]|new[23]|new[25]|new[30]	_	_
4-38	582-583	,	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-39	584-587	and	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-40	588-592	head	place[18]|abstract[23]|object[25]|person	giv[18]|new[23]|new[25]|new	_	_
4-41	593-596	and	place[18]|abstract[23]|object[25]	giv[18]|new[23]|new[25]	_	_
4-42	597-601	neck	place[18]|abstract[23]|object[25]|object|abstract[33]	giv[18]|new[23]|new[25]|new|new[33]	_	_
4-43	602-608	cancer	place[18]|abstract[23]|object[25]|abstract[33]	giv[18]|new[23]|new[25]|new[33]	_	_
4-44	609-610	.	_	_	_	_

#Text=Several forms of neural network have been derived from CNNs for image segmentation , including fully convolutional networks ( FCNs ) and mask-regional convolutional neural networks ( mask-RCNNs ) .
5-1	611-618	Several	abstract[34]	new[34]	_	_
5-2	619-624	forms	abstract[34]	new[34]	_	_
5-3	625-627	of	abstract[34]	new[34]	_	_
5-4	628-634	neural	abstract[34]|abstract[35]	new[34]|new[35]	_	_
5-5	635-642	network	abstract[34]|abstract[35]	new[34]|new[35]	_	_
5-6	643-647	have	_	_	_	_
5-7	648-652	been	_	_	_	_
5-8	653-660	derived	_	_	_	_
5-9	661-665	from	_	_	_	_
5-10	666-670	CNNs	organization[36]	giv[36]	coref	12-10[0_36]
5-11	671-674	for	organization[36]	giv[36]	_	_
5-12	675-680	image	organization[36]|object|abstract[38]	giv[36]|giv|new[38]	coref|coref	6-28|6-31[52_38]
5-13	681-693	segmentation	organization[36]|abstract[38]	giv[36]|new[38]	_	_
5-14	694-695	,	_	_	_	_
5-15	696-705	including	_	_	_	_
5-16	706-711	fully	place[39]	giv[39]	coref	5-23[41_39]
5-17	712-725	convolutional	place[39]	giv[39]	_	_
5-18	726-734	networks	place[39]	giv[39]	_	_
5-19	735-736	(	_	_	_	_
5-20	737-741	FCNs	object	new	_	_
5-21	742-743	)	_	_	_	_
5-22	744-747	and	_	_	_	_
5-23	748-761	mask-regional	place[41]	giv[41]	coref	6-1[43_41]
5-24	762-775	convolutional	place[41]	giv[41]	_	_
5-25	776-782	neural	place[41]	giv[41]	_	_
5-26	783-791	networks	place[41]	giv[41]	_	_
5-27	792-793	(	_	_	_	_
5-28	794-804	mask-RCNNs	abstract	new	_	_
5-29	805-806	)	_	_	_	_
5-30	807-808	.	_	_	_	_

#Text=Recurrent neural networks ( RNNs ) , which are well known for modeling dynamic sequence behavior such as speech recognition , have also been explored in multi-label image classification and image segmentation .
6-1	809-818	Recurrent	place[43]	giv[43]	coref	8-4[70_43]
6-2	819-825	neural	place[43]	giv[43]	_	_
6-3	826-834	networks	place[43]	giv[43]	_	_
6-4	835-836	(	_	_	_	_
6-5	837-841	RNNs	abstract	new	_	_
6-6	842-843	)	_	_	_	_
6-7	844-845	,	_	_	_	_
6-8	846-851	which	_	_	_	_
6-9	852-855	are	_	_	_	_
6-10	856-860	well	_	_	_	_
6-11	861-866	known	_	_	_	_
6-12	867-870	for	_	_	_	_
6-13	871-879	modeling	_	_	_	_
6-14	880-887	dynamic	abstract[46]	new[46]	_	_
6-15	888-896	sequence	abstract|abstract[46]	new|new[46]	_	_
6-16	897-905	behavior	abstract[46]	new[46]	_	_
6-17	906-910	such	abstract[46]	new[46]	_	_
6-18	911-913	as	abstract[46]	new[46]	_	_
6-19	914-920	speech	abstract[46]|abstract|abstract[48]	new[46]|new|new[48]	_	_
6-20	921-932	recognition	abstract[46]|abstract[48]	new[46]|new[48]	_	_
6-21	933-934	,	_	_	_	_
6-22	935-939	have	_	_	_	_
6-23	940-944	also	_	_	_	_
6-24	945-949	been	_	_	_	_
6-25	950-958	explored	_	_	_	_
6-26	959-961	in	_	_	_	_
6-27	962-973	multi-label	abstract[50]	giv[50]	_	_
6-28	974-979	image	object|abstract[50]	giv|giv[50]	coref	6-31
6-29	980-994	classification	abstract[50]	giv[50]	_	_
6-30	995-998	and	_	_	_	_
6-31	999-1004	image	object|abstract[52]	giv|giv[52]	coref	7-34
6-32	1005-1017	segmentation	abstract[52]	giv[52]	_	_
6-33	1018-1019	.	_	_	_	_

#Text=In additional to the aforementioned supervised deep learning models , autoencoder , an unsupervised deep learning model , has shown ability in analyzing pathology images through pre-training models , cell detection , and image feature extraction .
7-1	1020-1022	In	_	_	_	_
7-2	1023-1033	additional	_	_	_	_
7-3	1034-1036	to	_	_	_	_
7-4	1037-1040	the	abstract[54]|abstract[55]	new[54]|new[55]	coref|coref	7-27[62_54]|7-27[63_55]
7-5	1041-1055	aforementioned	abstract[54]|abstract[55]	new[54]|new[55]	_	_
7-6	1056-1066	supervised	abstract[54]|abstract[55]	new[54]|new[55]	_	_
7-7	1067-1071	deep	abstract[54]|abstract[55]	new[54]|new[55]	_	_
7-8	1072-1080	learning	abstract|abstract[54]|abstract[55]	giv|new[54]|new[55]	coref	7-16
7-9	1081-1087	models	abstract[54]|abstract[55]	new[54]|new[55]	_	_
7-10	1088-1089	,	abstract[55]	new[55]	_	_
7-11	1090-1101	autoencoder	abstract[55]|object	new[55]|new	_	_
7-12	1102-1103	,	abstract[55]	new[55]	_	_
7-13	1104-1106	an	abstract[55]|abstract[58]	new[55]|new[58]	coref	16-2[98_58]
7-14	1107-1119	unsupervised	abstract[55]|abstract[58]	new[55]|new[58]	_	_
7-15	1120-1124	deep	abstract[55]|abstract[58]	new[55]|new[58]	_	_
7-16	1125-1133	learning	abstract[55]|abstract|abstract[58]	new[55]|giv|new[58]	coref	13-17[84_0]
7-17	1134-1139	model	abstract[55]|abstract[58]	new[55]|new[58]	_	_
7-18	1140-1141	,	_	_	_	_
7-19	1142-1145	has	_	_	_	_
7-20	1146-1151	shown	_	_	_	_
7-21	1152-1159	ability	abstract	new	_	_
7-22	1160-1162	in	_	_	_	_
7-23	1163-1172	analyzing	_	_	_	_
7-24	1173-1182	pathology	abstract|object[61]	giv|giv[61]	coref|coref	25-6|25-18[175_61]
7-25	1183-1189	images	object[61]	giv[61]	_	_
7-26	1190-1197	through	object[61]	giv[61]	_	_
7-27	1198-1210	pre-training	object[61]|abstract[62]|abstract[63]	giv[61]|giv[62]|giv[63]	coref	21-25[0_62]
7-28	1211-1217	models	object[61]|abstract[62]|abstract[63]	giv[61]|giv[62]|giv[63]	_	_
7-29	1218-1219	,	object[61]|abstract[63]	giv[61]|giv[63]	_	_
7-30	1220-1224	cell	object[61]|abstract[63]|object|abstract[65]	giv[61]|giv[63]|giv|giv[65]	_	_
7-31	1225-1234	detection	object[61]|abstract[63]|abstract[65]	giv[61]|giv[63]|giv[65]	_	_
7-32	1235-1236	,	object[61]|abstract[63]	giv[61]|giv[63]	_	_
7-33	1237-1240	and	object[61]|abstract[63]	giv[61]|giv[63]	_	_
7-34	1241-1246	image	object[61]|abstract[63]|object|abstract[68]	giv[61]|giv[63]|giv|new[68]	coref|coref	8-10|19-19[129_68]
7-35	1247-1254	feature	object[61]|abstract[63]|abstract|abstract[68]	giv[61]|giv[63]|new|new[68]	coref	16-10[101_0]
7-36	1255-1265	extraction	object[61]|abstract[63]|abstract[68]	giv[61]|giv[63]|new[68]	_	_
7-37	1266-1267	.	_	_	_	_

#Text=The taxonomy of the common neural networks used in image analysis is summarized in
8-1	1268-1271	The	abstract[69]	new[69]	_	_
8-2	1272-1280	taxonomy	abstract[69]	new[69]	_	_
8-3	1281-1283	of	abstract[69]	new[69]	_	_
8-4	1284-1287	the	abstract[69]|place[70]	new[69]|giv[70]	coref	12-6[79_70]
8-5	1288-1294	common	abstract[69]|place[70]	new[69]|giv[70]	_	_
8-6	1295-1301	neural	abstract[69]|place[70]	new[69]|giv[70]	_	_
8-7	1302-1310	networks	abstract[69]|place[70]	new[69]|giv[70]	_	_
8-8	1311-1315	used	_	_	_	_
8-9	1316-1318	in	_	_	_	_
8-10	1319-1324	image	object|abstract[72]	giv|new[72]	coref|coref	18-25[121_0]|18-27[0_72]
8-11	1325-1333	analysis	abstract[72]	new[72]	_	_
8-12	1334-1336	is	_	_	_	_
8-13	1337-1347	summarized	_	_	_	_
8-14	1348-1350	in	_	_	_	_

#Text=Figure 2
9-1	1351-1357	Figure	abstract[73]	new[73]	_	_
9-2	1358-1359	2	abstract[73]	new[73]	_	_

#Text=.
10-1	1360-1361	.	_	_	_	_

#Text=3.1 .
11-1	1362-1365	3.1	abstract	new	_	_
11-2	1366-1367	.	_	_	_	_

#Text=Inherent Characteristics and Advantages of Convolutional Neural Networks ( CNNs )
12-1	1368-1376	Inherent	abstract[75]|abstract[76]	new[75]|new[76]	coref	19-6[126_76]
12-2	1377-1392	Characteristics	abstract[75]|abstract[76]	new[75]|new[76]	_	_
12-3	1393-1396	and	abstract[76]	new[76]	_	_
12-4	1397-1407	Advantages	abstract[76]|abstract[77]	new[76]|new[77]	coref	22-10[154_77]
12-5	1408-1410	of	abstract[76]|abstract[77]	new[76]|new[77]	_	_
12-6	1411-1424	Convolutional	abstract[76]|abstract[77]|abstract[79]	new[76]|new[77]|giv[79]	coref	13-10[83_79]
12-7	1425-1431	Neural	abstract[76]|abstract[77]|abstract|abstract[79]	new[76]|new[77]|new|giv[79]	_	_
12-8	1432-1440	Networks	abstract[76]|abstract[77]|abstract[79]	new[76]|new[77]|giv[79]	_	_
12-9	1441-1442	(	_	_	_	_
12-10	1443-1447	CNNs	object	giv	coref	31-4
12-11	1448-1449	)	_	_	_	_

#Text=Inspired by the working mechanisms of the brain , deep neural networks , also called “ deep learning ” , have one or more “ hidden ” layers between the input and output layers .
13-1	1450-1458	Inspired	_	_	_	_
13-2	1459-1461	by	_	_	_	_
13-3	1462-1465	the	abstract[81]	new[81]	_	_
13-4	1466-1473	working	abstract[81]	new[81]	_	_
13-5	1474-1484	mechanisms	abstract[81]	new[81]	_	_
13-6	1485-1487	of	abstract[81]	new[81]	_	_
13-7	1488-1491	the	abstract[81]|object[82]	new[81]|new[82]	_	_
13-8	1492-1497	brain	abstract[81]|object[82]	new[81]|new[82]	_	_
13-9	1498-1499	,	abstract[81]	new[81]	_	_
13-10	1500-1504	deep	abstract[81]|place[83]	new[81]|giv[83]	coref	20-4[138_83]
13-11	1505-1511	neural	abstract[81]|place[83]	new[81]|giv[83]	_	_
13-12	1512-1520	networks	abstract[81]|place[83]	new[81]|giv[83]	_	_
13-13	1521-1522	,	_	_	_	_
13-14	1523-1527	also	_	_	_	_
13-15	1528-1534	called	_	_	_	_
13-16	1535-1536	“	_	_	_	_
13-17	1537-1541	deep	abstract[84]	giv[84]	coref	19-3[0_84]
13-18	1542-1550	learning	abstract[84]	giv[84]	_	_
13-19	1551-1552	”	_	_	_	_
13-20	1553-1554	,	_	_	_	_
13-21	1555-1559	have	_	_	_	_
13-22	1560-1563	one	abstract[85]	new[85]	_	_
13-23	1564-1566	or	abstract[85]	new[85]	_	_
13-24	1567-1571	more	abstract[85]	new[85]	_	_
13-25	1572-1573	“	abstract[85]	new[85]	_	_
13-26	1574-1580	hidden	abstract[85]	new[85]	_	_
13-27	1581-1582	”	abstract[85]	new[85]	_	_
13-28	1583-1589	layers	abstract[85]	new[85]	_	_
13-29	1590-1597	between	abstract[85]	new[85]	_	_
13-30	1598-1601	the	abstract[85]|abstract[86]	new[85]|new[86]	coref	16-26[0_86]
13-31	1602-1607	input	abstract[85]|abstract[86]	new[85]|new[86]	_	_
13-32	1608-1611	and	abstract[85]	new[85]	_	_
13-33	1612-1618	output	abstract[85]|abstract|abstract[88]	new[85]|new|new[88]	coref|coref	15-14[96_0]|19-34[133_88]
13-34	1619-1625	layers	abstract[85]|abstract[88]	new[85]|new[88]	_	_
13-35	1626-1627	.	_	_	_	_

#Text=In each layer , there are many neurons , also called kernels .
14-1	1628-1630	In	_	_	_	_
14-2	1631-1635	each	abstract[89]	new[89]	coref	17-15[110_89]
14-3	1636-1641	layer	abstract[89]	new[89]	_	_
14-4	1642-1643	,	_	_	_	_
14-5	1644-1649	there	_	_	_	_
14-6	1650-1653	are	_	_	_	_
14-7	1654-1658	many	object[90]	new[90]	_	_
14-8	1659-1666	neurons	object[90]	new[90]	_	_
14-9	1667-1668	,	_	_	_	_
14-10	1669-1673	also	_	_	_	_
14-11	1674-1680	called	_	_	_	_
14-12	1681-1688	kernels	object	new	coref	19-37[134_0]
14-13	1689-1690	.	_	_	_	_

#Text=Each kernel ( usually a function in mathematics ) takes inputs and computes an output .
15-1	1691-1695	Each	object[92]	new[92]	coref	16-6[100_92]
15-2	1696-1702	kernel	object[92]	new[92]	_	_
15-3	1703-1704	(	_	_	_	_
15-4	1705-1712	usually	abstract[93]	new[93]	_	_
15-5	1713-1714	a	abstract[93]	new[93]	_	_
15-6	1715-1723	function	abstract[93]	new[93]	_	_
15-7	1724-1726	in	abstract[93]	new[93]	_	_
15-8	1727-1738	mathematics	abstract[93]|abstract	new[93]|new	_	_
15-9	1739-1740	)	_	_	_	_
15-10	1741-1746	takes	_	_	_	_
15-11	1747-1753	inputs	abstract	new	_	_
15-12	1754-1757	and	_	_	_	_
15-13	1758-1766	computes	abstract	new	none|none	15-13[0_205]|16-9
15-14	1767-1769	an	abstract[96]	giv[96]	_	_
15-15	1770-1776	output	abstract[96]	giv[96]	_	_
15-16	1777-1778	.	_	_	_	_

#Text=In a CNN model , a convolution kernel computes a feature at a specific location , called a “ receptive field ” , in the input space .
16-1	1779-1781	In	_	_	_	_
16-2	1782-1783	a	abstract[98]	giv[98]	coref	19-1[125_98]
16-3	1784-1787	CNN	object|abstract[98]	new|giv[98]	coref	18-20
16-4	1788-1793	model	abstract[98]	giv[98]	_	_
16-5	1794-1795	,	_	_	_	_
16-6	1796-1797	a	object[100]	giv[100]	_	_
16-7	1798-1809	convolution	abstract|object[100]	new|giv[100]	coref	17-27
16-8	1810-1816	kernel	object[100]	giv[100]	_	_
16-9	1817-1825	computes	abstract	new	coref	27-3[193_0]
16-10	1826-1827	a	abstract[101]	giv[101]	coref	17-22[0_101]
16-11	1828-1835	feature	abstract[101]	giv[101]	_	_
16-12	1836-1838	at	_	_	_	_
16-13	1839-1840	a	place[102]	new[102]	_	_
16-14	1841-1849	specific	place[102]	new[102]	_	_
16-15	1850-1858	location	place[102]	new[102]	_	_
16-16	1859-1860	,	_	_	_	_
16-17	1861-1867	called	_	_	_	_
16-18	1868-1869	a	abstract[103]	new[103]	_	_
16-19	1870-1871	“	abstract[103]	new[103]	_	_
16-20	1872-1881	receptive	abstract[103]	new[103]	_	_
16-21	1882-1887	field	abstract[103]	new[103]	_	_
16-22	1888-1889	”	abstract[103]	new[103]	_	_
16-23	1890-1891	,	_	_	_	_
16-24	1892-1894	in	_	_	_	_
16-25	1895-1898	the	place[105]	new[105]	_	_
16-26	1899-1904	input	abstract|place[105]	giv|new[105]	coref	17-16
16-27	1905-1910	space	place[105]	new[105]	_	_
16-28	1911-1912	.	_	_	_	_

#Text=The term “ convolutional ” denotes the operation of sliding the receptive fields through the input layer to generate the “ feature map ” from the convolution layer as the outputs .
17-1	1913-1916	The	abstract[106]	new[106]	_	_
17-2	1917-1921	term	abstract[106]	new[106]	_	_
17-3	1922-1923	“	abstract[106]	new[106]	_	_
17-4	1924-1937	convolutional	abstract[106]	new[106]	_	_
17-5	1938-1939	”	abstract[106]	new[106]	_	_
17-6	1940-1947	denotes	_	_	_	_
17-7	1948-1951	the	event[107]	new[107]	coref	18-4[115_107]
17-8	1952-1961	operation	event[107]	new[107]	_	_
17-9	1962-1964	of	_	_	_	_
17-10	1965-1972	sliding	_	_	_	_
17-11	1973-1976	the	abstract[108]	new[108]	_	_
17-12	1977-1986	receptive	abstract[108]	new[108]	_	_
17-13	1987-1993	fields	abstract[108]	new[108]	_	_
17-14	1994-2001	through	_	_	_	_
17-15	2002-2005	the	abstract[110]	giv[110]	coref	17-26[114_110]
17-16	2006-2011	input	abstract|abstract[110]	giv|giv[110]	coref	24-40
17-17	2012-2017	layer	abstract[110]	giv[110]	_	_
17-18	2018-2020	to	_	_	_	_
17-19	2021-2029	generate	_	_	_	_
17-20	2030-2033	the	object[112]	new[112]	_	_
17-21	2034-2035	“	object[112]	new[112]	_	_
17-22	2036-2043	feature	abstract|object[112]	giv|new[112]	coref	24-2
17-23	2044-2047	map	object[112]	new[112]	_	_
17-24	2048-2049	”	object[112]	new[112]	_	_
17-25	2050-2054	from	_	_	_	_
17-26	2055-2058	the	abstract[114]	giv[114]	_	_
17-27	2059-2070	convolution	substance|abstract[114]	giv|giv[114]	coref	32-50
17-28	2071-2076	layer	abstract[114]	giv[114]	_	_
17-29	2077-2079	as	abstract[114]	giv[114]	_	_
17-30	2080-2083	the	abstract[114]	giv[114]	_	_
17-31	2084-2091	outputs	abstract[114]	giv[114]	_	_
17-32	2092-2093	.	_	_	_	_

#Text=In essence , this operation was inspired by the functional mechanism of the visual cortex , and it makes CNN a great solution for many image analysis tasks .
18-1	2094-2096	In	_	_	_	_
18-2	2097-2104	essence	_	_	_	_
18-3	2105-2106	,	_	_	_	_
18-4	2107-2111	this	event[115]	giv[115]	coref	32-50[243_115]
18-5	2112-2121	operation	event[115]	giv[115]	_	_
18-6	2122-2125	was	_	_	_	_
18-7	2126-2134	inspired	_	_	_	_
18-8	2135-2137	by	_	_	_	_
18-9	2138-2141	the	abstract[116]	new[116]	_	_
18-10	2142-2152	functional	abstract[116]	new[116]	_	_
18-11	2153-2162	mechanism	abstract[116]	new[116]	_	_
18-12	2163-2165	of	abstract[116]	new[116]	_	_
18-13	2166-2169	the	abstract[116]|object[117]	new[116]|new[117]	ana	18-18[0_117]
18-14	2170-2176	visual	abstract[116]|object[117]	new[116]|new[117]	_	_
18-15	2177-2183	cortex	abstract[116]|object[117]	new[116]|new[117]	_	_
18-16	2184-2185	,	_	_	_	_
18-17	2186-2189	and	_	_	_	_
18-18	2190-2192	it	object	giv	_	_
18-19	2193-2198	makes	_	_	_	_
18-20	2199-2202	CNN	object	giv	coref	32-35
18-21	2203-2204	a	abstract[120]	new[120]	_	_
18-22	2205-2210	great	abstract[120]	new[120]	_	_
18-23	2211-2219	solution	abstract[120]	new[120]	_	_
18-24	2220-2223	for	abstract[120]	new[120]	_	_
18-25	2224-2228	many	abstract[120]|object[121]|abstract[123]	new[120]|giv[121]|new[123]	coref	25-7[0_121]
18-26	2229-2234	image	abstract[120]|object[121]|abstract[123]	new[120]|giv[121]|new[123]	_	_
18-27	2235-2243	analysis	abstract[120]|abstract|abstract[123]	new[120]|giv|new[123]	coref	25-6[173_0]
18-28	2244-2249	tasks	abstract[120]|abstract[123]	new[120]|new[123]	_	_
18-29	2250-2251	.	_	_	_	_

#Text=A deep learning model has two important characteristics : ( 1 ) it allows for the construction and extraction of flexible representational features from input data , and ( 2 ) it contains multiple layers and many kernels that enable it to approximate basically any complex functions using the extracted features .
19-1	2252-2253	A	abstract[125]	giv[125]	ana	19-13[0_125]
19-2	2254-2258	deep	abstract[125]	giv[125]	_	_
19-3	2259-2267	learning	abstract|abstract[125]	giv|giv[125]	coref	21-6
19-4	2268-2273	model	abstract[125]	giv[125]	_	_
19-5	2274-2277	has	_	_	_	_
19-6	2278-2281	two	abstract[126]	giv[126]	_	_
19-7	2282-2291	important	abstract[126]	giv[126]	_	_
19-8	2292-2307	characteristics	abstract[126]	giv[126]	_	_
19-9	2308-2309	:	abstract[126]	giv[126]	_	_
19-10	2310-2311	(	abstract[126]	giv[126]	_	_
19-11	2312-2313	1	abstract[126]	giv[126]	_	_
19-12	2314-2315	)	abstract[126]	giv[126]	_	_
19-13	2316-2318	it	abstract	giv	ana	19-32
19-14	2319-2325	allows	_	_	_	_
19-15	2326-2329	for	_	_	_	_
19-16	2330-2333	the	event[128]	new[128]	_	_
19-17	2334-2346	construction	event[128]	new[128]	_	_
19-18	2347-2350	and	_	_	_	_
19-19	2351-2361	extraction	abstract[129]	giv[129]	coref	24-1[160_129]
19-20	2362-2364	of	abstract[129]	giv[129]	_	_
19-21	2365-2373	flexible	abstract[129]|abstract[130]	giv[129]|new[130]	coref	19-49[137_130]
19-22	2374-2390	representational	abstract[129]|abstract[130]	giv[129]|new[130]	_	_
19-23	2391-2399	features	abstract[129]|abstract[130]	giv[129]|new[130]	_	_
19-24	2400-2404	from	abstract[129]|abstract[130]	giv[129]|new[130]	_	_
19-25	2405-2410	input	abstract[129]|abstract[130]|abstract[131]	giv[129]|new[130]|giv[131]	coref	24-40[168_131]
19-26	2411-2415	data	abstract[129]|abstract[130]|abstract[131]	giv[129]|new[130]|giv[131]	_	_
19-27	2416-2417	,	_	_	_	_
19-28	2418-2421	and	_	_	_	_
19-29	2422-2423	(	_	_	_	_
19-30	2424-2425	2	_	_	_	_
19-31	2426-2427	)	_	_	_	_
19-32	2428-2430	it	abstract	giv	ana	19-41
19-33	2431-2439	contains	_	_	_	_
19-34	2440-2448	multiple	abstract[133]	giv[133]	_	_
19-35	2449-2455	layers	abstract[133]	giv[133]	_	_
19-36	2456-2459	and	abstract[133]	giv[133]	_	_
19-37	2460-2464	many	abstract[133]|object[134]	giv[133]|giv[134]	_	_
19-38	2465-2472	kernels	abstract[133]|object[134]	giv[133]|giv[134]	_	_
19-39	2473-2477	that	_	_	_	_
19-40	2478-2484	enable	_	_	_	_
19-41	2485-2487	it	abstract	giv	coref	29-24[211_0]
19-42	2488-2490	to	_	_	_	_
19-43	2491-2502	approximate	abstract[136]	new[136]	_	_
19-44	2503-2512	basically	abstract[136]	new[136]	_	_
19-45	2513-2516	any	abstract[136]	new[136]	_	_
19-46	2517-2524	complex	abstract[136]	new[136]	_	_
19-47	2525-2534	functions	abstract[136]	new[136]	_	_
19-48	2535-2540	using	_	_	_	_
19-49	2541-2544	the	abstract[137]	giv[137]	coref	20-12[0_137]
19-50	2545-2554	extracted	abstract[137]	giv[137]	_	_
19-51	2555-2563	features	abstract[137]	giv[137]	_	_
19-52	2564-2565	.	_	_	_	_

#Text=In all , deep neural networks are capable of automatically extracting features and solving highly complex prediction problems .
20-1	2566-2568	In	_	_	_	_
20-2	2569-2572	all	_	_	_	_
20-3	2573-2574	,	_	_	_	_
20-4	2575-2579	deep	place[138]	giv[138]	_	_
20-5	2580-2586	neural	place[138]	giv[138]	_	_
20-6	2587-2595	networks	place[138]	giv[138]	_	_
20-7	2596-2599	are	_	_	_	_
20-8	2600-2607	capable	_	_	_	_
20-9	2608-2610	of	_	_	_	_
20-10	2611-2624	automatically	_	_	_	_
20-11	2625-2635	extracting	_	_	_	_
20-12	2636-2644	features	abstract	giv	coref	21-17[147_0]
20-13	2645-2648	and	_	_	_	_
20-14	2649-2656	solving	_	_	_	_
20-15	2657-2663	highly	abstract[141]	new[141]	coref	30-15[222_141]
20-16	2664-2671	complex	abstract[141]	new[141]	_	_
20-17	2672-2682	prediction	event|abstract[141]	new|new[141]	coref	24-22
20-18	2683-2691	problems	abstract[141]	new[141]	_	_
20-19	2692-2693	.	_	_	_	_

#Text=In contrast , traditional machine learning methods have two major steps : ( 1 ) defining the features , and ( 2 ) constructing models using these handcrafted features .
21-1	2694-2696	In	_	_	_	_
21-2	2697-2705	contrast	abstract	new	_	_
21-3	2706-2707	,	_	_	_	_
21-4	2708-2719	traditional	abstract[145]	giv[145]	coref	22-3[151_145]
21-5	2720-2727	machine	object|abstract[145]	new|giv[145]	_	_
21-6	2728-2736	learning	abstract|abstract[145]	giv|giv[145]	coref	22-7
21-7	2737-2744	methods	abstract[145]	giv[145]	_	_
21-8	2745-2749	have	_	_	_	_
21-9	2750-2753	two	abstract[146]	new[146]	coref	29-32[214_146]
21-10	2754-2759	major	abstract[146]	new[146]	_	_
21-11	2760-2765	steps	abstract[146]	new[146]	_	_
21-12	2766-2767	:	abstract[146]	new[146]	_	_
21-13	2768-2769	(	abstract[146]	new[146]	_	_
21-14	2770-2771	1	abstract[146]	new[146]	_	_
21-15	2772-2773	)	abstract[146]	new[146]	_	_
21-16	2774-2782	defining	_	_	_	_
21-17	2783-2786	the	abstract[147]	giv[147]	coref	21-27[150_147]
21-18	2787-2795	features	abstract[147]	giv[147]	_	_
21-19	2796-2797	,	_	_	_	_
21-20	2798-2801	and	_	_	_	_
21-21	2802-2803	(	_	_	_	_
21-22	2804-2805	2	quantity	new	_	_
21-23	2806-2807	)	_	_	_	_
21-24	2808-2820	constructing	_	_	_	_
21-25	2821-2827	models	abstract	giv	coref	22-6[153_0]
21-26	2828-2833	using	_	_	_	_
21-27	2834-2839	these	abstract[150]	giv[150]	coref	23-15[0_150]
21-28	2840-2851	handcrafted	abstract[150]	giv[150]	_	_
21-29	2852-2860	features	abstract[150]	giv[150]	_	_
21-30	2861-2862	.	_	_	_	_

#Text=Compared with traditional methods , deep learning models have the following advantages :
22-1	2863-2871	Compared	_	_	_	_
22-2	2872-2876	with	_	_	_	_
22-3	2877-2888	traditional	abstract[151]	giv[151]	coref	29-39[217_151]
22-4	2889-2896	methods	abstract[151]	giv[151]	_	_
22-5	2897-2898	,	_	_	_	_
22-6	2899-2903	deep	abstract[153]	giv[153]	coref	23-3[156_153]
22-7	2904-2912	learning	abstract|abstract[153]	giv|giv[153]	coref	23-4
22-8	2913-2919	models	abstract[153]	giv[153]	_	_
22-9	2920-2924	have	_	_	_	_
22-10	2925-2928	the	abstract[154]	giv[154]	_	_
22-11	2929-2938	following	abstract[154]	giv[154]	_	_
22-12	2939-2949	advantages	abstract[154]	giv[154]	_	_
22-13	2950-2951	:	_	_	_	_

#Text=First , deep learning models greatly simplify or remove the task of manually defining features .
23-1	2952-2957	First	_	_	_	_
23-2	2958-2959	,	_	_	_	_
23-3	2960-2964	deep	abstract[156]	giv[156]	coref	32-8[230_156]
23-4	2965-2973	learning	abstract|abstract[156]	giv|giv[156]	coref	26-21[192_0]
23-5	2974-2980	models	abstract[156]	giv[156]	_	_
23-6	2981-2988	greatly	_	_	_	_
23-7	2989-2997	simplify	_	_	_	_
23-8	2998-3000	or	_	_	_	_
23-9	3001-3007	remove	_	_	_	_
23-10	3008-3011	the	abstract[157]	new[157]	_	_
23-11	3012-3016	task	abstract[157]	new[157]	_	_
23-12	3017-3019	of	_	_	_	_
23-13	3020-3028	manually	_	_	_	_
23-14	3029-3037	defining	_	_	_	_
23-15	3038-3046	features	abstract	giv	coref	30-9[219_0]
23-16	3047-3048	.	_	_	_	_

#Text=Manual feature extraction is very challenging and time consuming , especially in the following two scenarios : ( 1 ) the prediction problem is complex , and/or ( 2 ) there is limited prior knowledge about the relationship between input data and the outcomes to be predicted .
24-1	3049-3055	Manual	abstract[160]	giv[160]	coref	29-34[0_160]
24-2	3056-3063	feature	abstract|abstract[160]	giv|giv[160]	coref	29-33
24-3	3064-3074	extraction	abstract[160]	giv[160]	_	_
24-4	3075-3077	is	_	_	_	_
24-5	3078-3082	very	_	_	_	_
24-6	3083-3094	challenging	_	_	_	_
24-7	3095-3098	and	_	_	_	_
24-8	3099-3103	time	time	new	_	_
24-9	3104-3113	consuming	_	_	_	_
24-10	3114-3115	,	_	_	_	_
24-11	3116-3126	especially	event[162]	new[162]	coref	25-1[170_162]
24-12	3127-3129	in	event[162]	new[162]	_	_
24-13	3130-3133	the	event[162]	new[162]	_	_
24-14	3134-3143	following	event[162]	new[162]	_	_
24-15	3144-3147	two	event[162]	new[162]	_	_
24-16	3148-3157	scenarios	event[162]	new[162]	_	_
24-17	3158-3159	:	event[162]	new[162]	_	_
24-18	3160-3161	(	event[162]	new[162]	_	_
24-19	3162-3163	1	event[162]	new[162]	_	_
24-20	3164-3165	)	event[162]	new[162]	_	_
24-21	3166-3169	the	abstract[164]	new[164]	_	_
24-22	3170-3180	prediction	event|abstract[164]	giv|new[164]	coref	25-12
24-23	3181-3188	problem	abstract[164]	new[164]	_	_
24-24	3189-3191	is	_	_	_	_
24-25	3192-3199	complex	_	_	_	_
24-26	3200-3201	,	_	_	_	_
24-27	3202-3208	and/or	_	_	_	_
24-28	3209-3210	(	_	_	_	_
24-29	3211-3212	2	_	_	_	_
24-30	3213-3214	)	_	_	_	_
24-31	3215-3220	there	_	_	_	_
24-32	3221-3223	is	_	_	_	_
24-33	3224-3231	limited	_	_	_	_
24-34	3232-3237	prior	_	_	_	_
24-35	3238-3247	knowledge	abstract[165]	new[165]	coref	25-41[182_165]
24-36	3248-3253	about	abstract[165]	new[165]	_	_
24-37	3254-3257	the	abstract[165]|abstract[166]	new[165]|new[166]	_	_
24-38	3258-3270	relationship	abstract[165]|abstract[166]	new[165]|new[166]	_	_
24-39	3271-3278	between	abstract[165]|abstract[166]	new[165]|new[166]	_	_
24-40	3279-3284	input	abstract[165]|abstract[166]|abstract|abstract[168]	new[165]|new[166]|giv|giv[168]	coref	32-19[232_168]
24-41	3285-3289	data	abstract[165]|abstract[166]|abstract[168]	new[165]|new[166]|giv[168]	_	_
24-42	3290-3293	and	abstract[165]|abstract[166]|abstract[168]	new[165]|new[166]|giv[168]	_	_
24-43	3294-3297	the	abstract[165]|abstract[166]|abstract[168]|event[169]	new[165]|new[166]|giv[168]|new[169]	coref	25-22[177_169]
24-44	3298-3306	outcomes	abstract[165]|abstract[166]|abstract[168]|event[169]	new[165]|new[166]|giv[168]|new[169]	_	_
24-45	3307-3309	to	_	_	_	_
24-46	3310-3312	be	_	_	_	_
24-47	3313-3322	predicted	_	_	_	_
24-48	3323-3324	.	_	_	_	_

#Text=Both scenarios are true of pathology image analysis , as the prediction problems ( such as using pathology images to predict patient outcomes or recognizing various tissue structures and cells from H&E-stained images ) are very complex , and despite the accumulated knowledge from pathologists , little is known about which quantitative image features predict the outcomes .
25-1	3325-3329	Both	event[170]	giv[170]	_	_
25-2	3330-3339	scenarios	event[170]	giv[170]	_	_
25-3	3340-3343	are	_	_	_	_
25-4	3344-3348	true	_	_	_	_
25-5	3349-3351	of	_	_	_	_
25-6	3352-3361	pathology	abstract|abstract[173]	giv|giv[173]	coref|coref	26-8|26-8[190_173]
25-7	3362-3367	image	object|abstract[173]	giv|giv[173]	coref	25-52[184_0]
25-8	3368-3376	analysis	abstract[173]	giv[173]	_	_
25-9	3377-3378	,	abstract[173]	giv[173]	_	_
25-10	3379-3381	as	abstract[173]	giv[173]	_	_
25-11	3382-3385	the	abstract[173]	giv[173]	_	_
25-12	3386-3396	prediction	abstract[173]|event	giv[173]|giv	coref	30-18
25-13	3397-3405	problems	abstract[173]	giv[173]	_	_
25-14	3406-3407	(	_	_	_	_
25-15	3408-3412	such	_	_	_	_
25-16	3413-3415	as	_	_	_	_
25-17	3416-3421	using	_	_	_	_
25-18	3422-3431	pathology	object[175]	giv[175]	coref	25-32[181_175]
25-19	3432-3438	images	object[175]	giv[175]	_	_
25-20	3439-3441	to	_	_	_	_
25-21	3442-3449	predict	_	_	_	_
25-22	3450-3457	patient	person|event[177]	new|giv[177]	coref	25-56[185_177]
25-23	3458-3466	outcomes	event[177]	giv[177]	_	_
25-24	3467-3469	or	_	_	_	_
25-25	3470-3481	recognizing	_	_	_	_
25-26	3482-3489	various	abstract[179]	new[179]	_	_
25-27	3490-3496	tissue	object|abstract[179]	new|new[179]	_	_
25-28	3497-3507	structures	abstract[179]	new[179]	_	_
25-29	3508-3511	and	_	_	_	_
25-30	3512-3517	cells	object[180]	new[180]	_	_
25-31	3518-3522	from	object[180]	new[180]	_	_
25-32	3523-3534	H&E-stained	object[180]|object[181]	new[180]|giv[181]	_	_
25-33	3535-3541	images	object[180]|object[181]	new[180]|giv[181]	_	_
25-34	3542-3543	)	_	_	_	_
25-35	3544-3547	are	_	_	_	_
25-36	3548-3552	very	_	_	_	_
25-37	3553-3560	complex	_	_	_	_
25-38	3561-3562	,	_	_	_	_
25-39	3563-3566	and	_	_	_	_
25-40	3567-3574	despite	_	_	_	_
25-41	3575-3578	the	abstract[182]	giv[182]	_	_
25-42	3579-3590	accumulated	abstract[182]	giv[182]	_	_
25-43	3591-3600	knowledge	abstract[182]	giv[182]	_	_
25-44	3601-3605	from	abstract[182]	giv[182]	_	_
25-45	3606-3618	pathologists	abstract[182]|person	giv[182]|new	_	_
25-46	3619-3620	,	_	_	_	_
25-47	3621-3627	little	_	_	_	_
25-48	3628-3630	is	_	_	_	_
25-49	3631-3636	known	_	_	_	_
25-50	3637-3642	about	_	_	_	_
25-51	3643-3648	which	_	_	_	_
25-52	3649-3661	quantitative	object[184]	giv[184]	coref	26-9[0_184]
25-53	3662-3667	image	object[184]	giv[184]	_	_
25-54	3668-3676	features	_	_	_	_
25-55	3677-3684	predict	_	_	_	_
25-56	3685-3688	the	event[185]	giv[185]	_	_
25-57	3689-3697	outcomes	event[185]	giv[185]	_	_
25-58	3698-3699	.	_	_	_	_

#Text=As a result , the advance of pathology image analysis had been slow and limited until the recent development of deep learning .
26-1	3700-3702	As	_	_	_	_
26-2	3703-3704	a	abstract[186]	new[186]	_	_
26-3	3705-3711	result	abstract[186]	new[186]	_	_
26-4	3712-3713	,	_	_	_	_
26-5	3714-3717	the	abstract[187]	new[187]	_	_
26-6	3718-3725	advance	abstract[187]	new[187]	_	_
26-7	3726-3728	of	abstract[187]	new[187]	_	_
26-8	3729-3738	pathology	abstract[187]|abstract|abstract[190]	new[187]|giv|giv[190]	coref|coref	32-84|32-84[255_190]
26-9	3739-3744	image	abstract[187]|object|abstract[190]	new[187]|giv|giv[190]	coref	29-11[208_0]
26-10	3745-3753	analysis	abstract[187]|abstract[190]	new[187]|giv[190]	_	_
26-11	3754-3757	had	_	_	_	_
26-12	3758-3762	been	_	_	_	_
26-13	3763-3767	slow	_	_	_	_
26-14	3768-3771	and	_	_	_	_
26-15	3772-3779	limited	_	_	_	_
26-16	3780-3785	until	_	_	_	_
26-17	3786-3789	the	abstract[191]	new[191]	_	_
26-18	3790-3796	recent	abstract[191]	new[191]	_	_
26-19	3797-3808	development	abstract[191]	new[191]	_	_
26-20	3809-3811	of	abstract[191]	new[191]	_	_
26-21	3812-3816	deep	abstract[191]|abstract[192]	new[191]|giv[192]	coref	27-7[0_192]
26-22	3817-3825	learning	abstract[191]|abstract[192]	new[191]|giv[192]	_	_
26-23	3826-3827	.	_	_	_	_

#Text=Second , the computation of deep learning algorithms can be highly parallel .
27-1	3828-3834	Second	_	_	_	_
27-2	3835-3836	,	_	_	_	_
27-3	3837-3840	the	abstract[193]	new[193]	coref	29-2[205_193]
27-4	3841-3852	computation	abstract[193]	new[193]	_	_
27-5	3853-3855	of	abstract[193]	new[193]	_	_
27-6	3856-3860	deep	abstract[193]|abstract[195]	new[193]|new[195]	_	_
27-7	3861-3869	learning	abstract[193]|abstract|abstract[195]	new[193]|giv|new[195]	coref	28-5[196_0]
27-8	3870-3880	algorithms	abstract[193]|abstract[195]	new[193]|new[195]	_	_
27-9	3881-3884	can	_	_	_	_
27-10	3885-3887	be	_	_	_	_
27-11	3888-3894	highly	_	_	_	_
27-12	3895-3903	parallel	_	_	_	_
27-13	3904-3905	.	_	_	_	_

#Text=As a result , deep learning can largely leverage the parallel computing power from the recent developments in GPU ( graphics processing unit ) hardware .
28-1	3906-3908	As	_	_	_	_
28-2	3909-3910	a	_	_	_	_
28-3	3911-3917	result	_	_	_	_
28-4	3918-3919	,	_	_	_	_
28-5	3920-3924	deep	abstract[196]	giv[196]	coref	29-26[0_196]
28-6	3925-3933	learning	abstract[196]	giv[196]	_	_
28-7	3934-3937	can	_	_	_	_
28-8	3938-3945	largely	_	_	_	_
28-9	3946-3954	leverage	_	_	_	_
28-10	3955-3958	the	abstract[198]	new[198]	coref	31-7[225_198]
28-11	3959-3967	parallel	abstract[198]	new[198]	_	_
28-12	3968-3977	computing	abstract|abstract[198]	new|new[198]	_	_
28-13	3978-3983	power	abstract[198]	new[198]	_	_
28-14	3984-3988	from	_	_	_	_
28-15	3989-3992	the	abstract[199]	new[199]	_	_
28-16	3993-3999	recent	abstract[199]	new[199]	_	_
28-17	4000-4012	developments	abstract[199]	new[199]	_	_
28-18	4013-4015	in	abstract[199]	new[199]	_	_
28-19	4016-4019	GPU	abstract[199]|object	new[199]|new	_	_
28-20	4020-4021	(	abstract[199]|abstract[204]	new[199]|new[204]	_	_
28-21	4022-4030	graphics	abstract[199]|abstract|object[203]|abstract[204]	new[199]|new|new[203]|new[204]	_	_
28-22	4031-4041	processing	abstract[199]|abstract|object[203]|abstract[204]	new[199]|new|new[203]|new[204]	_	_
28-23	4042-4046	unit	abstract[199]|object[203]|abstract[204]	new[199]|new[203]|new[204]	_	_
28-24	4047-4048	)	abstract[199]|abstract[204]	new[199]|new[204]	_	_
28-25	4049-4057	hardware	abstract[199]|abstract[204]	new[199]|new[204]	_	_
28-26	4058-4059	.	_	_	_	_

#Text=With GPU-aided computation , processing ( classifying or segmenting ) a 1000 × 1000 pixels image usually takes less than one second for a deep learning model , much faster than traditional feature extraction steps and non-deep-learning-based image segmentation methods .
29-1	4060-4064	With	_	_	_	_
29-2	4065-4074	GPU-aided	abstract[205]	giv[205]	_	_
29-3	4075-4086	computation	abstract[205]	giv[205]	_	_
29-4	4087-4088	,	_	_	_	_
29-5	4089-4099	processing	_	_	_	_
29-6	4100-4101	(	_	_	_	_
29-7	4102-4113	classifying	_	_	_	_
29-8	4114-4116	or	_	_	_	_
29-9	4117-4127	segmenting	_	_	_	_
29-10	4128-4129	)	_	_	_	_
29-11	4130-4131	a	object[208]	giv[208]	coref	29-37[215_208]
29-12	4132-4136	1000	abstract[207]|object[208]	new[207]|giv[208]	_	_
29-13	4137-4138	×	abstract[207]|object[208]	new[207]|giv[208]	_	_
29-14	4139-4143	1000	quantity|abstract[207]|object[208]	new|new[207]|giv[208]	_	_
29-15	4144-4150	pixels	abstract[207]|object[208]	new[207]|giv[208]	_	_
29-16	4151-4156	image	object[208]	giv[208]	_	_
29-17	4157-4164	usually	_	_	_	_
29-18	4165-4170	takes	_	_	_	_
29-19	4171-4175	less	_	_	_	_
29-20	4176-4180	than	_	_	_	_
29-21	4181-4184	one	time[209]	new[209]	_	_
29-22	4185-4191	second	time[209]	new[209]	_	_
29-23	4192-4195	for	_	_	_	_
29-24	4196-4197	a	abstract[211]	giv[211]	coref	32-29[235_211]
29-25	4198-4202	deep	abstract[211]	giv[211]	_	_
29-26	4203-4211	learning	abstract|abstract[211]	giv|giv[211]	coref	30-4[218_0]
29-27	4212-4217	model	abstract[211]	giv[211]	_	_
29-28	4218-4219	,	_	_	_	_
29-29	4220-4224	much	_	_	_	_
29-30	4225-4231	faster	_	_	_	_
29-31	4232-4236	than	_	_	_	_
29-32	4237-4248	traditional	abstract[214]	giv[214]	_	_
29-33	4249-4256	feature	abstract|abstract[214]	giv|giv[214]	_	_
29-34	4257-4267	extraction	abstract|abstract[214]	giv|giv[214]	_	_
29-35	4268-4273	steps	abstract[214]	giv[214]	_	_
29-36	4274-4277	and	_	_	_	_
29-37	4278-4301	non-deep-learning-based	object[215]	giv[215]	coref	32-19[0_215]
29-38	4302-4307	image	object[215]	giv[215]	_	_
29-39	4308-4320	segmentation	abstract|abstract[217]	new|giv[217]	coref	32-4[229_217]
29-40	4321-4328	methods	abstract[217]	giv[217]	_	_
29-41	4329-4330	.	_	_	_	_

#Text=Furthermore , since deep learning does not require handcrafted features , it can handle much more complex prediction problems and is able to recognize multiple objects simultaneously .
30-1	4331-4342	Furthermore	_	_	_	_
30-2	4343-4344	,	_	_	_	_
30-3	4345-4350	since	_	_	_	_
30-4	4351-4355	deep	abstract[218]	giv[218]	ana	30-12[0_218]
30-5	4356-4364	learning	abstract[218]	giv[218]	_	_
30-6	4365-4369	does	_	_	_	_
30-7	4370-4373	not	_	_	_	_
30-8	4374-4381	require	_	_	_	_
30-9	4382-4393	handcrafted	abstract[219]	giv[219]	_	_
30-10	4394-4402	features	abstract[219]	giv[219]	_	_
30-11	4403-4404	,	_	_	_	_
30-12	4405-4407	it	abstract	giv	coref	32-5
30-13	4408-4411	can	_	_	_	_
30-14	4412-4418	handle	_	_	_	_
30-15	4419-4423	much	abstract[222]	giv[222]	_	_
30-16	4424-4428	more	abstract[222]	giv[222]	_	_
30-17	4429-4436	complex	abstract[222]	giv[222]	_	_
30-18	4437-4447	prediction	event|abstract[222]	giv|giv[222]	coref	32-29
30-19	4448-4456	problems	abstract[222]	giv[222]	_	_
30-20	4457-4460	and	_	_	_	_
30-21	4461-4463	is	_	_	_	_
30-22	4464-4468	able	_	_	_	_
30-23	4469-4471	to	_	_	_	_
30-24	4472-4481	recognize	_	_	_	_
30-25	4482-4490	multiple	abstract[223]	new[223]	_	_
30-26	4491-4498	objects	abstract[223]	new[223]	_	_
30-27	4499-4513	simultaneously	_	_	_	_
30-28	4514-4515	.	_	_	_	_

#Text=For example , CNNs have shown great power in distinguishing as many as 1000 object categories .
31-1	4516-4519	For	_	_	_	_
31-2	4520-4527	example	_	_	_	_
31-3	4528-4529	,	_	_	_	_
31-4	4530-4534	CNNs	object	giv	_	_
31-5	4535-4539	have	_	_	_	_
31-6	4540-4545	shown	_	_	_	_
31-7	4546-4551	great	abstract[225]	giv[225]	_	_
31-8	4552-4557	power	abstract[225]	giv[225]	_	_
31-9	4558-4560	in	_	_	_	_
31-10	4561-4575	distinguishing	_	_	_	_
31-11	4576-4578	as	_	_	_	_
31-12	4579-4583	many	_	_	_	_
31-13	4584-4586	as	_	_	_	_
31-14	4587-4591	1000	_	_	_	_
31-15	4592-4598	object	abstract	new	coref	32-40
31-16	4599-4609	categories	_	_	_	_
31-17	4610-4611	.	_	_	_	_

#Text=Other advantages of deep learning methods include the following : ( 1 ) deep learning models fully utilize image data , as every pixel can be utilized in prediction model ; ( 2 ) CNN models are insensitive to object position on the image , an inherent property of convolution operation ; and ( 3 ) as discussed in the next section , by using extensive data augmentations in the model training process , CNN models are robust to different staining conditions in pathology image analysis .
32-1	4612-4617	Other	abstract[227]	new[227]	_	_
32-2	4618-4628	advantages	abstract[227]	new[227]	_	_
32-3	4629-4631	of	abstract[227]	new[227]	_	_
32-4	4632-4636	deep	abstract[227]|abstract[229]	new[227]|giv[229]	_	_
32-5	4637-4645	learning	abstract[227]|abstract|abstract[229]	new[227]|giv|giv[229]	_	_
32-6	4646-4653	methods	abstract[227]|abstract[229]	new[227]|giv[229]	_	_
32-7	4654-4661	include	_	_	_	_
32-8	4662-4665	the	abstract[230]	giv[230]	coref	32-35[237_230]
32-9	4666-4675	following	abstract[230]	giv[230]	_	_
32-10	4676-4677	:	abstract[230]	giv[230]	_	_
32-11	4678-4679	(	abstract[230]	giv[230]	_	_
32-12	4680-4681	1	abstract[230]	giv[230]	_	_
32-13	4682-4683	)	abstract[230]	giv[230]	_	_
32-14	4684-4688	deep	abstract[230]	giv[230]	_	_
32-15	4689-4697	learning	abstract[230]	giv[230]	_	_
32-16	4698-4704	models	abstract[230]	giv[230]	_	_
32-17	4705-4710	fully	_	_	_	_
32-18	4711-4718	utilize	_	_	_	_
32-19	4719-4724	image	object|abstract[232]	giv|giv[232]	coref|coref	32-43[240_0]|32-67[0_232]
32-20	4725-4729	data	abstract[232]	giv[232]	_	_
32-21	4730-4731	,	_	_	_	_
32-22	4732-4734	as	_	_	_	_
32-23	4735-4740	every	object[233]	new[233]	_	_
32-24	4741-4746	pixel	object[233]	new[233]	_	_
32-25	4747-4750	can	_	_	_	_
32-26	4751-4753	be	_	_	_	_
32-27	4754-4762	utilized	_	_	_	_
32-28	4763-4765	in	_	_	_	_
32-29	4766-4776	prediction	event|abstract[235]	giv|giv[235]	coref	32-71[0_235]
32-30	4777-4782	model	abstract[235]	giv[235]	_	_
32-31	4783-4784	;	_	_	_	_
32-32	4785-4786	(	_	_	_	_
32-33	4787-4788	2	_	_	_	_
32-34	4789-4790	)	_	_	_	_
32-35	4791-4794	CNN	organization|abstract[237]	giv|giv[237]	coref|coref	32-75|32-75[251_237]
32-36	4795-4801	models	abstract[237]	giv[237]	_	_
32-37	4802-4805	are	_	_	_	_
32-38	4806-4817	insensitive	_	_	_	_
32-39	4818-4820	to	_	_	_	_
32-40	4821-4827	object	object|object[239]	giv|new[239]	_	_
32-41	4828-4836	position	object[239]	new[239]	_	_
32-42	4837-4839	on	object[239]	new[239]	_	_
32-43	4840-4843	the	object[239]|object[240]	new[239]|giv[240]	coref	32-85[0_240]
32-44	4844-4849	image	object[239]|object[240]	new[239]|giv[240]	_	_
32-45	4850-4851	,	_	_	_	_
32-46	4852-4854	an	abstract[241]	new[241]	_	_
32-47	4855-4863	inherent	abstract[241]	new[241]	_	_
32-48	4864-4872	property	abstract[241]	new[241]	_	_
32-49	4873-4875	of	abstract[241]	new[241]	_	_
32-50	4876-4887	convolution	abstract[241]|abstract|event[243]	new[241]|giv|giv[243]	_	_
32-51	4888-4897	operation	abstract[241]|event[243]	new[241]|giv[243]	_	_
32-52	4898-4899	;	abstract[241]	new[241]	_	_
32-53	4900-4903	and	abstract[241]	new[241]	_	_
32-54	4904-4905	(	abstract[241]	new[241]	_	_
32-55	4906-4907	3	abstract[241]	new[241]	_	_
32-56	4908-4909	)	abstract[241]	new[241]	_	_
32-57	4910-4912	as	_	_	_	_
32-58	4913-4922	discussed	_	_	_	_
32-59	4923-4925	in	_	_	_	_
32-60	4926-4929	the	abstract[244]	new[244]	_	_
32-61	4930-4934	next	abstract[244]	new[244]	_	_
32-62	4935-4942	section	abstract[244]	new[244]	_	_
32-63	4943-4944	,	_	_	_	_
32-64	4945-4947	by	_	_	_	_
32-65	4948-4953	using	_	_	_	_
32-66	4954-4963	extensive	abstract[246]	new[246]	_	_
32-67	4964-4968	data	abstract|abstract[246]	giv|new[246]	_	_
32-68	4969-4982	augmentations	abstract[246]	new[246]	_	_
32-69	4983-4985	in	abstract[246]	new[246]	_	_
32-70	4986-4989	the	abstract[246]|abstract[249]	new[246]|new[249]	_	_
32-71	4990-4995	model	abstract[246]|abstract|abstract[249]	new[246]|giv|new[249]	_	_
32-72	4996-5004	training	abstract[246]|abstract|abstract[249]	new[246]|new|new[249]	_	_
32-73	5005-5012	process	abstract[246]|abstract[249]	new[246]|new[249]	_	_
32-74	5013-5014	,	_	_	_	_
32-75	5015-5018	CNN	organization|abstract[251]	giv|giv[251]	_	_
32-76	5019-5025	models	abstract[251]	giv[251]	_	_
32-77	5026-5029	are	_	_	_	_
32-78	5030-5036	robust	_	_	_	_
32-79	5037-5039	to	_	_	_	_
32-80	5040-5049	different	abstract[252]	new[252]	_	_
32-81	5050-5058	staining	abstract[252]	new[252]	_	_
32-82	5059-5069	conditions	abstract[252]	new[252]	_	_
32-83	5070-5072	in	abstract[252]	new[252]	_	_
32-84	5073-5082	pathology	abstract[252]|abstract|abstract[255]	new[252]|giv|giv[255]	_	_
32-85	5083-5088	image	abstract[252]|object|abstract[255]	new[252]|giv|giv[255]	_	_
32-86	5089-5097	analysis	abstract[252]|abstract[255]	new[252]|giv[255]	_	_
32-87	5098-5099	.	_	_	_	_
