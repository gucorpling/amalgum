<rst>
<header>
	<relations>
			<rel name="elaboration" type="rst"/>
			<rel name="means" type="rst"/>
			<rel name="attribution" type="rst"/>
			<rel name="circumstance" type="rst"/>
			<rel name="concession" type="rst"/>
			<rel name="purpose" type="rst"/>
			<rel name="cause" type="rst"/>
			<rel name="restatement" type="rst"/>
			<rel name="evidence" type="rst"/>
			<rel name="preparation" type="rst"/>
			<rel name="sequence" type="multinuc"/>
			<rel name="same_unit" type="multinuc"/>
			<rel name="joint" type="multinuc"/>
		</relations>
</header>
<body>
<segment id="1" parent="1001" relname="preparation">2. Gradient Descent Learning of Stochastic Neural Networks</segment>
<segment id="2" parent="1002" relname="preparation">2.1 .</segment>
<segment id="3" parent="1003" relname="preparation">Stochastic Neural Networks</segment>
<segment id="4" parent="5" relname="cause">Since the natural gradient is derived from stochastic neural network models ,</segment>
<segment id="5" parent="1004" relname="span">let us start from the brief description of the two popular stochastic models .</segment>
<segment id="6" parent="1006" relname="span">Although typical neural networks , including deep networks , are defined as deterministic input-output mapping functions of input , output , and parameter ,</segment>
<segment id="7" parent="1007" relname="span">the observed data</segment>
<segment id="8" parent="1008" relname="joint">for training the networks always have inevitable noises</segment>
<segment id="9" parent="1008" relname="joint">and thus the input-output relation can be described in the stochastic manner .</segment>
<segment id="10" parent="1010" relname="span">In other words , the observed output can be regarded as a random vector</segment>
<segment id="11" parent="1011" relname="span">that is dependent on the deterministic function and some additional stochastic process</segment>
<segment id="12" parent="11" relname="elaboration">that is described by the conditional probability .</segment>
<segment id="13" parent="1013" relname="span">Then the goal of learning is to find an optimal value of parameter</segment>
<segment id="14" parent="1014" relname="span">that minimizes the loss function</segment>
<segment id="15" parent="14" relname="elaboration">defined as negative log likelihood of given input-output sample .</segment>
<segment id="16" parent="1017" relname="span">The loss function can then be written as</segment>
<segment id="17" parent="16" relname="elaboration">( 1 )</segment>
<segment id="18" parent="1018" relname="span">and the optimal parameter is described as</segment>
<segment id="19" parent="18" relname="restatement">( 2 )</segment>
<segment id="20" parent="1022" relname="span">Note that the last term in equation</segment>
<segment id="21" parent="20" relname="elaboration">( 1 )</segment>
<segment id="22" parent="1021" relname="same_unit">is independent of parameter</segment>
<segment id="23" parent="1020" relname="joint">and can be ignored in the objective function for optimization .</segment>
<segment id="24" parent="1024" relname="preparation">Based on the general definition , the conventional neural networks can be regarded as a special case with a specific conditional probability distribution , .</segment>
<segment id="25" parent="1025" relname="span">For example , consider that the output is observed with additive noise to the deterministic neural networks function</segment>
<segment id="26" parent="1027" relname="span">such as</segment>
<segment id="27" parent="26" relname="evidence">( 3 )</segment>
<segment id="28" parent="1027" relname="elaboration">where is a random noise vector .</segment>
<segment id="29" parent="1029" relname="attribution">When we assume</segment>
<segment id="30" parent="1029" relname="joint">that the random vector is subject to the standard Gaussian distribution ,</segment>
<segment id="31" parent="1031" relname="joint">its probability density is defined as the normal distribution function ,</segment>
<segment id="32" parent="1032" relname="span">and its loss function becomes the well-known squared error function ,</segment>
<segment id="33" parent="1033" relname="span">which can be written as</segment>
<segment id="34" parent="33" relname="evidence">( 4 )</segment>
<segment id="35" parent="1036" relname="span">Therefore , the stochastic neural network model with additive Gaussian noise is equivalent to the typical neural network model</segment>
<segment id="36" parent="35" relname="elaboration">trained with squared error function ,</segment>
<segment id="37" parent="1036" relname="elaboration">which is widely used for regression task .</segment>
<segment id="38" parent="1038" relname="span">On the other hand , in case that the output is a binary vector ,</segment>
<segment id="39" parent="1039" relname="span">the corresponding probability distribution can be defined</segment>
<segment id="40" parent="1040" relname="span">by using a logistic model , such as ( 5 )</segment>
<segment id="41" parent="40" relname="elaboration">where and are the i -th component of L -dimensional vector and , respectively .</segment>
<segment id="42" parent="1042" relname="span">Since the typical problems with binary target output vector is pattern classification ,</segment>
<segment id="43" parent="42" relname="elaboration">the logistic model is appropriate for L -class classification tasks .</segment>
<segment id="44" parent="1043" relname="span">The corresponding loss function of the logistic model is obtained</segment>
<segment id="45" parent="1045" relname="span">by taking negative log likelihood of Equation</segment>
<segment id="46" parent="45" relname="evidence">( 5 ) ,</segment>
<segment id="47" parent="1047" relname="span">which can be written as ,</segment>
<segment id="48" parent="47" relname="evidence">( 6 )</segment>
<segment id="49" parent="1050" relname="attribution">Noting that this is the well-known cross-entropy error ,</segment>
<segment id="50" parent="1051" relname="attribution">we can say</segment>
<segment id="51" parent="1051" relname="span">that the logistic regression model is equivalent to the typical neural networks with cross-entropy error ,</segment>
<segment id="52" parent="51" relname="elaboration">which is widely used for classification task .</segment>
<segment id="53" parent="1053" relname="span">Likewise ,</segment>
<segment id="54" parent="1054" relname="span">by defining a proper stochastic model , we can derive various types of neural network models ,</segment>
<segment id="55" parent="1055" relname="joint">which can explain the given task more adequately</segment>
<segment id="56" parent="1056" relname="span">and get a new insight</segment>
<segment id="57" parent="56" relname="purpose">to solve many unresolved problems in the field of neural network learning .</segment>
<segment id="58" parent="1057" relname="span">Natural gradient is also derived from a new metric for the space of probability density function of stochastic neural networks .</segment>
<segment id="59" parent="1059" relname="span">In this paper , we present explicit algorithms of adaptive natural gradient learning method for two representative stochastic neural network models :</segment>
<segment id="60" parent="59" relname="elaboration">The additive Gaussian noise model and the logistic regression model .</segment>
<segment id="61" parent="1060" relname="joint">2.2 .</segment>
<segment id="62" parent="1062" relname="preparation">Gradient Descent Learning</segment>
<segment id="63" parent="64" relname="circumstance">Once a specific model of stochastic neural networks and its corresponding loss function are determined ,</segment>
<segment id="64" parent="1063" relname="span">the weight parameters are optimized by gradient descent method .</segment>
<segment id="65" parent="1064" relname="joint">The well-known error-backpropagation algorithm is the standard type of gradient descent learning method .</segment>
<segment id="66" parent="1066" relname="span">There have been numerous variations of the standard gradient descent method ,</segment>
<segment id="67" parent="66" relname="elaboration">including second-order methods , momentum method , and adaptive learning rate methods .</segment>
<segment id="68" parent="1068" relname="preparation">Since the natural gradient learning method is also based on the gradient descent method ,</segment>
<segment id="69" parent="1069" relname="span">we describe the basic formula of gradient descent learning and its online version</segment>
<segment id="70" parent="69" relname="elaboration">that is called stochastic gradient descent method .</segment>
<segment id="71" parent="1072" relname="circumstance">When a set of training data is given ,</segment>
<segment id="72" parent="1072" relname="span">a neural network is trained</segment>
<segment id="73" parent="1073" relname="span">in order to find an input-output mapping</segment>
<segment id="74" parent="73" relname="elaboration">that is specified with weight parameter vector .</segment>
<segment id="75" parent="1076" relname="span">The error of neural network for the whole data set can then be defined</segment>
<segment id="76" parent="1077" relname="same_unit">by using a loss function such as ( 7 )</segment>
<segment id="77" parent="1077" relname="same_unit">and the goal of learning is to get the optimal minimizing .</segment>
<segment id="78" parent="1078" relname="span">To achieve the goal , the weight parameter is updated</segment>
<segment id="79" parent="1079" relname="span">starting from the current position ,</segment>
<segment id="80" parent="1080" relname="span">according to the opposite direction of the gradient of ,</segment>
<segment id="81" parent="1081" relname="span">which can be written as</segment>
<segment id="82" parent="81" relname="evidence">( 8 )</segment>
<segment id="83" parent="1083" relname="span">This update rule is called batch learning mode ,</segment>
<segment id="84" parent="83" relname="elaboration">meaning that an update is done for the whole batch set .</segment>
<segment id="85" parent="86" relname="concession">Theoretically , the batch mode learning gives the steepest descent direction of at the current position of ,</segment>
<segment id="86" parent="1085" relname="span">but it has two practical drawbacks .</segment>
<segment id="87" parent="1087" relname="span">First , it is too stable</segment>
<segment id="88" parent="87" relname="purpose">to be easily trapped in undesirable local minima .</segment>
<segment id="89" parent="1089" relname="preparation">In addition ,</segment>
<segment id="90" parent="1090" relname="circumstance">when the number of data is large ,</segment>
<segment id="91" parent="1090" relname="joint">it needs large amounts of computation for just a single update ,</segment>
<segment id="92" parent="1091" relname="joint">making the learning process slow .</segment>
<segment id="93" parent="1092" relname="span">To overcome this practical inefficiency , online stochastic gradient decent learning is proposed ,</segment>
<segment id="94" parent="1093" relname="span">in which parameters are updated for each data sample</segment>
<segment id="95" parent="1094" relname="span">by using gradient of loss function</segment>
<segment id="96" parent="1095" relname="span">defined with a single data pair , such as</segment>
<segment id="97" parent="96" relname="evidence">( 9 )</segment>
<group id="1000" type="span" />
<group id="1001" type="span" parent="1000" relname="span"/>
<group id="1002" type="span" parent="1001" relname="span"/>
<group id="1003" type="span" parent="1002" relname="span"/>
<group id="1004" type="span" parent="1003" relname="span"/>
<group id="1005" type="multinuc" parent="1004" relname="concession"/>
<group id="1006" type="span" parent="1005" relname="joint"/>
<group id="1007" type="span" parent="6" relname="elaboration"/>
<group id="1008" type="multinuc" parent="7" relname="purpose"/>
<group id="1009" type="multinuc" parent="1005" relname="joint"/>
<group id="1010" type="span" parent="1009" relname="sequence"/>
<group id="1011" type="span" parent="10" relname="elaboration"/>
<group id="1012" type="multinuc" parent="1009" relname="sequence"/>
<group id="1013" type="span" parent="1012" relname="sequence"/>
<group id="1014" type="span" parent="13" relname="elaboration"/>
<group id="1015" type="multinuc" parent="1012" relname="sequence"/>
<group id="1016" type="multinuc" parent="1015" relname="joint"/>
<group id="1017" type="span" parent="1016" relname="joint"/>
<group id="1018" type="span" parent="1016" relname="joint"/>
<group id="1019" type="multinuc" parent="1015" relname="joint"/>
<group id="1020" type="multinuc" parent="1019" relname="sequence"/>
<group id="1021" type="multinuc" parent="1020" relname="joint"/>
<group id="1022" type="span" parent="1021" relname="same_unit"/>
<group id="1023" type="span" parent="1019" relname="sequence"/>
<group id="1024" type="multinuc" parent="1023" relname="span"/>
<group id="1025" type="span" parent="1024" relname="joint"/>
<group id="1026" type="span" parent="25" relname="elaboration"/>
<group id="1027" type="span" parent="1026" relname="span"/>
<group id="1028" type="span" parent="1024" relname="joint"/>
<group id="1029" type="multinuc" parent="1028" relname="span"/>
<group id="1030" type="multinuc" parent="1029" relname="joint"/>
<group id="1031" type="multinuc" parent="1030" relname="joint"/>
<group id="1032" type="span" parent="1031" relname="joint"/>
<group id="1033" type="span" parent="32" relname="elaboration"/>
<group id="1034" type="multinuc" parent="1030" relname="joint"/>
<group id="1035" type="span" parent="1034" relname="joint"/>
<group id="1036" type="span" parent="1035" relname="span"/>
<group id="1037" type="multinuc" parent="1034" relname="joint"/>
<group id="1038" type="span" parent="1037" relname="sequence"/>
<group id="1039" type="span" parent="38" relname="elaboration"/>
<group id="1040" type="span" parent="39" relname="means"/>
<group id="1041" type="span" parent="1037" relname="sequence"/>
<group id="1042" type="span" parent="1043" relname="preparation"/>
<group id="1043" type="span" parent="1041" relname="span"/>
<group id="1044" type="multinuc" parent="44" relname="means"/>
<group id="1045" type="span" parent="1044" relname="joint"/>
<group id="1046" type="multinuc" parent="1044" relname="joint"/>
<group id="1047" type="span" parent="1046" relname="joint"/>
<group id="1048" type="multinuc" parent="1046" relname="joint"/>
<group id="1049" type="span" parent="1048" relname="joint"/>
<group id="1050" type="span" parent="1049" relname="span"/>
<group id="1051" type="span" parent="1050" relname="span"/>
<group id="1052" type="multinuc" parent="1048" relname="joint"/>
<group id="1053" type="span" parent="1052" relname="joint"/>
<group id="1054" type="span" parent="53" relname="means"/>
<group id="1055" type="multinuc" parent="54" relname="elaboration"/>
<group id="1056" type="span" parent="1055" relname="joint"/>
<group id="1057" type="span" parent="1052" relname="joint"/>
<group id="1058" type="multinuc" parent="58" relname="elaboration"/>
<group id="1059" type="span" parent="1058" relname="joint"/>
<group id="1060" type="multinuc" parent="1058" relname="joint"/>
<group id="1061" type="span" parent="1060" relname="joint"/>
<group id="1062" type="multinuc" parent="1061" relname="span"/>
<group id="1063" type="span" parent="1062" relname="joint"/>
<group id="1064" type="multinuc" parent="1062" relname="joint"/>
<group id="1065" type="multinuc" parent="1064" relname="joint"/>
<group id="1066" type="span" parent="1065" relname="joint"/>
<group id="1067" type="span" parent="1065" relname="joint"/>
<group id="1068" type="multinuc" parent="1067" relname="span"/>
<group id="1069" type="span" parent="1068" relname="joint"/>
<group id="1070" type="multinuc" parent="1068" relname="joint"/>
<group id="1071" type="span" parent="1070" relname="joint"/>
<group id="1072" type="span" parent="1071" relname="span"/>
<group id="1073" type="span" parent="72" relname="purpose"/>
<group id="1074" type="multinuc" parent="1070" relname="joint"/>
<group id="1075" type="span" parent="1074" relname="joint"/>
<group id="1076" type="span" parent="1075" relname="span"/>
<group id="1077" type="multinuc" parent="75" relname="means"/>
<group id="1078" type="span" parent="1076" relname="purpose"/>
<group id="1079" type="span" parent="78" relname="elaboration"/>
<group id="1080" type="span" parent="79" relname="attribution"/>
<group id="1081" type="span" parent="80" relname="elaboration"/>
<group id="1082" type="multinuc" parent="1074" relname="joint"/>
<group id="1083" type="span" parent="1082" relname="joint"/>
<group id="1084" type="span" parent="1082" relname="joint"/>
<group id="1085" type="span" parent="1086" relname="preparation"/>
<group id="1086" type="multinuc" parent="1084" relname="span"/>
<group id="1087" type="span" parent="1086" relname="joint"/>
<group id="1088" type="span" parent="1086" relname="joint"/>
<group id="1089" type="span" parent="1088" relname="span"/>
<group id="1090" type="multinuc" parent="1089" relname="span"/>
<group id="1091" type="multinuc" parent="1090" relname="joint"/>
<group id="1092" type="span" parent="1091" relname="joint"/>
<group id="1093" type="span" parent="93" relname="elaboration"/>
<group id="1094" type="span" parent="94" relname="means"/>
<group id="1095" type="span" parent="95" relname="elaboration"/>
	</body>
</rst>
