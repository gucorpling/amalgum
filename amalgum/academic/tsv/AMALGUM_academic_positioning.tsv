#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=1. Introduction
1-1	0-2	1.	_	_	_	_
1-2	3-15	Introduction	abstract	new	_	_

#Text=Nowadays , position information has become key information in people ’s daily lives .
2-1	16-24	Nowadays	_	_	_	_
2-2	25-26	,	_	_	_	_
2-3	27-35	position	abstract|abstract[3]	new|new[3]	ana|coref	3-1[0_3]|4-4[11_0]
2-4	36-47	information	abstract[3]	new[3]	_	_
2-5	48-51	has	_	_	_	_
2-6	52-58	become	_	_	_	_
2-7	59-62	key	_	_	_	_
2-8	63-74	information	_	_	_	_
2-9	75-77	in	_	_	_	_
2-10	78-84	people	person[4]|abstract[5]	new[4]|new[5]	coref	6-7[0_4]
2-11	85-87	’s	person[4]|abstract[5]	new[4]|new[5]	_	_
2-12	88-93	daily	abstract[5]	new[5]	_	_
2-13	94-99	lives	abstract[5]	new[5]	_	_
2-14	100-101	.	_	_	_	_

#Text=This has inspired position-based services , which aim to provide personalized services to mobile users whose positions are changing .
3-1	102-106	This	abstract	giv	coref	18-10[94_0]
3-2	107-110	has	_	_	_	_
3-3	111-119	inspired	_	_	_	_
3-4	120-134	position-based	abstract[7]	new[7]	coref	3-11[8_7]
3-5	135-143	services	abstract[7]	new[7]	_	_
3-6	144-145	,	abstract[7]	new[7]	_	_
3-7	146-151	which	abstract[7]	new[7]	_	_
3-8	152-155	aim	abstract[7]	new[7]	_	_
3-9	156-158	to	abstract[7]	new[7]	_	_
3-10	159-166	provide	abstract[7]	new[7]	_	_
3-11	167-179	personalized	abstract[7]|abstract[8]	new[7]|giv[8]	coref	4-11[13_8]
3-12	180-188	services	abstract[7]|abstract[8]	new[7]|giv[8]	_	_
3-13	189-191	to	_	_	_	_
3-14	192-198	mobile	person[9]	new[9]	_	_
3-15	199-204	users	person[9]	new[9]	_	_
3-16	205-210	whose	person[9]|abstract[10]	new[9]|new[10]	_	_
3-17	211-220	positions	person[9]|abstract[10]	new[9]|new[10]	_	_
3-18	221-224	are	person[9]	new[9]	_	_
3-19	225-233	changing	person[9]	new[9]	_	_
3-20	234-235	.	_	_	_	_

#Text=Therefore , obtaining a precise position is a prerequisite for these services .
4-1	236-245	Therefore	_	_	_	_
4-2	246-247	,	_	_	_	_
4-3	248-257	obtaining	_	_	_	_
4-4	258-259	a	abstract[11]	giv[11]	coref	19-23[0_11]
4-5	260-267	precise	abstract[11]	giv[11]	_	_
4-6	268-276	position	abstract[11]	giv[11]	_	_
4-7	277-279	is	_	_	_	_
4-8	280-281	a	object[12]	new[12]	_	_
4-9	282-294	prerequisite	object[12]	new[12]	_	_
4-10	295-298	for	object[12]	new[12]	_	_
4-11	299-304	these	object[12]|abstract[13]	new[12]|giv[13]	_	_
4-12	305-313	services	object[12]|abstract[13]	new[12]|giv[13]	_	_
4-13	314-315	.	_	_	_	_

#Text=The most commonly used positioning method in the outdoor environment is the Global Navigation Satellite System ( GNSS ) .
5-1	316-319	The	abstract[15]	new[15]	coref	5-12[19_15]
5-2	320-324	most	abstract[15]	new[15]	_	_
5-3	325-333	commonly	abstract[15]	new[15]	_	_
5-4	334-338	used	abstract[15]	new[15]	_	_
5-5	339-350	positioning	abstract|abstract[15]	new|new[15]	coref	7-3[26_0]
5-6	351-357	method	abstract[15]	new[15]	_	_
5-7	358-360	in	abstract[15]	new[15]	_	_
5-8	361-364	the	abstract[15]|place[16]	new[15]|new[16]	_	_
5-9	365-372	outdoor	abstract[15]|place[16]	new[15]|new[16]	_	_
5-10	373-384	environment	abstract[15]|place[16]	new[15]|new[16]	_	_
5-11	385-387	is	_	_	_	_
5-12	388-391	the	abstract[19]	giv[19]	appos	5-18[0_19]
5-13	392-398	Global	abstract[19]	giv[19]	_	_
5-14	399-409	Navigation	abstract|abstract[19]	new|giv[19]	_	_
5-15	410-419	Satellite	organization|abstract[19]	new|giv[19]	_	_
5-16	420-426	System	abstract[19]	giv[19]	_	_
5-17	427-428	(	_	_	_	_
5-18	429-433	GNSS	abstract	giv	coref	8-2
5-19	434-435	)	_	_	_	_
5-20	436-437	.	_	_	_	_

#Text=In most cases , however , people spend more than 70 % of their time indoors .
6-1	438-440	In	_	_	_	_
6-2	441-445	most	event[21]	new[21]	_	_
6-3	446-451	cases	event[21]	new[21]	_	_
6-4	452-453	,	_	_	_	_
6-5	454-461	however	_	_	_	_
6-6	462-463	,	_	_	_	_
6-7	464-470	people	person	giv	ana	6-14
6-8	471-476	spend	_	_	_	_
6-9	477-481	more	abstract[23]	new[23]	_	_
6-10	482-486	than	abstract[23]	new[23]	_	_
6-11	487-489	70	abstract[23]	new[23]	_	_
6-12	490-491	%	abstract[23]	new[23]	_	_
6-13	492-494	of	abstract[23]	new[23]	_	_
6-14	495-500	their	abstract[23]|person|time[25]	new[23]|giv|new[25]	coref|coref	9-4|26-6[0_25]
6-15	501-505	time	abstract[23]|time[25]	new[23]|new[25]	_	_
6-16	506-513	indoors	_	_	_	_
6-17	514-515	.	_	_	_	_

#Text=Therefore , accurate indoor positioning has important practical significance .
7-1	516-525	Therefore	_	_	_	_
7-2	526-527	,	_	_	_	_
7-3	528-536	accurate	abstract[26]	giv[26]	coref	13-14[61_26]
7-4	537-543	indoor	abstract[26]	giv[26]	_	_
7-5	544-555	positioning	abstract[26]	giv[26]	_	_
7-6	556-559	has	_	_	_	_
7-7	560-569	important	abstract[27]	new[27]	_	_
7-8	570-579	practical	abstract[27]	new[27]	_	_
7-9	580-592	significance	abstract[27]	new[27]	_	_
7-10	593-594	.	_	_	_	_

#Text=Although GNSS is a good choice for outdoor positioning , due to signal occlusion and attenuations , it is often useless in indoor environments .
8-1	595-603	Although	_	_	_	_
8-2	604-608	GNSS	abstract	giv	coref	8-4[29_0]
8-3	609-611	is	_	_	_	_
8-4	612-613	a	abstract[29]	giv[29]	coref	24-5[141_29]
8-5	614-618	good	abstract[29]	giv[29]	_	_
8-6	619-625	choice	abstract[29]	giv[29]	_	_
8-7	626-629	for	abstract[29]	giv[29]	_	_
8-8	630-637	outdoor	abstract[29]|abstract[30]	giv[29]|new[30]	ana	8-18[0_30]
8-9	638-649	positioning	abstract[29]|abstract[30]	giv[29]|new[30]	_	_
8-10	650-651	,	_	_	_	_
8-11	652-655	due	_	_	_	_
8-12	656-658	to	_	_	_	_
8-13	659-665	signal	abstract|abstract[32]	new|new[32]	_	_
8-14	666-675	occlusion	abstract[32]	new[32]	_	_
8-15	676-679	and	_	_	_	_
8-16	680-692	attenuations	abstract	new	_	_
8-17	693-694	,	_	_	_	_
8-18	695-697	it	abstract	giv	_	_
8-19	698-700	is	_	_	_	_
8-20	701-706	often	_	_	_	_
8-21	707-714	useless	_	_	_	_
8-22	715-717	in	_	_	_	_
8-23	718-724	indoor	place[35]	new[35]	_	_
8-24	725-737	environments	place[35]	new[35]	_	_
8-25	738-739	.	_	_	_	_

#Text=Thus , positioning people accurately in indoor scenes remains a challenge and it has stimulated a large number of indoor-positioning methods in recent years .
9-1	740-744	Thus	_	_	_	_
9-2	745-746	,	_	_	_	_
9-3	747-758	positioning	_	_	_	_
9-4	759-765	people	person	giv	_	_
9-5	766-776	accurately	_	_	_	_
9-6	777-779	in	_	_	_	_
9-7	780-786	indoor	abstract[37]	new[37]	_	_
9-8	787-793	scenes	abstract[37]	new[37]	_	_
9-9	794-801	remains	_	_	_	_
9-10	802-803	a	abstract[38]	new[38]	ana	9-13[0_38]
9-11	804-813	challenge	abstract[38]	new[38]	_	_
9-12	814-817	and	_	_	_	_
9-13	818-820	it	abstract	giv	_	_
9-14	821-824	has	_	_	_	_
9-15	825-835	stimulated	_	_	_	_
9-16	836-837	a	abstract[40]	new[40]	_	_
9-17	838-843	large	abstract[40]	new[40]	_	_
9-18	844-850	number	abstract[40]	new[40]	_	_
9-19	851-853	of	abstract[40]	new[40]	_	_
9-20	854-872	indoor-positioning	abstract[40]|abstract[41]	new[40]|new[41]	coref	10-2[43_41]
9-21	873-880	methods	abstract[40]|abstract[41]	new[40]|new[41]	_	_
9-22	881-883	in	_	_	_	_
9-23	884-890	recent	time[42]	new[42]	coref	15-11[71_42]
9-24	891-896	years	time[42]	new[42]	_	_
9-25	897-898	.	_	_	_	_

#Text=Among these methods , fingerprint-based algorithms are widely used .
10-1	899-904	Among	_	_	_	_
10-2	905-910	these	abstract[43]	giv[43]	coref	12-2[53_43]
10-3	911-918	methods	abstract[43]	giv[43]	_	_
10-4	919-920	,	_	_	_	_
10-5	921-938	fingerprint-based	abstract[44]	new[44]	ana	11-1[0_44]
10-6	939-949	algorithms	abstract[44]	new[44]	_	_
10-7	950-953	are	_	_	_	_
10-8	954-960	widely	_	_	_	_
10-9	961-965	used	_	_	_	_
10-10	966-967	.	_	_	_	_

#Text=Their fingerprint databases include Wi-Fi , Bluetooth , and magnetic field strengths .
11-1	968-973	Their	abstract|abstract[47]	giv|new[47]	coref	27-4[176_0]
11-2	974-985	fingerprint	object|abstract[47]	new|new[47]	coref	12-12
11-3	986-995	databases	abstract[47]	new[47]	_	_
11-4	996-1003	include	_	_	_	_
11-5	1004-1009	Wi-Fi	abstract|abstract[49]	new|new[49]	ana	13-7[0_49]
11-6	1010-1011	,	abstract[49]	new[49]	_	_
11-7	1012-1021	Bluetooth	abstract[49]|person	new[49]|new	_	_
11-8	1022-1023	,	abstract[49]	new[49]	_	_
11-9	1024-1027	and	abstract[49]	new[49]	_	_
11-10	1028-1036	magnetic	abstract[49]|abstract[51]|abstract[52]	new[49]|new[51]|new[52]	_	_
11-11	1037-1042	field	abstract[49]|abstract[51]|abstract[52]	new[49]|new[51]|new[52]	_	_
11-12	1043-1052	strengths	abstract[49]|abstract[52]	new[49]|new[52]	_	_
11-13	1053-1054	.	_	_	_	_

#Text=Although these methods are easy to implement , construction of a fingerprint database is usually labor-intensive and time-consuming .
12-1	1055-1063	Although	_	_	_	_
12-2	1064-1069	these	abstract[53]	giv[53]	coref	15-1[70_53]
12-3	1070-1077	methods	abstract[53]	giv[53]	_	_
12-4	1078-1081	are	_	_	_	_
12-5	1082-1086	easy	_	_	_	_
12-6	1087-1089	to	_	_	_	_
12-7	1090-1099	implement	_	_	_	_
12-8	1100-1101	,	_	_	_	_
12-9	1102-1114	construction	event[54]	new[54]	ana	13-3[0_54]
12-10	1115-1117	of	event[54]	new[54]	_	_
12-11	1118-1119	a	event[54]|abstract[56]	new[54]|new[56]	coref	18-4[93_56]
12-12	1120-1131	fingerprint	event[54]|object|abstract[56]	new[54]|giv|new[56]	_	_
12-13	1132-1140	database	event[54]|abstract[56]	new[54]|new[56]	_	_
12-14	1141-1143	is	_	_	_	_
12-15	1144-1151	usually	_	_	_	_
12-16	1152-1167	labor-intensive	_	_	_	_
12-17	1168-1171	and	_	_	_	_
12-18	1172-1186	time-consuming	_	_	_	_
12-19	1187-1188	.	_	_	_	_

#Text=Moreover , it is difficult for their results to meet the needs of high-accuracy indoor positioning .
13-1	1189-1197	Moreover	_	_	_	_
13-2	1198-1199	,	_	_	_	_
13-3	1200-1202	it	event	giv	_	_
13-4	1203-1205	is	_	_	_	_
13-5	1206-1215	difficult	_	_	_	_
13-6	1216-1219	for	_	_	_	_
13-7	1220-1225	their	abstract|abstract[59]	giv|new[59]	_	_
13-8	1226-1233	results	abstract[59]	new[59]	_	_
13-9	1234-1236	to	_	_	_	_
13-10	1237-1241	meet	_	_	_	_
13-11	1242-1245	the	abstract[60]	new[60]	_	_
13-12	1246-1251	needs	abstract[60]	new[60]	_	_
13-13	1252-1254	of	abstract[60]	new[60]	_	_
13-14	1255-1268	high-accuracy	abstract[60]|abstract[61]	new[60]|giv[61]	ana	14-18[0_61]
13-15	1269-1275	indoor	abstract[60]|abstract[61]	new[60]|giv[61]	_	_
13-16	1276-1287	positioning	abstract[60]|abstract[61]	new[60]|giv[61]	_	_
13-17	1288-1289	.	_	_	_	_

#Text=Given that humans use their eyes to see where they are , mobile platforms can also do this with cameras .
14-1	1290-1295	Given	_	_	_	_
14-2	1296-1300	that	_	_	_	_
14-3	1301-1307	humans	person	new	ana	14-5
14-4	1308-1311	use	_	_	_	_
14-5	1312-1317	their	person|object[64]	giv|new[64]	ana	14-10
14-6	1318-1322	eyes	object[64]	new[64]	_	_
14-7	1323-1325	to	_	_	_	_
14-8	1326-1329	see	_	_	_	_
14-9	1330-1335	where	_	_	_	_
14-10	1336-1340	they	person	giv	_	_
14-11	1341-1344	are	_	_	_	_
14-12	1345-1346	,	_	_	_	_
14-13	1347-1353	mobile	organization[66]	new[66]	_	_
14-14	1354-1363	platforms	organization[66]	new[66]	_	_
14-15	1364-1367	can	_	_	_	_
14-16	1368-1372	also	_	_	_	_
14-17	1373-1375	do	_	_	_	_
14-18	1376-1380	this	abstract	giv	coref	15-5
14-19	1381-1385	with	_	_	_	_
14-20	1386-1393	cameras	object	new	_	_
14-21	1394-1395	.	_	_	_	_

#Text=A number of visual positioning methods have been proposed in recent years .
15-1	1396-1397	A	abstract[70]	giv[70]	coref	16-1[73_70]
15-2	1398-1404	number	abstract[70]	giv[70]	_	_
15-3	1405-1407	of	abstract[70]	giv[70]	_	_
15-4	1408-1414	visual	abstract[70]	giv[70]	_	_
15-5	1415-1426	positioning	abstract|abstract[70]	giv|giv[70]	coref	16-2
15-6	1427-1434	methods	abstract[70]	giv[70]	_	_
15-7	1435-1439	have	_	_	_	_
15-8	1440-1444	been	_	_	_	_
15-9	1445-1453	proposed	_	_	_	_
15-10	1454-1456	in	_	_	_	_
15-11	1457-1463	recent	time[71]	giv[71]	_	_
15-12	1464-1469	years	time[71]	giv[71]	_	_
15-13	1470-1471	.	_	_	_	_

#Text=These positioning methods are divided into three categories : image retrieval based methods , visual landmarks-based methods , and learning-based methods .
16-1	1472-1477	These	abstract[73]	giv[73]	_	_
16-2	1478-1489	positioning	abstract|abstract[73]	giv|giv[73]	coref	17-7
16-3	1490-1497	methods	abstract[73]	giv[73]	_	_
16-4	1498-1501	are	_	_	_	_
16-5	1502-1509	divided	_	_	_	_
16-6	1510-1514	into	_	_	_	_
16-7	1515-1520	three	abstract[74]	new[74]	appos	16-10[77_74]
16-8	1521-1531	categories	abstract[74]	new[74]	_	_
16-9	1532-1533	:	_	_	_	_
16-10	1534-1539	image	abstract|event[76]|abstract[77]|abstract[78]	new|new[76]|giv[77]|giv[78]	appos|appos|coref|coref	16-10[78_77]|16-15[79_78]|17-1|17-1[83_76]
16-11	1540-1549	retrieval	event[76]|abstract[77]|abstract[78]	new[76]|giv[77]|giv[78]	_	_
16-12	1550-1555	based	abstract[77]|abstract[78]	giv[77]|giv[78]	_	_
16-13	1556-1563	methods	abstract[77]|abstract[78]	giv[77]|giv[78]	_	_
16-14	1564-1565	,	abstract[78]	giv[78]	_	_
16-15	1566-1572	visual	abstract[78]|abstract[79]	giv[78]|giv[79]	appos	16-20[81_79]
16-16	1573-1588	landmarks-based	abstract[78]|abstract[79]	giv[78]|giv[79]	_	_
16-17	1589-1596	methods	abstract[78]|abstract[79]	giv[78]|giv[79]	_	_
16-18	1597-1598	,	abstract[78]	giv[78]	_	_
16-19	1599-1602	and	abstract[78]	giv[78]	_	_
16-20	1603-1617	learning-based	abstract[78]|abstract|abstract[81]	giv[78]|new|giv[81]	coref|coref	17-1[84_81]|28-6
16-21	1618-1625	methods	abstract[78]|abstract[81]	giv[78]|giv[81]	_	_
16-22	1626-1627	.	_	_	_	_

#Text=Image retrieval based methods treat the positioning task as an image retrieval or recognition process .
17-1	1628-1633	Image	abstract|event[83]|abstract[84]	giv|giv[83]|giv[84]	coref|coref|coref	17-11|17-10[88_83]|19-1[98_84]
17-2	1634-1643	retrieval	event[83]|abstract[84]	giv[83]|giv[84]	_	_
17-3	1644-1649	based	abstract[84]	giv[84]	_	_
17-4	1650-1657	methods	abstract[84]	giv[84]	_	_
17-5	1658-1663	treat	_	_	_	_
17-6	1664-1667	the	abstract[86]	new[86]	_	_
17-7	1668-1679	positioning	abstract|abstract[86]	giv|new[86]	coref	22-2
17-8	1680-1684	task	abstract[86]	new[86]	_	_
17-9	1685-1687	as	_	_	_	_
17-10	1688-1690	an	event[88]|abstract[89]	giv[88]|new[89]	ana|coref	18-1[0_89]|20-8[0_88]
17-11	1691-1696	image	abstract|event[88]|abstract[89]	giv|giv[88]|new[89]	coref	18-14[95_0]
17-12	1697-1706	retrieval	event[88]|abstract[89]	giv[88]|new[89]	_	_
17-13	1707-1709	or	abstract[89]	new[89]	_	_
17-14	1710-1721	recognition	abstract[89]|abstract|abstract[91]	new[89]|new|new[91]	_	_
17-15	1722-1729	process	abstract[89]|abstract[91]	new[89]|new[91]	_	_
17-16	1730-1731	.	_	_	_	_

#Text=They usually have a database that are augmented with geospatial information , and every image in the database is described through the same specific features .
18-1	1732-1736	They	abstract	giv	_	_
18-2	1737-1744	usually	_	_	_	_
18-3	1745-1749	have	_	_	_	_
18-4	1750-1751	a	abstract[93]	giv[93]	coref	18-17[96_93]
18-5	1752-1760	database	abstract[93]	giv[93]	_	_
18-6	1761-1765	that	abstract[93]	giv[93]	_	_
18-7	1766-1769	are	abstract[93]	giv[93]	_	_
18-8	1770-1779	augmented	abstract[93]	giv[93]	_	_
18-9	1780-1784	with	abstract[93]	giv[93]	_	_
18-10	1785-1795	geospatial	abstract[93]|abstract[94]	giv[93]|giv[94]	coref	19-21[105_94]
18-11	1796-1807	information	abstract[93]|abstract[94]	giv[93]|giv[94]	_	_
18-12	1808-1809	,	_	_	_	_
18-13	1810-1813	and	_	_	_	_
18-14	1814-1819	every	abstract[95]	giv[95]	coref	19-26[107_95]
18-15	1820-1825	image	abstract[95]	giv[95]	_	_
18-16	1826-1828	in	abstract[95]	giv[95]	_	_
18-17	1829-1832	the	abstract[95]|abstract[96]	giv[95]|giv[96]	coref	19-12[101_96]
18-18	1833-1841	database	abstract[95]|abstract[96]	giv[95]|giv[96]	_	_
18-19	1842-1844	is	_	_	_	_
18-20	1845-1854	described	_	_	_	_
18-21	1855-1862	through	_	_	_	_
18-22	1863-1866	the	abstract[97]	new[97]	coref	23-16[136_97]
18-23	1867-1871	same	abstract[97]	new[97]	_	_
18-24	1872-1880	specific	abstract[97]	new[97]	_	_
18-25	1881-1889	features	abstract[97]	new[97]	_	_
18-26	1890-1891	.	_	_	_	_

#Text=These methods perform a first step to retrieve candidate images from the database according to a similarity search , and the coarse position information of the query image is then obtained based on the geospatial information of these candidate images .
19-1	1892-1897	These	abstract[98]	giv[98]	coref	22-1[124_98]
19-2	1898-1905	methods	abstract[98]	giv[98]	_	_
19-3	1906-1913	perform	_	_	_	_
19-4	1914-1915	a	abstract[99]	new[99]	coref	20-2[110_99]
19-5	1916-1921	first	abstract[99]	new[99]	_	_
19-6	1922-1926	step	abstract[99]	new[99]	_	_
19-7	1927-1929	to	abstract[99]	new[99]	_	_
19-8	1930-1938	retrieve	abstract[99]	new[99]	_	_
19-9	1939-1948	candidate	abstract[99]|object[100]	new[99]|new[100]	coref	19-38[109_100]
19-10	1949-1955	images	abstract[99]|object[100]	new[99]|new[100]	_	_
19-11	1956-1960	from	abstract[99]	new[99]	_	_
19-12	1961-1964	the	abstract[99]|abstract[101]	new[99]|giv[101]	coref	23-11[135_101]
19-13	1965-1973	database	abstract[99]|abstract[101]	new[99]|giv[101]	_	_
19-14	1974-1983	according	abstract[99]	new[99]	_	_
19-15	1984-1986	to	abstract[99]	new[99]	_	_
19-16	1987-1988	a	abstract[99]|abstract[103]	new[99]|new[103]	coref	21-19[122_103]
19-17	1989-1999	similarity	abstract[99]|abstract|abstract[103]	new[99]|new|new[103]	coref	21-19
19-18	2000-2006	search	abstract[99]|abstract[103]	new[99]|new[103]	_	_
19-19	2007-2008	,	_	_	_	_
19-20	2009-2012	and	_	_	_	_
19-21	2013-2016	the	abstract[105]	giv[105]	_	_
19-22	2017-2023	coarse	abstract[105]	giv[105]	_	_
19-23	2024-2032	position	abstract|abstract[105]	giv|giv[105]	coref	31-11
19-24	2033-2044	information	abstract[105]	giv[105]	_	_
19-25	2045-2047	of	abstract[105]	giv[105]	_	_
19-26	2048-2051	the	abstract[105]|abstract[107]	giv[105]|giv[107]	coref	20-7[0_107]
19-27	2052-2057	query	abstract[105]|abstract|abstract[107]	giv[105]|new|giv[107]	coref	27-21
19-28	2058-2063	image	abstract[105]|abstract[107]	giv[105]|giv[107]	_	_
19-29	2064-2066	is	_	_	_	_
19-30	2067-2071	then	_	_	_	_
19-31	2072-2080	obtained	_	_	_	_
19-32	2081-2086	based	_	_	_	_
19-33	2087-2089	on	_	_	_	_
19-34	2090-2093	the	abstract[108]	new[108]	coref	31-21[211_108]
19-35	2094-2104	geospatial	abstract[108]	new[108]	_	_
19-36	2105-2116	information	abstract[108]	new[108]	_	_
19-37	2117-2119	of	abstract[108]	new[108]	_	_
19-38	2120-2125	these	abstract[108]|object[109]	new[108]|giv[109]	coref	23-8[134_109]
19-39	2126-2135	candidate	abstract[108]|object[109]	new[108]|giv[109]	_	_
19-40	2136-2142	images	abstract[108]|object[109]	new[108]|giv[109]	_	_
19-41	2143-2144	.	_	_	_	_

#Text=So the first step , similar image retrieval process , is critical .
20-1	2145-2147	So	_	_	_	_
20-2	2148-2151	the	abstract[110]	giv[110]	appos	20-6[113_110]
20-3	2152-2157	first	abstract[110]	giv[110]	_	_
20-4	2158-2162	step	abstract[110]	giv[110]	_	_
20-5	2163-2164	,	_	_	_	_
20-6	2165-2172	similar	abstract[113]	giv[113]	coref	22-12[130_113]
20-7	2173-2178	image	abstract|abstract[113]	giv|giv[113]	coref	22-17
20-8	2179-2188	retrieval	event|abstract[113]	giv|giv[113]	coref	22-17[132_0]
20-9	2189-2196	process	abstract[113]	giv[113]	_	_
20-10	2197-2198	,	_	_	_	_
20-11	2199-2201	is	_	_	_	_
20-12	2202-2210	critical	_	_	_	_
20-13	2211-2212	.	_	_	_	_

#Text=The brute-force approach , which is a distance comparison between feature descriptor vectors , is often used for similarity search .
21-1	2213-2216	The	abstract[115]	new[115]	_	_
21-2	2217-2228	brute-force	person|abstract[115]	new|new[115]	_	_
21-3	2229-2237	approach	abstract[115]	new[115]	_	_
21-4	2238-2239	,	abstract[115]	new[115]	_	_
21-5	2240-2245	which	abstract[115]|abstract[117]	new[115]|new[117]	coref	22-9[127_117]
21-6	2246-2248	is	abstract[115]|abstract[117]	new[115]|new[117]	_	_
21-7	2249-2250	a	abstract[115]|abstract[117]	new[115]|new[117]	_	_
21-8	2251-2259	distance	abstract[115]|abstract|abstract[117]	new[115]|new|new[117]	_	_
21-9	2260-2270	comparison	abstract[115]|abstract[117]	new[115]|new[117]	_	_
21-10	2271-2278	between	abstract[115]|abstract[117]	new[115]|new[117]	_	_
21-11	2279-2286	feature	abstract[115]|abstract[117]|abstract|abstract[120]	new[115]|new[117]|new|new[120]	coref|coref	22-6|26-29[171_120]
21-12	2287-2297	descriptor	abstract[115]|abstract[117]|person|abstract[120]	new[115]|new[117]|new|new[120]	_	_
21-13	2298-2305	vectors	abstract[115]|abstract[117]|abstract[120]	new[115]|new[117]|new[120]	_	_
21-14	2306-2307	,	_	_	_	_
21-15	2308-2310	is	_	_	_	_
21-16	2311-2316	often	_	_	_	_
21-17	2317-2321	used	_	_	_	_
21-18	2322-2325	for	_	_	_	_
21-19	2326-2336	similarity	abstract|abstract[122]	giv|giv[122]	coref|coref	22-13|22-14[0_122]
21-20	2337-2343	search	abstract[122]	giv[122]	_	_
21-21	2344-2345	.	_	_	_	_

#Text=Some positioning methods based on feature descriptors adopt brute-force comparison for the similarity search process of image retrieval .
22-1	2346-2350	Some	abstract[124]	giv[124]	coref	28-12[189_124]
22-2	2351-2362	positioning	abstract|abstract[124]	giv|giv[124]	coref	32-28[222_0]
22-3	2363-2370	methods	abstract[124]	giv[124]	_	_
22-4	2371-2376	based	abstract[124]	giv[124]	_	_
22-5	2377-2379	on	abstract[124]	giv[124]	_	_
22-6	2380-2387	feature	abstract[124]|abstract|abstract[126]	giv[124]|giv|new[126]	coref|coref	24-25|26-32[0_126]
22-7	2388-2399	descriptors	abstract[124]|abstract[126]	giv[124]|new[126]	_	_
22-8	2400-2405	adopt	_	_	_	_
22-9	2406-2417	brute-force	abstract[127]	giv[127]	_	_
22-10	2418-2428	comparison	abstract[127]	giv[127]	_	_
22-11	2429-2432	for	_	_	_	_
22-12	2433-2436	the	abstract[130]	giv[130]	ana	23-3[0_130]
22-13	2437-2447	similarity	abstract|abstract[130]	giv|giv[130]	coref	26-10
22-14	2448-2454	search	abstract|abstract[130]	giv|giv[130]	coref	24-12
22-15	2455-2462	process	abstract[130]	giv[130]	_	_
22-16	2463-2465	of	abstract[130]	giv[130]	_	_
22-17	2466-2471	image	abstract[130]|abstract|event[132]	giv[130]|giv|giv[132]	coref|coref	27-21[182_0]|29-15[196_132]
22-18	2472-2481	retrieval	abstract[130]|event[132]	giv[130]|giv[132]	_	_
22-19	2482-2483	.	_	_	_	_

#Text=However , it is computationally intensive when the images of a database are described with high-dimensional features , limiting its scope of applications .
23-1	2484-2491	However	_	_	_	_
23-2	2492-2493	,	_	_	_	_
23-3	2494-2496	it	abstract	giv	ana	23-20
23-4	2497-2499	is	_	_	_	_
23-5	2500-2515	computationally	_	_	_	_
23-6	2516-2525	intensive	_	_	_	_
23-7	2526-2530	when	_	_	_	_
23-8	2531-2534	the	object[134]	giv[134]	coref	24-16[144_134]
23-9	2535-2541	images	object[134]	giv[134]	_	_
23-10	2542-2544	of	object[134]	giv[134]	_	_
23-11	2545-2546	a	object[134]|abstract[135]	giv[134]|giv[135]	coref	24-19[145_135]
23-12	2547-2555	database	object[134]|abstract[135]	giv[134]|giv[135]	_	_
23-13	2556-2559	are	_	_	_	_
23-14	2560-2569	described	_	_	_	_
23-15	2570-2574	with	_	_	_	_
23-16	2575-2591	high-dimensional	abstract[136]	giv[136]	coref	29-5[191_136]
23-17	2592-2600	features	abstract[136]	giv[136]	_	_
23-18	2601-2602	,	_	_	_	_
23-19	2603-2611	limiting	_	_	_	_
23-20	2612-2615	its	abstract|abstract[138]	giv|new[138]	_	_
23-21	2616-2621	scope	abstract[138]	new[138]	_	_
23-22	2622-2624	of	abstract[138]	new[138]	_	_
23-23	2625-2637	applications	abstract[138]|abstract	new[138]|new	_	_
23-24	2638-2639	.	_	_	_	_

#Text=Azzi et al. use a global feature-based system to reduce the search space and find candidate images in the database , then the local feature scale-invariant feature transform ( SIFT ) is adopted for points matching in pose estimation .
24-1	2640-2644	Azzi	person	new	_	_
24-2	2645-2647	et	_	_	_	_
24-3	2648-2651	al.	_	_	_	_
24-4	2652-2655	use	_	_	_	_
24-5	2656-2657	a	abstract[141]	giv[141]	_	_
24-6	2658-2664	global	abstract[141]	giv[141]	_	_
24-7	2665-2678	feature-based	abstract[141]	giv[141]	_	_
24-8	2679-2685	system	abstract[141]	giv[141]	_	_
24-9	2686-2688	to	_	_	_	_
24-10	2689-2695	reduce	_	_	_	_
24-11	2696-2699	the	abstract[143]	new[143]	_	_
24-12	2700-2706	search	abstract|abstract[143]	giv|new[143]	coref	25-11[157_0]
24-13	2707-2712	space	abstract[143]	new[143]	_	_
24-14	2713-2716	and	_	_	_	_
24-15	2717-2721	find	_	_	_	_
24-16	2722-2731	candidate	object[144]	giv[144]	coref	27-24[184_144]
24-17	2732-2738	images	object[144]	giv[144]	_	_
24-18	2739-2741	in	_	_	_	_
24-19	2742-2745	the	abstract[145]	giv[145]	coref	27-24[0_145]
24-20	2746-2754	database	abstract[145]	giv[145]	_	_
24-21	2755-2756	,	_	_	_	_
24-22	2757-2761	then	_	_	_	_
24-23	2762-2765	the	abstract[148]	new[148]	appos	24-30[0_148]
24-24	2766-2771	local	abstract[148]	new[148]	_	_
24-25	2772-2779	feature	abstract|abstract[148]	giv|new[148]	coref	24-27
24-26	2780-2795	scale-invariant	abstract[148]	new[148]	_	_
24-27	2796-2803	feature	abstract|abstract[148]	giv|new[148]	coref	26-29
24-28	2804-2813	transform	abstract[148]	new[148]	_	_
24-29	2814-2815	(	_	_	_	_
24-30	2816-2820	SIFT	abstract	giv	_	_
24-31	2821-2822	)	_	_	_	_
24-32	2823-2825	is	_	_	_	_
24-33	2826-2833	adopted	_	_	_	_
24-34	2834-2837	for	_	_	_	_
24-35	2838-2844	points	abstract[150]	new[150]	_	_
24-36	2845-2853	matching	abstract[150]	new[150]	_	_
24-37	2854-2856	in	abstract[150]	new[150]	_	_
24-38	2857-2861	pose	abstract[150]|abstract|abstract[152]	new[150]|new|new[152]	coref	33-8[228_0]
24-39	2862-2872	estimation	abstract[150]|abstract[152]	new[150]|new[152]	_	_
24-40	2873-2874	.	_	_	_	_

#Text=Some researchers try to trade accuracy for rapidity by using approximate nearest neighbor search , such as quantization and vocabulary tree .
25-1	2875-2879	Some	person[153]	new[153]	_	_
25-2	2880-2891	researchers	person[153]	new[153]	_	_
25-3	2892-2895	try	_	_	_	_
25-4	2896-2898	to	_	_	_	_
25-5	2899-2904	trade	_	_	_	_
25-6	2905-2913	accuracy	abstract	new	coref	38-9[258_0]
25-7	2914-2917	for	_	_	_	_
25-8	2918-2926	rapidity	abstract	new	_	_
25-9	2927-2929	by	_	_	_	_
25-10	2930-2935	using	_	_	_	_
25-11	2936-2947	approximate	abstract[157]	giv[157]	coref	26-10[165_157]
25-12	2948-2955	nearest	person[156]|abstract[157]	new[156]|giv[157]	_	_
25-13	2956-2964	neighbor	person[156]|abstract[157]	new[156]|giv[157]	_	_
25-14	2965-2971	search	abstract[157]	giv[157]	_	_
25-15	2972-2973	,	abstract[157]	giv[157]	_	_
25-16	2974-2978	such	abstract[157]	giv[157]	_	_
25-17	2979-2981	as	abstract[157]	giv[157]	_	_
25-18	2982-2994	quantization	abstract[157]|abstract	giv[157]|new	_	_
25-19	2995-2998	and	abstract[157]	giv[157]	_	_
25-20	2999-3009	vocabulary	abstract[157]|abstract|abstract[160]	giv[157]|new|new[160]	_	_
25-21	3010-3014	tree	abstract[157]|abstract[160]	giv[157]|new[160]	_	_
25-22	3015-3016	.	_	_	_	_

#Text=Another common way to save time and memory of similarity search is principal component analysis ( PCA ) , which has been used to reduce the size of feature vectors and descriptors .
26-1	3017-3024	Another	abstract[161]	new[161]	coref	26-13[167_161]
26-2	3025-3031	common	abstract[161]	new[161]	_	_
26-3	3032-3035	way	abstract[161]	new[161]	_	_
26-4	3036-3038	to	abstract[161]	new[161]	_	_
26-5	3039-3043	save	abstract[161]	new[161]	_	_
26-6	3044-3048	time	abstract[161]|time	new[161]|giv	coref	38-14[260_0]
26-7	3049-3052	and	abstract[161]	new[161]	_	_
26-8	3053-3059	memory	abstract[161]|abstract[163]	new[161]|new[163]	_	_
26-9	3060-3062	of	abstract[161]|abstract[163]	new[161]|new[163]	_	_
26-10	3063-3073	similarity	abstract[161]|abstract[163]|abstract|abstract[165]	new[161]|new[163]|giv|giv[165]	coref	27-19[180_0]
26-11	3074-3080	search	abstract[161]|abstract[163]|abstract[165]	new[161]|new[163]|giv[165]	_	_
26-12	3081-3083	is	_	_	_	_
26-13	3084-3093	principal	abstract[167]	giv[167]	appos	26-17[0_167]
26-14	3094-3103	component	abstract|abstract[167]	new|giv[167]	_	_
26-15	3104-3112	analysis	abstract[167]	giv[167]	_	_
26-16	3113-3114	(	_	_	_	_
26-17	3115-3118	PCA	abstract	giv	_	_
26-18	3119-3120	)	_	_	_	_
26-19	3121-3122	,	_	_	_	_
26-20	3123-3128	which	_	_	_	_
26-21	3129-3132	has	_	_	_	_
26-22	3133-3137	been	_	_	_	_
26-23	3138-3142	used	_	_	_	_
26-24	3143-3145	to	_	_	_	_
26-25	3146-3152	reduce	_	_	_	_
26-26	3153-3156	the	abstract[169]	new[169]	_	_
26-27	3157-3161	size	abstract[169]	new[169]	_	_
26-28	3162-3164	of	abstract[169]	new[169]	_	_
26-29	3165-3172	feature	abstract[169]|abstract|abstract[171]|abstract[172]	new[169]|giv|giv[171]|giv[172]	coref|coref	26-29[172_171]|35-15
26-30	3173-3180	vectors	abstract[169]|abstract[171]|abstract[172]	new[169]|giv[171]|giv[172]	_	_
26-31	3181-3184	and	abstract[169]|abstract[172]	new[169]|giv[172]	_	_
26-32	3185-3196	descriptors	abstract[169]|abstract[172]|abstract	new[169]|giv[172]|giv	coref	30-13[203_0]
26-33	3197-3198	.	_	_	_	_

#Text=Some works use correlation algorithms , such as sum of absolute difference ( SAD ) , for computing similarity between query image and database images .
27-1	3199-3203	Some	abstract[174]	new[174]	_	_
27-2	3204-3209	works	abstract[174]	new[174]	_	_
27-3	3210-3213	use	_	_	_	_
27-4	3214-3225	correlation	abstract|abstract[176]	new|giv[176]	coref	28-5[187_176]
27-5	3226-3236	algorithms	abstract[176]	giv[176]	_	_
27-6	3237-3238	,	abstract[176]	giv[176]	_	_
27-7	3239-3243	such	abstract[176]	giv[176]	_	_
27-8	3244-3246	as	abstract[176]	giv[176]	_	_
27-9	3247-3250	sum	abstract[176]|abstract[177]	giv[176]|new[177]	_	_
27-10	3251-3253	of	abstract[176]|abstract[177]	giv[176]|new[177]	_	_
27-11	3254-3262	absolute	abstract[176]|abstract[177]|abstract[178]	giv[176]|new[177]|new[178]	appos	27-14[0_178]
27-12	3263-3273	difference	abstract[176]|abstract[177]|abstract[178]	giv[176]|new[177]|new[178]	_	_
27-13	3274-3275	(	_	_	_	_
27-14	3276-3279	SAD	abstract	giv	_	_
27-15	3280-3281	)	_	_	_	_
27-16	3282-3283	,	_	_	_	_
27-17	3284-3287	for	_	_	_	_
27-18	3288-3297	computing	_	_	_	_
27-19	3298-3308	similarity	abstract[180]	giv[180]	_	_
27-20	3309-3316	between	abstract[180]	giv[180]	_	_
27-21	3317-3322	query	abstract[180]|abstract|abstract[182]	giv[180]|giv|giv[182]	coref|coref	29-12[0_182]|31-15
27-22	3323-3328	image	abstract[180]|abstract[182]	giv[180]|giv[182]	_	_
27-23	3329-3332	and	abstract[180]	giv[180]	_	_
27-24	3333-3341	database	abstract[180]|abstract|object[184]	giv[180]|giv|giv[184]	coref|coref	31-2[206_184]|35-6[237_0]
27-25	3342-3348	images	abstract[180]|object[184]	giv[180]|giv[184]	_	_
27-26	3349-3350	.	_	_	_	_

#Text=In recent studies , deep learning-based algorithms are an alternative to aforementioned methods .
28-1	3351-3353	In	_	_	_	_
28-2	3354-3360	recent	abstract[185]	new[185]	_	_
28-3	3361-3368	studies	abstract[185]	new[185]	_	_
28-4	3369-3370	,	_	_	_	_
28-5	3371-3375	deep	abstract[187]	giv[187]	coref	28-9[188_187]
28-6	3376-3390	learning-based	abstract|abstract[187]	giv|giv[187]	_	_
28-7	3391-3401	algorithms	abstract[187]	giv[187]	_	_
28-8	3402-3405	are	_	_	_	_
28-9	3406-3408	an	abstract[188]	giv[188]	_	_
28-10	3409-3420	alternative	abstract[188]	giv[188]	_	_
28-11	3421-3423	to	abstract[188]	giv[188]	_	_
28-12	3424-3438	aforementioned	abstract[188]|abstract[189]	giv[188]|giv[189]	coref	33-1[224_189]
28-13	3439-3446	methods	abstract[188]|abstract[189]	giv[188]|giv[189]	_	_
28-14	3447-3448	.	_	_	_	_

#Text=Razavian et al. use features extracted from a network as an image representation for image retrieval in a diverse set of datasets .
29-1	3449-3457	Razavian	person	new	_	_
29-2	3458-3460	et	_	_	_	_
29-3	3461-3464	al.	_	_	_	_
29-4	3465-3468	use	_	_	_	_
29-5	3469-3477	features	abstract[191]	giv[191]	coref	30-9[201_191]
29-6	3478-3487	extracted	abstract[191]	giv[191]	_	_
29-7	3488-3492	from	abstract[191]	giv[191]	_	_
29-8	3493-3494	a	abstract[191]|abstract[192]	giv[191]|new[192]	_	_
29-9	3495-3502	network	abstract[191]|abstract[192]	giv[191]|new[192]	_	_
29-10	3503-3505	as	abstract[191]	giv[191]	_	_
29-11	3506-3508	an	abstract[191]|abstract[194]	giv[191]|new[194]	_	_
29-12	3509-3514	image	abstract[191]|abstract|abstract[194]	giv[191]|giv|new[194]	coref	29-15
29-13	3515-3529	representation	abstract[191]|abstract[194]	giv[191]|new[194]	_	_
29-14	3530-3533	for	_	_	_	_
29-15	3534-3539	image	abstract|event[196]	giv|giv[196]	coref|coref	30-16|30-16[205_196]
29-16	3540-3549	retrieval	event[196]	giv[196]	_	_
29-17	3550-3552	in	event[196]	giv[196]	_	_
29-18	3553-3554	a	event[196]|abstract[197]	giv[196]|new[197]	_	_
29-19	3555-3562	diverse	event[196]|abstract[197]	giv[196]|new[197]	_	_
29-20	3563-3566	set	event[196]|abstract[197]	giv[196]|new[197]	_	_
29-21	3567-3569	of	event[196]|abstract[197]	giv[196]|new[197]	_	_
29-22	3570-3578	datasets	event[196]|abstract[197]|abstract	giv[196]|new[197]|new	_	_
29-23	3579-3580	.	_	_	_	_

#Text=Yandex et al. propose a method that aggregates local deep features to product descriptors for image retrieval .
30-1	3581-3587	Yandex	person	new	_	_
30-2	3588-3590	et	_	_	_	_
30-3	3591-3594	al.	_	_	_	_
30-4	3595-3602	propose	_	_	_	_
30-5	3603-3604	a	abstract[200]	new[200]	coref	40-7[277_200]
30-6	3605-3611	method	abstract[200]	new[200]	_	_
30-7	3612-3616	that	abstract[200]	new[200]	_	_
30-8	3617-3627	aggregates	abstract[200]	new[200]	_	_
30-9	3628-3633	local	abstract[200]|abstract[201]	new[200]|giv[201]	coref	38-19[261_201]
30-10	3634-3638	deep	abstract[200]|abstract[201]	new[200]|giv[201]	_	_
30-11	3639-3647	features	abstract[200]|abstract[201]	new[200]|giv[201]	_	_
30-12	3648-3650	to	abstract[200]	new[200]	_	_
30-13	3651-3658	product	abstract[200]|abstract|abstract[203]	new[200]|new|giv[203]	coref	35-15[239_203]
30-14	3659-3670	descriptors	abstract[200]|abstract[203]	new[200]|giv[203]	_	_
30-15	3671-3674	for	abstract[200]	new[200]	_	_
30-16	3675-3680	image	abstract[200]|abstract|event[205]	new[200]|giv|giv[205]	coref	31-14[210_0]
30-17	3681-3690	retrieval	abstract[200]|event[205]	new[200]|giv[205]	_	_
30-18	3691-3692	.	_	_	_	_

#Text=After a set of candidate images are retrieved , the position information of the query image is calculated according to the geospatial information of these candidate images through a weighting scheme or linear combination .
31-1	3693-3698	After	_	_	_	_
31-2	3699-3700	a	object[206]	giv[206]	coref	31-25[212_206]
31-3	3701-3704	set	object[206]	giv[206]	_	_
31-4	3705-3707	of	object[206]	giv[206]	_	_
31-5	3708-3717	candidate	object[206]	giv[206]	_	_
31-6	3718-3724	images	object[206]	giv[206]	_	_
31-7	3725-3728	are	_	_	_	_
31-8	3729-3738	retrieved	_	_	_	_
31-9	3739-3740	,	_	_	_	_
31-10	3741-3744	the	abstract[208]	new[208]	_	_
31-11	3745-3753	position	abstract|abstract[208]	giv|new[208]	coref	32-5
31-12	3754-3765	information	abstract[208]	new[208]	_	_
31-13	3766-3768	of	abstract[208]	new[208]	_	_
31-14	3769-3772	the	abstract[208]|abstract[210]	new[208]|giv[210]	coref	33-18[230_210]
31-15	3773-3778	query	abstract[208]|abstract|abstract[210]	new[208]|giv|giv[210]	coref	33-19
31-16	3779-3784	image	abstract[208]|abstract[210]	new[208]|giv[210]	_	_
31-17	3785-3787	is	_	_	_	_
31-18	3788-3798	calculated	_	_	_	_
31-19	3799-3808	according	_	_	_	_
31-20	3809-3811	to	_	_	_	_
31-21	3812-3815	the	abstract[211]	giv[211]	_	_
31-22	3816-3826	geospatial	abstract[211]	giv[211]	_	_
31-23	3827-3838	information	abstract[211]	giv[211]	_	_
31-24	3839-3841	of	abstract[211]	giv[211]	_	_
31-25	3842-3847	these	abstract[211]|object[212]	giv[211]|giv[212]	coref	35-18[241_212]
31-26	3848-3857	candidate	abstract[211]|object[212]	giv[211]|giv[212]	_	_
31-27	3858-3864	images	abstract[211]|object[212]	giv[211]|giv[212]	_	_
31-28	3865-3872	through	_	_	_	_
31-29	3873-3874	a	abstract[214]	new[214]	_	_
31-30	3875-3884	weighting	abstract|abstract[214]	new|new[214]	_	_
31-31	3885-3891	scheme	abstract[214]	new[214]	_	_
31-32	3892-3894	or	_	_	_	_
31-33	3895-3901	linear	abstract[215]	new[215]	_	_
31-34	3902-3913	combination	abstract[215]	new[215]	_	_
31-35	3914-3915	.	_	_	_	_

#Text=However , because this position result is not calculated by strict geometric relations , it is rough in most cases and difficult to meet the requirement of high-accuracy positioning .
32-1	3916-3923	However	_	_	_	_
32-2	3924-3925	,	_	_	_	_
32-3	3926-3933	because	_	_	_	_
32-4	3934-3938	this	abstract[217]	new[217]	ana	32-15[0_217]
32-5	3939-3947	position	abstract|abstract[217]	giv|new[217]	_	_
32-6	3948-3954	result	abstract[217]	new[217]	_	_
32-7	3955-3957	is	_	_	_	_
32-8	3958-3961	not	_	_	_	_
32-9	3962-3972	calculated	_	_	_	_
32-10	3973-3975	by	_	_	_	_
32-11	3976-3982	strict	abstract[218]	new[218]	_	_
32-12	3983-3992	geometric	abstract[218]	new[218]	_	_
32-13	3993-4002	relations	abstract[218]	new[218]	_	_
32-14	4003-4004	,	_	_	_	_
32-15	4005-4007	it	abstract	giv	_	_
32-16	4008-4010	is	_	_	_	_
32-17	4011-4016	rough	_	_	_	_
32-18	4017-4019	in	_	_	_	_
32-19	4020-4024	most	abstract[220]	new[220]	_	_
32-20	4025-4030	cases	abstract[220]	new[220]	_	_
32-21	4031-4034	and	_	_	_	_
32-22	4035-4044	difficult	_	_	_	_
32-23	4045-4047	to	_	_	_	_
32-24	4048-4052	meet	_	_	_	_
32-25	4053-4056	the	abstract[221]	new[221]	_	_
32-26	4057-4068	requirement	abstract[221]	new[221]	_	_
32-27	4069-4071	of	abstract[221]	new[221]	_	_
32-28	4072-4085	high-accuracy	abstract[221]|abstract[222]	new[221]|giv[222]	coref	33-3[0_222]
32-29	4086-4097	positioning	abstract[221]|abstract[222]	new[221]|giv[222]	_	_
32-30	4098-4099	.	_	_	_	_

#Text=Visual landmarks-based positioning methods aim to provide a six degrees of freedom ( DoF ) pose of the query image .
33-1	4100-4106	Visual	abstract[224]	giv[224]	coref	38-5[257_224]
33-2	4107-4122	landmarks-based	abstract[224]	giv[224]	_	_
33-3	4123-4134	positioning	abstract|abstract[224]	giv|giv[224]	coref	39-10
33-4	4135-4142	methods	abstract[224]	giv[224]	_	_
33-5	4143-4146	aim	_	_	_	_
33-6	4147-4149	to	_	_	_	_
33-7	4150-4157	provide	_	_	_	_
33-8	4158-4159	a	abstract[228]	giv[228]	_	_
33-9	4160-4163	six	abstract[225]|abstract[228]	new[225]|giv[228]	appos	33-14[0_225]
33-10	4164-4171	degrees	abstract[225]|abstract[228]	new[225]|giv[228]	_	_
33-11	4172-4174	of	abstract[225]|abstract[228]	new[225]|giv[228]	_	_
33-12	4175-4182	freedom	abstract[225]|abstract|abstract[228]	new[225]|new|giv[228]	_	_
33-13	4183-4184	(	abstract[228]	giv[228]	_	_
33-14	4185-4188	DoF	abstract|abstract[228]	giv|giv[228]	_	_
33-15	4189-4190	)	abstract[228]	giv[228]	_	_
33-16	4191-4195	pose	abstract[228]	giv[228]	_	_
33-17	4196-4198	of	abstract[228]	giv[228]	_	_
33-18	4199-4202	the	abstract[228]|abstract[230]	giv[228]|giv[230]	coref	37-5[251_230]
33-19	4203-4208	query	abstract[228]|abstract|abstract[230]	giv[228]|giv|giv[230]	coref	37-5
33-20	4209-4214	image	abstract[228]|abstract[230]	giv[228]|giv[230]	_	_
33-21	4215-4216	.	_	_	_	_

#Text=Generally , visual landmarks in the indoor environments includes natural landmarks and artificial landmarks .
34-1	4217-4226	Generally	_	_	_	_
34-2	4227-4228	,	_	_	_	_
34-3	4229-4235	visual	place[231]	new[231]	_	_
34-4	4236-4245	landmarks	place[231]	new[231]	_	_
34-5	4246-4248	in	place[231]	new[231]	_	_
34-6	4249-4252	the	place[231]|place[232]	new[231]|new[232]	coref	42-8[287_232]
34-7	4253-4259	indoor	place[231]|place[232]	new[231]|new[232]	_	_
34-8	4260-4272	environments	place[231]|place[232]	new[231]|new[232]	_	_
34-9	4273-4281	includes	_	_	_	_
34-10	4282-4289	natural	place[233]	new[233]	coref	35-1[235_233]
34-11	4290-4299	landmarks	place[233]	new[233]	_	_
34-12	4300-4303	and	_	_	_	_
34-13	4304-4314	artificial	place[234]	new[234]	coref	39-14[270_234]
34-14	4315-4324	landmarks	place[234]	new[234]	_	_
34-15	4325-4326	.	_	_	_	_

#Text=The natural landmarks refer to the geo-tagged 3D database , which is represented by feature descriptors or images with poses .
35-1	4327-4330	The	place[235]	giv[235]	coref	39-4[267_235]
35-2	4331-4338	natural	place[235]	giv[235]	_	_
35-3	4339-4348	landmarks	place[235]	giv[235]	_	_
35-4	4349-4354	refer	_	_	_	_
35-5	4355-4357	to	_	_	_	_
35-6	4358-4361	the	abstract[237]	giv[237]	coref	36-1[243_237]
35-7	4362-4372	geo-tagged	abstract[237]	giv[237]	_	_
35-8	4373-4375	3D	abstract|abstract[237]	new|giv[237]	coref	38-26
35-9	4376-4384	database	abstract[237]	giv[237]	_	_
35-10	4385-4386	,	abstract[237]	giv[237]	_	_
35-11	4387-4392	which	abstract[237]	giv[237]	_	_
35-12	4393-4395	is	abstract[237]	giv[237]	_	_
35-13	4396-4407	represented	abstract[237]	giv[237]	_	_
35-14	4408-4410	by	abstract[237]	giv[237]	_	_
35-15	4411-4418	feature	abstract[237]|abstract|abstract[239]|abstract[240]	giv[237]|giv|giv[239]|giv[240]	coref|coref	35-15[240_239]|37-15
35-16	4419-4430	descriptors	abstract[237]|abstract[239]|abstract[240]	giv[237]|giv[239]|giv[240]	_	_
35-17	4431-4433	or	abstract[237]|abstract[240]	giv[237]|giv[240]	_	_
35-18	4434-4440	images	abstract[237]|abstract[240]|object[241]	giv[237]|giv[240]|giv[241]	_	_
35-19	4441-4445	with	abstract[237]|abstract[240]|object[241]	giv[237]|giv[240]|giv[241]	_	_
35-20	4446-4451	poses	abstract[237]|abstract[240]|object[241]|abstract	giv[237]|giv[240]|giv[241]|new	_	_
35-21	4452-4453	.	_	_	_	_

#Text=This database could have been built thanks to the mapping module of simultaneous localization and mapping ( SLAM ) .
36-1	4454-4458	This	abstract[243]	giv[243]	coref	38-25[265_243]
36-2	4459-4467	database	abstract[243]	giv[243]	_	_
36-3	4468-4473	could	_	_	_	_
36-4	4474-4478	have	_	_	_	_
36-5	4479-4483	been	_	_	_	_
36-6	4484-4489	built	_	_	_	_
36-7	4490-4496	thanks	_	_	_	_
36-8	4497-4499	to	_	_	_	_
36-9	4500-4503	the	abstract[245]	new[245]	_	_
36-10	4504-4511	mapping	object|abstract[245]	new|new[245]	_	_
36-11	4512-4518	module	abstract[245]	new[245]	_	_
36-12	4519-4521	of	abstract[245]	new[245]	_	_
36-13	4522-4534	simultaneous	abstract[245]|abstract[246]	new[245]|new[246]	_	_
36-14	4535-4547	localization	abstract[245]|abstract[246]	new[245]|new[246]	_	_
36-15	4548-4551	and	abstract[245]	new[245]	_	_
36-16	4552-4559	mapping	abstract[245]|abstract	new[245]|new	appos	36-18
36-17	4560-4561	(	_	_	_	_
36-18	4562-4566	SLAM	abstract	giv	_	_
36-19	4567-4568	)	_	_	_	_
36-20	4569-4570	.	_	_	_	_

#Text=Then the pose of query image is estimated by means of re-localization module and feature correspondence .
37-1	4571-4575	Then	_	_	_	_
37-2	4576-4579	the	abstract[249]	new[249]	_	_
37-3	4580-4584	pose	abstract[249]	new[249]	_	_
37-4	4585-4587	of	abstract[249]	new[249]	_	_
37-5	4588-4593	query	abstract[249]|abstract|abstract[251]	new[249]|giv|giv[251]	coref|coref	38-22|38-22[263_251]
37-6	4594-4599	image	abstract[249]|abstract[251]	new[249]|giv[251]	_	_
37-7	4600-4602	is	_	_	_	_
37-8	4603-4612	estimated	_	_	_	_
37-9	4613-4615	by	_	_	_	_
37-10	4616-4621	means	_	_	_	_
37-11	4622-4624	of	_	_	_	_
37-12	4625-4640	re-localization	abstract|abstract[253]	new|new[253]	_	_
37-13	4641-4647	module	abstract[253]	new[253]	_	_
37-14	4648-4651	and	_	_	_	_
37-15	4652-4659	feature	abstract|abstract[255]	giv|new[255]	ana	38-12[0_255]
37-16	4660-4674	correspondence	abstract[255]	new[255]	_	_
37-17	4675-4676	.	_	_	_	_

#Text=Although the results of these methods are of good accuracy , it takes a long time to match the features of query image with geo-tagged 3D database , especially when the indoor scenes are large .
38-1	4677-4685	Although	_	_	_	_
38-2	4686-4689	the	abstract[256]	new[256]	_	_
38-3	4690-4697	results	abstract[256]	new[256]	_	_
38-4	4698-4700	of	abstract[256]	new[256]	_	_
38-5	4701-4706	these	abstract[256]|abstract[257]	new[256]|giv[257]	coref	39-10[269_257]
38-6	4707-4714	methods	abstract[256]|abstract[257]	new[256]|giv[257]	_	_
38-7	4715-4718	are	_	_	_	_
38-8	4719-4721	of	_	_	_	_
38-9	4722-4726	good	abstract[258]	giv[258]	_	_
38-10	4727-4735	accuracy	abstract[258]	giv[258]	_	_
38-11	4736-4737	,	_	_	_	_
38-12	4738-4740	it	abstract	giv	_	_
38-13	4741-4746	takes	_	_	_	_
38-14	4747-4748	a	time[260]	giv[260]	_	_
38-15	4749-4753	long	time[260]	giv[260]	_	_
38-16	4754-4758	time	time[260]	giv[260]	_	_
38-17	4759-4761	to	_	_	_	_
38-18	4762-4767	match	_	_	_	_
38-19	4768-4771	the	abstract[261]	giv[261]	_	_
38-20	4772-4780	features	abstract[261]	giv[261]	_	_
38-21	4781-4783	of	abstract[261]	giv[261]	_	_
38-22	4784-4789	query	abstract[261]|abstract|abstract[263]	giv[261]|giv|giv[263]	_	_
38-23	4790-4795	image	abstract[261]|abstract[263]	giv[261]|giv[263]	_	_
38-24	4796-4800	with	_	_	_	_
38-25	4801-4811	geo-tagged	abstract[265]	giv[265]	_	_
38-26	4812-4814	3D	abstract|abstract[265]	giv|giv[265]	_	_
38-27	4815-4823	database	abstract[265]	giv[265]	_	_
38-28	4824-4825	,	_	_	_	_
38-29	4826-4836	especially	_	_	_	_
38-30	4837-4841	when	_	_	_	_
38-31	4842-4845	the	abstract[266]	new[266]	_	_
38-32	4846-4852	indoor	abstract[266]	new[266]	_	_
38-33	4853-4859	scenes	abstract[266]	new[266]	_	_
38-34	4860-4863	are	_	_	_	_
38-35	4864-4869	large	_	_	_	_
38-36	4870-4871	.	_	_	_	_

#Text=In addition to natural landmarks , there are also positioning methods based on artificial landmarks , e.g. , Degol et al. proposed a fiducial marker and detection algorithm .
39-1	4872-4874	In	_	_	_	_
39-2	4875-4883	addition	_	_	_	_
39-3	4884-4886	to	_	_	_	_
39-4	4887-4894	natural	place[267]	giv[267]	_	_
39-5	4895-4904	landmarks	place[267]	giv[267]	_	_
39-6	4905-4906	,	_	_	_	_
39-7	4907-4912	there	_	_	_	_
39-8	4913-4916	are	_	_	_	_
39-9	4917-4921	also	_	_	_	_
39-10	4922-4933	positioning	abstract|abstract[269]	giv|giv[269]	coref|coref	40-15[279_0]|42-2[285_269]
39-11	4934-4941	methods	abstract[269]	giv[269]	_	_
39-12	4942-4947	based	abstract[269]	giv[269]	_	_
39-13	4948-4950	on	abstract[269]	giv[269]	_	_
39-14	4951-4961	artificial	abstract[269]|place[270]	giv[269]|giv[270]	_	_
39-15	4962-4971	landmarks	abstract[269]|place[270]	giv[269]|giv[270]	_	_
39-16	4972-4973	,	_	_	_	_
39-17	4974-4978	e.g.	_	_	_	_
39-18	4979-4980	,	_	_	_	_
39-19	4981-4986	Degol	person	new	_	_
39-20	4987-4989	et	_	_	_	_
39-21	4990-4993	al.	_	_	_	_
39-22	4994-5002	proposed	_	_	_	_
39-23	5003-5004	a	abstract[272]|abstract[274]	new[272]|new[274]	coref	41-5[282_272]
39-24	5005-5013	fiducial	abstract[272]|abstract[274]	new[272]|new[274]	_	_
39-25	5014-5020	marker	abstract[272]|abstract[274]	new[272]|new[274]	_	_
39-26	5021-5024	and	abstract[274]	new[274]	_	_
39-27	5025-5034	detection	abstract|abstract[274]	new|new[274]	_	_
39-28	5035-5044	algorithm	abstract[274]	new[274]	_	_
39-29	5045-5046	.	_	_	_	_

#Text=In reference , the authors proposed a method to simultaneously solve the problems of positioning from a set of squared planar markers .
40-1	5047-5049	In	_	_	_	_
40-2	5050-5059	reference	abstract	new	_	_
40-3	5060-5061	,	_	_	_	_
40-4	5062-5065	the	person[276]	new[276]	_	_
40-5	5066-5073	authors	person[276]	new[276]	_	_
40-6	5074-5082	proposed	_	_	_	_
40-7	5083-5084	a	abstract[277]	giv[277]	_	_
40-8	5085-5091	method	abstract[277]	giv[277]	_	_
40-9	5092-5094	to	abstract[277]	giv[277]	_	_
40-10	5095-5109	simultaneously	abstract[277]	giv[277]	_	_
40-11	5110-5115	solve	abstract[277]	giv[277]	_	_
40-12	5116-5119	the	abstract[277]|abstract[278]	giv[277]|new[278]	_	_
40-13	5120-5128	problems	abstract[277]|abstract[278]	giv[277]|new[278]	_	_
40-14	5129-5131	of	abstract[277]|abstract[278]	giv[277]|new[278]	_	_
40-15	5132-5143	positioning	abstract[277]|abstract[278]|abstract[279]	giv[277]|new[278]|giv[279]	coref	41-3[281_279]
40-16	5144-5148	from	abstract[277]|abstract[278]|abstract[279]	giv[277]|new[278]|giv[279]	_	_
40-17	5149-5150	a	abstract[277]|abstract[278]|abstract[279]|object[280]	giv[277]|new[278]|giv[279]|new[280]	coref	42-6[0_280]
40-18	5151-5154	set	abstract[277]|abstract[278]|abstract[279]|object[280]	giv[277]|new[278]|giv[279]|new[280]	_	_
40-19	5155-5157	of	abstract[277]|abstract[278]|abstract[279]|object[280]	giv[277]|new[278]|giv[279]|new[280]	_	_
40-20	5158-5165	squared	abstract[277]|abstract[278]|abstract[279]|object[280]	giv[277]|new[278]|giv[279]|new[280]	_	_
40-21	5166-5172	planar	abstract[277]|abstract[278]|abstract[279]|object[280]	giv[277]|new[278]|giv[279]|new[280]	_	_
40-22	5173-5180	markers	abstract[277]|abstract[278]|abstract[279]|object[280]	giv[277]|new[278]|giv[279]|new[280]	_	_
40-23	5181-5182	.	_	_	_	_

#Text=However , positioning from a planar marker suffers from the ambiguity problem .
41-1	5183-5190	However	_	_	_	_
41-2	5191-5192	,	_	_	_	_
41-3	5193-5204	positioning	abstract[281]	giv[281]	_	_
41-4	5205-5209	from	abstract[281]	giv[281]	_	_
41-5	5210-5211	a	abstract[281]|abstract[282]	giv[281]|giv[282]	_	_
41-6	5212-5218	planar	abstract[281]|abstract[282]	giv[281]|giv[282]	_	_
41-7	5219-5225	marker	abstract[281]|abstract[282]	giv[281]|giv[282]	_	_
41-8	5226-5233	suffers	_	_	_	_
41-9	5234-5238	from	_	_	_	_
41-10	5239-5242	the	abstract[284]	new[284]	_	_
41-11	5243-5252	ambiguity	abstract|abstract[284]	new|new[284]	_	_
41-12	5253-5260	problem	abstract[284]	new[284]	_	_
41-13	5261-5262	.	_	_	_	_

#Text=Since these methods require posting markers in the environments , they are not suitable for places such as shopping malls that maintain a clean appearance .
42-1	5263-5268	Since	_	_	_	_
42-2	5269-5274	these	abstract[285]	giv[285]	ana	42-11[0_285]
42-3	5275-5282	methods	abstract[285]	giv[285]	_	_
42-4	5283-5290	require	_	_	_	_
42-5	5291-5298	posting	_	_	_	_
42-6	5299-5306	markers	object	giv	_	_
42-7	5307-5309	in	_	_	_	_
42-8	5310-5313	the	place[287]	giv[287]	_	_
42-9	5314-5326	environments	place[287]	giv[287]	_	_
42-10	5327-5328	,	_	_	_	_
42-11	5329-5333	they	abstract	giv	_	_
42-12	5334-5337	are	_	_	_	_
42-13	5338-5341	not	_	_	_	_
42-14	5342-5350	suitable	_	_	_	_
42-15	5351-5354	for	_	_	_	_
42-16	5355-5361	places	place[289]	new[289]	_	_
42-17	5362-5366	such	place[289]	new[289]	_	_
42-18	5367-5369	as	place[289]	new[289]	_	_
42-19	5370-5378	shopping	place[289]|event|place[291]	new[289]|new|new[291]	_	_
42-20	5379-5384	malls	place[289]|place[291]	new[289]|new[291]	_	_
42-21	5385-5389	that	place[289]|place[291]	new[289]|new[291]	_	_
42-22	5390-5398	maintain	place[289]|place[291]	new[289]|new[291]	_	_
42-23	5399-5400	a	place[289]|place[291]|abstract[292]	new[289]|new[291]|new[292]	_	_
42-24	5401-5406	clean	place[289]|place[291]|abstract[292]	new[289]|new[291]|new[292]	_	_
42-25	5407-5417	appearance	place[289]|place[291]|abstract[292]	new[289]|new[291]|new[292]	_	_
42-26	5418-5419	.	_	_	_	_
