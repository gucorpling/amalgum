#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=3. Advantages of Deep Learning Methods
1-1	0-2	3.	_	_	_	_
1-2	3-13	Advantages	abstract[1]	new[1]	_	_
1-3	14-16	of	abstract[1]	new[1]	_	_
1-4	17-21	Deep	abstract[1]|person[2]|abstract[3]	new[1]|new[2]|new[3]	coref	2-7[9_3]
1-5	22-30	Learning	abstract[1]|person[2]|abstract[3]	new[1]|new[2]|new[3]	_	_
1-6	31-38	Methods	abstract[1]|abstract[3]	new[1]|new[3]	_	_

#Text=To overcome the aforementioned challenges , various image processing and machine learning methods have been proposed and have so far achieved great progress .
2-1	39-41	To	_	_	_	_
2-2	42-50	overcome	_	_	_	_
2-3	51-54	the	abstract[4]	new[4]	_	_
2-4	55-69	aforementioned	abstract[4]	new[4]	_	_
2-5	70-80	challenges	abstract[4]	new[4]	_	_
2-6	81-82	,	_	_	_	_
2-7	83-90	various	abstract[9]	giv[9]	coref	3-11[14_9]
2-8	91-96	image	abstract|abstract[6]|abstract[9]	new|new[6]|giv[9]	coref	4-18
2-9	97-107	processing	abstract[6]|abstract[9]	new[6]|giv[9]	_	_
2-10	108-111	and	abstract[9]	giv[9]	_	_
2-11	112-119	machine	object|abstract[8]|abstract[9]	new|new[8]|giv[9]	coref|coref	3-12[0_8]|21-5
2-12	120-128	learning	abstract[8]|abstract[9]	new[8]|giv[9]	_	_
2-13	129-136	methods	abstract[9]	giv[9]	_	_
2-14	137-141	have	_	_	_	_
2-15	142-146	been	_	_	_	_
2-16	147-155	proposed	_	_	_	_
2-17	156-159	and	_	_	_	_
2-18	160-164	have	_	_	_	_
2-19	165-167	so	_	_	_	_
2-20	168-171	far	_	_	_	_
2-21	172-180	achieved	_	_	_	_
2-22	181-186	great	abstract[10]	new[10]	ana	3-3[0_10]
2-23	187-195	progress	abstract[10]	new[10]	_	_
2-24	196-197	.	_	_	_	_

#Text=However , it is important to note the advantages of deep learning methods over non-deep-learning methods ( also called shallow-learning methods ) .
3-1	198-205	However	_	_	_	_
3-2	206-207	,	_	_	_	_
3-3	208-210	it	abstract	giv	_	_
3-4	211-213	is	_	_	_	_
3-5	214-223	important	_	_	_	_
3-6	224-226	to	_	_	_	_
3-7	227-231	note	_	_	_	_
3-8	232-235	the	abstract[12]	new[12]	_	_
3-9	236-246	advantages	abstract[12]	new[12]	_	_
3-10	247-249	of	abstract[12]	new[12]	_	_
3-11	250-254	deep	abstract[12]|abstract[14]	new[12]|giv[14]	coref	3-15[15_14]
3-12	255-263	learning	abstract[12]|abstract|abstract[14]	new[12]|giv|giv[14]	coref	4-14[20_0]
3-13	264-271	methods	abstract[12]|abstract[14]	new[12]|giv[14]	_	_
3-14	272-276	over	abstract[12]	new[12]	_	_
3-15	277-294	non-deep-learning	abstract[12]|abstract[15]	new[12]|giv[15]	coref	3-20[17_15]
3-16	295-302	methods	abstract[12]|abstract[15]	new[12]|giv[15]	_	_
3-17	303-304	(	abstract[12]|abstract[15]	new[12]|giv[15]	_	_
3-18	305-309	also	abstract[12]|abstract[15]	new[12]|giv[15]	_	_
3-19	310-316	called	abstract[12]|abstract[15]	new[12]|giv[15]	_	_
3-20	317-333	shallow-learning	abstract[12]|abstract[15]|abstract|abstract[17]	new[12]|giv[15]|new|giv[17]	coref	21-4[147_17]
3-21	334-341	methods	abstract[12]|abstract[15]|abstract[17]	new[12]|giv[15]|giv[17]	_	_
3-22	342-343	)	abstract[12]|abstract[15]	new[12]|giv[15]	_	_
3-23	344-345	.	_	_	_	_

#Text=Currently , convolutional neural networks ( CNNs ) are the most frequently used deep learning model for image data classification , including tumor detection in pathology images of breast cancer , renal cell carcinoma , prostate cancer , and head and neck cancer .
4-1	346-355	Currently	_	_	_	_
4-2	356-357	,	_	_	_	_
4-3	358-371	convolutional	abstract[18]	new[18]	appos	4-7[0_18]
4-4	372-378	neural	abstract[18]	new[18]	_	_
4-5	379-387	networks	abstract[18]	new[18]	_	_
4-6	388-389	(	_	_	_	_
4-7	390-394	CNNs	abstract	giv	coref	4-10[21_0]
4-8	395-396	)	_	_	_	_
4-9	397-400	are	_	_	_	_
4-10	401-404	the	abstract[21]	giv[21]	coref	5-10[0_21]
4-11	405-409	most	abstract[21]	giv[21]	_	_
4-12	410-420	frequently	abstract[21]	giv[21]	_	_
4-13	421-425	used	abstract[21]	giv[21]	_	_
4-14	426-430	deep	abstract[20]|abstract[21]	giv[20]|giv[21]	coref	7-7[56_20]
4-15	431-439	learning	abstract[20]|abstract[21]	giv[20]|giv[21]	_	_
4-16	440-445	model	abstract[21]	giv[21]	_	_
4-17	446-449	for	abstract[21]	giv[21]	_	_
4-18	450-455	image	abstract[21]|abstract|abstract[23]|abstract[24]	giv[21]|giv|new[23]|new[24]	coref|coref|coref	5-12|6-27[53_24]|19-25[134_23]
4-19	456-460	data	abstract[21]|abstract[23]|abstract[24]	giv[21]|new[23]|new[24]	_	_
4-20	461-475	classification	abstract[21]|abstract[24]	giv[21]|new[24]	_	_
4-21	476-477	,	abstract[21]	giv[21]	_	_
4-22	478-487	including	abstract[21]	giv[21]	_	_
4-23	488-493	tumor	abstract[21]|abstract|abstract[26]	giv[21]|new|new[26]	coref	7-30[67_26]
4-24	494-503	detection	abstract[21]|abstract[26]	giv[21]|new[26]	_	_
4-25	504-506	in	abstract[21]|abstract[26]	giv[21]|new[26]	_	_
4-26	507-516	pathology	abstract[21]|abstract[26]|abstract|abstract[28]	giv[21]|new[26]|new|new[28]	coref|coref	7-24|7-24[63_28]
4-27	517-523	images	abstract[21]|abstract[26]|abstract[28]	giv[21]|new[26]|new[28]	_	_
4-28	524-526	of	abstract[21]|abstract[26]|abstract[28]	giv[21]|new[26]|new[28]	_	_
4-29	527-533	breast	abstract[21]|abstract[26]|abstract[28]|abstract[29]	giv[21]|new[26]|new[28]|new[29]	_	_
4-30	534-540	cancer	abstract[21]|abstract[26]|abstract[28]|abstract[29]	giv[21]|new[26]|new[28]|new[29]	_	_
4-31	541-542	,	abstract[21]|abstract[26]|abstract[28]	giv[21]|new[26]|new[28]	_	_
4-32	543-548	renal	abstract[21]|abstract[26]|abstract[28]|object[31]	giv[21]|new[26]|new[28]|new[31]	_	_
4-33	549-553	cell	abstract[21]|abstract[26]|abstract[28]|place|object[31]	giv[21]|new[26]|new[28]|new|new[31]	coref	7-30
4-34	554-563	carcinoma	abstract[21]|abstract[26]|abstract[28]|object[31]	giv[21]|new[26]|new[28]|new[31]	_	_
4-35	564-565	,	abstract[21]|abstract[26]|abstract[28]	giv[21]|new[26]|new[28]	_	_
4-36	566-574	prostate	abstract[21]|abstract[26]|abstract[28]|place|abstract[33]	giv[21]|new[26]|new[28]|new|new[33]	coref	4-40[36_33]
4-37	575-581	cancer	abstract[21]|abstract[26]|abstract[28]|abstract[33]	giv[21]|new[26]|new[28]|new[33]	_	_
4-38	582-583	,	abstract[21]	giv[21]	_	_
4-39	584-587	and	abstract[21]	giv[21]	_	_
4-40	588-592	head	abstract[21]|object|abstract[36]	giv[21]|new|giv[36]	_	_
4-41	593-596	and	abstract[21]|abstract[36]	giv[21]|giv[36]	_	_
4-42	597-601	neck	abstract[21]|object|abstract[36]	giv[21]|new|giv[36]	_	_
4-43	602-608	cancer	abstract[21]|abstract[36]	giv[21]|giv[36]	_	_
4-44	609-610	.	_	_	_	_

#Text=Several forms of neural network have been derived from CNNs for image segmentation , including fully convolutional networks ( FCNs ) and mask-regional convolutional neural networks ( mask-RCNNs ) .
5-1	611-618	Several	abstract[37]	new[37]	_	_
5-2	619-624	forms	abstract[37]	new[37]	_	_
5-3	625-627	of	abstract[37]	new[37]	_	_
5-4	628-634	neural	abstract[37]|abstract[38]	new[37]|new[38]	_	_
5-5	635-642	network	abstract[37]|abstract[38]	new[37]|new[38]	_	_
5-6	643-647	have	_	_	_	_
5-7	648-652	been	_	_	_	_
5-8	653-660	derived	_	_	_	_
5-9	661-665	from	_	_	_	_
5-10	666-670	CNNs	abstract	giv	coref	5-16[42_0]
5-11	671-674	for	_	_	_	_
5-12	675-680	image	abstract|abstract[41]	giv|new[41]	coref|coref	6-28|6-31[55_41]
5-13	681-693	segmentation	abstract[41]	new[41]	_	_
5-14	694-695	,	_	_	_	_
5-15	696-705	including	_	_	_	_
5-16	706-711	fully	abstract[42]	giv[42]	appos	5-20[0_42]
5-17	712-725	convolutional	abstract[42]	giv[42]	_	_
5-18	726-734	networks	abstract[42]	giv[42]	_	_
5-19	735-736	(	_	_	_	_
5-20	737-741	FCNs	abstract	giv	coref	5-23[44_0]
5-21	742-743	)	_	_	_	_
5-22	744-747	and	_	_	_	_
5-23	748-761	mask-regional	abstract[44]	giv[44]	appos	5-28[0_44]
5-24	762-775	convolutional	abstract[44]	giv[44]	_	_
5-25	776-782	neural	abstract[44]	giv[44]	_	_
5-26	783-791	networks	abstract[44]	giv[44]	_	_
5-27	792-793	(	_	_	_	_
5-28	794-804	mask-RCNNs	abstract	giv	coref	6-1[46_0]
5-29	805-806	)	_	_	_	_
5-30	807-808	.	_	_	_	_

#Text=Recurrent neural networks ( RNNs ) , which are well known for modeling dynamic sequence behavior such as speech recognition , have also been explored in multi-label image classification and image segmentation .
6-1	809-818	Recurrent	abstract[46]	giv[46]	appos	6-5[0_46]
6-2	819-825	neural	abstract[46]	giv[46]	_	_
6-3	826-834	networks	abstract[46]	giv[46]	_	_
6-4	835-836	(	_	_	_	_
6-5	837-841	RNNs	abstract	giv	coref	8-4[72_0]
6-6	842-843	)	_	_	_	_
6-7	844-845	,	_	_	_	_
6-8	846-851	which	_	_	_	_
6-9	852-855	are	_	_	_	_
6-10	856-860	well	_	_	_	_
6-11	861-866	known	_	_	_	_
6-12	867-870	for	_	_	_	_
6-13	871-879	modeling	_	_	_	_
6-14	880-887	dynamic	abstract[49]	new[49]	_	_
6-15	888-896	sequence	abstract|abstract[49]	new|new[49]	_	_
6-16	897-905	behavior	abstract[49]	new[49]	_	_
6-17	906-910	such	abstract[49]	new[49]	_	_
6-18	911-913	as	abstract[49]	new[49]	_	_
6-19	914-920	speech	abstract[49]|abstract|abstract[51]	new[49]|new|new[51]	_	_
6-20	921-932	recognition	abstract[49]|abstract[51]	new[49]|new[51]	_	_
6-21	933-934	,	_	_	_	_
6-22	935-939	have	_	_	_	_
6-23	940-944	also	_	_	_	_
6-24	945-949	been	_	_	_	_
6-25	950-958	explored	_	_	_	_
6-26	959-961	in	_	_	_	_
6-27	962-973	multi-label	abstract[53]	giv[53]	_	_
6-28	974-979	image	abstract|abstract[53]	giv|giv[53]	coref	6-31
6-29	980-994	classification	abstract[53]	giv[53]	_	_
6-30	995-998	and	_	_	_	_
6-31	999-1004	image	abstract|abstract[55]	giv|giv[55]	coref	7-34
6-32	1005-1017	segmentation	abstract[55]	giv[55]	_	_
6-33	1018-1019	.	_	_	_	_

#Text=In additional to the aforementioned supervised deep learning models , autoencoder , an unsupervised deep learning model , has shown ability in analyzing pathology images through pre-training models , cell detection , and image feature extraction .
7-1	1020-1022	In	_	_	_	_
7-2	1023-1033	additional	_	_	_	_
7-3	1034-1036	to	_	_	_	_
7-4	1037-1040	the	abstract[57]	new[57]	coref	7-27[64_57]
7-5	1041-1055	aforementioned	abstract[57]	new[57]	_	_
7-6	1056-1066	supervised	abstract[57]	new[57]	_	_
7-7	1067-1071	deep	abstract[56]|abstract[57]	giv[56]|new[57]	coref	7-15[59_56]
7-8	1072-1080	learning	abstract[56]|abstract[57]	giv[56]|new[57]	_	_
7-9	1081-1087	models	abstract[57]	new[57]	_	_
7-10	1088-1089	,	_	_	_	_
7-11	1090-1101	autoencoder	abstract	new	appos	7-13[60_0]
7-12	1102-1103	,	_	_	_	_
7-13	1104-1106	an	abstract[60]	giv[60]	coref	16-2[100_60]
7-14	1107-1119	unsupervised	abstract[60]	giv[60]	_	_
7-15	1120-1124	deep	abstract[59]|abstract[60]	giv[59]|giv[60]	coref	13-17[86_59]
7-16	1125-1133	learning	abstract[59]|abstract[60]	giv[59]|giv[60]	_	_
7-17	1134-1139	model	abstract[60]	giv[60]	_	_
7-18	1140-1141	,	_	_	_	_
7-19	1142-1145	has	_	_	_	_
7-20	1146-1151	shown	_	_	_	_
7-21	1152-1159	ability	abstract[61]	new[61]	_	_
7-22	1160-1162	in	abstract[61]	new[61]	_	_
7-23	1163-1172	analyzing	abstract[61]	new[61]	_	_
7-24	1173-1182	pathology	abstract[61]|abstract|abstract[63]	new[61]|giv|giv[63]	coref|coref	25-6|25-18[178_63]
7-25	1183-1189	images	abstract[61]|abstract[63]	new[61]|giv[63]	_	_
7-26	1190-1197	through	abstract[61]	new[61]	_	_
7-27	1198-1210	pre-training	abstract[61]|abstract[64]|abstract[65]	new[61]|giv[64]|giv[65]	coref|coref	7-27[65_64]|21-25[150_65]
7-28	1211-1217	models	abstract[61]|abstract[64]|abstract[65]	new[61]|giv[64]|giv[65]	_	_
7-29	1218-1219	,	abstract[61]|abstract[65]	new[61]|giv[65]	_	_
7-30	1220-1224	cell	abstract[61]|abstract[65]|place|abstract[67]	new[61]|giv[65]|giv|giv[67]	_	_
7-31	1225-1234	detection	abstract[61]|abstract[65]|abstract[67]	new[61]|giv[65]|giv[67]	_	_
7-32	1235-1236	,	abstract[61]|abstract[65]	new[61]|giv[65]	_	_
7-33	1237-1240	and	abstract[61]|abstract[65]	new[61]|giv[65]	_	_
7-34	1241-1246	image	abstract[61]|abstract[65]|abstract|abstract[70]	new[61]|giv[65]|giv|new[70]	coref|coref	8-10|19-19[132_70]
7-35	1247-1254	feature	abstract[61]|abstract[65]|abstract|abstract[70]	new[61]|giv[65]|new|new[70]	coref	16-10[103_0]
7-36	1255-1265	extraction	abstract[61]|abstract[65]|abstract[70]	new[61]|giv[65]|new[70]	_	_
7-37	1266-1267	.	_	_	_	_

#Text=The taxonomy of the common neural networks used in image analysis is summarized in
8-1	1268-1271	The	abstract[71]	new[71]	_	_
8-2	1272-1280	taxonomy	abstract[71]	new[71]	_	_
8-3	1281-1283	of	abstract[71]	new[71]	_	_
8-4	1284-1287	the	abstract[71]|abstract[72]	new[71]|giv[72]	coref	12-6[81_72]
8-5	1288-1294	common	abstract[71]|abstract[72]	new[71]|giv[72]	_	_
8-6	1295-1301	neural	abstract[71]|abstract[72]	new[71]|giv[72]	_	_
8-7	1302-1310	networks	abstract[71]|abstract[72]	new[71]|giv[72]	_	_
8-8	1311-1315	used	abstract[71]|abstract[72]	new[71]|giv[72]	_	_
8-9	1316-1318	in	abstract[71]|abstract[72]	new[71]|giv[72]	_	_
8-10	1319-1324	image	abstract[71]|abstract[72]|abstract|abstract[74]	new[71]|giv[72]|giv|new[74]	coref|coref	18-26|18-26[125_74]
8-11	1325-1333	analysis	abstract[71]|abstract[72]|abstract[74]	new[71]|giv[72]|new[74]	_	_
8-12	1334-1336	is	_	_	_	_
8-13	1337-1347	summarized	_	_	_	_
8-14	1348-1350	in	_	_	_	_

#Text=Figure 2
9-1	1351-1357	Figure	abstract[75]	new[75]	_	_
9-2	1358-1359	2	abstract[75]	new[75]	_	_

#Text=.
10-1	1360-1361	.	_	_	_	_

#Text=3.1.
11-1	1362-1366	3.1.	abstract	new	_	_

#Text=Inherent Characteristics and Advantages of Convolutional Neural Networks ( CNNs )
12-1	1367-1375	Inherent	abstract[77]|abstract[78]	new[77]|new[78]	coref	19-6[129_78]
12-2	1376-1391	Characteristics	abstract[77]|abstract[78]	new[77]|new[78]	_	_
12-3	1392-1395	and	abstract[78]	new[78]	_	_
12-4	1396-1406	Advantages	abstract[78]|abstract[79]	new[78]|new[79]	coref	22-10[155_79]
12-5	1407-1409	of	abstract[78]|abstract[79]	new[78]|new[79]	_	_
12-6	1410-1423	Convolutional	abstract[78]|abstract[79]|abstract[81]	new[78]|new[79]|giv[81]	coref	12-10[0_81]
12-7	1424-1430	Neural	abstract[78]|abstract[79]|abstract|abstract[81]	new[78]|new[79]|new|giv[81]	_	_
12-8	1431-1439	Networks	abstract[78]|abstract[79]|abstract[81]	new[78]|new[79]|giv[81]	_	_
12-9	1440-1441	(	_	_	_	_
12-10	1442-1446	CNNs	abstract	giv	coref	13-10[85_0]
12-11	1447-1448	)	_	_	_	_

#Text=Inspired by the working mechanisms of the brain , deep neural networks , also called “ deep learning ” , have one or more “ hidden ” layers between the input and output layers .
13-1	1449-1457	Inspired	_	_	_	_
13-2	1458-1460	by	_	_	_	_
13-3	1461-1464	the	abstract[83]	new[83]	_	_
13-4	1465-1472	working	abstract[83]	new[83]	_	_
13-5	1473-1483	mechanisms	abstract[83]	new[83]	_	_
13-6	1484-1486	of	abstract[83]	new[83]	_	_
13-7	1487-1490	the	abstract[83]|abstract[84]	new[83]|new[84]	_	_
13-8	1491-1496	brain	abstract[83]|abstract[84]	new[83]|new[84]	_	_
13-9	1497-1498	,	_	_	_	_
13-10	1499-1503	deep	abstract[85]	giv[85]	coref	20-4[141_85]
13-11	1504-1510	neural	abstract[85]	giv[85]	_	_
13-12	1511-1519	networks	abstract[85]	giv[85]	_	_
13-13	1520-1521	,	abstract[85]	giv[85]	_	_
13-14	1522-1526	also	abstract[85]	giv[85]	_	_
13-15	1527-1533	called	abstract[85]	giv[85]	_	_
13-16	1534-1535	“	abstract[85]	giv[85]	_	_
13-17	1536-1540	deep	abstract[85]|abstract[86]	giv[85]|giv[86]	coref	19-2[127_86]
13-18	1541-1549	learning	abstract[85]|abstract[86]	giv[85]|giv[86]	_	_
13-19	1550-1551	”	abstract[85]	giv[85]	_	_
13-20	1552-1553	,	_	_	_	_
13-21	1554-1558	have	_	_	_	_
13-22	1559-1562	one	abstract[87]	new[87]	coref	13-30[90_87]
13-23	1563-1565	or	abstract[87]	new[87]	_	_
13-24	1566-1570	more	abstract[87]	new[87]	_	_
13-25	1571-1572	“	abstract[87]	new[87]	_	_
13-26	1573-1579	hidden	abstract[87]	new[87]	_	_
13-27	1580-1581	”	abstract[87]	new[87]	_	_
13-28	1582-1588	layers	abstract[87]	new[87]	_	_
13-29	1589-1596	between	abstract[87]	new[87]	_	_
13-30	1597-1600	the	abstract[87]|abstract[90]	new[87]|giv[90]	coref	19-34[136_90]
13-31	1601-1606	input	abstract[87]|abstract|abstract[90]	new[87]|new|giv[90]	coref	16-26
13-32	1607-1610	and	abstract[87]|abstract[90]	new[87]|giv[90]	_	_
13-33	1611-1617	output	abstract[87]|abstract|abstract[90]	new[87]|new|giv[90]	coref	15-14[98_0]
13-34	1618-1624	layers	abstract[87]|abstract[90]	new[87]|giv[90]	_	_
13-35	1625-1626	.	_	_	_	_

#Text=In each layer , there are many neurons , also called kernels .
14-1	1627-1629	In	_	_	_	_
14-2	1630-1634	each	abstract[91]	new[91]	coref	17-15[112_91]
14-3	1635-1640	layer	abstract[91]	new[91]	_	_
14-4	1641-1642	,	_	_	_	_
14-5	1643-1648	there	_	_	_	_
14-6	1649-1652	are	_	_	_	_
14-7	1653-1657	many	abstract[92]	new[92]	_	_
14-8	1658-1665	neurons	abstract[92]	new[92]	_	_
14-9	1666-1667	,	abstract[92]	new[92]	_	_
14-10	1668-1672	also	abstract[92]	new[92]	_	_
14-11	1673-1679	called	abstract[92]	new[92]	_	_
14-12	1680-1687	kernels	abstract[92]|abstract	new[92]|new	coref	19-37[137_0]
14-13	1688-1689	.	_	_	_	_

#Text=Each kernel ( usually a function in mathematics ) takes inputs and computes an output .
15-1	1690-1694	Each	abstract[94]	new[94]	appos	15-4[95_94]
15-2	1695-1701	kernel	abstract[94]	new[94]	_	_
15-3	1702-1703	(	_	_	_	_
15-4	1704-1711	usually	abstract[95]	giv[95]	coref	16-6[102_95]
15-5	1712-1713	a	abstract[95]	giv[95]	_	_
15-6	1714-1722	function	abstract[95]	giv[95]	_	_
15-7	1723-1725	in	abstract[95]	giv[95]	_	_
15-8	1726-1737	mathematics	abstract[95]|abstract	giv[95]|new	_	_
15-9	1738-1739	)	_	_	_	_
15-10	1740-1745	takes	_	_	_	_
15-11	1746-1752	inputs	abstract	new	_	_
15-12	1753-1756	and	_	_	_	_
15-13	1757-1765	computes	abstract	new	none|none	15-13[0_209]|16-9
15-14	1766-1768	an	abstract[98]	giv[98]	_	_
15-15	1769-1775	output	abstract[98]	giv[98]	_	_
15-16	1776-1777	.	_	_	_	_

#Text=In a CNN model , a convolution kernel computes a feature at a specific location , called a “ receptive field ” , in the input space .
16-1	1778-1780	In	_	_	_	_
16-2	1781-1782	a	abstract[100]	giv[100]	coref	19-1[128_100]
16-3	1783-1786	CNN	organization|abstract[100]	new|giv[100]	coref	18-20
16-4	1787-1792	model	abstract[100]	giv[100]	_	_
16-5	1793-1794	,	_	_	_	_
16-6	1795-1796	a	abstract[102]	giv[102]	_	_
16-7	1797-1808	convolution	abstract|abstract[102]	new|giv[102]	coref	17-27
16-8	1809-1815	kernel	abstract[102]	giv[102]	_	_
16-9	1816-1824	computes	abstract	new	coref	27-3[197_0]
16-10	1825-1826	a	abstract[103]	giv[103]	coref	17-22[0_103]
16-11	1827-1834	feature	abstract[103]	giv[103]	_	_
16-12	1835-1837	at	_	_	_	_
16-13	1838-1839	a	abstract[104]	new[104]	ana	18-18[0_104]
16-14	1840-1848	specific	abstract[104]	new[104]	_	_
16-15	1849-1857	location	abstract[104]	new[104]	_	_
16-16	1858-1859	,	abstract[104]	new[104]	_	_
16-17	1860-1866	called	abstract[104]	new[104]	_	_
16-18	1867-1868	a	abstract[104]|abstract[105]	new[104]|new[105]	_	_
16-19	1869-1870	“	abstract[104]|abstract[105]	new[104]|new[105]	_	_
16-20	1871-1880	receptive	abstract[104]|abstract[105]	new[104]|new[105]	_	_
16-21	1881-1886	field	abstract[104]|abstract[105]	new[104]|new[105]	_	_
16-22	1887-1888	”	abstract[104]|abstract[105]	new[104]|new[105]	_	_
16-23	1889-1890	,	_	_	_	_
16-24	1891-1893	in	_	_	_	_
16-25	1894-1897	the	abstract[107]	new[107]	_	_
16-26	1898-1903	input	abstract|abstract[107]	giv|new[107]	coref	17-16
16-27	1904-1909	space	abstract[107]	new[107]	_	_
16-28	1910-1911	.	_	_	_	_

#Text=The term “ convolutional ” denotes the operation of sliding the receptive fields through the input layer to generate the “ feature map ” from the convolution layer as the outputs .
17-1	1912-1915	The	abstract[108]	new[108]	_	_
17-2	1916-1920	term	abstract[108]	new[108]	_	_
17-3	1921-1922	“	_	_	_	_
17-4	1923-1936	convolutional	_	_	_	_
17-5	1937-1938	”	_	_	_	_
17-6	1939-1946	denotes	_	_	_	_
17-7	1947-1950	the	event[109]	new[109]	coref	18-4[118_109]
17-8	1951-1960	operation	event[109]	new[109]	_	_
17-9	1961-1963	of	event[109]	new[109]	_	_
17-10	1964-1971	sliding	event[109]	new[109]	_	_
17-11	1972-1975	the	event[109]|abstract[110]	new[109]|new[110]	_	_
17-12	1976-1985	receptive	event[109]|abstract[110]	new[109]|new[110]	_	_
17-13	1986-1992	fields	event[109]|abstract[110]	new[109]|new[110]	_	_
17-14	1993-2000	through	event[109]	new[109]	_	_
17-15	2001-2004	the	event[109]|abstract[112]	new[109]|giv[112]	coref	17-26[116_112]
17-16	2005-2010	input	event[109]|abstract|abstract[112]	new[109]|giv|giv[112]	coref	24-40
17-17	2011-2016	layer	event[109]|abstract[112]	new[109]|giv[112]	_	_
17-18	2017-2019	to	_	_	_	_
17-19	2020-2028	generate	_	_	_	_
17-20	2029-2032	the	abstract[114]	new[114]	_	_
17-21	2033-2034	“	abstract[114]	new[114]	_	_
17-22	2035-2042	feature	abstract|abstract[114]	giv|new[114]	coref	24-2
17-23	2043-2046	map	abstract[114]	new[114]	_	_
17-24	2047-2048	”	abstract[114]	new[114]	_	_
17-25	2049-2053	from	_	_	_	_
17-26	2054-2057	the	abstract[116]	giv[116]	_	_
17-27	2058-2069	convolution	abstract|abstract[116]	giv|giv[116]	coref	34-16
17-28	2070-2075	layer	abstract[116]	giv[116]	_	_
17-29	2076-2078	as	_	_	_	_
17-30	2079-2082	the	abstract[117]	new[117]	_	_
17-31	2083-2090	outputs	abstract[117]	new[117]	_	_
17-32	2091-2092	.	_	_	_	_

#Text=In essence , this operation was inspired by the functional mechanism of the visual cortex , and it makes CNN a great solution for many image analysis tasks .
18-1	2093-2095	In	_	_	_	_
18-2	2096-2103	essence	_	_	_	_
18-3	2104-2105	,	_	_	_	_
18-4	2106-2110	this	event[118]	giv[118]	coref	34-16[250_118]
18-5	2111-2120	operation	event[118]	giv[118]	_	_
18-6	2121-2124	was	_	_	_	_
18-7	2125-2133	inspired	_	_	_	_
18-8	2134-2136	by	_	_	_	_
18-9	2137-2140	the	abstract[119]	new[119]	_	_
18-10	2141-2151	functional	abstract[119]	new[119]	_	_
18-11	2152-2161	mechanism	abstract[119]	new[119]	_	_
18-12	2162-2164	of	abstract[119]	new[119]	_	_
18-13	2165-2168	the	abstract[119]|abstract[120]	new[119]|new[120]	_	_
18-14	2169-2175	visual	abstract[119]|abstract[120]	new[119]|new[120]	_	_
18-15	2176-2182	cortex	abstract[119]|abstract[120]	new[119]|new[120]	_	_
18-16	2183-2184	,	_	_	_	_
18-17	2185-2188	and	_	_	_	_
18-18	2189-2191	it	abstract	giv	ana	19-13
18-19	2192-2197	makes	_	_	_	_
18-20	2198-2201	CNN	organization	giv	coref	34-1
18-21	2202-2203	a	abstract[123]	new[123]	_	_
18-22	2204-2209	great	abstract[123]	new[123]	_	_
18-23	2210-2218	solution	abstract[123]	new[123]	_	_
18-24	2219-2222	for	abstract[123]	new[123]	_	_
18-25	2223-2227	many	abstract[123]|abstract[126]	new[123]|new[126]	_	_
18-26	2228-2233	image	abstract[123]|abstract|abstract[125]|abstract[126]	new[123]|giv|giv[125]|new[126]	coref|coref	25-7|25-6[174_125]
18-27	2234-2242	analysis	abstract[123]|abstract[125]|abstract[126]	new[123]|giv[125]|new[126]	_	_
18-28	2243-2248	tasks	abstract[123]|abstract[126]	new[123]|new[126]	_	_
18-29	2249-2250	.	_	_	_	_

#Text=A deep learning model has two important characteristics : ( 1 ) it allows for the construction and extraction of flexible representational features from input data , and ( 2 ) it contains multiple layers and many kernels that enable it to approximate basically any complex functions using the extracted features .
19-1	2251-2252	A	abstract[128]	giv[128]	coref	29-24[215_128]
19-2	2253-2257	deep	abstract[127]|abstract[128]	giv[127]|giv[128]	coref	21-5[146_127]
19-3	2258-2266	learning	abstract[127]|abstract[128]	giv[127]|giv[128]	_	_
19-4	2267-2272	model	abstract[128]	giv[128]	_	_
19-5	2273-2276	has	_	_	_	_
19-6	2277-2280	two	abstract[129]	giv[129]	_	_
19-7	2281-2290	important	abstract[129]	giv[129]	_	_
19-8	2291-2306	characteristics	abstract[129]	giv[129]	_	_
19-9	2307-2308	:	abstract[129]	giv[129]	_	_
19-10	2309-2310	(	abstract[129]	giv[129]	_	_
19-11	2311-2312	1	abstract[129]	giv[129]	_	_
19-12	2313-2314	)	abstract[129]	giv[129]	_	_
19-13	2315-2317	it	abstract	giv	ana	19-32
19-14	2318-2324	allows	_	_	_	_
19-15	2325-2328	for	_	_	_	_
19-16	2329-2332	the	event[131]	new[131]	_	_
19-17	2333-2345	construction	event[131]	new[131]	_	_
19-18	2346-2349	and	_	_	_	_
19-19	2350-2360	extraction	abstract[132]	giv[132]	coref	24-1[161_132]
19-20	2361-2363	of	abstract[132]	giv[132]	_	_
19-21	2364-2372	flexible	abstract[132]|abstract[133]	giv[132]|new[133]	coref	19-49[140_133]
19-22	2373-2389	representational	abstract[132]|abstract[133]	giv[132]|new[133]	_	_
19-23	2390-2398	features	abstract[132]|abstract[133]	giv[132]|new[133]	_	_
19-24	2399-2403	from	abstract[132]|abstract[133]	giv[132]|new[133]	_	_
19-25	2404-2409	input	abstract[132]|abstract[133]|abstract[134]	giv[132]|new[133]|giv[134]	coref	24-40[168_134]
19-26	2410-2414	data	abstract[132]|abstract[133]|abstract[134]	giv[132]|new[133]|giv[134]	_	_
19-27	2415-2416	,	_	_	_	_
19-28	2417-2420	and	_	_	_	_
19-29	2421-2422	(	_	_	_	_
19-30	2423-2424	2	_	_	_	_
19-31	2425-2426	)	_	_	_	_
19-32	2427-2429	it	abstract	giv	ana	19-41
19-33	2430-2438	contains	_	_	_	_
19-34	2439-2447	multiple	abstract[136]	giv[136]	_	_
19-35	2448-2454	layers	abstract[136]	giv[136]	_	_
19-36	2455-2458	and	_	_	_	_
19-37	2459-2463	many	abstract[137]	giv[137]	_	_
19-38	2464-2471	kernels	abstract[137]	giv[137]	_	_
19-39	2472-2476	that	abstract[137]	giv[137]	_	_
19-40	2477-2483	enable	abstract[137]	giv[137]	_	_
19-41	2484-2486	it	abstract[137]|abstract	giv[137]|giv	_	_
19-42	2487-2489	to	abstract[137]	giv[137]	_	_
19-43	2490-2501	approximate	abstract[137]	giv[137]	_	_
19-44	2502-2511	basically	abstract[137]|abstract[139]	giv[137]|new[139]	_	_
19-45	2512-2515	any	abstract[137]|abstract[139]	giv[137]|new[139]	_	_
19-46	2516-2523	complex	abstract[137]|abstract[139]	giv[137]|new[139]	_	_
19-47	2524-2533	functions	abstract[137]|abstract[139]	giv[137]|new[139]	_	_
19-48	2534-2539	using	abstract[137]|abstract[139]	giv[137]|new[139]	_	_
19-49	2540-2543	the	abstract[137]|abstract[139]|abstract[140]	giv[137]|new[139]|giv[140]	coref	20-12[0_140]
19-50	2544-2553	extracted	abstract[137]|abstract[139]|abstract[140]	giv[137]|new[139]|giv[140]	_	_
19-51	2554-2562	features	abstract[137]|abstract[139]|abstract[140]	giv[137]|new[139]|giv[140]	_	_
19-52	2563-2564	.	_	_	_	_

#Text=In all , deep neural networks are capable of automatically extracting features and solving highly complex prediction problems .
20-1	2565-2567	In	_	_	_	_
20-2	2568-2571	all	_	_	_	_
20-3	2572-2573	,	_	_	_	_
20-4	2574-2578	deep	abstract[141]	giv[141]	coref	31-4[0_141]
20-5	2579-2585	neural	abstract[141]	giv[141]	_	_
20-6	2586-2594	networks	abstract[141]	giv[141]	_	_
20-7	2595-2598	are	_	_	_	_
20-8	2599-2606	capable	_	_	_	_
20-9	2607-2609	of	_	_	_	_
20-10	2610-2623	automatically	_	_	_	_
20-11	2624-2634	extracting	_	_	_	_
20-12	2635-2643	features	abstract	giv	coref	21-17[149_0]
20-13	2644-2647	and	_	_	_	_
20-14	2648-2655	solving	_	_	_	_
20-15	2656-2662	highly	abstract[144]	new[144]	coref	25-11[176_144]
20-16	2663-2670	complex	abstract[144]	new[144]	_	_
20-17	2671-2681	prediction	abstract|abstract[144]	new|new[144]	coref	24-22
20-18	2682-2690	problems	abstract[144]	new[144]	_	_
20-19	2691-2692	.	_	_	_	_

#Text=In contrast , traditional machine learning methods have two major steps : ( 1 ) defining the features , and ( 2 ) constructing models using these handcrafted features .
21-1	2693-2695	In	_	_	_	_
21-2	2696-2704	contrast	_	_	_	_
21-3	2705-2706	,	_	_	_	_
21-4	2707-2718	traditional	abstract[147]	giv[147]	coref	22-3[152_147]
21-5	2719-2726	machine	object|abstract[146]|abstract[147]	giv|giv[146]|giv[147]	coref	22-6[153_146]
21-6	2727-2735	learning	abstract[146]|abstract[147]	giv[146]|giv[147]	_	_
21-7	2736-2743	methods	abstract[147]	giv[147]	_	_
21-8	2744-2748	have	_	_	_	_
21-9	2749-2752	two	abstract[148]	new[148]	coref	29-32[218_148]
21-10	2753-2758	major	abstract[148]	new[148]	_	_
21-11	2759-2764	steps	abstract[148]	new[148]	_	_
21-12	2765-2766	:	abstract[148]	new[148]	_	_
21-13	2767-2768	(	abstract[148]	new[148]	_	_
21-14	2769-2770	1	abstract[148]	new[148]	_	_
21-15	2771-2772	)	abstract[148]	new[148]	_	_
21-16	2773-2781	defining	abstract[148]	new[148]	_	_
21-17	2782-2785	the	abstract[148]|abstract[149]	new[148]|giv[149]	coref	21-27[151_149]
21-18	2786-2794	features	abstract[148]|abstract[149]	new[148]|giv[149]	_	_
21-19	2795-2796	,	abstract[148]	new[148]	_	_
21-20	2797-2800	and	abstract[148]	new[148]	_	_
21-21	2801-2802	(	abstract[148]	new[148]	_	_
21-22	2803-2804	2	abstract[148]	new[148]	_	_
21-23	2805-2806	)	abstract[148]	new[148]	_	_
21-24	2807-2819	constructing	abstract[148]	new[148]	_	_
21-25	2820-2826	models	abstract[148]|abstract[150]	new[148]|giv[150]	coref	22-6[154_150]
21-26	2827-2832	using	abstract[148]|abstract[150]	new[148]|giv[150]	_	_
21-27	2833-2838	these	abstract[148]|abstract[150]|abstract[151]	new[148]|giv[150]|giv[151]	coref	23-15[0_151]
21-28	2839-2850	handcrafted	abstract[148]|abstract[150]|abstract[151]	new[148]|giv[150]|giv[151]	_	_
21-29	2851-2859	features	abstract[148]|abstract[150]|abstract[151]	new[148]|giv[150]|giv[151]	_	_
21-30	2860-2861	.	_	_	_	_

#Text=Compared with traditional methods , deep learning models have the following advantages :
22-1	2862-2870	Compared	_	_	_	_
22-2	2871-2875	with	_	_	_	_
22-3	2876-2887	traditional	abstract[152]	giv[152]	coref	29-37[222_152]
22-4	2888-2895	methods	abstract[152]	giv[152]	_	_
22-5	2896-2897	,	_	_	_	_
22-6	2898-2902	deep	abstract[153]|abstract[154]	giv[153]|giv[154]	coref|coref	23-3[156_153]|23-3[157_154]
22-7	2903-2911	learning	abstract[153]|abstract[154]	giv[153]|giv[154]	_	_
22-8	2912-2918	models	abstract[154]	giv[154]	_	_
22-9	2919-2923	have	_	_	_	_
22-10	2924-2927	the	abstract[155]	giv[155]	_	_
22-11	2928-2937	following	abstract[155]	giv[155]	_	_
22-12	2938-2948	advantages	abstract[155]	giv[155]	_	_
22-13	2949-2950	:	_	_	_	_

#Text=First , deep learning models greatly simplify or remove the task of manually defining features .
23-1	2951-2956	First	_	_	_	_
23-2	2957-2958	,	_	_	_	_
23-3	2959-2963	deep	abstract[156]|abstract[157]	giv[156]|giv[157]	coref|coref	26-21[196_156]|33-1[237_157]
23-4	2964-2972	learning	abstract[156]|abstract[157]	giv[156]|giv[157]	_	_
23-5	2973-2979	models	abstract[157]	giv[157]	_	_
23-6	2980-2987	greatly	_	_	_	_
23-7	2988-2996	simplify	_	_	_	_
23-8	2997-2999	or	_	_	_	_
23-9	3000-3006	remove	_	_	_	_
23-10	3007-3010	the	abstract[158]	new[158]	_	_
23-11	3011-3015	task	abstract[158]	new[158]	_	_
23-12	3016-3018	of	abstract[158]	new[158]	_	_
23-13	3019-3027	manually	abstract[158]	new[158]	_	_
23-14	3028-3036	defining	abstract[158]	new[158]	_	_
23-15	3037-3045	features	abstract[158]|abstract	new[158]|giv	coref	25-51[189_0]
23-16	3046-3047	.	_	_	_	_

#Text=Manual feature extraction is very challenging and time consuming , especially in the following two scenarios : ( 1 ) the prediction problem is complex , and/or ( 2 ) there is limited prior knowledge about the relationship between input data and the outcomes to be predicted .
24-1	3048-3054	Manual	abstract[161]	giv[161]	coref	29-33[217_161]
24-2	3055-3062	feature	abstract|abstract[161]	giv|giv[161]	coref	29-33
24-3	3063-3073	extraction	abstract[161]	giv[161]	_	_
24-4	3074-3076	is	_	_	_	_
24-5	3077-3081	very	_	_	_	_
24-6	3082-3093	challenging	_	_	_	_
24-7	3094-3097	and	_	_	_	_
24-8	3098-3102	time	_	_	_	_
24-9	3103-3112	consuming	_	_	_	_
24-10	3113-3114	,	_	_	_	_
24-11	3115-3125	especially	abstract[162]	new[162]	coref	25-1[171_162]
24-12	3126-3128	in	abstract[162]	new[162]	_	_
24-13	3129-3132	the	abstract[162]	new[162]	_	_
24-14	3133-3142	following	abstract[162]	new[162]	_	_
24-15	3143-3146	two	abstract[162]	new[162]	_	_
24-16	3147-3156	scenarios	abstract[162]	new[162]	_	_
24-17	3157-3158	:	_	_	_	_
24-18	3159-3160	(	_	_	_	_
24-19	3161-3162	1	_	_	_	_
24-20	3163-3164	)	_	_	_	_
24-21	3165-3168	the	abstract[164]	new[164]	_	_
24-22	3169-3179	prediction	abstract|abstract[164]	giv|new[164]	coref	25-12
24-23	3180-3187	problem	abstract[164]	new[164]	_	_
24-24	3188-3190	is	_	_	_	_
24-25	3191-3198	complex	_	_	_	_
24-26	3199-3200	,	_	_	_	_
24-27	3201-3207	and/or	_	_	_	_
24-28	3208-3209	(	_	_	_	_
24-29	3210-3211	2	_	_	_	_
24-30	3212-3213	)	_	_	_	_
24-31	3214-3219	there	_	_	_	_
24-32	3220-3222	is	_	_	_	_
24-33	3223-3230	limited	abstract[165]	new[165]	coref	25-41[186_165]
24-34	3231-3236	prior	abstract[165]	new[165]	_	_
24-35	3237-3246	knowledge	abstract[165]	new[165]	_	_
24-36	3247-3252	about	abstract[165]	new[165]	_	_
24-37	3253-3256	the	abstract[165]|abstract[166]	new[165]|new[166]	_	_
24-38	3257-3269	relationship	abstract[165]|abstract[166]	new[165]|new[166]	_	_
24-39	3270-3277	between	abstract[165]|abstract[166]	new[165]|new[166]	_	_
24-40	3278-3283	input	abstract[165]|abstract[166]|abstract|abstract[168]|abstract[169]	new[165]|new[166]|giv|giv[168]|giv[169]	coref|coref	24-40[169_168]|33-6[239_169]
24-41	3284-3288	data	abstract[165]|abstract[166]|abstract[168]|abstract[169]	new[165]|new[166]|giv[168]|giv[169]	_	_
24-42	3289-3292	and	abstract[165]|abstract[166]|abstract[169]	new[165]|new[166]|giv[169]	_	_
24-43	3293-3296	the	abstract[165]|abstract[166]|abstract[169]|abstract[170]	new[165]|new[166]|giv[169]|new[170]	coref	25-22[180_170]
24-44	3297-3305	outcomes	abstract[165]|abstract[166]|abstract[169]|abstract[170]	new[165]|new[166]|giv[169]|new[170]	_	_
24-45	3306-3308	to	abstract[165]|abstract[166]|abstract[169]|abstract[170]	new[165]|new[166]|giv[169]|new[170]	_	_
24-46	3309-3311	be	abstract[165]|abstract[166]|abstract[169]|abstract[170]	new[165]|new[166]|giv[169]|new[170]	_	_
24-47	3312-3321	predicted	abstract[165]|abstract[166]|abstract[169]|abstract[170]	new[165]|new[166]|giv[169]|new[170]	_	_
24-48	3322-3323	.	_	_	_	_

#Text=Both scenarios are true of pathology image analysis , as the prediction problems ( such as using pathology images to predict patient outcomes or recognizing various tissue structures and cells from H&E-stained images ) are very complex , and despite the accumulated knowledge from pathologists , little is known about which quantitative image features predict the outcomes .
25-1	3324-3328	Both	abstract[171]	giv[171]	_	_
25-2	3329-3338	scenarios	abstract[171]	giv[171]	_	_
25-3	3339-3342	are	_	_	_	_
25-4	3343-3347	true	_	_	_	_
25-5	3348-3350	of	_	_	_	_
25-6	3351-3360	pathology	abstract|abstract[174]	giv|giv[174]	coref|coref	25-18|26-8[194_174]
25-7	3361-3366	image	abstract|abstract[174]	giv|giv[174]	coref	25-53
25-8	3367-3375	analysis	abstract[174]	giv[174]	_	_
25-9	3376-3377	,	_	_	_	_
25-10	3378-3380	as	_	_	_	_
25-11	3381-3384	the	abstract[176]	giv[176]	coref	30-15[227_176]
25-12	3385-3395	prediction	abstract|abstract[176]	giv|giv[176]	coref	30-18
25-13	3396-3404	problems	abstract[176]	giv[176]	_	_
25-14	3405-3406	(	abstract[176]	giv[176]	_	_
25-15	3407-3411	such	abstract[176]	giv[176]	_	_
25-16	3412-3414	as	abstract[176]	giv[176]	_	_
25-17	3415-3420	using	abstract[176]	giv[176]	_	_
25-18	3421-3430	pathology	abstract[176]|abstract|abstract[178]	giv[176]|giv|giv[178]	coref|coref	25-32[185_178]|26-8
25-19	3431-3437	images	abstract[176]|abstract[178]	giv[176]|giv[178]	_	_
25-20	3438-3440	to	abstract[176]	giv[176]	_	_
25-21	3441-3448	predict	abstract[176]	giv[176]	_	_
25-22	3449-3456	patient	abstract[176]|person|abstract[180]	giv[176]|new|giv[180]	coref	25-56[190_180]
25-23	3457-3465	outcomes	abstract[176]|abstract[180]	giv[176]|giv[180]	_	_
25-24	3466-3468	or	abstract[176]	giv[176]	_	_
25-25	3469-3480	recognizing	abstract[176]	giv[176]	_	_
25-26	3481-3488	various	abstract[176]|abstract[182]	giv[176]|new[182]	_	_
25-27	3489-3495	tissue	abstract[176]|object|abstract[182]	giv[176]|new|new[182]	_	_
25-28	3496-3506	structures	abstract[176]|abstract[182]	giv[176]|new[182]	_	_
25-29	3507-3510	and	abstract[176]	giv[176]	_	_
25-30	3511-3516	cells	abstract[176]|object	giv[176]|new	_	_
25-31	3517-3521	from	_	_	_	_
25-32	3522-3533	H&E-stained	event|abstract[185]	new|giv[185]	_	_
25-33	3534-3540	images	abstract[185]	giv[185]	_	_
25-34	3541-3542	)	_	_	_	_
25-35	3543-3546	are	_	_	_	_
25-36	3547-3551	very	_	_	_	_
25-37	3552-3559	complex	_	_	_	_
25-38	3560-3561	,	_	_	_	_
25-39	3562-3565	and	_	_	_	_
25-40	3566-3573	despite	_	_	_	_
25-41	3574-3577	the	abstract[186]	giv[186]	_	_
25-42	3578-3589	accumulated	abstract[186]	giv[186]	_	_
25-43	3590-3599	knowledge	abstract[186]	giv[186]	_	_
25-44	3600-3604	from	abstract[186]	giv[186]	_	_
25-45	3605-3617	pathologists	abstract[186]|person	giv[186]|new	_	_
25-46	3618-3619	,	_	_	_	_
25-47	3620-3626	little	_	_	_	_
25-48	3627-3629	is	_	_	_	_
25-49	3630-3635	known	_	_	_	_
25-50	3636-3641	about	_	_	_	_
25-51	3642-3647	which	abstract[189]	giv[189]	coref	30-9[224_189]
25-52	3648-3660	quantitative	abstract[189]	giv[189]	_	_
25-53	3661-3666	image	abstract|abstract[189]	giv|giv[189]	coref	26-9
25-54	3667-3675	features	abstract[189]	giv[189]	_	_
25-55	3676-3683	predict	_	_	_	_
25-56	3684-3687	the	abstract[190]	giv[190]	_	_
25-57	3688-3696	outcomes	abstract[190]	giv[190]	_	_
25-58	3697-3698	.	_	_	_	_

#Text=As a result , the advance of pathology image analysis had been slow and limited until the recent development of deep learning .
26-1	3699-3701	As	_	_	_	_
26-2	3702-3703	a	_	_	_	_
26-3	3704-3710	result	_	_	_	_
26-4	3711-3712	,	_	_	_	_
26-5	3713-3716	the	abstract[191]	new[191]	_	_
26-6	3717-3724	advance	abstract[191]	new[191]	_	_
26-7	3725-3727	of	abstract[191]	new[191]	_	_
26-8	3728-3737	pathology	abstract[191]|abstract|abstract[194]	new[191]|giv|giv[194]	coref|coref	34-50|34-50[262_194]
26-9	3738-3743	image	abstract[191]|abstract|abstract[194]	new[191]|giv|giv[194]	coref	29-11[212_0]
26-10	3744-3752	analysis	abstract[191]|abstract[194]	new[191]|giv[194]	_	_
26-11	3753-3756	had	_	_	_	_
26-12	3757-3761	been	_	_	_	_
26-13	3762-3766	slow	_	_	_	_
26-14	3767-3770	and	_	_	_	_
26-15	3771-3778	limited	_	_	_	_
26-16	3779-3784	until	_	_	_	_
26-17	3785-3788	the	event[195]	new[195]	_	_
26-18	3789-3795	recent	event[195]	new[195]	_	_
26-19	3796-3807	development	event[195]	new[195]	_	_
26-20	3808-3810	of	event[195]	new[195]	_	_
26-21	3811-3815	deep	event[195]|abstract[196]	new[195]|giv[196]	coref	27-7[0_196]
26-22	3816-3824	learning	event[195]|abstract[196]	new[195]|giv[196]	_	_
26-23	3825-3826	.	_	_	_	_

#Text=Second , the computation of deep learning algorithms can be highly parallel .
27-1	3827-3833	Second	_	_	_	_
27-2	3834-3835	,	_	_	_	_
27-3	3836-3839	the	abstract[197]	new[197]	coref	29-2[209_197]
27-4	3840-3851	computation	abstract[197]	new[197]	_	_
27-5	3852-3854	of	abstract[197]	new[197]	_	_
27-6	3855-3859	deep	abstract[197]|abstract[199]	new[197]|new[199]	_	_
27-7	3860-3868	learning	abstract[197]|abstract|abstract[199]	new[197]|giv|new[199]	coref	28-5[200_0]
27-8	3869-3879	algorithms	abstract[197]|abstract[199]	new[197]|new[199]	_	_
27-9	3880-3883	can	_	_	_	_
27-10	3884-3886	be	_	_	_	_
27-11	3887-3893	highly	_	_	_	_
27-12	3894-3902	parallel	_	_	_	_
27-13	3903-3904	.	_	_	_	_

#Text=As a result , deep learning can largely leverage the parallel computing power from the recent developments in GPU ( graphics processing unit ) hardware .
28-1	3905-3907	As	_	_	_	_
28-2	3908-3909	a	_	_	_	_
28-3	3910-3916	result	_	_	_	_
28-4	3917-3918	,	_	_	_	_
28-5	3919-3923	deep	abstract[200]	giv[200]	coref	29-25[214_200]
28-6	3924-3932	learning	abstract[200]	giv[200]	_	_
28-7	3933-3936	can	_	_	_	_
28-8	3937-3944	largely	_	_	_	_
28-9	3945-3953	leverage	_	_	_	_
28-10	3954-3957	the	abstract[202]	new[202]	coref	31-7[230_202]
28-11	3958-3966	parallel	abstract[202]	new[202]	_	_
28-12	3967-3976	computing	abstract|abstract[202]	new|new[202]	_	_
28-13	3977-3982	power	abstract[202]	new[202]	_	_
28-14	3983-3987	from	_	_	_	_
28-15	3988-3991	the	abstract[203]	new[203]	_	_
28-16	3992-3998	recent	abstract[203]	new[203]	_	_
28-17	3999-4011	developments	abstract[203]	new[203]	_	_
28-18	4012-4014	in	abstract[203]	new[203]	_	_
28-19	4015-4018	GPU	abstract[203]|abstract|abstract[208]	new[203]|new|new[208]	_	_
28-20	4019-4020	(	abstract[203]|abstract[208]	new[203]|new[208]	_	_
28-21	4021-4029	graphics	abstract[203]|abstract|object[207]|abstract[208]	new[203]|new|new[207]|new[208]	_	_
28-22	4030-4040	processing	abstract[203]|abstract|object[207]|abstract[208]	new[203]|new|new[207]|new[208]	coref	29-5
28-23	4041-4045	unit	abstract[203]|object[207]|abstract[208]	new[203]|new[207]|new[208]	_	_
28-24	4046-4047	)	abstract[203]|abstract[208]	new[203]|new[208]	_	_
28-25	4048-4056	hardware	abstract[203]|abstract[208]	new[203]|new[208]	_	_
28-26	4057-4058	.	_	_	_	_

#Text=With GPU-aided computation , processing ( classifying or segmenting ) a 1000 × 1000 pixels image usually takes less than one second for a deep learning model , much faster than traditional feature extraction steps and non-deep-learning-based image segmentation methods .
29-1	4059-4063	With	_	_	_	_
29-2	4064-4073	GPU-aided	abstract[209]	giv[209]	_	_
29-3	4074-4085	computation	abstract[209]	giv[209]	_	_
29-4	4086-4087	,	_	_	_	_
29-5	4088-4098	processing	abstract	giv	_	_
29-6	4099-4100	(	_	_	_	_
29-7	4101-4112	classifying	_	_	_	_
29-8	4113-4115	or	_	_	_	_
29-9	4116-4126	segmenting	_	_	_	_
29-10	4127-4128	)	_	_	_	_
29-11	4129-4130	a	abstract[212]	giv[212]	coref	29-38[0_212]
29-12	4131-4135	1000	abstract[211]|abstract[212]	new[211]|giv[212]	_	_
29-13	4136-4137	×	abstract[211]|abstract[212]	new[211]|giv[212]	_	_
29-14	4138-4142	1000	abstract[211]|abstract[212]	new[211]|giv[212]	_	_
29-15	4143-4149	pixels	abstract[211]|abstract[212]	new[211]|giv[212]	_	_
29-16	4150-4155	image	abstract[212]	giv[212]	_	_
29-17	4156-4163	usually	_	_	_	_
29-18	4164-4169	takes	_	_	_	_
29-19	4170-4174	less	time[213]	new[213]	_	_
29-20	4175-4179	than	time[213]	new[213]	_	_
29-21	4180-4183	one	time[213]	new[213]	_	_
29-22	4184-4190	second	time[213]	new[213]	_	_
29-23	4191-4194	for	_	_	_	_
29-24	4195-4196	a	abstract[215]	giv[215]	coref	33-16[242_215]
29-25	4197-4201	deep	abstract[214]|abstract[215]	giv[214]|giv[215]	coref	30-4[223_214]
29-26	4202-4210	learning	abstract[214]|abstract[215]	giv[214]|giv[215]	_	_
29-27	4211-4216	model	abstract[215]	giv[215]	_	_
29-28	4217-4218	,	_	_	_	_
29-29	4219-4223	much	_	_	_	_
29-30	4224-4230	faster	_	_	_	_
29-31	4231-4235	than	_	_	_	_
29-32	4236-4247	traditional	abstract[218]|abstract[219]	giv[218]|giv[219]	coref	29-32[219_218]
29-33	4248-4255	feature	abstract|abstract[217]|abstract[218]|abstract[219]	giv|giv[217]|giv[218]|giv[219]	_	_
29-34	4256-4266	extraction	abstract[217]|abstract[218]|abstract[219]	giv[217]|giv[218]|giv[219]	_	_
29-35	4267-4272	steps	abstract[218]|abstract[219]	giv[218]|giv[219]	_	_
29-36	4273-4276	and	abstract[219]	giv[219]	_	_
29-37	4277-4300	non-deep-learning-based	abstract[219]|abstract[222]	giv[219]|giv[222]	coref	32-4[235_222]
29-38	4301-4306	image	abstract[219]|abstract|abstract[221]|abstract[222]	giv[219]|giv|new[221]|giv[222]	coref	33-6
29-39	4307-4319	segmentation	abstract[219]|abstract[221]|abstract[222]	giv[219]|new[221]|giv[222]	_	_
29-40	4320-4327	methods	abstract[219]|abstract[222]	giv[219]|giv[222]	_	_
29-41	4328-4329	.	_	_	_	_

#Text=Furthermore , since deep learning does not require handcrafted features , it can handle much more complex prediction problems and is able to recognize multiple objects simultaneously .
30-1	4330-4341	Furthermore	_	_	_	_
30-2	4342-4343	,	_	_	_	_
30-3	4344-4349	since	_	_	_	_
30-4	4350-4354	deep	abstract[223]	giv[223]	ana	30-12[0_223]
30-5	4355-4363	learning	abstract[223]	giv[223]	_	_
30-6	4364-4368	does	_	_	_	_
30-7	4369-4372	not	_	_	_	_
30-8	4373-4380	require	_	_	_	_
30-9	4381-4392	handcrafted	abstract[224]	giv[224]	_	_
30-10	4393-4401	features	abstract[224]	giv[224]	_	_
30-11	4402-4403	,	_	_	_	_
30-12	4404-4406	it	abstract	giv	coref	32-4[234_0]
30-13	4407-4410	can	_	_	_	_
30-14	4411-4417	handle	_	_	_	_
30-15	4418-4422	much	abstract[227]	giv[227]	_	_
30-16	4423-4427	more	abstract[227]	giv[227]	_	_
30-17	4428-4435	complex	abstract[227]	giv[227]	_	_
30-18	4436-4446	prediction	abstract|abstract[227]	giv|giv[227]	coref	33-16
30-19	4447-4455	problems	abstract[227]	giv[227]	_	_
30-20	4456-4459	and	_	_	_	_
30-21	4460-4462	is	_	_	_	_
30-22	4463-4467	able	_	_	_	_
30-23	4468-4470	to	_	_	_	_
30-24	4471-4480	recognize	_	_	_	_
30-25	4481-4489	multiple	abstract[228]	new[228]	_	_
30-26	4490-4497	objects	abstract[228]	new[228]	_	_
30-27	4498-4512	simultaneously	_	_	_	_
30-28	4513-4514	.	_	_	_	_

#Text=For example , CNNs have shown great power in distinguishing as many as 1000 object categories .
31-1	4515-4518	For	_	_	_	_
31-2	4519-4526	example	_	_	_	_
31-3	4527-4528	,	_	_	_	_
31-4	4529-4533	CNNs	abstract	giv	_	_
31-5	4534-4538	have	_	_	_	_
31-6	4539-4544	shown	_	_	_	_
31-7	4545-4550	great	abstract[230]	giv[230]	_	_
31-8	4551-4556	power	abstract[230]	giv[230]	_	_
31-9	4557-4559	in	abstract[230]	giv[230]	_	_
31-10	4560-4574	distinguishing	abstract[230]	giv[230]	_	_
31-11	4575-4577	as	abstract[230]	giv[230]	_	_
31-12	4578-4582	many	abstract[230]	giv[230]	_	_
31-13	4583-4585	as	abstract[230]	giv[230]	_	_
31-14	4586-4590	1000	abstract[230]|abstract[232]	giv[230]|new[232]	_	_
31-15	4591-4597	object	abstract[230]|abstract|abstract[232]	giv[230]|new|new[232]	coref	34-6
31-16	4598-4608	categories	abstract[230]|abstract[232]	giv[230]|new[232]	_	_
31-17	4609-4610	.	_	_	_	_

#Text=Other advantages of deep learning methods include the following : ( 1 )
32-1	4611-4616	Other	abstract[233]	new[233]	_	_
32-2	4617-4627	advantages	abstract[233]	new[233]	_	_
32-3	4628-4630	of	abstract[233]	new[233]	_	_
32-4	4631-4635	deep	abstract[233]|abstract[234]|abstract[235]	new[233]|giv[234]|giv[235]	coref	33-1[236_234]
32-5	4636-4644	learning	abstract[233]|abstract[234]|abstract[235]	new[233]|giv[234]|giv[235]	_	_
32-6	4645-4652	methods	abstract[233]|abstract[235]	new[233]|giv[235]	_	_
32-7	4653-4660	include	_	_	_	_
32-8	4661-4664	the	_	_	_	_
32-9	4665-4674	following	_	_	_	_
32-10	4675-4676	:	_	_	_	_
32-11	4677-4678	(	_	_	_	_
32-12	4679-4680	1	_	_	_	_
32-13	4681-4682	)	_	_	_	_

#Text=deep learning models fully utilize image data , as every pixel can be utilized in prediction model ; ( 2 )
33-1	4683-4687	deep	abstract[236]|abstract[237]	giv[236]|giv[237]	coref	34-1[244_237]
33-2	4688-4696	learning	abstract[236]|abstract[237]	giv[236]|giv[237]	_	_
33-3	4697-4703	models	abstract[237]	giv[237]	_	_
33-4	4704-4709	fully	_	_	_	_
33-5	4710-4717	utilize	_	_	_	_
33-6	4718-4723	image	abstract|abstract[239]	giv|giv[239]	coref|coref	34-9[247_0]|34-33[0_239]
33-7	4724-4728	data	abstract[239]	giv[239]	_	_
33-8	4729-4730	,	_	_	_	_
33-9	4731-4733	as	_	_	_	_
33-10	4734-4739	every	abstract[240]	new[240]	_	_
33-11	4740-4745	pixel	abstract[240]	new[240]	_	_
33-12	4746-4749	can	_	_	_	_
33-13	4750-4752	be	_	_	_	_
33-14	4753-4761	utilized	_	_	_	_
33-15	4762-4764	in	_	_	_	_
33-16	4765-4775	prediction	abstract|abstract[242]	giv|giv[242]	coref	34-37[0_242]
33-17	4776-4781	model	abstract[242]	giv[242]	_	_
33-18	4782-4783	;	_	_	_	_
33-19	4784-4785	(	_	_	_	_
33-20	4786-4787	2	_	_	_	_
33-21	4788-4789	)	_	_	_	_

#Text=CNN models are insensitive to object position on the image , an inherent property of convolution operation ; and ( 3 ) as discussed in the next section , by using extensive data augmentations in the model training process , CNN models are robust to different staining conditions in pathology image analysis .
34-1	4790-4793	CNN	organization|abstract[244]	giv|giv[244]	coref|coref	34-41|34-41[258_244]
34-2	4794-4800	models	abstract[244]	giv[244]	_	_
34-3	4801-4804	are	_	_	_	_
34-4	4805-4816	insensitive	_	_	_	_
34-5	4817-4819	to	_	_	_	_
34-6	4820-4826	object	abstract|abstract[246]	giv|new[246]	appos	34-12[248_246]
34-7	4827-4835	position	abstract[246]	new[246]	_	_
34-8	4836-4838	on	abstract[246]	new[246]	_	_
34-9	4839-4842	the	abstract[246]|abstract[247]	new[246]|giv[247]	coref	34-51[0_247]
34-10	4843-4848	image	abstract[246]|abstract[247]	new[246]|giv[247]	_	_
34-11	4849-4850	,	_	_	_	_
34-12	4851-4853	an	abstract[248]	giv[248]	_	_
34-13	4854-4862	inherent	abstract[248]	giv[248]	_	_
34-14	4863-4871	property	abstract[248]	giv[248]	_	_
34-15	4872-4874	of	abstract[248]	giv[248]	_	_
34-16	4875-4886	convolution	abstract[248]|abstract|event[250]	giv[248]|giv|giv[250]	_	_
34-17	4887-4896	operation	abstract[248]|event[250]	giv[248]|giv[250]	_	_
34-18	4897-4898	;	_	_	_	_
34-19	4899-4902	and	_	_	_	_
34-20	4903-4904	(	_	_	_	_
34-21	4905-4906	3	_	_	_	_
34-22	4907-4908	)	_	_	_	_
34-23	4909-4911	as	_	_	_	_
34-24	4912-4921	discussed	_	_	_	_
34-25	4922-4924	in	_	_	_	_
34-26	4925-4928	the	abstract[251]	new[251]	_	_
34-27	4929-4933	next	abstract[251]	new[251]	_	_
34-28	4934-4941	section	abstract[251]	new[251]	_	_
34-29	4942-4943	,	_	_	_	_
34-30	4944-4946	by	_	_	_	_
34-31	4947-4952	using	_	_	_	_
34-32	4953-4962	extensive	abstract[253]	new[253]	_	_
34-33	4963-4967	data	abstract|abstract[253]	giv|new[253]	_	_
34-34	4968-4981	augmentations	abstract[253]	new[253]	_	_
34-35	4982-4984	in	_	_	_	_
34-36	4985-4988	the	abstract[256]	new[256]	_	_
34-37	4989-4994	model	abstract|abstract[255]|abstract[256]	giv|new[255]|new[256]	_	_
34-38	4995-5003	training	abstract[255]|abstract[256]	new[255]|new[256]	_	_
34-39	5004-5011	process	abstract[256]	new[256]	_	_
34-40	5012-5013	,	_	_	_	_
34-41	5014-5017	CNN	organization|abstract[258]	giv|giv[258]	_	_
34-42	5018-5024	models	abstract[258]	giv[258]	_	_
34-43	5025-5028	are	_	_	_	_
34-44	5029-5035	robust	_	_	_	_
34-45	5036-5038	to	_	_	_	_
34-46	5039-5048	different	abstract[259]	new[259]	_	_
34-47	5049-5057	staining	abstract[259]	new[259]	_	_
34-48	5058-5068	conditions	abstract[259]	new[259]	_	_
34-49	5069-5071	in	abstract[259]	new[259]	_	_
34-50	5072-5081	pathology	abstract[259]|abstract|abstract[262]	new[259]|giv|giv[262]	_	_
34-51	5082-5087	image	abstract[259]|abstract|abstract[262]	new[259]|giv|giv[262]	_	_
34-52	5088-5096	analysis	abstract[259]|abstract[262]	new[259]|giv[262]	_	_
34-53	5097-5098	.	_	_	_	_
