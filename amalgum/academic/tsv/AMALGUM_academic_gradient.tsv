#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=2. Gradient Descent Learning of Stochastic Neural Networks
1-1	0-2	2.	_	_	_	_
1-2	3-11	Gradient	abstract[1]|abstract[2]	new[1]|new[2]	coref|coref	6-5[0_2]|21-1[153_1]
1-3	12-19	Descent	abstract[1]|abstract[2]	new[1]|new[2]	_	_
1-4	20-28	Learning	abstract[2]	new[2]	_	_
1-5	29-31	of	abstract[2]	new[2]	_	_
1-6	32-42	Stochastic	abstract[2]|abstract[3]	new[2]|new[3]	coref	2-2[5_3]
1-7	43-49	Neural	abstract[2]|abstract[3]	new[2]|new[3]	_	_
1-8	50-58	Networks	abstract[2]|abstract[3]	new[2]|new[3]	_	_

#Text=2.1. Stochastic Neural Networks
2-1	59-63	2.1.	_	_	_	_
2-2	64-74	Stochastic	abstract[5]	giv[5]	coref	4-2[12_5]
2-3	75-81	Neural	abstract|abstract[5]	new|giv[5]	_	_
2-4	82-90	Networks	abstract[5]	giv[5]	_	_

#Text=Since the natural gradient is derived from stochastic neural network models , let us start from the brief description of the two popular stochastic models .
3-1	91-96	Since	_	_	_	_
3-2	97-100	the	abstract[6]	new[6]	coref	18-1[130_6]
3-3	101-108	natural	abstract[6]	new[6]	_	_
3-4	109-117	gradient	abstract[6]	new[6]	_	_
3-5	118-120	is	_	_	_	_
3-6	121-128	derived	_	_	_	_
3-7	129-133	from	_	_	_	_
3-8	134-144	stochastic	abstract[8]	new[8]	coref	3-21[11_8]
3-9	145-151	neural	abstract[7]|abstract[8]	new[7]|new[8]	coref	12-5[73_7]
3-10	152-159	network	abstract[7]|abstract[8]	new[7]|new[8]	_	_
3-11	160-166	models	abstract[8]	new[8]	_	_
3-12	167-168	,	_	_	_	_
3-13	169-172	let	_	_	_	_
3-14	173-175	us	person	acc	ana	11-2
3-15	176-181	start	_	_	_	_
3-16	182-186	from	_	_	_	_
3-17	187-190	the	abstract[10]	new[10]	_	_
3-18	191-196	brief	abstract[10]	new[10]	_	_
3-19	197-208	description	abstract[10]	new[10]	_	_
3-20	209-211	of	abstract[10]	new[10]	_	_
3-21	212-215	the	abstract[10]|abstract[11]	new[10]|giv[11]	coref	17-16[123_11]
3-22	216-219	two	abstract[10]|abstract[11]	new[10]|giv[11]	_	_
3-23	220-227	popular	abstract[10]|abstract[11]	new[10]|giv[11]	_	_
3-24	228-238	stochastic	abstract[10]|abstract[11]	new[10]|giv[11]	_	_
3-25	239-245	models	abstract[10]|abstract[11]	new[10]|giv[11]	_	_
3-26	246-247	.	_	_	_	_

#Text=Although typical neural networks , including deep networks , are defined as deterministic input-output mapping functions of input , output , and parameter , the observed data for training the networks always have inevitable noises and thus the input-output relation can be described in the stochastic manner .
4-1	248-256	Although	_	_	_	_
4-2	257-264	typical	abstract[12]	giv[12]	coref	4-7[13_12]
4-3	265-271	neural	abstract[12]	giv[12]	_	_
4-4	272-280	networks	abstract[12]	giv[12]	_	_
4-5	281-282	,	abstract[12]	giv[12]	_	_
4-6	283-292	including	abstract[12]	giv[12]	_	_
4-7	293-297	deep	abstract[12]|abstract[13]	giv[12]|giv[13]	coref	4-30[20_13]
4-8	298-306	networks	abstract[12]|abstract[13]	giv[12]|giv[13]	_	_
4-9	307-308	,	_	_	_	_
4-10	309-312	are	_	_	_	_
4-11	313-320	defined	_	_	_	_
4-12	321-323	as	_	_	_	_
4-13	324-337	deterministic	abstract[15]	new[15]	_	_
4-14	338-350	input-output	abstract[15]	new[15]	_	_
4-15	351-358	mapping	abstract|abstract[15]	new|new[15]	_	_
4-16	359-368	functions	abstract[15]	new[15]	_	_
4-17	369-371	of	abstract[15]	new[15]	_	_
4-18	372-377	input	abstract[15]|abstract	new[15]|new	_	_
4-19	378-379	,	abstract[15]	new[15]	_	_
4-20	380-386	output	abstract[15]|abstract	new[15]|new	coref	5-5[24_0]
4-21	387-388	,	abstract[15]	new[15]	_	_
4-22	389-392	and	abstract[15]	new[15]	_	_
4-23	393-402	parameter	abstract[15]|abstract	new[15]|new	coref	6-13
4-24	403-404	,	_	_	_	_
4-25	405-408	the	abstract[19]	new[19]	_	_
4-26	409-417	observed	abstract[19]	new[19]	_	_
4-27	418-422	data	abstract[19]	new[19]	_	_
4-28	423-426	for	abstract[19]	new[19]	_	_
4-29	427-435	training	abstract[19]	new[19]	_	_
4-30	436-439	the	abstract[19]|abstract[20]	new[19]|giv[20]	coref	9-7[49_20]
4-31	440-448	networks	abstract[19]|abstract[20]	new[19]|giv[20]	_	_
4-32	449-455	always	_	_	_	_
4-33	456-460	have	_	_	_	_
4-34	461-471	inevitable	abstract[21]	new[21]	_	_
4-35	472-478	noises	abstract[21]	new[21]	_	_
4-36	479-482	and	_	_	_	_
4-37	483-487	thus	_	_	_	_
4-38	488-491	the	abstract[22]	new[22]	_	_
4-39	492-504	input-output	abstract[22]	new[22]	_	_
4-40	505-513	relation	abstract[22]	new[22]	_	_
4-41	514-517	can	_	_	_	_
4-42	518-520	be	_	_	_	_
4-43	521-530	described	_	_	_	_
4-44	531-533	in	_	_	_	_
4-45	534-537	the	abstract[23]	new[23]	_	_
4-46	538-548	stochastic	abstract[23]	new[23]	_	_
4-47	549-555	manner	abstract[23]	new[23]	_	_
4-48	556-557	.	_	_	_	_

#Text=In other words , the observed output can be regarded as a random vector that is dependent on the deterministic function and some additional stochastic process that is described by the conditional probability .
5-1	558-560	In	_	_	_	_
5-2	561-566	other	_	_	_	_
5-3	567-572	words	_	_	_	_
5-4	573-574	,	_	_	_	_
5-5	575-578	the	abstract[24]	giv[24]	coref	10-6[53_24]
5-6	579-587	observed	abstract[24]	giv[24]	_	_
5-7	588-594	output	abstract[24]	giv[24]	_	_
5-8	595-598	can	_	_	_	_
5-9	599-601	be	_	_	_	_
5-10	602-610	regarded	_	_	_	_
5-11	611-613	as	_	_	_	_
5-12	614-615	a	abstract[25]	new[25]	coref	10-24[59_25]
5-13	616-622	random	abstract[25]	new[25]	_	_
5-14	623-629	vector	abstract[25]	new[25]	_	_
5-15	630-634	that	abstract[25]	new[25]	_	_
5-16	635-637	is	abstract[25]	new[25]	_	_
5-17	638-647	dependent	abstract[25]	new[25]	_	_
5-18	648-650	on	abstract[25]	new[25]	_	_
5-19	651-654	the	abstract[25]|abstract[26]	new[25]|new[26]	coref	6-16[34_26]
5-20	655-668	deterministic	abstract[25]|abstract[26]	new[25]|new[26]	_	_
5-21	669-677	function	abstract[25]|abstract[26]	new[25]|new[26]	_	_
5-22	678-681	and	abstract[25]	new[25]	_	_
5-23	682-686	some	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-24	687-697	additional	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-25	698-708	stochastic	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-26	709-716	process	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-27	717-721	that	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-28	722-724	is	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-29	725-734	described	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-30	735-737	by	abstract[25]|abstract[27]	new[25]|new[27]	_	_
5-31	738-741	the	abstract[25]|abstract[27]|abstract[28]	new[25]|new[27]|new[28]	coref	9-22[0_28]
5-32	742-753	conditional	abstract[25]|abstract[27]|abstract[28]	new[25]|new[27]|new[28]	_	_
5-33	754-765	probability	abstract[25]|abstract[27]|abstract[28]	new[25]|new[27]|new[28]	_	_
5-34	766-767	.	_	_	_	_

#Text=Then the goal of learning is to find an optimal value of parameter that minimizes the loss function defined as negative log likelihood of given input-output sample .
6-1	768-772	Then	_	_	_	_
6-2	773-776	the	abstract[29]	new[29]	_	_
6-3	777-781	goal	abstract[29]	new[29]	_	_
6-4	782-784	of	abstract[29]	new[29]	_	_
6-5	785-793	learning	abstract[29]|abstract	new[29]|giv	coref	17-42[129_0]
6-6	794-796	is	_	_	_	_
6-7	797-799	to	_	_	_	_
6-8	800-804	find	_	_	_	_
6-9	805-807	an	abstract[31]	new[31]	_	_
6-10	808-815	optimal	abstract[31]	new[31]	_	_
6-11	816-821	value	abstract[31]	new[31]	_	_
6-12	822-824	of	abstract[31]	new[31]	_	_
6-13	825-834	parameter	abstract[31]|abstract	new[31]|giv	coref	7-13[41_0]
6-14	835-839	that	abstract[31]	new[31]	_	_
6-15	840-849	minimizes	abstract[31]|abstract	new[31]|new	coref|none	27-33[214_0]|6-15[0_214]
6-16	850-853	the	abstract[31]|abstract[34]	new[31]|giv[34]	coref	7-1[39_34]
6-17	854-858	loss	abstract[31]|abstract|abstract[34]	new[31]|new|giv[34]	coref	7-2
6-18	859-867	function	abstract[31]|abstract[34]	new[31]|giv[34]	_	_
6-19	868-875	defined	abstract[31]|abstract[34]	new[31]|giv[34]	_	_
6-20	876-878	as	abstract[31]|abstract[34]	new[31]|giv[34]	_	_
6-21	879-887	negative	abstract[31]|abstract[34]|abstract[36]	new[31]|giv[34]|new[36]	_	_
6-22	888-891	log	abstract[31]|abstract[34]|object|abstract[36]	new[31]|giv[34]|new|new[36]	coref	15-14
6-23	892-902	likelihood	abstract[31]|abstract[34]|abstract[36]	new[31]|giv[34]|new[36]	_	_
6-24	903-905	of	abstract[31]|abstract[34]|abstract[36]	new[31]|giv[34]|new[36]	_	_
6-25	906-911	given	abstract[31]|abstract[34]|abstract[36]|abstract[37]	new[31]|giv[34]|new[36]|new[37]	_	_
6-26	912-924	input-output	abstract[31]|abstract[34]|abstract[36]|abstract[37]	new[31]|giv[34]|new[36]|new[37]	_	_
6-27	925-931	sample	abstract[31]|abstract[34]|abstract[36]|abstract[37]	new[31]|giv[34]|new[36]|new[37]	_	_
6-28	932-933	.	_	_	_	_

#Text=The loss function can then be written as ( 1 ) and the optimal parameter is described as ( 2 )
7-1	934-937	The	abstract[39]	giv[39]	coref	8-20[46_39]
7-2	938-942	loss	abstract|abstract[39]	giv|giv[39]	coref	11-29
7-3	943-951	function	abstract[39]	giv[39]	_	_
7-4	952-955	can	_	_	_	_
7-5	956-960	then	_	_	_	_
7-6	961-963	be	_	_	_	_
7-7	964-971	written	_	_	_	_
7-8	972-974	as	_	_	_	_
7-9	975-976	(	_	_	_	_
7-10	977-978	1	abstract	new	_	_
7-11	979-980	)	_	_	_	_
7-12	981-984	and	_	_	_	_
7-13	985-988	the	abstract[41]	giv[41]	coref	8-14[0_41]
7-14	989-996	optimal	abstract[41]	giv[41]	_	_
7-15	997-1006	parameter	abstract[41]	giv[41]	_	_
7-16	1007-1009	is	_	_	_	_
7-17	1010-1019	described	_	_	_	_
7-18	1020-1022	as	_	_	_	_
7-19	1023-1024	(	_	_	_	_
7-20	1025-1026	2	_	_	_	_
7-21	1027-1028	)	_	_	_	_

#Text=Note that the last term in equation ( 1 ) is independent of parameter and can be ignored in the objective function for optimization .
8-1	1029-1033	Note	_	_	_	_
8-2	1034-1038	that	_	_	_	_
8-3	1039-1042	the	abstract[42]	new[42]	_	_
8-4	1043-1047	last	abstract[42]	new[42]	_	_
8-5	1048-1052	term	abstract[42]	new[42]	_	_
8-6	1053-1055	in	abstract[42]	new[42]	_	_
8-7	1056-1064	equation	abstract[42]|abstract	new[42]|new	appos	8-9
8-8	1065-1066	(	_	_	_	_
8-9	1067-1068	1	abstract	giv	coref	15-17[106_0]
8-10	1069-1070	)	_	_	_	_
8-11	1071-1073	is	_	_	_	_
8-12	1074-1085	independent	_	_	_	_
8-13	1086-1088	of	_	_	_	_
8-14	1089-1098	parameter	abstract	giv	coref	26-27
8-15	1099-1102	and	_	_	_	_
8-16	1103-1106	can	_	_	_	_
8-17	1107-1109	be	_	_	_	_
8-18	1110-1117	ignored	_	_	_	_
8-19	1118-1120	in	_	_	_	_
8-20	1121-1124	the	abstract[46]	giv[46]	coref	10-14[57_46]
8-21	1125-1134	objective	abstract[46]	giv[46]	_	_
8-22	1135-1143	function	abstract[46]	giv[46]	_	_
8-23	1144-1147	for	abstract[46]	giv[46]	_	_
8-24	1148-1160	optimization	abstract[46]|abstract	giv[46]|new	_	_
8-25	1161-1162	.	_	_	_	_

#Text=Based on the general definition , the conventional neural networks can be regarded as a special case with a specific conditional probability distribution , .
9-1	1163-1168	Based	_	_	_	_
9-2	1169-1171	on	_	_	_	_
9-3	1172-1175	the	abstract[48]	new[48]	_	_
9-4	1176-1183	general	abstract[48]	new[48]	_	_
9-5	1184-1194	definition	abstract[48]	new[48]	_	_
9-6	1195-1196	,	_	_	_	_
9-7	1197-1200	the	abstract[49]	giv[49]	coref	10-16[56_49]
9-8	1201-1213	conventional	abstract[49]	giv[49]	_	_
9-9	1214-1220	neural	abstract[49]	giv[49]	_	_
9-10	1221-1229	networks	abstract[49]	giv[49]	_	_
9-11	1230-1233	can	_	_	_	_
9-12	1234-1236	be	_	_	_	_
9-13	1237-1245	regarded	_	_	_	_
9-14	1246-1248	as	_	_	_	_
9-15	1249-1250	a	abstract[50]	new[50]	_	_
9-16	1251-1258	special	abstract[50]	new[50]	_	_
9-17	1259-1263	case	abstract[50]	new[50]	_	_
9-18	1264-1268	with	abstract[50]	new[50]	_	_
9-19	1269-1270	a	abstract[50]|abstract[52]	new[50]|new[52]	coref	11-11[62_52]
9-20	1271-1279	specific	abstract[50]|abstract[52]	new[50]|new[52]	_	_
9-21	1280-1291	conditional	abstract[50]|abstract[52]	new[50]|new[52]	_	_
9-22	1292-1303	probability	abstract[50]|abstract|abstract[52]	new[50]|giv|new[52]	coref	11-17
9-23	1304-1316	distribution	abstract[50]|abstract[52]	new[50]|new[52]	_	_
9-24	1317-1318	,	_	_	_	_
9-25	1319-1320	.	_	_	_	_

#Text=For example , consider that the output is observed with additive noise to the deterministic neural networks function such as ( 3 ) where is a random noise vector .
10-1	1321-1324	For	_	_	_	_
10-2	1325-1332	example	_	_	_	_
10-3	1333-1334	,	_	_	_	_
10-4	1335-1343	consider	_	_	_	_
10-5	1344-1348	that	_	_	_	_
10-6	1349-1352	the	abstract[53]	giv[53]	coref	13-9[83_53]
10-7	1353-1359	output	abstract[53]	giv[53]	_	_
10-8	1360-1362	is	_	_	_	_
10-9	1363-1371	observed	_	_	_	_
10-10	1372-1376	with	_	_	_	_
10-11	1377-1385	additive	person|abstract[55]	new|new[55]	coref|coref	10-27[58_55]|19-24
10-12	1386-1391	noise	abstract[55]	new[55]	_	_
10-13	1392-1394	to	_	_	_	_
10-14	1395-1398	the	abstract[57]	giv[57]	coref	11-22[67_57]
10-15	1399-1412	deterministic	abstract[57]	giv[57]	_	_
10-16	1413-1419	neural	abstract[56]|abstract[57]	giv[56]|giv[57]	coref	16-21[114_56]
10-17	1420-1428	networks	abstract[56]|abstract[57]	giv[56]|giv[57]	_	_
10-18	1429-1437	function	abstract[57]	giv[57]	_	_
10-19	1438-1442	such	abstract[57]	giv[57]	_	_
10-20	1443-1445	as	abstract[57]	giv[57]	_	_
10-21	1446-1447	(	abstract[57]	giv[57]	_	_
10-22	1448-1449	3	abstract[57]	giv[57]	_	_
10-23	1450-1451	)	abstract[57]	giv[57]	_	_
10-24	1452-1457	where	abstract[57]|abstract[59]	giv[57]|giv[59]	coref	11-5[61_59]
10-25	1458-1460	is	abstract[57]|abstract[59]	giv[57]|giv[59]	_	_
10-26	1461-1462	a	abstract[57]|abstract[59]	giv[57]|giv[59]	_	_
10-27	1463-1469	random	abstract[57]|abstract[58]|abstract[59]	giv[57]|giv[58]|giv[59]	coref	12-9[76_58]
10-28	1470-1475	noise	abstract[57]|abstract[58]|abstract[59]	giv[57]|giv[58]|giv[59]	_	_
10-29	1476-1482	vector	abstract[57]|abstract[59]	giv[57]|giv[59]	_	_
10-30	1483-1484	.	_	_	_	_

#Text=When we assume that the random vector is subject to the standard Gaussian distribution , its probability density is defined as the normal distribution function , and its loss function becomes the well-known squared error function , which can be written as ( 4 )
11-1	1485-1489	When	_	_	_	_
11-2	1490-1492	we	person	giv	ana	16-10
11-3	1493-1499	assume	_	_	_	_
11-4	1500-1504	that	_	_	_	_
11-5	1505-1508	the	abstract[61]	giv[61]	coref	13-9[84_61]
11-6	1509-1515	random	abstract[61]	giv[61]	_	_
11-7	1516-1522	vector	abstract[61]	giv[61]	_	_
11-8	1523-1525	is	_	_	_	_
11-9	1526-1533	subject	_	_	_	_
11-10	1534-1536	to	_	_	_	_
11-11	1537-1540	the	abstract[62]	giv[62]	ana	11-16[0_62]
11-12	1541-1549	standard	abstract[62]	giv[62]	_	_
11-13	1550-1558	Gaussian	abstract[62]	giv[62]	_	_
11-14	1559-1571	distribution	abstract[62]	giv[62]	_	_
11-15	1572-1573	,	_	_	_	_
11-16	1574-1577	its	abstract|abstract[65]	giv|new[65]	coref|coref	11-24|18-15[0_65]
11-17	1578-1589	probability	abstract|abstract[65]	giv|new[65]	coref	13-18
11-18	1590-1597	density	abstract[65]	new[65]	_	_
11-19	1598-1600	is	_	_	_	_
11-20	1601-1608	defined	_	_	_	_
11-21	1609-1611	as	_	_	_	_
11-22	1612-1615	the	abstract[67]	giv[67]	coref	11-28[70_67]
11-23	1616-1622	normal	abstract[67]	giv[67]	_	_
11-24	1623-1635	distribution	abstract|abstract[67]	giv|giv[67]	ana	11-28
11-25	1636-1644	function	abstract[67]	giv[67]	_	_
11-26	1645-1646	,	_	_	_	_
11-27	1647-1650	and	_	_	_	_
11-28	1651-1654	its	abstract|abstract[70]	giv|giv[70]	coref|coref	11-32[72_70]|13-16[86_0]
11-29	1655-1659	loss	abstract|abstract[70]	giv|giv[70]	coref	15-3
11-30	1660-1668	function	abstract[70]	giv[70]	_	_
11-31	1669-1676	becomes	_	_	_	_
11-32	1677-1680	the	abstract[72]	giv[72]	coref	12-22[80_72]
11-33	1681-1691	well-known	abstract[72]	giv[72]	_	_
11-34	1692-1699	squared	abstract[71]|abstract[72]	new[71]|giv[72]	coref	12-23[0_71]
11-35	1700-1705	error	abstract[71]|abstract[72]	new[71]|giv[72]	_	_
11-36	1706-1714	function	abstract[72]	giv[72]	_	_
11-37	1715-1716	,	abstract[72]	giv[72]	_	_
11-38	1717-1722	which	abstract[72]	giv[72]	_	_
11-39	1723-1726	can	abstract[72]	giv[72]	_	_
11-40	1727-1729	be	abstract[72]	giv[72]	_	_
11-41	1730-1737	written	abstract[72]	giv[72]	_	_
11-42	1738-1740	as	abstract[72]	giv[72]	_	_
11-43	1741-1742	(	_	_	_	_
11-44	1743-1744	4	_	_	_	_
11-45	1745-1746	)	_	_	_	_

#Text=Therefore , the stochastic neural network model with additive Gaussian noise is equivalent to the typical neural network model trained with squared error function , which is widely used for regression task .
12-1	1747-1756	Therefore	_	_	_	_
12-2	1757-1758	,	_	_	_	_
12-3	1759-1762	the	abstract[74]	new[74]	_	_
12-4	1763-1773	stochastic	abstract[74]	new[74]	_	_
12-5	1774-1780	neural	abstract[73]|abstract[74]	giv[73]|new[74]	coref	12-17[77_73]
12-6	1781-1788	network	abstract[73]|abstract[74]	giv[73]|new[74]	_	_
12-7	1789-1794	model	abstract[74]	new[74]	_	_
12-8	1795-1799	with	abstract[74]	new[74]	_	_
12-9	1800-1808	additive	abstract[74]|abstract[76]	new[74]|giv[76]	coref	19-26[0_76]
12-10	1809-1817	Gaussian	abstract[74]|person|abstract[76]	new[74]|new|giv[76]	coref	19-25
12-11	1818-1823	noise	abstract[74]|abstract[76]	new[74]|giv[76]	_	_
12-12	1824-1826	is	_	_	_	_
12-13	1827-1837	equivalent	_	_	_	_
12-14	1838-1840	to	_	_	_	_
12-15	1841-1844	the	abstract[78]	new[78]	coref	13-25[87_78]
12-16	1845-1852	typical	abstract[78]	new[78]	_	_
12-17	1853-1859	neural	abstract[77]|abstract[78]	giv[77]|new[78]	coref	17-16[122_77]
12-18	1860-1867	network	abstract[77]|abstract[78]	giv[77]|new[78]	_	_
12-19	1868-1873	model	abstract[78]	new[78]	_	_
12-20	1874-1881	trained	abstract[78]	new[78]	_	_
12-21	1882-1886	with	abstract[78]	new[78]	_	_
12-22	1887-1894	squared	abstract[78]|abstract[80]	new[78]|giv[80]	coref	15-1[102_80]
12-23	1895-1900	error	abstract[78]|abstract|abstract[80]	new[78]|giv|giv[80]	_	_
12-24	1901-1909	function	abstract[78]|abstract[80]	new[78]|giv[80]	_	_
12-25	1910-1911	,	abstract[78]	new[78]	_	_
12-26	1912-1917	which	abstract[78]	new[78]	_	_
12-27	1918-1920	is	abstract[78]	new[78]	_	_
12-28	1921-1927	widely	abstract[78]	new[78]	_	_
12-29	1928-1932	used	abstract[78]	new[78]	_	_
12-30	1933-1936	for	abstract[78]	new[78]	_	_
12-31	1937-1947	regression	abstract[78]|abstract|abstract[82]	new[78]|new|new[82]	coref|coref	16-16|16-34[118_82]
12-32	1948-1952	task	abstract[78]|abstract[82]	new[78]|new[82]	_	_
12-33	1953-1954	.	_	_	_	_

#Text=On the other hand , in case that the output is a binary vector , the corresponding probability distribution can be defined by using a logistic model , such as ( 5 ) where and are the i -th component of L -dimensional vector and , respectively .
13-1	1955-1957	On	_	_	_	_
13-2	1958-1961	the	_	_	_	_
13-3	1962-1967	other	_	_	_	_
13-4	1968-1972	hand	_	_	_	_
13-5	1973-1974	,	_	_	_	_
13-6	1975-1977	in	_	_	_	_
13-7	1978-1982	case	_	_	_	_
13-8	1983-1987	that	_	_	_	_
13-9	1988-1991	the	abstract[83]|abstract[84]	giv[83]|giv[84]	coref|coref	13-42[90_84]|14-8[0_83]
13-10	1992-1998	output	abstract[83]|abstract[84]	giv[83]|giv[84]	_	_
13-11	1999-2001	is	abstract[84]	giv[84]	_	_
13-12	2002-2003	a	abstract[84]	giv[84]	_	_
13-13	2004-2010	binary	abstract[84]	giv[84]	_	_
13-14	2011-2017	vector	abstract[84]	giv[84]	_	_
13-15	2018-2019	,	_	_	_	_
13-16	2020-2023	the	abstract[86]	giv[86]	_	_
13-17	2024-2037	corresponding	abstract[86]	giv[86]	_	_
13-18	2038-2049	probability	abstract|abstract[86]	giv|giv[86]	coref	18-14
13-19	2050-2062	distribution	abstract[86]	giv[86]	_	_
13-20	2063-2066	can	_	_	_	_
13-21	2067-2069	be	_	_	_	_
13-22	2070-2077	defined	_	_	_	_
13-23	2078-2080	by	_	_	_	_
13-24	2081-2086	using	_	_	_	_
13-25	2087-2088	a	abstract[87]	giv[87]	coref	14-14[97_87]
13-26	2089-2097	logistic	abstract[87]	giv[87]	_	_
13-27	2098-2103	model	abstract[87]	giv[87]	_	_
13-28	2104-2105	,	abstract[87]	giv[87]	_	_
13-29	2106-2110	such	abstract[87]	giv[87]	_	_
13-30	2111-2113	as	abstract[87]	giv[87]	_	_
13-31	2114-2115	(	abstract[87]	giv[87]	_	_
13-32	2116-2117	5	abstract[87]	giv[87]	_	_
13-33	2118-2119	)	abstract[87]	giv[87]	_	_
13-34	2120-2125	where	abstract[87]	giv[87]	_	_
13-35	2126-2129	and	abstract[87]|abstract[88]	giv[87]|new[88]	_	_
13-36	2130-2133	are	abstract[87]|abstract[88]	giv[87]|new[88]	_	_
13-37	2134-2137	the	abstract[87]|abstract[88]	giv[87]|new[88]	_	_
13-38	2138-2139	i	abstract[87]|abstract[88]	giv[87]|new[88]	_	_
13-39	2140-2143	-th	abstract[87]|abstract[88]	giv[87]|new[88]	_	_
13-40	2144-2153	component	abstract[87]|abstract[88]	giv[87]|new[88]	_	_
13-41	2154-2156	of	abstract[87]|abstract[88]	giv[87]|new[88]	_	_
13-42	2157-2158	L	abstract[87]|abstract[88]|person|abstract[90]	giv[87]|new[88]|new|giv[90]	coref|coref	14-6[94_90]|14-20
13-43	2159-2171	-dimensional	abstract[87]|abstract[88]|abstract[90]	giv[87]|new[88]|giv[90]	_	_
13-44	2172-2178	vector	abstract[87]|abstract[88]|abstract[90]	giv[87]|new[88]|giv[90]	_	_
13-45	2179-2182	and	abstract[87]	giv[87]	_	_
13-46	2183-2184	,	abstract[87]	giv[87]	_	_
13-47	2185-2197	respectively	abstract[87]	giv[87]	_	_
13-48	2198-2199	.	_	_	_	_

#Text=Since the typical problems with binary target output vector is pattern classification , the logistic model is appropriate for L -class classification tasks .
14-1	2200-2205	Since	_	_	_	_
14-2	2206-2209	the	abstract[91]	new[91]	coref	14-11[96_91]
14-3	2210-2217	typical	abstract[91]	new[91]	_	_
14-4	2218-2226	problems	abstract[91]	new[91]	_	_
14-5	2227-2231	with	abstract[91]	new[91]	_	_
14-6	2232-2238	binary	abstract[91]|abstract[94]	new[91]|giv[94]	coref	26-26[205_94]
14-7	2239-2245	target	abstract[91]|abstract|abstract[94]	new[91]|new|giv[94]	_	_
14-8	2246-2252	output	abstract[91]|abstract|abstract[94]	new[91]|giv|giv[94]	_	_
14-9	2253-2259	vector	abstract[91]|abstract[94]	new[91]|giv[94]	_	_
14-10	2260-2262	is	_	_	_	_
14-11	2263-2270	pattern	abstract|abstract[96]	new|giv[96]	coref	17-35[126_96]
14-12	2271-2285	classification	abstract[96]	giv[96]	_	_
14-13	2286-2287	,	_	_	_	_
14-14	2288-2291	the	abstract[97]	giv[97]	coref	15-6[103_97]
14-15	2292-2300	logistic	abstract[97]	giv[97]	_	_
14-16	2301-2306	model	abstract[97]	giv[97]	_	_
14-17	2307-2309	is	_	_	_	_
14-18	2310-2321	appropriate	_	_	_	_
14-19	2322-2325	for	_	_	_	_
14-20	2326-2327	L	person|abstract[100]	giv|new[100]	_	_
14-21	2328-2334	-class	abstract[100]	new[100]	_	_
14-22	2335-2349	classification	abstract|abstract[100]	new|new[100]	coref	16-34
14-23	2350-2355	tasks	abstract[100]	new[100]	_	_
14-24	2356-2357	.	_	_	_	_

#Text=The corresponding loss function of the logistic model is obtained by taking negative log likelihood of Equation ( 5 ) , which can be written as , ( 6 )
15-1	2358-2361	The	abstract[102]	giv[102]	_	_
15-2	2362-2375	corresponding	abstract[102]	giv[102]	_	_
15-3	2376-2380	loss	abstract|abstract[102]	giv|giv[102]	coref	22-12
15-4	2381-2389	function	abstract[102]	giv[102]	_	_
15-5	2390-2392	of	abstract[102]	giv[102]	_	_
15-6	2393-2396	the	abstract[102]|abstract[103]	giv[102]|giv[103]	coref	16-14[113_103]
15-7	2397-2405	logistic	abstract[102]|abstract[103]	giv[102]|giv[103]	_	_
15-8	2406-2411	model	abstract[102]|abstract[103]	giv[102]|giv[103]	_	_
15-9	2412-2414	is	_	_	_	_
15-10	2415-2423	obtained	_	_	_	_
15-11	2424-2426	by	_	_	_	_
15-12	2427-2433	taking	_	_	_	_
15-13	2434-2442	negative	abstract[105]	new[105]	_	_
15-14	2443-2446	log	object|abstract[105]	giv|new[105]	_	_
15-15	2447-2457	likelihood	abstract[105]	new[105]	_	_
15-16	2458-2460	of	abstract[105]	new[105]	_	_
15-17	2461-2469	Equation	abstract[105]|abstract[106]	new[105]|giv[106]	ana	16-3[0_106]
15-18	2470-2471	(	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-19	2472-2473	5	abstract[105]|abstract[106]|abstract	new[105]|giv[106]|new	_	_
15-20	2474-2475	)	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-21	2476-2477	,	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-22	2478-2483	which	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-23	2484-2487	can	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-24	2488-2490	be	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-25	2491-2498	written	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-26	2499-2501	as	abstract[105]|abstract[106]	new[105]|giv[106]	_	_
15-27	2502-2503	,	_	_	_	_
15-28	2504-2505	(	_	_	_	_
15-29	2506-2507	6	_	_	_	_
15-30	2508-2509	)	_	_	_	_

#Text=Noting that this is the well-known cross-entropy error , we can say that the logistic regression model is equivalent to the typical neural networks with cross-entropy error , which is widely used for classification task .
16-1	2510-2516	Noting	_	_	_	_
16-2	2517-2521	that	_	_	_	_
16-3	2522-2526	this	abstract	giv	coref	16-5[110_0]
16-4	2527-2529	is	_	_	_	_
16-5	2530-2533	the	abstract[110]	giv[110]	coref	16-26[116_110]
16-6	2534-2544	well-known	abstract[110]	giv[110]	_	_
16-7	2545-2558	cross-entropy	abstract|abstract[110]	new|giv[110]	coref	16-26
16-8	2559-2564	error	abstract[110]	giv[110]	_	_
16-9	2565-2566	,	_	_	_	_
16-10	2567-2569	we	person	giv	ana	17-10
16-11	2570-2573	can	_	_	_	_
16-12	2574-2577	say	_	_	_	_
16-13	2578-2582	that	_	_	_	_
16-14	2583-2586	the	abstract[113]	giv[113]	coref	17-5[119_113]
16-15	2587-2595	logistic	abstract[113]	giv[113]	_	_
16-16	2596-2606	regression	abstract|abstract[113]	giv|giv[113]	coref	19-30[150_0]
16-17	2607-2612	model	abstract[113]	giv[113]	_	_
16-18	2613-2615	is	_	_	_	_
16-19	2616-2626	equivalent	_	_	_	_
16-20	2627-2629	to	_	_	_	_
16-21	2630-2633	the	abstract[114]	giv[114]	coref	18-18[136_114]
16-22	2634-2641	typical	abstract[114]	giv[114]	_	_
16-23	2642-2648	neural	abstract[114]	giv[114]	_	_
16-24	2649-2657	networks	abstract[114]	giv[114]	_	_
16-25	2658-2662	with	abstract[114]	giv[114]	_	_
16-26	2663-2676	cross-entropy	abstract[114]|abstract|abstract[116]	giv[114]|giv|giv[116]	coref	27-1[206_116]
16-27	2677-2682	error	abstract[114]|abstract[116]	giv[114]|giv[116]	_	_
16-28	2683-2684	,	abstract[114]	giv[114]	_	_
16-29	2685-2690	which	abstract[114]	giv[114]	_	_
16-30	2691-2693	is	abstract[114]	giv[114]	_	_
16-31	2694-2700	widely	abstract[114]	giv[114]	_	_
16-32	2701-2705	used	abstract[114]	giv[114]	_	_
16-33	2706-2709	for	abstract[114]	giv[114]	_	_
16-34	2710-2724	classification	abstract[114]|abstract|abstract[118]	giv[114]|giv|giv[118]	coref	17-23[124_118]
16-35	2725-2729	task	abstract[114]|abstract[118]	giv[114]|giv[118]	_	_
16-36	2730-2731	.	_	_	_	_

#Text=Likewise , by defining a proper stochastic model , we can derive various types of neural network models , which can explain the given task more adequately and get a new insight to solve many unresolved problems in the field of neural network learning .
17-1	2732-2740	Likewise	_	_	_	_
17-2	2741-2742	,	_	_	_	_
17-3	2743-2745	by	_	_	_	_
17-4	2746-2754	defining	_	_	_	_
17-5	2755-2756	a	abstract[119]	giv[119]	coref	22-2[155_119]
17-6	2757-2763	proper	abstract[119]	giv[119]	_	_
17-7	2764-2774	stochastic	abstract[119]	giv[119]	_	_
17-8	2775-2780	model	abstract[119]	giv[119]	_	_
17-9	2781-2782	,	_	_	_	_
17-10	2783-2785	we	person	giv	ana	19-5
17-11	2786-2789	can	_	_	_	_
17-12	2790-2796	derive	_	_	_	_
17-13	2797-2804	various	abstract[121]	new[121]	_	_
17-14	2805-2810	types	abstract[121]	new[121]	_	_
17-15	2811-2813	of	abstract[121]	new[121]	_	_
17-16	2814-2820	neural	abstract[121]|abstract[122]|abstract[123]	new[121]|giv[122]|giv[123]	coref|coref	17-42[128_122]|19-16[144_123]
17-17	2821-2828	network	abstract[121]|abstract[122]|abstract[123]	new[121]|giv[122]|giv[123]	_	_
17-18	2829-2835	models	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-19	2836-2837	,	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-20	2838-2843	which	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-21	2844-2847	can	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-22	2848-2855	explain	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-23	2856-2859	the	abstract[121]|abstract[123]|abstract[124]	new[121]|giv[123]|giv[124]	_	_
17-24	2860-2865	given	abstract[121]|abstract[123]|abstract[124]	new[121]|giv[123]|giv[124]	_	_
17-25	2866-2870	task	abstract[121]|abstract[123]|abstract[124]	new[121]|giv[123]|giv[124]	_	_
17-26	2871-2875	more	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-27	2876-2886	adequately	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-28	2887-2890	and	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-29	2891-2894	get	abstract[121]|abstract[123]	new[121]|giv[123]	_	_
17-30	2895-2896	a	abstract[121]|abstract[123]|abstract[125]	new[121]|giv[123]|new[125]	_	_
17-31	2897-2900	new	abstract[121]|abstract[123]|abstract[125]	new[121]|giv[123]|new[125]	_	_
17-32	2901-2908	insight	abstract[121]|abstract[123]|abstract[125]	new[121]|giv[123]|new[125]	_	_
17-33	2909-2911	to	abstract[121]|abstract[123]|abstract[125]	new[121]|giv[123]|new[125]	_	_
17-34	2912-2917	solve	abstract[121]|abstract[123]|abstract[125]	new[121]|giv[123]|new[125]	_	_
17-35	2918-2922	many	abstract[121]|abstract[123]|abstract[125]|abstract[126]	new[121]|giv[123]|new[125]|giv[126]	_	_
17-36	2923-2933	unresolved	abstract[121]|abstract[123]|abstract[125]|abstract[126]	new[121]|giv[123]|new[125]|giv[126]	_	_
17-37	2934-2942	problems	abstract[121]|abstract[123]|abstract[125]|abstract[126]	new[121]|giv[123]|new[125]|giv[126]	_	_
17-38	2943-2945	in	abstract[121]|abstract[123]|abstract[125]|abstract[126]	new[121]|giv[123]|new[125]|giv[126]	_	_
17-39	2946-2949	the	abstract[121]|abstract[123]|abstract[125]|abstract[126]|abstract[127]	new[121]|giv[123]|new[125]|giv[126]|new[127]	_	_
17-40	2950-2955	field	abstract[121]|abstract[123]|abstract[125]|abstract[126]|abstract[127]	new[121]|giv[123]|new[125]|giv[126]|new[127]	_	_
17-41	2956-2958	of	abstract[121]|abstract[123]|abstract[125]|abstract[126]|abstract[127]	new[121]|giv[123]|new[125]|giv[126]|new[127]	_	_
17-42	2959-2965	neural	abstract[121]|abstract[123]|abstract[125]|abstract[126]|abstract[127]|abstract[128]|abstract[129]	new[121]|giv[123]|new[125]|giv[126]|new[127]|giv[128]|giv[129]	coref|coref	19-12[141_129]|19-19[143_128]
17-43	2966-2973	network	abstract[121]|abstract[123]|abstract[125]|abstract[126]|abstract[127]|abstract[128]|abstract[129]	new[121]|giv[123]|new[125]|giv[126]|new[127]|giv[128]|giv[129]	_	_
17-44	2974-2982	learning	abstract[121]|abstract[123]|abstract[125]|abstract[126]|abstract[127]|abstract[129]	new[121]|giv[123]|new[125]|giv[126]|new[127]|giv[129]	_	_
17-45	2983-2984	.	_	_	_	_

#Text=Natural gradient is also derived from a new metric for the space of probability density function of stochastic neural networks .
18-1	2985-2992	Natural	abstract[130]	giv[130]	coref	19-12[0_130]
18-2	2993-3001	gradient	abstract[130]	giv[130]	_	_
18-3	3002-3004	is	_	_	_	_
18-4	3005-3009	also	_	_	_	_
18-5	3010-3017	derived	_	_	_	_
18-6	3018-3022	from	_	_	_	_
18-7	3023-3024	a	abstract[131]	new[131]	_	_
18-8	3025-3028	new	abstract[131]	new[131]	_	_
18-9	3029-3035	metric	abstract[131]	new[131]	_	_
18-10	3036-3039	for	abstract[131]	new[131]	_	_
18-11	3040-3043	the	abstract[131]|abstract[132]	new[131]|new[132]	_	_
18-12	3044-3049	space	abstract[131]|abstract[132]	new[131]|new[132]	_	_
18-13	3050-3052	of	abstract[131]|abstract[132]	new[131]|new[132]	_	_
18-14	3053-3064	probability	abstract[131]|abstract[132]|abstract|abstract[135]	new[131]|new[132]|giv|new[135]	coref	22-10[160_135]
18-15	3065-3072	density	abstract[131]|abstract[132]|abstract|abstract[135]	new[131]|new[132]|giv|new[135]	_	_
18-16	3073-3081	function	abstract[131]|abstract[132]|abstract[135]	new[131]|new[132]|new[135]	_	_
18-17	3082-3084	of	abstract[131]|abstract[132]|abstract[135]	new[131]|new[132]|new[135]	_	_
18-18	3085-3095	stochastic	abstract[131]|abstract[132]|abstract[135]|abstract[136]	new[131]|new[132]|new[135]|giv[136]	coref	22-6[156_136]
18-19	3096-3102	neural	abstract[131]|abstract[132]|abstract[135]|abstract[136]	new[131]|new[132]|new[135]|giv[136]	_	_
18-20	3103-3111	networks	abstract[131]|abstract[132]|abstract[135]|abstract[136]	new[131]|new[132]|new[135]|giv[136]	_	_
18-21	3112-3113	.	_	_	_	_

#Text=In this paper , we present explicit algorithms of adaptive natural gradient learning method for two representative stochastic neural network models : The additive Gaussian noise model and the logistic regression model .
19-1	3114-3116	In	_	_	_	_
19-2	3117-3121	this	abstract[137]	new[137]	_	_
19-3	3122-3127	paper	abstract[137]	new[137]	_	_
19-4	3128-3129	,	_	_	_	_
19-5	3130-3132	we	person	giv	ana	25-16
19-6	3133-3140	present	_	_	_	_
19-7	3141-3149	explicit	abstract[139]	new[139]	_	_
19-8	3150-3160	algorithms	abstract[139]	new[139]	_	_
19-9	3161-3163	of	abstract[139]	new[139]	_	_
19-10	3164-3172	adaptive	abstract[139]|abstract[142]	new[139]|new[142]	coref	22-23[165_142]
19-11	3173-3180	natural	abstract[139]|abstract[142]	new[139]|new[142]	_	_
19-12	3181-3189	gradient	abstract[139]|abstract|abstract[141]|abstract[142]	new[139]|giv|giv[141]|new[142]	coref|coref	21-1[154_141]|22-23
19-13	3190-3198	learning	abstract[139]|abstract[141]|abstract[142]	new[139]|giv[141]|new[142]	_	_
19-14	3199-3205	method	abstract[139]|abstract[142]	new[139]|new[142]	_	_
19-15	3206-3209	for	abstract[139]	new[139]	_	_
19-16	3210-3213	two	abstract[139]|abstract[144]	new[139]|giv[144]	appos	19-23[148_144]
19-17	3214-3228	representative	abstract[139]|abstract[144]	new[139]|giv[144]	_	_
19-18	3229-3239	stochastic	abstract[139]|abstract[144]	new[139]|giv[144]	_	_
19-19	3240-3246	neural	abstract[139]|abstract[143]|abstract[144]	new[139]|giv[143]|giv[144]	coref	26-10[201_143]
19-20	3247-3254	network	abstract[139]|abstract[143]|abstract[144]	new[139]|giv[143]|giv[144]	_	_
19-21	3255-3261	models	abstract[139]|abstract[144]	new[139]|giv[144]	_	_
19-22	3262-3263	:	_	_	_	_
19-23	3264-3267	The	abstract[148]|abstract[149]	giv[148]|giv[149]	appos|appos	19-23[149_148]|19-29[151_149]
19-24	3268-3276	additive	person|abstract[148]|abstract[149]	giv|giv[148]|giv[149]	_	_
19-25	3277-3285	Gaussian	person|abstract[148]|abstract[149]	giv|giv[148]|giv[149]	_	_
19-26	3286-3291	noise	abstract|abstract[148]|abstract[149]	giv|giv[148]|giv[149]	_	_
19-27	3292-3297	model	abstract[148]|abstract[149]	giv[148]|giv[149]	_	_
19-28	3298-3301	and	abstract[149]	giv[149]	_	_
19-29	3302-3305	the	abstract[149]|abstract[151]	giv[149]|giv[151]	_	_
19-30	3306-3314	logistic	abstract[149]|abstract[150]|abstract[151]	giv[149]|giv[150]|giv[151]	_	_
19-31	3315-3325	regression	abstract[149]|abstract[150]|abstract[151]	giv[149]|giv[150]|giv[151]	_	_
19-32	3326-3331	model	abstract[149]|abstract[151]	giv[149]|giv[151]	_	_
19-33	3332-3333	.	_	_	_	_

#Text=2.2.
20-1	3334-3338	2.2.	abstract	new	_	_

#Text=Gradient Descent Learning
21-1	3339-3347	Gradient	abstract[153]|abstract[154]	giv[153]|giv[154]	coref|coref	22-23[164_153]|23-12[0_154]
21-2	3348-3355	Descent	abstract[153]|abstract[154]	giv[153]|giv[154]	_	_
21-3	3356-3364	Learning	abstract[154]	giv[154]	_	_

#Text=Once a specific model of stochastic neural networks and its corresponding loss function are determined , the weight parameters are optimized by gradient descent method .
22-1	3365-3369	Once	_	_	_	_
22-2	3370-3371	a	abstract[155]	giv[155]	ana	22-10[0_155]
22-3	3372-3380	specific	abstract[155]	giv[155]	_	_
22-4	3381-3386	model	abstract[155]	giv[155]	_	_
22-5	3387-3389	of	abstract[155]	giv[155]	_	_
22-6	3390-3400	stochastic	abstract[155]|abstract[156]|abstract[157]	giv[155]|giv[156]|giv[157]	coref	22-6[157_156]
22-7	3401-3407	neural	abstract[155]|abstract[156]|abstract[157]	giv[155]|giv[156]|giv[157]	_	_
22-8	3408-3416	networks	abstract[155]|abstract[156]|abstract[157]	giv[155]|giv[156]|giv[157]	_	_
22-9	3417-3420	and	abstract[155]|abstract[157]	giv[155]|giv[157]	_	_
22-10	3421-3424	its	abstract[155]|abstract[157]|abstract|abstract[160]	giv[155]|giv[157]|giv|giv[160]	coref	27-17[211_160]
22-11	3425-3438	corresponding	abstract[155]|abstract[157]|abstract[160]	giv[155]|giv[157]|giv[160]	_	_
22-12	3439-3443	loss	abstract[155]|abstract[157]|abstract|abstract[160]	giv[155]|giv[157]|giv|giv[160]	coref	27-18
22-13	3444-3452	function	abstract[155]|abstract[157]|abstract[160]	giv[155]|giv[157]|giv[160]	_	_
22-14	3453-3456	are	_	_	_	_
22-15	3457-3467	determined	_	_	_	_
22-16	3468-3469	,	_	_	_	_
22-17	3470-3473	the	abstract[162]	new[162]	coref	33-17[0_162]
22-18	3474-3480	weight	abstract|abstract[162]	new|new[162]	coref	26-26
22-19	3481-3491	parameters	abstract[162]	new[162]	_	_
22-20	3492-3495	are	_	_	_	_
22-21	3496-3505	optimized	_	_	_	_
22-22	3506-3508	by	_	_	_	_
22-23	3509-3517	gradient	abstract|abstract[164]|abstract[165]	giv|giv[164]|giv[165]	coref|coref|coref	23-10|23-10[170_164]|23-10[172_165]
22-24	3518-3525	descent	abstract[164]|abstract[165]	giv[164]|giv[165]	_	_
22-25	3526-3532	method	abstract[165]	giv[165]	_	_
22-26	3533-3534	.	_	_	_	_

#Text=The well-known error-backpropagation algorithm is the standard type of gradient descent learning method .
23-1	3535-3538	The	abstract[167]	new[167]	coref	23-6[168_167]
23-2	3539-3549	well-known	abstract[167]	new[167]	_	_
23-3	3550-3571	error-backpropagation	abstract|abstract[167]	new|new[167]	_	_
23-4	3572-3581	algorithm	abstract[167]	new[167]	_	_
23-5	3582-3584	is	_	_	_	_
23-6	3585-3588	the	abstract[168]	giv[168]	ana	25-26[0_168]
23-7	3589-3597	standard	abstract[168]	giv[168]	_	_
23-8	3598-3602	type	abstract[168]	giv[168]	_	_
23-9	3603-3605	of	abstract[168]	giv[168]	_	_
23-10	3606-3614	gradient	abstract[168]|abstract|abstract[170]|abstract[172]	giv[168]|giv|giv[170]|giv[172]	coref|coref|coref	24-9|24-9[175_170]|24-7[176_172]
23-11	3615-3622	descent	abstract[168]|abstract[170]|abstract[172]	giv[168]|giv[170]|giv[172]	_	_
23-12	3623-3631	learning	abstract[168]|abstract|abstract[172]	giv[168]|giv|giv[172]	coref	24-22
23-13	3632-3638	method	abstract[168]|abstract[172]	giv[168]|giv[172]	_	_
23-14	3639-3640	.	_	_	_	_

#Text=There have been numerous variations of the standard gradient descent method , including second-order methods , momentum method , and adaptive learning rate methods .
24-1	3641-3646	There	_	_	_	_
24-2	3647-3651	have	_	_	_	_
24-3	3652-3656	been	_	_	_	_
24-4	3657-3665	numerous	abstract[173]	new[173]	_	_
24-5	3666-3676	variations	abstract[173]	new[173]	_	_
24-6	3677-3679	of	abstract[173]	new[173]	_	_
24-7	3680-3683	the	abstract[173]|abstract[176]	new[173]|giv[176]	coref	24-17[179_176]
24-8	3684-3692	standard	abstract[173]|abstract[176]	new[173]|giv[176]	_	_
24-9	3693-3701	gradient	abstract[173]|abstract|abstract[175]|abstract[176]	new[173]|giv|giv[175]|giv[176]	coref|coref	25-4|25-12[187_175]
24-10	3702-3709	descent	abstract[173]|abstract[175]|abstract[176]	new[173]|giv[175]|giv[176]	_	_
24-11	3710-3716	method	abstract[173]|abstract[176]	new[173]|giv[176]	_	_
24-12	3717-3718	,	abstract[173]	new[173]	_	_
24-13	3719-3728	including	abstract[173]	new[173]	_	_
24-14	3729-3741	second-order	abstract[173]|abstract[177]	new[173]|new[177]	_	_
24-15	3742-3749	methods	abstract[173]|abstract[177]	new[173]|new[177]	_	_
24-16	3750-3751	,	abstract[173]	new[173]	_	_
24-17	3752-3760	momentum	abstract[173]|abstract|abstract[179]	new[173]|new|giv[179]	coref	25-2[185_179]
24-18	3761-3767	method	abstract[173]|abstract[179]	new[173]|giv[179]	_	_
24-19	3768-3769	,	abstract[173]	new[173]	_	_
24-20	3770-3773	and	abstract[173]	new[173]	_	_
24-21	3774-3782	adaptive	abstract[173]|abstract[181]|abstract[182]	new[173]|new[181]|new[182]	_	_
24-22	3783-3791	learning	abstract[173]|abstract|abstract[181]|abstract[182]	new[173]|giv|new[181]|new[182]	coref	25-3[184_0]
24-23	3792-3796	rate	abstract[173]|abstract[181]|abstract[182]	new[173]|new[181]|new[182]	_	_
24-24	3797-3804	methods	abstract[173]|abstract[182]	new[173]|new[182]	_	_
24-25	3805-3806	.	_	_	_	_

#Text=Since the natural gradient learning method is also based on the gradient descent method , we describe the basic formula of gradient descent learning and its online version that is called stochastic gradient descent method .
25-1	3807-3812	Since	_	_	_	_
25-2	3813-3816	the	abstract[185]	giv[185]	coref	25-11[188_185]
25-3	3817-3824	natural	abstract[184]|abstract[185]	giv[184]|giv[185]	coref	25-22[193_184]
25-4	3825-3833	gradient	abstract|abstract[184]|abstract[185]	giv|giv[184]|giv[185]	coref	25-12
25-5	3834-3842	learning	abstract[184]|abstract[185]	giv[184]|giv[185]	_	_
25-6	3843-3849	method	abstract[185]	giv[185]	_	_
25-7	3850-3852	is	_	_	_	_
25-8	3853-3857	also	_	_	_	_
25-9	3858-3863	based	_	_	_	_
25-10	3864-3866	on	_	_	_	_
25-11	3867-3870	the	abstract[188]	giv[188]	coref	25-32[198_188]
25-12	3871-3879	gradient	abstract|abstract[187]|abstract[188]	giv|giv[187]|giv[188]	coref|coref	25-22|25-22[192_187]
25-13	3880-3887	descent	abstract[187]|abstract[188]	giv[187]|giv[188]	_	_
25-14	3888-3894	method	abstract[188]	giv[188]	_	_
25-15	3895-3896	,	_	_	_	_
25-16	3897-3899	we	person	giv	_	_
25-17	3900-3908	describe	_	_	_	_
25-18	3909-3912	the	abstract[190]	new[190]	_	_
25-19	3913-3918	basic	abstract[190]	new[190]	_	_
25-20	3919-3926	formula	abstract[190]	new[190]	_	_
25-21	3927-3929	of	abstract[190]	new[190]	_	_
25-22	3930-3938	gradient	abstract[190]|abstract|abstract[192]|abstract[193]	new[190]|giv|giv[192]|giv[193]	coref|coref|coref	25-33|25-33[197_192]|27-29[0_193]
25-23	3939-3946	descent	abstract[190]|abstract[192]|abstract[193]	new[190]|giv[192]|giv[193]	_	_
25-24	3947-3955	learning	abstract[190]|abstract[193]	new[190]|giv[193]	_	_
25-25	3956-3959	and	abstract[190]	new[190]	_	_
25-26	3960-3963	its	abstract[190]|abstract|abstract[195]	new[190]|giv|new[195]	_	_
25-27	3964-3970	online	abstract[190]|abstract[195]	new[190]|new[195]	_	_
25-28	3971-3978	version	abstract[190]|abstract[195]	new[190]|new[195]	_	_
25-29	3979-3983	that	abstract[190]|abstract[195]	new[190]|new[195]	_	_
25-30	3984-3986	is	abstract[190]|abstract[195]	new[190]|new[195]	_	_
25-31	3987-3993	called	abstract[190]|abstract[195]	new[190]|new[195]	_	_
25-32	3994-4004	stochastic	abstract[190]|abstract[195]|abstract[198]	new[190]|new[195]|giv[198]	_	_
25-33	4005-4013	gradient	abstract[190]|abstract[195]|abstract|abstract[197]|abstract[198]	new[190]|new[195]|giv|giv[197]|giv[198]	coref|coref	28-23[220_0]|30-10[0_197]
25-34	4014-4021	descent	abstract[190]|abstract[195]|abstract[197]|abstract[198]	new[190]|new[195]|giv[197]|giv[198]	_	_
25-35	4022-4028	method	abstract[190]|abstract[195]|abstract[198]	new[190]|new[195]|giv[198]	_	_
25-36	4029-4030	.	_	_	_	_

#Text=When a set of training data is given , a neural network is trained in order to find an input-output mapping that is specified with weight parameter vector .
26-1	4031-4035	When	_	_	_	_
26-2	4036-4037	a	abstract[200]	new[200]	coref	32-8[0_200]
26-3	4038-4041	set	abstract[200]	new[200]	_	_
26-4	4042-4044	of	abstract[200]	new[200]	_	_
26-5	4045-4053	training	abstract|abstract[200]	new|new[200]	_	_
26-6	4054-4058	data	abstract[200]	new[200]	_	_
26-7	4059-4061	is	_	_	_	_
26-8	4062-4067	given	_	_	_	_
26-9	4068-4069	,	_	_	_	_
26-10	4070-4071	a	abstract[201]	giv[201]	coref	27-4[207_201]
26-11	4072-4078	neural	abstract[201]	giv[201]	_	_
26-12	4079-4086	network	abstract[201]	giv[201]	_	_
26-13	4087-4089	is	_	_	_	_
26-14	4090-4097	trained	_	_	_	_
26-15	4098-4100	in	_	_	_	_
26-16	4101-4106	order	_	_	_	_
26-17	4107-4109	to	_	_	_	_
26-18	4110-4114	find	_	_	_	_
26-19	4115-4117	an	abstract[202]	new[202]	_	_
26-20	4118-4130	input-output	abstract[202]	new[202]	_	_
26-21	4131-4138	mapping	abstract[202]	new[202]	_	_
26-22	4139-4143	that	abstract[202]	new[202]	_	_
26-23	4144-4146	is	abstract[202]	new[202]	_	_
26-24	4147-4156	specified	abstract[202]	new[202]	_	_
26-25	4157-4161	with	abstract[202]	new[202]	_	_
26-26	4162-4168	weight	abstract[202]|abstract|abstract[205]	new[202]|giv|giv[205]	coref	28-7
26-27	4169-4178	parameter	abstract[202]|abstract|abstract[205]	new[202]|giv|giv[205]	coref	28-6[217_0]
26-28	4179-4185	vector	abstract[202]|abstract[205]	new[202]|giv[205]	_	_
26-29	4186-4187	.	_	_	_	_

#Text=The error of neural network for the whole data set can then be defined by using a loss function such as ( 7 ) and the goal of learning is to get the optimal minimizing .
27-1	4188-4191	The	abstract[206]	giv[206]	_	_
27-2	4192-4197	error	abstract[206]	giv[206]	_	_
27-3	4198-4200	of	abstract[206]	giv[206]	_	_
27-4	4201-4207	neural	abstract[206]|abstract[207]	giv[206]|giv[207]	_	_
27-5	4208-4215	network	abstract[206]|abstract[207]	giv[206]|giv[207]	_	_
27-6	4216-4219	for	abstract[206]	giv[206]	_	_
27-7	4220-4223	the	abstract[206]|abstract[209]	giv[206]|new[209]	coref	29-17[228_209]
27-8	4224-4229	whole	abstract[206]|abstract[209]	giv[206]|new[209]	_	_
27-9	4230-4234	data	abstract[206]|abstract|abstract[209]	giv[206]|new|new[209]	_	_
27-10	4235-4238	set	abstract[206]|abstract[209]	giv[206]|new[209]	_	_
27-11	4239-4242	can	_	_	_	_
27-12	4243-4247	then	_	_	_	_
27-13	4248-4250	be	_	_	_	_
27-14	4251-4258	defined	_	_	_	_
27-15	4259-4261	by	_	_	_	_
27-16	4262-4267	using	_	_	_	_
27-17	4268-4269	a	abstract[211]	giv[211]	coref	33-26[254_211]
27-18	4270-4274	loss	abstract|abstract[211]	giv|giv[211]	coref	33-28
27-19	4275-4283	function	abstract[211]	giv[211]	_	_
27-20	4284-4288	such	abstract[211]	giv[211]	_	_
27-21	4289-4291	as	abstract[211]	giv[211]	_	_
27-22	4292-4293	(	abstract[211]	giv[211]	_	_
27-23	4294-4295	7	abstract[211]	giv[211]	_	_
27-24	4296-4297	)	abstract[211]	giv[211]	_	_
27-25	4298-4301	and	_	_	_	_
27-26	4302-4305	the	abstract[212]	new[212]	coref	28-3[215_212]
27-27	4306-4310	goal	abstract[212]	new[212]	_	_
27-28	4311-4313	of	abstract[212]	new[212]	_	_
27-29	4314-4322	learning	abstract[212]|abstract	new[212]|giv	coref	29-6[224_0]
27-30	4323-4325	is	_	_	_	_
27-31	4326-4328	to	_	_	_	_
27-32	4329-4332	get	_	_	_	_
27-33	4333-4336	the	abstract[214]	new[214]	_	_
27-34	4337-4344	optimal	abstract[214]	new[214]	_	_
27-35	4345-4355	minimizing	abstract[214]	new[214]	_	_
27-36	4356-4357	.	_	_	_	_

#Text=To achieve the goal , the weight parameter is updated starting from the current position , according to the opposite direction of the gradient of , which can be written as ( 8 )
28-1	4358-4360	To	_	_	_	_
28-2	4361-4368	achieve	_	_	_	_
28-3	4369-4372	the	abstract[215]	giv[215]	_	_
28-4	4373-4377	goal	abstract[215]	giv[215]	_	_
28-5	4378-4379	,	_	_	_	_
28-6	4380-4383	the	abstract[217]	giv[217]	_	_
28-7	4384-4390	weight	abstract|abstract[217]	giv|giv[217]	_	_
28-8	4391-4400	parameter	abstract[217]	giv[217]	_	_
28-9	4401-4403	is	_	_	_	_
28-10	4404-4411	updated	_	_	_	_
28-11	4412-4420	starting	_	_	_	_
28-12	4421-4425	from	_	_	_	_
28-13	4426-4429	the	abstract[218]	new[218]	coref	30-14[234_218]
28-14	4430-4437	current	abstract[218]	new[218]	_	_
28-15	4438-4446	position	abstract[218]	new[218]	_	_
28-16	4447-4448	,	_	_	_	_
28-17	4449-4458	according	_	_	_	_
28-18	4459-4461	to	_	_	_	_
28-19	4462-4465	the	abstract[219]	new[219]	coref	30-8[233_219]
28-20	4466-4474	opposite	abstract[219]	new[219]	_	_
28-21	4475-4484	direction	abstract[219]	new[219]	_	_
28-22	4485-4487	of	abstract[219]	new[219]	_	_
28-23	4488-4491	the	abstract[219]|abstract[220]	new[219]|giv[220]	coref	33-9[0_220]
28-24	4492-4500	gradient	abstract[219]|abstract[220]	new[219]|giv[220]	_	_
28-25	4501-4503	of	abstract[219]|abstract[220]	new[219]|giv[220]	_	_
28-26	4504-4505	,	abstract[219]	new[219]	_	_
28-27	4506-4511	which	abstract[219]	new[219]	_	_
28-28	4512-4515	can	abstract[219]	new[219]	_	_
28-29	4516-4518	be	abstract[219]	new[219]	_	_
28-30	4519-4526	written	abstract[219]	new[219]	_	_
28-31	4527-4529	as	_	_	_	_
28-32	4530-4531	(	_	_	_	_
28-33	4532-4533	8	_	_	_	_
28-34	4534-4535	)	_	_	_	_

#Text=This update rule is called batch learning mode , meaning that an update is done for the whole batch set .
29-1	4536-4540	This	abstract[222]	new[222]	_	_
29-2	4541-4547	update	event|abstract[222]	new|new[222]	coref	29-12[226_0]
29-3	4548-4552	rule	abstract[222]	new[222]	_	_
29-4	4553-4555	is	_	_	_	_
29-5	4556-4562	called	_	_	_	_
29-6	4563-4568	batch	abstract|abstract[224]|abstract[225]	new|giv[224]|new[225]	coref|coref|coref	29-19|30-4[230_225]|30-3[231_224]
29-7	4569-4577	learning	abstract[224]|abstract[225]	giv[224]|new[225]	_	_
29-8	4578-4582	mode	abstract[225]	new[225]	_	_
29-9	4583-4584	,	_	_	_	_
29-10	4585-4592	meaning	_	_	_	_
29-11	4593-4597	that	_	_	_	_
29-12	4598-4600	an	event[226]	giv[226]	coref	32-19[244_226]
29-13	4601-4607	update	event[226]	giv[226]	_	_
29-14	4608-4610	is	_	_	_	_
29-15	4611-4615	done	_	_	_	_
29-16	4616-4619	for	_	_	_	_
29-17	4620-4623	the	abstract[228]	giv[228]	_	_
29-18	4624-4629	whole	abstract[228]	giv[228]	_	_
29-19	4630-4635	batch	abstract|abstract[228]	giv|giv[228]	coref	30-4
29-20	4636-4639	set	abstract[228]	giv[228]	_	_
29-21	4640-4641	.	_	_	_	_

#Text=Theoretically , the batch mode learning gives the steepest descent direction of at the current position of , but it has two practical drawbacks .
30-1	4642-4655	Theoretically	_	_	_	_
30-2	4656-4657	,	_	_	_	_
30-3	4658-4661	the	abstract[231]	giv[231]	coref	33-7[248_231]
30-4	4662-4667	batch	abstract|abstract[230]|abstract[231]	giv|giv[230]|giv[231]	_	_
30-5	4668-4672	mode	abstract[230]|abstract[231]	giv[230]|giv[231]	_	_
30-6	4673-4681	learning	abstract[231]	giv[231]	_	_
30-7	4682-4687	gives	_	_	_	_
30-8	4688-4691	the	abstract[233]	giv[233]	_	_
30-9	4692-4700	steepest	abstract[233]	giv[233]	_	_
30-10	4701-4708	descent	abstract|abstract[233]	giv|giv[233]	_	_
30-11	4709-4718	direction	abstract[233]	giv[233]	_	_
30-12	4719-4721	of	abstract[233]	giv[233]	_	_
30-13	4722-4724	at	_	_	_	_
30-14	4725-4728	the	abstract[234]	giv[234]	ana	30-20[0_234]
30-15	4729-4736	current	abstract[234]	giv[234]	_	_
30-16	4737-4745	position	abstract[234]	giv[234]	_	_
30-17	4746-4748	of	abstract[234]	giv[234]	_	_
30-18	4749-4750	,	_	_	_	_
30-19	4751-4754	but	_	_	_	_
30-20	4755-4757	it	abstract	giv	ana	31-3
30-21	4758-4761	has	_	_	_	_
30-22	4762-4765	two	abstract[236]	new[236]	_	_
30-23	4766-4775	practical	abstract[236]	new[236]	_	_
30-24	4776-4785	drawbacks	abstract[236]	new[236]	_	_
30-25	4786-4787	.	_	_	_	_

#Text=First , it is too stable to be easily trapped in undesirable local minima .
31-1	4788-4793	First	_	_	_	_
31-2	4794-4795	,	_	_	_	_
31-3	4796-4798	it	abstract	giv	_	_
31-4	4799-4801	is	_	_	_	_
31-5	4802-4805	too	_	_	_	_
31-6	4806-4812	stable	_	_	_	_
31-7	4813-4815	to	_	_	_	_
31-8	4816-4818	be	_	_	_	_
31-9	4819-4825	easily	_	_	_	_
31-10	4826-4833	trapped	_	_	_	_
31-11	4834-4836	in	_	_	_	_
31-12	4837-4848	undesirable	abstract[238]	new[238]	_	_
31-13	4849-4854	local	abstract[238]	new[238]	_	_
31-14	4855-4861	minima	abstract[238]	new[238]	_	_
31-15	4862-4863	.	_	_	_	_

#Text=In addition , when the number of data is large , it needs large amounts of computation for just a single update , making the learning process slow .
32-1	4864-4866	In	_	_	_	_
32-2	4867-4875	addition	_	_	_	_
32-3	4876-4877	,	_	_	_	_
32-4	4878-4882	when	_	_	_	_
32-5	4883-4886	the	abstract[239]	new[239]	ana	32-12[0_239]
32-6	4887-4893	number	abstract[239]	new[239]	_	_
32-7	4894-4896	of	abstract[239]	new[239]	_	_
32-8	4897-4901	data	abstract[239]|abstract	new[239]|giv	coref	33-22
32-9	4902-4904	is	_	_	_	_
32-10	4905-4910	large	_	_	_	_
32-11	4911-4912	,	_	_	_	_
32-12	4913-4915	it	abstract	giv	_	_
32-13	4916-4921	needs	_	_	_	_
32-14	4922-4927	large	abstract[242]	new[242]	_	_
32-15	4928-4935	amounts	abstract[242]	new[242]	_	_
32-16	4936-4938	of	abstract[242]	new[242]	_	_
32-17	4939-4950	computation	abstract[242]|abstract	new[242]|new	_	_
32-18	4951-4954	for	_	_	_	_
32-19	4955-4959	just	event[244]	giv[244]	_	_
32-20	4960-4961	a	event[244]	giv[244]	_	_
32-21	4962-4968	single	event[244]	giv[244]	_	_
32-22	4969-4975	update	event[244]	giv[244]	_	_
32-23	4976-4977	,	_	_	_	_
32-24	4978-4984	making	_	_	_	_
32-25	4985-4988	the	abstract[245]	new[245]	_	_
32-26	4989-4997	learning	abstract[245]	new[245]	_	_
32-27	4998-5005	process	abstract[245]	new[245]	_	_
32-28	5006-5010	slow	_	_	_	_
32-29	5011-5012	.	_	_	_	_

#Text=To overcome this practical inefficiency , online stochastic gradient decent learning is proposed , in which parameters are updated for each data sample by using gradient of loss function defined with a single data pair , such as ( 9 )
33-1	5013-5015	To	_	_	_	_
33-2	5016-5024	overcome	_	_	_	_
33-3	5025-5029	this	abstract[246]	new[246]	_	_
33-4	5030-5039	practical	abstract[246]	new[246]	_	_
33-5	5040-5052	inefficiency	abstract[246]	new[246]	_	_
33-6	5053-5054	,	_	_	_	_
33-7	5055-5061	online	abstract[248]	giv[248]	_	_
33-8	5062-5072	stochastic	abstract[248]	giv[248]	_	_
33-9	5073-5081	gradient	abstract|abstract[248]	giv|giv[248]	coref	33-26[252_0]
33-10	5082-5088	decent	abstract[248]	giv[248]	_	_
33-11	5089-5097	learning	abstract[248]	giv[248]	_	_
33-12	5098-5100	is	_	_	_	_
33-13	5101-5109	proposed	_	_	_	_
33-14	5110-5111	,	_	_	_	_
33-15	5112-5114	in	_	_	_	_
33-16	5115-5120	which	_	_	_	_
33-17	5121-5131	parameters	abstract	giv	_	_
33-18	5132-5135	are	_	_	_	_
33-19	5136-5143	updated	_	_	_	_
33-20	5144-5147	for	_	_	_	_
33-21	5148-5152	each	abstract[251]	new[251]	_	_
33-22	5153-5157	data	abstract|abstract[251]	giv|new[251]	coref	33-34
33-23	5158-5164	sample	abstract[251]	new[251]	_	_
33-24	5165-5167	by	_	_	_	_
33-25	5168-5173	using	_	_	_	_
33-26	5174-5182	gradient	abstract[252]|abstract[254]	giv[252]|giv[254]	_	_
33-27	5183-5185	of	abstract[252]|abstract[254]	giv[252]|giv[254]	_	_
33-28	5186-5190	loss	abstract[252]|abstract|abstract[254]	giv[252]|giv|giv[254]	_	_
33-29	5191-5199	function	abstract[254]	giv[254]	_	_
33-30	5200-5207	defined	abstract[254]	giv[254]	_	_
33-31	5208-5212	with	abstract[254]	giv[254]	_	_
33-32	5213-5214	a	abstract[254]|abstract[256]	giv[254]|new[256]	_	_
33-33	5215-5221	single	abstract[254]|abstract[256]	giv[254]|new[256]	_	_
33-34	5222-5226	data	abstract[254]|abstract|abstract[256]	giv[254]|giv|new[256]	_	_
33-35	5227-5231	pair	abstract[254]|abstract[256]	giv[254]|new[256]	_	_
33-36	5232-5233	,	_	_	_	_
33-37	5234-5238	such	_	_	_	_
33-38	5239-5241	as	_	_	_	_
33-39	5242-5243	(	_	_	_	_
33-40	5244-5245	9	_	_	_	_
33-41	5246-5247	)	_	_	_	_
