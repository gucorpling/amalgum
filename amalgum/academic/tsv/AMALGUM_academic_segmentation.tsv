#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=6. Results and Discussion
1-1	0-2	6.	_	_	_	_
1-2	3-10	Results	abstract|abstract[2]	new|new[2]	coref|coref	2-1[4_0]|6-4[29_2]
1-3	11-14	and	abstract[2]	new[2]	_	_
1-4	15-25	Discussion	abstract[2]|abstract	new[2]|new	_	_

#Text=The results of the evaluation experiments with our algorithm are presented in Table 5 .
2-1	26-29	The	abstract[4]	giv[4]	coref	3-12[15_4]
2-2	30-37	results	abstract[4]	giv[4]	_	_
2-3	38-40	of	abstract[4]	giv[4]	_	_
2-4	41-44	the	abstract[4]|event[6]	giv[4]|new[6]	coref	6-7[30_6]
2-5	45-55	evaluation	abstract[4]|abstract|event[6]	giv[4]|new|new[6]	_	_
2-6	56-67	experiments	abstract[4]|event[6]	giv[4]|new[6]	_	_
2-7	68-72	with	abstract[4]|event[6]	giv[4]|new[6]	_	_
2-8	73-76	our	abstract[4]|event[6]|person|abstract[8]	giv[4]|new[6]|acc|new[8]	ana	22-8
2-9	77-86	algorithm	abstract[4]|event[6]|abstract[8]	giv[4]|new[6]|new[8]	_	_
2-10	87-90	are	_	_	_	_
2-11	91-100	presented	_	_	_	_
2-12	101-103	in	_	_	_	_
2-13	104-109	Table	abstract[9]	new[9]	_	_
2-14	110-111	5	abstract[9]	new[9]	_	_
2-15	112-113	.	_	_	_	_

#Text=The variant without the limit of n-grams per input segment produces unbalanced results ( especially on SYOS ) , with relatively low Precision .
3-1	114-117	The	abstract[10]	new[10]	_	_
3-2	118-125	variant	abstract[10]	new[10]	_	_
3-3	126-133	without	abstract[10]	new[10]	_	_
3-4	134-137	the	abstract[10]|abstract[11]	new[10]|new[11]	coref	4-3[18_11]
3-5	138-143	limit	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-6	144-146	of	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-7	147-154	n-grams	abstract[10]|abstract[11]|abstract	new[10]|new[11]|new	coref	7-25[42_0]
3-8	155-158	per	abstract[10]|abstract[11]	new[10]|new[11]	_	_
3-9	159-164	input	abstract[10]|abstract[11]|abstract|abstract[14]	new[10]|new[11]|new|new[14]	coref|coref	7-27|7-27[44_14]
3-10	165-172	segment	abstract[10]|abstract[11]|abstract[14]	new[10]|new[11]|new[14]	_	_
3-11	173-181	produces	_	_	_	_
3-12	182-192	unbalanced	abstract[15]	giv[15]	_	_
3-13	193-200	results	abstract[15]	giv[15]	_	_
3-14	201-202	(	abstract[15]	giv[15]	_	_
3-15	203-213	especially	abstract[15]|abstract[16]	giv[15]|new[16]	coref	5-6[0_16]
3-16	214-216	on	abstract[15]|abstract[16]	giv[15]|new[16]	_	_
3-17	217-221	SYOS	abstract[15]|abstract[16]	giv[15]|new[16]	_	_
3-18	222-223	)	abstract[15]	giv[15]	_	_
3-19	224-225	,	abstract[15]	giv[15]	_	_
3-20	226-230	with	abstract[15]	giv[15]	_	_
3-21	231-241	relatively	abstract[15]|abstract[17]	giv[15]|new[17]	coref	4-8[0_17]
3-22	242-245	low	abstract[15]|abstract[17]	giv[15]|new[17]	_	_
3-23	246-255	Precision	abstract[15]|abstract[17]	giv[15]|new[17]	_	_
3-24	256-257	.	_	_	_	_

#Text=After setting the limit to 2 , Precision improves at the cost of a drop in Recall .
4-1	258-263	After	_	_	_	_
4-2	264-271	setting	_	_	_	_
4-3	272-275	the	abstract[18]	giv[18]	_	_
4-4	276-281	limit	abstract[18]	giv[18]	_	_
4-5	282-284	to	_	_	_	_
4-6	285-286	2	abstract	new	_	_
4-7	287-288	,	_	_	_	_
4-8	289-298	Precision	abstract	giv	coref	8-14
4-9	299-307	improves	_	_	_	_
4-10	308-310	at	_	_	_	_
4-11	311-314	the	abstract[21]	new[21]	_	_
4-12	315-319	cost	abstract[21]	new[21]	_	_
4-13	320-322	of	abstract[21]	new[21]	_	_
4-14	323-324	a	abstract[21]|object[22]	new[21]|new[22]	coref	5-13[27_22]
4-15	325-329	drop	abstract[21]|object[22]	new[21]|new[22]	_	_
4-16	330-332	in	abstract[21]|object[22]	new[21]|new[22]	_	_
4-17	333-339	Recall	abstract[21]|object[22]|abstract	new[21]|new[22]|new	coref	8-26
4-18	340-341	.	_	_	_	_

#Text=The F-score is better for SYOS , while on AKJ there is a very slight drop .
5-1	342-345	The	abstract[24]	new[24]	coref	8-16[0_24]
5-2	346-353	F-score	abstract[24]	new[24]	_	_
5-3	354-356	is	_	_	_	_
5-4	357-363	better	_	_	_	_
5-5	364-367	for	_	_	_	_
5-6	368-372	SYOS	abstract	giv	coref	19-14[146_0]
5-7	373-374	,	_	_	_	_
5-8	375-380	while	_	_	_	_
5-9	381-383	on	_	_	_	_
5-10	384-387	AKJ	organization	new	_	_
5-11	388-393	there	_	_	_	_
5-12	394-396	is	_	_	_	_
5-13	397-398	a	object[27]	giv[27]	coref	12-39[99_27]
5-14	399-403	very	object[27]	giv[27]	_	_
5-15	404-410	slight	object[27]	giv[27]	_	_
5-16	411-415	drop	object[27]	giv[27]	_	_
5-17	416-417	.	_	_	_	_

#Text=Table 6 shows the results of experiments with the Stupid Backoff model .
6-1	418-423	Table	abstract[28]	new[28]	_	_
6-2	424-425	6	abstract[28]	new[28]	_	_
6-3	426-431	shows	_	_	_	_
6-4	432-435	the	abstract[29]	giv[29]	coref	7-8[35_29]
6-5	436-443	results	abstract[29]	giv[29]	_	_
6-6	444-446	of	abstract[29]	giv[29]	_	_
6-7	447-458	experiments	abstract[29]|event[30]	giv[29]|giv[30]	_	_
6-8	459-463	with	abstract[29]|event[30]	giv[29]|giv[30]	_	_
6-9	464-467	the	abstract[29]|event[30]|abstract[32]	giv[29]|giv[30]|new[32]	coref	11-31[85_32]
6-10	468-474	Stupid	abstract[29]|event[30]|abstract[32]	giv[29]|giv[30]|new[32]	_	_
6-11	475-482	Backoff	abstract[29]|event[30]|abstract|abstract[32]	giv[29]|giv[30]|new|new[32]	coref	7-3
6-12	483-488	model	abstract[29]|event[30]|abstract[32]	giv[29]|giv[30]|new[32]	_	_
6-13	489-490	.	_	_	_	_

#Text=When no backoff factor is applied , results for both test sets are similar to those from the MiNgMatch Segmenter without the limit of n-grams per input segment .
7-1	491-495	When	_	_	_	_
7-2	496-498	no	abstract[34]	new[34]	ana	7-16[38_34]
7-3	499-506	backoff	abstract|abstract[34]	giv|new[34]	coref	8-3
7-4	507-513	factor	abstract[34]	new[34]	_	_
7-5	514-516	is	_	_	_	_
7-6	517-524	applied	_	_	_	_
7-7	525-526	,	_	_	_	_
7-8	527-534	results	abstract[35]	giv[35]	coref	11-20[82_35]
7-9	535-538	for	abstract[35]	giv[35]	_	_
7-10	539-543	both	abstract[35]|abstract[37]	giv[35]|new[37]	_	_
7-11	544-548	test	abstract[35]|abstract|abstract[37]	giv[35]|new|new[37]	coref	21-6
7-12	549-553	sets	abstract[35]|abstract[37]	giv[35]|new[37]	_	_
7-13	554-557	are	_	_	_	_
7-14	558-565	similar	_	_	_	_
7-15	566-568	to	_	_	_	_
7-16	569-574	those	abstract[38]	giv[38]	coref	8-2[46_38]
7-17	575-579	from	abstract[38]	giv[38]	_	_
7-18	580-583	the	abstract[38]|person[40]	giv[38]|new[40]	_	_
7-19	584-593	MiNgMatch	abstract[38]|abstract|person[40]	giv[38]|new|new[40]	_	_
7-20	594-603	Segmenter	abstract[38]|person[40]	giv[38]|new[40]	_	_
7-21	604-611	without	_	_	_	_
7-22	612-615	the	abstract[41]	new[41]	coref	12-12[89_41]
7-23	616-621	limit	abstract[41]	new[41]	_	_
7-24	622-624	of	abstract[41]	new[41]	_	_
7-25	625-632	n-grams	abstract[41]|abstract[42]	new[41]|giv[42]	coref	11-28[84_42]
7-26	633-636	per	abstract[41]|abstract[42]	new[41]|giv[42]	_	_
7-27	637-642	input	abstract[41]|abstract[42]|abstract|abstract[44]	new[41]|giv[42]|giv|giv[44]	coref|coref	12-17|12-17[92_44]
7-28	643-650	segment	abstract[41]|abstract[42]|abstract[44]	new[41]|giv[42]|giv[44]	_	_
7-29	651-652	.	_	_	_	_

#Text=Setting the backoff factor to an appropriate value allows for significant improvement in Precision and F-score ( and in some cases also small improvements in Recall ) .
8-1	653-660	Setting	_	_	_	_
8-2	661-664	the	abstract[46]	giv[46]	coref	9-10[57_46]
8-3	665-672	backoff	abstract|abstract[46]	giv|giv[46]	coref	9-12
8-4	673-679	factor	abstract[46]	giv[46]	_	_
8-5	680-682	to	_	_	_	_
8-6	683-685	an	abstract[47]	new[47]	coref	9-28[61_47]
8-7	686-697	appropriate	abstract[47]	new[47]	_	_
8-8	698-703	value	abstract[47]	new[47]	_	_
8-9	704-710	allows	_	_	_	_
8-10	711-714	for	_	_	_	_
8-11	715-726	significant	abstract[48]	new[48]	coref	10-7[69_48]
8-12	727-738	improvement	abstract[48]	new[48]	_	_
8-13	739-741	in	abstract[48]	new[48]	_	_
8-14	742-751	Precision	abstract[48]|abstract	new[48]|giv	coref	10-10[70_0]
8-15	752-755	and	abstract[48]	new[48]	_	_
8-16	756-763	F-score	abstract[48]|abstract	new[48]|giv	coref	9-2[54_0]
8-17	764-765	(	_	_	_	_
8-18	766-769	and	_	_	_	_
8-19	770-772	in	_	_	_	_
8-20	773-777	some	abstract[51]|abstract[52]	new[51]|new[52]	coref	27-16[199_51]
8-21	778-783	cases	abstract[51]|abstract[52]	new[51]|new[52]	_	_
8-22	784-788	also	abstract[52]	new[52]	_	_
8-23	789-794	small	abstract[52]	new[52]	_	_
8-24	795-807	improvements	abstract[52]	new[52]	_	_
8-25	808-810	in	abstract[52]	new[52]	_	_
8-26	811-817	Recall	abstract[52]|abstract	new[52]|giv	ana	9-5
8-27	818-819	)	_	_	_	_
8-28	820-821	.	_	_	_	_

#Text=For the F-score , it is better to set a low backoff factor ( e.g. , 0.09 ) for 1-grams only , than to set it to a fixed value for all backoff steps ( e.g. , 0.4 , as Brants et al. did ) .
9-1	822-825	For	_	_	_	_
9-2	826-829	the	abstract[54]	giv[54]	coref	25-15[186_54]
9-3	830-837	F-score	abstract[54]	giv[54]	_	_
9-4	838-839	,	_	_	_	_
9-5	840-842	it	abstract	giv	ana	9-26
9-6	843-845	is	_	_	_	_
9-7	846-852	better	_	_	_	_
9-8	853-855	to	_	_	_	_
9-9	856-859	set	_	_	_	_
9-10	860-861	a	abstract[57]	giv[57]	appos	9-15[58_57]
9-11	862-865	low	abstract[57]	giv[57]	_	_
9-12	866-873	backoff	abstract|abstract[57]	giv|giv[57]	coref	9-33
9-13	874-880	factor	abstract[57]	giv[57]	_	_
9-14	881-882	(	_	_	_	_
9-15	883-887	e.g.	abstract[58]	giv[58]	coref	10-1[67_58]
9-16	888-889	,	abstract[58]	giv[58]	_	_
9-17	890-894	0.09	abstract[58]	giv[58]	_	_
9-18	895-896	)	_	_	_	_
9-19	897-900	for	_	_	_	_
9-20	901-908	1-grams	abstract[59]	new[59]	_	_
9-21	909-913	only	abstract[59]	new[59]	_	_
9-22	914-915	,	_	_	_	_
9-23	916-920	than	_	_	_	_
9-24	921-923	to	_	_	_	_
9-25	924-927	set	_	_	_	_
9-26	928-930	it	abstract	giv	coref	10-22
9-27	931-933	to	_	_	_	_
9-28	934-935	a	abstract[61]	giv[61]	_	_
9-29	936-941	fixed	abstract[61]	giv[61]	_	_
9-30	942-947	value	abstract[61]	giv[61]	_	_
9-31	948-951	for	_	_	_	_
9-32	952-955	all	abstract[63]	new[63]	appos	9-36[64_63]
9-33	956-963	backoff	abstract|abstract[63]	giv|new[63]	coref	10-2
9-34	964-969	steps	abstract[63]	new[63]	_	_
9-35	970-971	(	_	_	_	_
9-36	972-976	e.g.	abstract[64]	giv[64]	_	_
9-37	977-978	,	abstract[64]	giv[64]	_	_
9-38	979-982	0.4	abstract[64]	giv[64]	_	_
9-39	983-984	,	_	_	_	_
9-40	985-987	as	_	_	_	_
9-41	988-994	Brants	person	new	coref	36-31
9-42	995-997	et	_	_	_	_
9-43	998-1001	al.	_	_	_	_
9-44	1002-1005	did	_	_	_	_
9-45	1006-1007	)	_	_	_	_
9-46	1008-1009	.	_	_	_	_

#Text=A backoff factor of 0.4 gives significant improvement in Precision with higher order n-gram models , but at the same time Recall drops drastically and overall performance deteriorates .
10-1	1010-1011	A	abstract[67]	giv[67]	coref	11-12[80_67]
10-2	1012-1019	backoff	abstract|abstract[67]	giv|giv[67]	coref	11-13
10-3	1020-1026	factor	abstract[67]	giv[67]	_	_
10-4	1027-1029	of	abstract[67]	giv[67]	_	_
10-5	1030-1033	0.4	abstract[67]|abstract	giv[67]|new	_	_
10-6	1034-1039	gives	_	_	_	_
10-7	1040-1051	significant	abstract[69]	giv[69]	coref	27-20[200_69]
10-8	1052-1063	improvement	abstract[69]	giv[69]	_	_
10-9	1064-1066	in	abstract[69]	giv[69]	_	_
10-10	1067-1076	Precision	abstract[69]|abstract[70]	giv[69]|giv[70]	coref	12-30[95_70]
10-11	1077-1081	with	abstract[69]|abstract[70]	giv[69]|giv[70]	_	_
10-12	1082-1088	higher	abstract[69]|abstract[70]|abstract[72]	giv[69]|giv[70]|new[72]	coref	11-2[76_72]
10-13	1089-1094	order	abstract[69]|abstract[70]|abstract[72]	giv[69]|giv[70]|new[72]	_	_
10-14	1095-1101	n-gram	abstract[69]|abstract[70]|abstract|abstract[72]	giv[69]|giv[70]|new|new[72]	coref	17-20
10-15	1102-1108	models	abstract[69]|abstract[70]|abstract[72]	giv[69]|giv[70]|new[72]	_	_
10-16	1109-1110	,	_	_	_	_
10-17	1111-1114	but	_	_	_	_
10-18	1115-1117	at	_	_	_	_
10-19	1118-1121	the	time[73]	new[73]	_	_
10-20	1122-1126	same	time[73]	new[73]	_	_
10-21	1127-1131	time	time[73]	new[73]	_	_
10-22	1132-1138	Recall	abstract	giv	coref	12-44
10-23	1139-1144	drops	_	_	_	_
10-24	1145-1156	drastically	_	_	_	_
10-25	1157-1160	and	_	_	_	_
10-26	1161-1168	overall	abstract[75]	new[75]	coref	26-18[194_75]
10-27	1169-1180	performance	abstract[75]	new[75]	_	_
10-28	1181-1193	deteriorates	_	_	_	_
10-29	1194-1195	.	_	_	_	_

#Text=For models with an n-gram order of 3 or higher , the backoff factor has a bigger impact on the results than further increasing the order of n-grams included in the model .
11-1	1196-1199	For	_	_	_	_
11-2	1200-1206	models	abstract[76]	giv[76]	coref	13-7[103_76]
11-3	1207-1211	with	abstract[76]	giv[76]	_	_
11-4	1212-1214	an	abstract[76]|abstract[77]	giv[76]|new[77]	_	_
11-5	1215-1221	n-gram	abstract[76]|abstract[77]	giv[76]|new[77]	_	_
11-6	1222-1227	order	abstract[76]|abstract[77]	giv[76]|new[77]	_	_
11-7	1228-1230	of	abstract[76]|abstract[77]	giv[76]|new[77]	_	_
11-8	1231-1232	3	abstract[76]|abstract[77]|abstract	giv[76]|new[77]|new	_	_
11-9	1233-1235	or	abstract[76]|abstract[77]	giv[76]|new[77]	_	_
11-10	1236-1242	higher	abstract[76]|abstract[77]	giv[76]|new[77]	_	_
11-11	1243-1244	,	_	_	_	_
11-12	1245-1248	the	abstract[80]	giv[80]	coref	34-19[0_80]
11-13	1249-1256	backoff	abstract|abstract[80]	giv|giv[80]	coref	12-23[93_0]
11-14	1257-1263	factor	abstract[80]	giv[80]	_	_
11-15	1264-1267	has	_	_	_	_
11-16	1268-1269	a	abstract[81]	new[81]	_	_
11-17	1270-1276	bigger	abstract[81]	new[81]	_	_
11-18	1277-1283	impact	abstract[81]	new[81]	_	_
11-19	1284-1286	on	abstract[81]	new[81]	_	_
11-20	1287-1290	the	abstract[81]|abstract[82]	new[81]|giv[82]	coref	12-4[87_82]
11-21	1291-1298	results	abstract[81]|abstract[82]	new[81]|giv[82]	_	_
11-22	1299-1303	than	_	_	_	_
11-23	1304-1311	further	_	_	_	_
11-24	1312-1322	increasing	_	_	_	_
11-25	1323-1326	the	abstract[83]	new[83]	_	_
11-26	1327-1332	order	abstract[83]	new[83]	_	_
11-27	1333-1335	of	abstract[83]	new[83]	_	_
11-28	1336-1343	n-grams	abstract[83]|abstract[84]	new[83]|giv[84]	coref	12-15[90_84]
11-29	1344-1352	included	abstract[83]|abstract[84]	new[83]|giv[84]	_	_
11-30	1353-1355	in	abstract[83]|abstract[84]	new[83]|giv[84]	_	_
11-31	1356-1359	the	abstract[83]|abstract[84]|abstract[85]	new[83]|giv[84]|giv[85]	coref	17-1[116_85]
11-32	1360-1365	model	abstract[83]|abstract[84]|abstract[85]	new[83]|giv[84]|giv[85]	_	_
11-33	1366-1367	.	_	_	_	_

#Text=A comparison with the results yielded by MiNgMatch shows that setting the limit of n-grams per input segment is more effective than Stupid Backoff as a method for improving precision of the segmentation process — it leads to a much smaller drop in Recall .
12-1	1368-1369	A	abstract[86]	new[86]	_	_
12-2	1370-1380	comparison	abstract[86]	new[86]	_	_
12-3	1381-1385	with	abstract[86]	new[86]	_	_
12-4	1386-1389	the	abstract[86]|abstract[87]	new[86]|giv[87]	_	_
12-5	1390-1397	results	abstract[86]|abstract[87]	new[86]|giv[87]	_	_
12-6	1398-1405	yielded	abstract[86]|abstract[87]	new[86]|giv[87]	_	_
12-7	1406-1408	by	abstract[86]|abstract[87]	new[86]|giv[87]	_	_
12-8	1409-1418	MiNgMatch	abstract[86]|abstract[87]|person	new[86]|giv[87]|new	_	_
12-9	1419-1424	shows	_	_	_	_
12-10	1425-1429	that	_	_	_	_
12-11	1430-1437	setting	_	_	_	_
12-12	1438-1441	the	abstract[89]	giv[89]	_	_
12-13	1442-1447	limit	abstract[89]	giv[89]	_	_
12-14	1448-1450	of	abstract[89]	giv[89]	_	_
12-15	1451-1458	n-grams	abstract[89]|abstract[90]	giv[89]|giv[90]	coref	32-16[0_90]
12-16	1459-1462	per	abstract[89]|abstract[90]	giv[89]|giv[90]	_	_
12-17	1463-1468	input	abstract[89]|abstract[90]|abstract|abstract[92]	giv[89]|giv[90]|giv|giv[92]	coref|coref	17-56[133_92]|18-14[139_0]
12-18	1469-1476	segment	abstract[89]|abstract[90]|abstract[92]	giv[89]|giv[90]|giv[92]	_	_
12-19	1477-1479	is	_	_	_	_
12-20	1480-1484	more	_	_	_	_
12-21	1485-1494	effective	_	_	_	_
12-22	1495-1499	than	_	_	_	_
12-23	1500-1506	Stupid	abstract[93]	giv[93]	coref	34-3[0_93]
12-24	1507-1514	Backoff	abstract[93]	giv[93]	_	_
12-25	1515-1517	as	_	_	_	_
12-26	1518-1519	a	abstract[94]	new[94]	_	_
12-27	1520-1526	method	abstract[94]	new[94]	_	_
12-28	1527-1530	for	abstract[94]	new[94]	_	_
12-29	1531-1540	improving	abstract[94]	new[94]	_	_
12-30	1541-1550	precision	abstract[94]|abstract[95]	new[94]|giv[95]	_	_
12-31	1551-1553	of	abstract[94]|abstract[95]	new[94]|giv[95]	_	_
12-32	1554-1557	the	abstract[94]|abstract[95]|abstract[97]	new[94]|giv[95]|new[97]	ana	12-36[0_97]
12-33	1558-1570	segmentation	abstract[94]|abstract[95]|abstract|abstract[97]	new[94]|giv[95]|new|new[97]	coref	18-20[140_0]
12-34	1571-1578	process	abstract[94]|abstract[95]|abstract[97]	new[94]|giv[95]|new[97]	_	_
12-35	1579-1580	—	_	_	_	_
12-36	1581-1583	it	abstract	giv	coref	21-13[160_0]
12-37	1584-1589	leads	_	_	_	_
12-38	1590-1592	to	_	_	_	_
12-39	1593-1594	a	object[99]	giv[99]	_	_
12-40	1595-1599	much	object[99]	giv[99]	_	_
12-41	1600-1607	smaller	object[99]	giv[99]	_	_
12-42	1608-1612	drop	object[99]	giv[99]	_	_
12-43	1613-1615	in	object[99]	giv[99]	_	_
12-44	1616-1622	Recall	object[99]|abstract	giv[99]|giv	coref	15-5[111_0]
12-45	1623-1624	.	_	_	_	_

#Text=The results of the experiment with models employing modified Kneser-Ney smoothing are shown in Table 7 .
13-1	1625-1628	The	abstract[101]	new[101]	coref	15-9[112_101]
13-2	1629-1636	results	abstract[101]	new[101]	_	_
13-3	1637-1639	of	abstract[101]	new[101]	_	_
13-4	1640-1643	the	abstract[101]|event[102]	new[101]|new[102]	_	_
13-5	1644-1654	experiment	abstract[101]|event[102]	new[101]|new[102]	_	_
13-6	1655-1659	with	abstract[101]|event[102]	new[101]|new[102]	_	_
13-7	1660-1666	models	abstract[101]|event[102]|abstract[103]	new[101]|new[102]|giv[103]	ana	14-1[0_103]
13-8	1667-1676	employing	abstract[101]|event[102]|abstract[103]	new[101]|new[102]|giv[103]	_	_
13-9	1677-1685	modified	abstract[101]|event[102]|abstract[103]|abstract[105]	new[101]|new[102]|giv[103]|new[105]	coref	24-13[180_105]
13-10	1686-1696	Kneser-Ney	abstract[101]|event[102]|abstract[103]|person|abstract[105]	new[101]|new[102]|giv[103]|new|new[105]	coref	24-13
13-11	1697-1706	smoothing	abstract[101]|event[102]|abstract[103]|abstract[105]	new[101]|new[102]|giv[103]|new[105]	_	_
13-12	1707-1710	are	_	_	_	_
13-13	1711-1716	shown	_	_	_	_
13-14	1717-1719	in	_	_	_	_
13-15	1720-1725	Table	abstract[106]	new[106]	_	_
13-16	1726-1727	7	abstract[106]	new[106]	_	_
13-17	1728-1729	.	_	_	_	_

#Text=They achieve higher Precision than both the other types of n-gram models .
14-1	1730-1734	They	abstract	giv	coref	14-11[110_0]
14-2	1735-1742	achieve	_	_	_	_
14-3	1743-1749	higher	abstract[108]	new[108]	coref	24-1[175_108]
14-4	1750-1759	Precision	abstract[108]	new[108]	_	_
14-5	1760-1764	than	_	_	_	_
14-6	1765-1769	both	place[109]	new[109]	_	_
14-7	1770-1773	the	place[109]	new[109]	_	_
14-8	1774-1779	other	place[109]	new[109]	_	_
14-9	1780-1785	types	place[109]	new[109]	_	_
14-10	1786-1788	of	place[109]	new[109]	_	_
14-11	1789-1795	n-gram	place[109]|abstract[110]	new[109]|giv[110]	coref	19-1[143_110]
14-12	1796-1802	models	place[109]|abstract[110]	new[109]|giv[110]	_	_
14-13	1803-1804	.	_	_	_	_

#Text=Nevertheless , due to very low Recall , the overall results are low .
15-1	1805-1817	Nevertheless	_	_	_	_
15-2	1818-1819	,	_	_	_	_
15-3	1820-1823	due	_	_	_	_
15-4	1824-1826	to	_	_	_	_
15-5	1827-1831	very	abstract[111]	giv[111]	coref	24-18[181_111]
15-6	1832-1835	low	abstract[111]	giv[111]	_	_
15-7	1836-1842	Recall	abstract[111]	giv[111]	_	_
15-8	1843-1844	,	_	_	_	_
15-9	1845-1848	the	abstract[112]	giv[112]	coref	16-1[113_112]
15-10	1849-1856	overall	abstract[112]	giv[112]	_	_
15-11	1857-1864	results	abstract[112]	giv[112]	_	_
15-12	1865-1868	are	_	_	_	_
15-13	1869-1872	low	_	_	_	_
15-14	1873-1874	.	_	_	_	_

#Text=The results obtained by the Universal Segmenter are presented in Table 8 .
16-1	1875-1878	The	abstract[113]	giv[113]	coref	23-6[174_113]
16-2	1879-1886	results	abstract[113]	giv[113]	_	_
16-3	1887-1895	obtained	abstract[113]	giv[113]	_	_
16-4	1896-1898	by	abstract[113]	giv[113]	_	_
16-5	1899-1902	the	abstract[113]|abstract[114]	giv[113]|new[114]	_	_
16-6	1903-1912	Universal	abstract[113]|abstract[114]	giv[113]|new[114]	_	_
16-7	1913-1922	Segmenter	abstract[113]|abstract[114]	giv[113]|new[114]	_	_
16-8	1923-1926	are	_	_	_	_
16-9	1927-1936	presented	_	_	_	_
16-10	1937-1939	in	_	_	_	_
16-11	1940-1945	Table	abstract[115]	new[115]	_	_
16-12	1946-1947	8	abstract[115]	new[115]	_	_
16-13	1948-1949	.	_	_	_	_

#Text=The default model ( regardless of what kind of character representations are used — conventional character embeddings or concatenated n-gram vectors ) learns from the training data that the first and the last character of a word ( corresponding to B , E and S tags ) are always adjacent either to the boundary of a space-delimited segment or to a punctuation mark .
17-1	1950-1953	The	abstract[116]	giv[116]	appos	17-15[120_116]
17-2	1954-1961	default	abstract[116]	giv[116]	_	_
17-3	1962-1967	model	abstract[116]	giv[116]	_	_
17-4	1968-1969	(	_	_	_	_
17-5	1970-1980	regardless	_	_	_	_
17-6	1981-1983	of	_	_	_	_
17-7	1984-1988	what	_	_	_	_
17-8	1989-1993	kind	abstract[118]	new[118]	_	_
17-9	1994-1996	of	abstract[118]	new[118]	_	_
17-10	1997-2006	character	abstract|abstract[118]	new|new[118]	coref	17-16
17-11	2007-2022	representations	abstract[118]	new[118]	_	_
17-12	2023-2026	are	_	_	_	_
17-13	2027-2031	used	_	_	_	_
17-14	2032-2033	—	_	_	_	_
17-15	2034-2046	conventional	abstract[120]|abstract[121]	giv[120]|giv[121]	appos|appos	17-15[121_120]|17-19[123_121]
17-16	2047-2056	character	abstract|abstract[120]|abstract[121]	giv|giv[120]|giv[121]	coref	17-29[126_0]
17-17	2057-2067	embeddings	abstract[120]|abstract[121]	giv[120]|giv[121]	_	_
17-18	2068-2070	or	abstract[121]	giv[121]	_	_
17-19	2071-2083	concatenated	abstract[121]|abstract[123]	giv[121]|giv[123]	coref	18-5[136_123]
17-20	2084-2090	n-gram	abstract[121]|abstract|abstract[123]	giv[121]|giv|giv[123]	coref	19-11
17-21	2091-2098	vectors	abstract[121]|abstract[123]	giv[121]|giv[123]	_	_
17-22	2099-2100	)	_	_	_	_
17-23	2101-2107	learns	_	_	_	_
17-24	2108-2112	from	_	_	_	_
17-25	2113-2116	the	abstract[125]	new[125]	coref	20-10[149_125]
17-26	2117-2125	training	abstract|abstract[125]	new|new[125]	coref	32-27
17-27	2126-2130	data	abstract[125]	new[125]	_	_
17-28	2131-2135	that	_	_	_	_
17-29	2136-2139	the	abstract[126]	giv[126]	coref	29-6[0_126]
17-30	2140-2145	first	abstract[126]	giv[126]	_	_
17-31	2146-2149	and	abstract[126]	giv[126]	_	_
17-32	2150-2153	the	abstract[126]	giv[126]	_	_
17-33	2154-2158	last	abstract[126]	giv[126]	_	_
17-34	2159-2168	character	abstract[126]	giv[126]	_	_
17-35	2169-2171	of	abstract[126]	giv[126]	_	_
17-36	2172-2173	a	abstract[126]|abstract[127]	giv[126]|new[127]	coref	20-16[0_127]
17-37	2174-2178	word	abstract[126]|abstract[127]	giv[126]|new[127]	_	_
17-38	2179-2180	(	abstract[126]	giv[126]	_	_
17-39	2181-2194	corresponding	abstract[126]	giv[126]	_	_
17-40	2195-2197	to	abstract[126]	giv[126]	_	_
17-41	2198-2199	B	abstract[126]|abstract|abstract[131]	giv[126]|new|new[131]	_	_
17-42	2200-2201	,	abstract[126]|abstract[131]	giv[126]|new[131]	_	_
17-43	2202-2203	E	abstract[126]|abstract|abstract[131]	giv[126]|new|new[131]	_	_
17-44	2204-2207	and	abstract[126]|abstract[131]	giv[126]|new[131]	_	_
17-45	2208-2209	S	abstract[126]|person|abstract[131]	giv[126]|new|new[131]	_	_
17-46	2210-2214	tags	abstract[126]|abstract[131]	giv[126]|new[131]	_	_
17-47	2215-2216	)	abstract[126]	giv[126]	_	_
17-48	2217-2220	are	_	_	_	_
17-49	2221-2227	always	_	_	_	_
17-50	2228-2236	adjacent	_	_	_	_
17-51	2237-2243	either	_	_	_	_
17-52	2244-2246	to	_	_	_	_
17-53	2247-2250	the	place[132]	new[132]	_	_
17-54	2251-2259	boundary	place[132]	new[132]	_	_
17-55	2260-2262	of	place[132]	new[132]	_	_
17-56	2263-2264	a	place[132]|abstract[133]	new[132]|giv[133]	coref	36-81[282_133]
17-57	2265-2280	space-delimited	place[132]|abstract[133]	new[132]|giv[133]	_	_
17-58	2281-2288	segment	place[132]|abstract[133]	new[132]|giv[133]	_	_
17-59	2289-2291	or	_	_	_	_
17-60	2292-2294	to	_	_	_	_
17-61	2295-2296	a	abstract[135]	new[135]	_	_
17-62	2297-2308	punctuation	abstract|abstract[135]	new|new[135]	coref	18-8
17-63	2309-2313	mark	abstract[135]	new[135]	_	_
17-64	2314-2315	.	_	_	_	_

#Text=As a result , the model separates punctuation from alpha-numeric strings found in the input , but never applies further segmentation to them .
18-1	2316-2318	As	_	_	_	_
18-2	2319-2320	a	_	_	_	_
18-3	2321-2327	result	_	_	_	_
18-4	2328-2329	,	_	_	_	_
18-5	2330-2333	the	abstract[136]	giv[136]	coref	20-6[148_136]
18-6	2334-2339	model	abstract[136]	giv[136]	_	_
18-7	2340-2349	separates	_	_	_	_
18-8	2350-2361	punctuation	abstract	giv	_	_
18-9	2362-2366	from	_	_	_	_
18-10	2367-2380	alpha-numeric	abstract[138]	new[138]	ana	18-23[0_138]
18-11	2381-2388	strings	abstract[138]	new[138]	_	_
18-12	2389-2394	found	abstract[138]	new[138]	_	_
18-13	2395-2397	in	abstract[138]	new[138]	_	_
18-14	2398-2401	the	abstract[138]|abstract[139]	new[138]|giv[139]	_	_
18-15	2402-2407	input	abstract[138]|abstract[139]	new[138]|giv[139]	_	_
18-16	2408-2409	,	_	_	_	_
18-17	2410-2413	but	_	_	_	_
18-18	2414-2419	never	_	_	_	_
18-19	2420-2427	applies	_	_	_	_
18-20	2428-2435	further	abstract[140]	giv[140]	coref	21-14[0_140]
18-21	2436-2448	segmentation	abstract[140]	giv[140]	_	_
18-22	2449-2451	to	_	_	_	_
18-23	2452-2456	them	abstract	giv	coref	20-19[153_0]
18-24	2457-2458	.	_	_	_	_

#Text=US-ISP models are better but still notably worse than lexical n-gram models ( especially on SYOS ) .
19-1	2459-2465	US-ISP	object|abstract[143]	new|giv[143]	coref|coref	19-10[145_143]|26-6
19-2	2466-2472	models	abstract[143]	giv[143]	_	_
19-3	2473-2476	are	_	_	_	_
19-4	2477-2483	better	_	_	_	_
19-5	2484-2487	but	_	_	_	_
19-6	2488-2493	still	_	_	_	_
19-7	2494-2501	notably	_	_	_	_
19-8	2502-2507	worse	_	_	_	_
19-9	2508-2512	than	_	_	_	_
19-10	2513-2520	lexical	abstract[145]	giv[145]	coref	23-1[172_145]
19-11	2521-2527	n-gram	abstract|abstract[145]	giv|giv[145]	coref	32-6
19-12	2528-2534	models	abstract[145]	giv[145]	_	_
19-13	2535-2536	(	abstract[145]	giv[145]	_	_
19-14	2537-2547	especially	abstract[145]|abstract[146]	giv[145]|giv[146]	coref	26-9[0_146]
19-15	2548-2550	on	abstract[145]|abstract[146]	giv[145]|giv[146]	_	_
19-16	2551-2555	SYOS	abstract[145]|abstract[146]	giv[145]|giv[146]	_	_
19-17	2556-2557	)	abstract[145]	giv[145]	_	_
19-18	2558-2559	.	_	_	_	_

#Text=Unlike with default settings , the model trained on data without whitespaces learns to predict word boundaries within strings of alpha-numeric characters .
20-1	2560-2566	Unlike	_	_	_	_
20-2	2567-2571	with	_	_	_	_
20-3	2572-2579	default	abstract[147]	new[147]	_	_
20-4	2580-2588	settings	abstract[147]	new[147]	_	_
20-5	2589-2590	,	_	_	_	_
20-6	2591-2594	the	abstract[148]	giv[148]	coref	22-25[168_148]
20-7	2595-2600	model	abstract[148]	giv[148]	_	_
20-8	2601-2608	trained	abstract[148]	giv[148]	_	_
20-9	2609-2611	on	abstract[148]	giv[148]	_	_
20-10	2612-2616	data	abstract[148]|abstract[149]	giv[148]|giv[149]	coref	21-6[156_149]
20-11	2617-2624	without	abstract[148]|abstract[149]	giv[148]|giv[149]	_	_
20-12	2625-2636	whitespaces	abstract[148]|abstract[149]|abstract	giv[148]|giv[149]|new	coref	22-34[171_0]
20-13	2637-2643	learns	_	_	_	_
20-14	2644-2646	to	_	_	_	_
20-15	2647-2654	predict	_	_	_	_
20-16	2655-2659	word	abstract|abstract[152]	giv|new[152]	coref|coref	22-14|22-13[165_152]
20-17	2660-2670	boundaries	abstract[152]	new[152]	_	_
20-18	2671-2677	within	_	_	_	_
20-19	2678-2685	strings	abstract[153]	giv[153]	coref	34-25[251_153]
20-20	2686-2688	of	abstract[153]	giv[153]	_	_
20-21	2689-2702	alpha-numeric	abstract[153]|abstract[154]	giv[153]|new[154]	coref	28-20[207_154]
20-22	2703-2713	characters	abstract[153]|abstract[154]	giv[153]|new[154]	_	_
20-23	2714-2715	.	_	_	_	_

#Text=However , when presented with test data including spaces , they impede the segmentation process rather than supporting it .
21-1	2716-2723	However	_	_	_	_
21-2	2724-2725	,	_	_	_	_
21-3	2726-2730	when	_	_	_	_
21-4	2731-2740	presented	_	_	_	_
21-5	2741-2745	with	_	_	_	_
21-6	2746-2750	test	abstract|abstract[156]	giv|giv[156]	coref|coref	22-22|22-32[170_156]
21-7	2751-2755	data	abstract[156]	giv[156]	_	_
21-8	2756-2765	including	abstract[156]	giv[156]	_	_
21-9	2766-2772	spaces	abstract[156]|abstract	giv[156]|new	ana	21-11
21-10	2773-2774	,	_	_	_	_
21-11	2775-2779	they	abstract	giv	_	_
21-12	2780-2786	impede	_	_	_	_
21-13	2787-2790	the	abstract[160]	giv[160]	ana	21-19[0_160]
21-14	2791-2803	segmentation	abstract|abstract[160]	giv|giv[160]	coref	29-17[216_0]
21-15	2804-2811	process	abstract[160]	giv[160]	_	_
21-16	2812-2818	rather	_	_	_	_
21-17	2819-2823	than	_	_	_	_
21-18	2824-2834	supporting	_	_	_	_
21-19	2835-2837	it	abstract	giv	_	_
21-20	2838-2839	.	_	_	_	_

#Text=As shown in Table 9 , if we only take into account the word boundaries not already indicated in the raw test set , the model makes more correct predictions in data where the whitespaces have all been removed .
22-1	2840-2842	As	_	_	_	_
22-2	2843-2848	shown	_	_	_	_
22-3	2849-2851	in	_	_	_	_
22-4	2852-2857	Table	abstract[162]	new[162]	_	_
22-5	2858-2859	9	abstract[162]	new[162]	_	_
22-6	2860-2861	,	_	_	_	_
22-7	2862-2864	if	_	_	_	_
22-8	2865-2867	we	person	giv	_	_
22-9	2868-2872	only	_	_	_	_
22-10	2873-2877	take	_	_	_	_
22-11	2878-2882	into	_	_	_	_
22-12	2883-2890	account	_	_	_	_
22-13	2891-2894	the	abstract[165]	giv[165]	_	_
22-14	2895-2899	word	abstract|abstract[165]	giv|giv[165]	coref	29-17
22-15	2900-2910	boundaries	abstract[165]	giv[165]	_	_
22-16	2911-2914	not	abstract[165]	giv[165]	_	_
22-17	2915-2922	already	abstract[165]	giv[165]	_	_
22-18	2923-2932	indicated	abstract[165]	giv[165]	_	_
22-19	2933-2935	in	abstract[165]	giv[165]	_	_
22-20	2936-2939	the	abstract[165]|abstract[167]	giv[165]|new[167]	coref	29-6[211_167]
22-21	2940-2943	raw	abstract[165]|abstract[167]	giv[165]|new[167]	_	_
22-22	2944-2948	test	abstract[165]|abstract|abstract[167]	giv[165]|giv|new[167]	coref	32-10
22-23	2949-2952	set	abstract[165]|abstract[167]	giv[165]|new[167]	_	_
22-24	2953-2954	,	_	_	_	_
22-25	2955-2958	the	abstract[168]	giv[168]	coref	24-3[177_168]
22-26	2959-2964	model	abstract[168]	giv[168]	_	_
22-27	2965-2970	makes	_	_	_	_
22-28	2971-2975	more	abstract[169]	new[169]	_	_
22-29	2976-2983	correct	abstract[169]	new[169]	_	_
22-30	2984-2995	predictions	abstract[169]	new[169]	_	_
22-31	2996-2998	in	abstract[169]	new[169]	_	_
22-32	2999-3003	data	abstract[169]|abstract[170]	new[169]|giv[170]	coref	32-18[231_170]
22-33	3004-3009	where	abstract[169]|abstract[170]	new[169]|giv[170]	_	_
22-34	3010-3013	the	abstract[169]|abstract[170]|abstract[171]	new[169]|giv[170]|giv[171]	_	_
22-35	3014-3025	whitespaces	abstract[169]|abstract[170]|abstract[171]	new[169]|giv[170]|giv[171]	_	_
22-36	3026-3030	have	abstract[169]|abstract[170]	new[169]|giv[170]	_	_
22-37	3031-3034	all	abstract[169]|abstract[170]	new[169]|giv[170]	_	_
22-38	3035-3039	been	abstract[169]|abstract[170]	new[169]|giv[170]	_	_
22-39	3040-3047	removed	abstract[169]|abstract[170]	new[169]|giv[170]	_	_
22-40	3048-3049	.	_	_	_	_

#Text=Models with multi-word tokens achieve significantly higher results .
23-1	3050-3056	Models	abstract[172]	giv[172]	coref	29-24[218_172]
23-2	3057-3061	with	abstract[172]	giv[172]	_	_
23-3	3062-3072	multi-word	abstract[172]|abstract[173]	giv[172]|new[173]	coref	25-9[185_173]
23-4	3073-3079	tokens	abstract[172]|abstract[173]	giv[172]|new[173]	_	_
23-5	3080-3087	achieve	_	_	_	_
23-6	3088-3101	significantly	abstract[174]	giv[174]	coref	27-4[197_174]
23-7	3102-3108	higher	abstract[174]	giv[174]	_	_
23-8	3109-3116	results	abstract[174]	giv[174]	_	_
23-9	3117-3118	.	_	_	_	_

#Text=Precision of the US-MWTs model is on par with the segmenter applying Kneser-Ney smoothing , while maintaining relatively high Recall .
24-1	3119-3128	Precision	abstract[175]	giv[175]	coref	25-21[187_175]
24-2	3129-3131	of	abstract[175]	giv[175]	_	_
24-3	3132-3135	the	abstract[175]|abstract[177]	giv[175]|giv[177]	coref	25-6[184_177]
24-4	3136-3143	US-MWTs	abstract[175]|object|abstract[177]	giv[175]|new|giv[177]	_	_
24-5	3144-3149	model	abstract[175]|abstract[177]	giv[175]|giv[177]	_	_
24-6	3150-3152	is	_	_	_	_
24-7	3153-3155	on	_	_	_	_
24-8	3156-3159	par	_	_	_	_
24-9	3160-3164	with	_	_	_	_
24-10	3165-3168	the	object[178]	new[178]	coref	26-14[193_178]
24-11	3169-3178	segmenter	object[178]	new[178]	_	_
24-12	3179-3187	applying	object[178]	new[178]	_	_
24-13	3188-3198	Kneser-Ney	object[178]|person|abstract[180]	new[178]|giv|giv[180]	coref|coref	36-42|36-41[272_180]
24-14	3199-3208	smoothing	object[178]|abstract[180]	new[178]|giv[180]	_	_
24-15	3209-3210	,	_	_	_	_
24-16	3211-3216	while	_	_	_	_
24-17	3217-3228	maintaining	_	_	_	_
24-18	3229-3239	relatively	abstract[181]	giv[181]	ana	25-1[0_181]
24-19	3240-3244	high	abstract[181]	giv[181]	_	_
24-20	3245-3251	Recall	abstract[181]	giv[181]	_	_
24-21	3252-3253	.	_	_	_	_

#Text=It yields lower Recall than the model with randomly generated multi-word tokens , but the F-score is higher due to better Precision .
25-1	3254-3256	It	abstract	giv	_	_
25-2	3257-3263	yields	_	_	_	_
25-3	3264-3269	lower	abstract[183]	new[183]	coref	36-90[283_183]
25-4	3270-3276	Recall	abstract[183]	new[183]	_	_
25-5	3277-3281	than	_	_	_	_
25-6	3282-3285	the	abstract[184]	giv[184]	coref	26-5[190_184]
25-7	3286-3291	model	abstract[184]	giv[184]	_	_
25-8	3292-3296	with	abstract[184]	giv[184]	_	_
25-9	3297-3305	randomly	abstract[184]|abstract[185]	giv[184]|giv[185]	coref	33-4[236_185]
25-10	3306-3315	generated	abstract[184]|abstract[185]	giv[184]|giv[185]	_	_
25-11	3316-3326	multi-word	abstract[184]|abstract[185]	giv[184]|giv[185]	_	_
25-12	3327-3333	tokens	abstract[184]|abstract[185]	giv[184]|giv[185]	_	_
25-13	3334-3335	,	_	_	_	_
25-14	3336-3339	but	_	_	_	_
25-15	3340-3343	the	abstract[186]	giv[186]	_	_
25-16	3344-3351	F-score	abstract[186]	giv[186]	_	_
25-17	3352-3354	is	_	_	_	_
25-18	3355-3361	higher	_	_	_	_
25-19	3362-3365	due	_	_	_	_
25-20	3366-3368	to	_	_	_	_
25-21	3369-3375	better	abstract[187]	giv[187]	_	_
25-22	3376-3385	Precision	abstract[187]	giv[187]	_	_
25-23	3386-3387	.	_	_	_	_

#Text=With the exception of the US-ISP model on SYOS , all variants of the neural segmenter achieved the best performance with concatenated 9-gram vectors .
26-1	3388-3392	With	_	_	_	_
26-2	3393-3396	the	abstract[188]	new[188]	_	_
26-3	3397-3406	exception	abstract[188]	new[188]	_	_
26-4	3407-3409	of	abstract[188]	new[188]	_	_
26-5	3410-3413	the	abstract[188]|abstract[190]	new[188]|giv[190]	coref	26-22[195_190]
26-6	3414-3420	US-ISP	abstract[188]|object|abstract[190]	new[188]|giv|giv[190]	_	_
26-7	3421-3426	model	abstract[188]|abstract[190]	new[188]|giv[190]	_	_
26-8	3427-3429	on	abstract[188]|abstract[190]	new[188]|giv[190]	_	_
26-9	3430-3434	SYOS	abstract[188]|abstract[190]|abstract	new[188]|giv[190]|giv	_	_
26-10	3435-3436	,	_	_	_	_
26-11	3437-3440	all	abstract[192]	new[192]	_	_
26-12	3441-3449	variants	abstract[192]	new[192]	_	_
26-13	3450-3452	of	abstract[192]	new[192]	_	_
26-14	3453-3456	the	abstract[192]|object[193]	new[192]|giv[193]	_	_
26-15	3457-3463	neural	abstract[192]|object[193]	new[192]|giv[193]	_	_
26-16	3464-3473	segmenter	abstract[192]|object[193]	new[192]|giv[193]	_	_
26-17	3474-3482	achieved	_	_	_	_
26-18	3483-3486	the	abstract[194]	giv[194]	ana	27-1[0_194]
26-19	3487-3491	best	abstract[194]	giv[194]	_	_
26-20	3492-3503	performance	abstract[194]	giv[194]	_	_
26-21	3504-3508	with	abstract[194]	giv[194]	_	_
26-22	3509-3521	concatenated	abstract[194]|abstract[195]	giv[194]|giv[195]	coref	34-1[245_195]
26-23	3522-3528	9-gram	abstract[194]|abstract[195]	giv[194]|giv[195]	_	_
26-24	3529-3536	vectors	abstract[194]|abstract[195]	giv[194]|giv[195]	_	_
26-25	3537-3538	.	_	_	_	_

#Text=This contrasts with the results reported by Shao et al. for Chinese , where in most cases there was no further improvement beyond 3-grams .
27-1	3539-3543	This	abstract	giv	coref	29-20[217_0]
27-2	3544-3553	contrasts	_	_	_	_
27-3	3554-3558	with	_	_	_	_
27-4	3559-3562	the	abstract[197]	giv[197]	_	_
27-5	3563-3570	results	abstract[197]	giv[197]	_	_
27-6	3571-3579	reported	abstract[197]	giv[197]	_	_
27-7	3580-3582	by	abstract[197]	giv[197]	_	_
27-8	3583-3587	Shao	abstract[197]|person	giv[197]|new	_	_
27-9	3588-3590	et	abstract[197]	giv[197]	_	_
27-10	3591-3594	al.	abstract[197]	giv[197]	_	_
27-11	3595-3598	for	abstract[197]	giv[197]	_	_
27-12	3599-3606	Chinese	abstract[197]	giv[197]	_	_
27-13	3607-3608	,	abstract[197]	giv[197]	_	_
27-14	3609-3614	where	abstract[197]	giv[197]	_	_
27-15	3615-3617	in	abstract[197]	giv[197]	_	_
27-16	3618-3622	most	abstract[197]|abstract[199]	giv[197]|giv[199]	coref	36-66[277_199]
27-17	3623-3628	cases	abstract[197]|abstract[199]	giv[197]|giv[199]	_	_
27-18	3629-3634	there	abstract[197]	giv[197]	_	_
27-19	3635-3638	was	abstract[197]	giv[197]	_	_
27-20	3639-3641	no	abstract[197]|abstract[200]	giv[197]|giv[200]	_	_
27-21	3642-3649	further	abstract[197]|abstract[200]	giv[197]|giv[200]	_	_
27-22	3650-3661	improvement	abstract[197]|abstract[200]	giv[197]|giv[200]	_	_
27-23	3662-3668	beyond	abstract[197]|abstract[200]	giv[197]|giv[200]	_	_
27-24	3669-3676	3-grams	abstract[197]|abstract[200]|substance	giv[197]|giv[200]|new	_	_
27-25	3677-3678	.	_	_	_	_

#Text=This behavior is a consequence of differences between writing systems : words in Chinese are on average composed of less characters than in languages using alphabetic scripts .
28-1	3679-3683	This	abstract[202]	new[202]	coref	28-4[203_202]
28-2	3684-3692	behavior	abstract[202]	new[202]	_	_
28-3	3693-3695	is	_	_	_	_
28-4	3696-3697	a	abstract[203]	giv[203]	_	_
28-5	3698-3709	consequence	abstract[203]	giv[203]	_	_
28-6	3710-3712	of	abstract[203]	giv[203]	_	_
28-7	3713-3724	differences	abstract[203]|abstract[204]	giv[203]|new[204]	_	_
28-8	3725-3732	between	abstract[203]|abstract[204]	giv[203]|new[204]	_	_
28-9	3733-3740	writing	abstract[203]|abstract[204]|abstract[205]	giv[203]|new[204]|new[205]	_	_
28-10	3741-3748	systems	abstract[203]|abstract[204]|abstract[205]	giv[203]|new[204]|new[205]	_	_
28-11	3749-3750	:	_	_	_	_
28-12	3751-3756	words	abstract[206]	new[206]	coref	36-49[273_206]
28-13	3757-3759	in	abstract[206]	new[206]	_	_
28-14	3760-3767	Chinese	abstract[206]	new[206]	_	_
28-15	3768-3771	are	_	_	_	_
28-16	3772-3774	on	_	_	_	_
28-17	3775-3782	average	_	_	_	_
28-18	3783-3791	composed	_	_	_	_
28-19	3792-3794	of	_	_	_	_
28-20	3795-3799	less	abstract[207]	giv[207]	coref	29-10[214_207]
28-21	3800-3810	characters	abstract[207]	giv[207]	_	_
28-22	3811-3815	than	_	_	_	_
28-23	3816-3818	in	_	_	_	_
28-24	3819-3828	languages	abstract[208]	new[208]	_	_
28-25	3829-3834	using	abstract[208]	new[208]	_	_
28-26	3835-3845	alphabetic	abstract[208]|abstract[209]	new[208]|new[209]	_	_
28-27	3846-3853	scripts	abstract[208]|abstract[209]	new[208]|new[209]	_	_
28-28	3854-3855	.	_	_	_	_

#Text=Due to a much bigger character set size , hanzi characters are also more informative to word segmentation , hence better performance with models using shorter context .
29-1	3856-3859	Due	_	_	_	_
29-2	3860-3862	to	_	_	_	_
29-3	3863-3864	a	abstract[212]	new[212]	_	_
29-4	3865-3869	much	abstract[212]	new[212]	_	_
29-5	3870-3876	bigger	abstract[212]	new[212]	_	_
29-6	3877-3886	character	abstract|abstract[211]|abstract[212]	giv|giv[211]|new[212]	coref	32-9[227_211]
29-7	3887-3890	set	abstract[211]|abstract[212]	giv[211]|new[212]	_	_
29-8	3891-3895	size	abstract[212]	new[212]	_	_
29-9	3896-3897	,	_	_	_	_
29-10	3898-3903	hanzi	person|abstract[214]	new|giv[214]	_	_
29-11	3904-3914	characters	abstract[214]	giv[214]	_	_
29-12	3915-3918	are	_	_	_	_
29-13	3919-3923	also	_	_	_	_
29-14	3924-3928	more	_	_	_	_
29-15	3929-3940	informative	_	_	_	_
29-16	3941-3943	to	_	_	_	_
29-17	3944-3948	word	abstract|abstract[216]	giv|giv[216]	_	_
29-18	3949-3961	segmentation	abstract[216]	giv[216]	_	_
29-19	3962-3963	,	_	_	_	_
29-20	3964-3969	hence	abstract[217]	giv[217]	_	_
29-21	3970-3976	better	abstract[217]	giv[217]	_	_
29-22	3977-3988	performance	abstract[217]	giv[217]	_	_
29-23	3989-3993	with	abstract[217]	giv[217]	_	_
29-24	3994-4000	models	abstract[217]|abstract[218]	giv[217]|giv[218]	coref	33-14[240_218]
29-25	4001-4006	using	abstract[217]|abstract[218]	giv[217]|giv[218]	_	_
29-26	4007-4014	shorter	abstract[217]|abstract[218]|abstract[219]	giv[217]|giv[218]|new[219]	_	_
29-27	4015-4022	context	abstract[217]|abstract[218]|abstract[219]	giv[217]|giv[218]|new[219]	_	_
29-28	4023-4024	.	_	_	_	_

#Text=6.1.
30-1	4025-4029	6.1.	abstract	new	_	_

#Text=General Observations
31-1	4030-4037	General	abstract[221]	new[221]	_	_
31-2	4038-4050	Observations	abstract[221]	new[221]	_	_

#Text=Due to data sparsity , n-gram coverage in the test set ( the fraction of n-grams in the test data that can be found in the training set ) is low ( see Table 10 ) .
32-1	4051-4054	Due	_	_	_	_
32-2	4055-4057	to	_	_	_	_
32-3	4058-4062	data	abstract|abstract[223]	new|new[223]	_	_
32-4	4063-4071	sparsity	abstract[223]	new[223]	_	_
32-5	4072-4073	,	_	_	_	_
32-6	4074-4080	n-gram	abstract|abstract[225]	giv|new[225]	appos|coref	32-13[228_225]|33-14
32-7	4081-4089	coverage	abstract[225]	new[225]	_	_
32-8	4090-4092	in	abstract[225]	new[225]	_	_
32-9	4093-4096	the	abstract[225]|abstract[227]	new[225]|giv[227]	coref	32-26[233_227]
32-10	4097-4101	test	abstract[225]|abstract|abstract[227]	new[225]|giv|giv[227]	coref	32-19
32-11	4102-4105	set	abstract[225]|abstract[227]	new[225]|giv[227]	_	_
32-12	4106-4107	(	_	_	_	_
32-13	4108-4111	the	abstract[228]	giv[228]	_	_
32-14	4112-4120	fraction	abstract[228]	giv[228]	_	_
32-15	4121-4123	of	abstract[228]	giv[228]	_	_
32-16	4124-4131	n-grams	abstract[228]|abstract	giv[228]|giv	_	_
32-17	4132-4134	in	abstract[228]	giv[228]	_	_
32-18	4135-4138	the	abstract[228]|abstract[231]	giv[228]|giv[231]	_	_
32-19	4139-4143	test	abstract[228]|abstract|abstract[231]	giv[228]|giv|giv[231]	coref	33-9
32-20	4144-4148	data	abstract[228]|abstract[231]	giv[228]|giv[231]	_	_
32-21	4149-4153	that	abstract[228]|abstract[231]	giv[228]|giv[231]	_	_
32-22	4154-4157	can	abstract[228]|abstract[231]	giv[228]|giv[231]	_	_
32-23	4158-4160	be	abstract[228]|abstract[231]	giv[228]|giv[231]	_	_
32-24	4161-4166	found	abstract[228]|abstract[231]	giv[228]|giv[231]	_	_
32-25	4167-4169	in	abstract[228]|abstract[231]	giv[228]|giv[231]	_	_
32-26	4170-4173	the	abstract[228]|abstract[231]|abstract[233]	giv[228]|giv[231]|giv[233]	coref	33-8[238_233]
32-27	4174-4182	training	abstract[228]|abstract[231]|abstract|abstract[233]	giv[228]|giv[231]|giv|giv[233]	_	_
32-28	4183-4186	set	abstract[228]|abstract[231]|abstract[233]	giv[228]|giv[231]|giv[233]	_	_
32-29	4187-4188	)	_	_	_	_
32-30	4189-4191	is	_	_	_	_
32-31	4192-4195	low	_	_	_	_
32-32	4196-4197	(	_	_	_	_
32-33	4198-4201	see	_	_	_	_
32-34	4202-4207	Table	abstract[234]	new[234]	_	_
32-35	4208-4210	10	abstract[234]	new[234]	_	_
32-36	4211-4212	)	_	_	_	_
32-37	4213-4214	.	_	_	_	_

#Text=It means that many multi-word tokens from the test set are known to n-gram models as separate unigrams , but not in the form of a single n-gram .
33-1	4215-4217	It	abstract	giv	_	_
33-2	4218-4223	means	_	_	_	_
33-3	4224-4228	that	_	_	_	_
33-4	4229-4233	many	abstract[236]	giv[236]	coref	35-7[254_236]
33-5	4234-4244	multi-word	abstract[236]	giv[236]	_	_
33-6	4245-4251	tokens	abstract[236]	giv[236]	_	_
33-7	4252-4256	from	abstract[236]	giv[236]	_	_
33-8	4257-4260	the	abstract[236]|abstract[238]	giv[236]|giv[238]	_	_
33-9	4261-4265	test	abstract[236]|abstract|abstract[238]	giv[236]|giv|giv[238]	_	_
33-10	4266-4269	set	abstract[236]|abstract[238]	giv[236]|giv[238]	_	_
33-11	4270-4273	are	_	_	_	_
33-12	4274-4279	known	_	_	_	_
33-13	4280-4282	to	_	_	_	_
33-14	4283-4289	n-gram	abstract|abstract[240]	giv|giv[240]	coref|coref	33-26[243_0]|36-6[260_240]
33-15	4290-4296	models	abstract[240]	giv[240]	_	_
33-16	4297-4299	as	_	_	_	_
33-17	4300-4308	separate	abstract[241]	new[241]	coref	34-10[0_241]
33-18	4309-4317	unigrams	abstract[241]	new[241]	_	_
33-19	4318-4319	,	_	_	_	_
33-20	4320-4323	but	_	_	_	_
33-21	4324-4327	not	_	_	_	_
33-22	4328-4330	in	_	_	_	_
33-23	4331-4334	the	abstract[242]	new[242]	_	_
33-24	4335-4339	form	abstract[242]	new[242]	_	_
33-25	4340-4342	of	abstract[242]	new[242]	_	_
33-26	4343-4344	a	abstract[242]|abstract[243]	new[242]|giv[243]	_	_
33-27	4345-4351	single	abstract[242]|abstract[243]	new[242]|giv[243]	_	_
33-28	4352-4358	n-gram	abstract[242]|abstract[243]	new[242]|giv[243]	_	_
33-29	4359-4360	.	_	_	_	_

#Text=The Stupid Backoff model with a backoff factor for unigrams set to a moderate value ( such as 0.09 ) is able to segment such strings correctly .
34-1	4361-4364	The	abstract[245]	giv[245]	coref	36-15[263_245]
34-2	4365-4371	Stupid	abstract[245]	giv[245]	_	_
34-3	4372-4379	Backoff	abstract|abstract[245]	giv|giv[245]	coref	34-7
34-4	4380-4385	model	abstract[245]	giv[245]	_	_
34-5	4386-4390	with	abstract[245]	giv[245]	_	_
34-6	4391-4392	a	abstract[245]|abstract[247]	giv[245]|new[247]	coref	36-25[268_247]
34-7	4393-4400	backoff	abstract[245]|abstract|abstract[247]	giv[245]|giv|new[247]	coref	36-21[266_0]
34-8	4401-4407	factor	abstract[245]|abstract[247]	giv[245]|new[247]	_	_
34-9	4408-4411	for	abstract[245]|abstract[247]	giv[245]|new[247]	_	_
34-10	4412-4420	unigrams	abstract[245]|abstract[247]|abstract	giv[245]|new[247]|giv	coref	35-22[258_0]
34-11	4421-4424	set	abstract[245]|abstract[247]	giv[245]|new[247]	_	_
34-12	4425-4427	to	abstract[245]|abstract[247]	giv[245]|new[247]	_	_
34-13	4428-4429	a	abstract[245]|abstract[247]|abstract[249]	giv[245]|new[247]|new[249]	ana	35-3[0_249]
34-14	4430-4438	moderate	abstract[245]|abstract[247]|abstract[249]	giv[245]|new[247]|new[249]	_	_
34-15	4439-4444	value	abstract[245]|abstract[247]|abstract[249]	giv[245]|new[247]|new[249]	_	_
34-16	4445-4446	(	abstract[245]|abstract[247]|abstract[249]	giv[245]|new[247]|new[249]	_	_
34-17	4447-4451	such	abstract[245]|abstract[247]|abstract[249]	giv[245]|new[247]|new[249]	_	_
34-18	4452-4454	as	abstract[245]|abstract[247]|abstract[249]	giv[245]|new[247]|new[249]	_	_
34-19	4455-4459	0.09	abstract[245]|abstract[247]|abstract[249]|abstract	giv[245]|new[247]|new[249]|giv	_	_
34-20	4460-4461	)	abstract[245]|abstract[247]|abstract[249]	giv[245]|new[247]|new[249]	_	_
34-21	4462-4464	is	_	_	_	_
34-22	4465-4469	able	_	_	_	_
34-23	4470-4472	to	_	_	_	_
34-24	4473-4480	segment	_	_	_	_
34-25	4481-4485	such	abstract[251]	giv[251]	_	_
34-26	4486-4493	strings	abstract[251]	giv[251]	_	_
34-27	4494-4503	correctly	_	_	_	_
34-28	4504-4505	.	_	_	_	_

#Text=However , it also erroneously segments some OoV single-word tokens whose surface forms happen to be interpretable as a sequence of concatenated in-vocabulary unigrams , resulting in lower Precision .
35-1	4506-4513	However	_	_	_	_
35-2	4514-4515	,	_	_	_	_
35-3	4516-4518	it	abstract	giv	_	_
35-4	4519-4523	also	_	_	_	_
35-5	4524-4535	erroneously	_	_	_	_
35-6	4536-4544	segments	_	_	_	_
35-7	4545-4549	some	abstract[254]	giv[254]	_	_
35-8	4550-4553	OoV	abstract|abstract[254]	new|giv[254]	_	_
35-9	4554-4565	single-word	abstract[254]	giv[254]	_	_
35-10	4566-4572	tokens	abstract[254]	giv[254]	_	_
35-11	4573-4578	whose	abstract[254]|abstract[256]	giv[254]|new[256]	_	_
35-12	4579-4586	surface	abstract[254]|place|abstract[256]	giv[254]|new|new[256]	_	_
35-13	4587-4592	forms	abstract[254]|abstract[256]	giv[254]|new[256]	_	_
35-14	4593-4599	happen	abstract[254]	giv[254]	_	_
35-15	4600-4602	to	abstract[254]	giv[254]	_	_
35-16	4603-4605	be	abstract[254]	giv[254]	_	_
35-17	4606-4619	interpretable	abstract[254]	giv[254]	_	_
35-18	4620-4622	as	_	_	_	_
35-19	4623-4624	a	abstract[257]	new[257]	_	_
35-20	4625-4633	sequence	abstract[257]	new[257]	_	_
35-21	4634-4636	of	abstract[257]	new[257]	_	_
35-22	4637-4649	concatenated	abstract[257]|abstract[258]	new[257]|giv[258]	coref	36-11[0_258]
35-23	4650-4663	in-vocabulary	abstract[257]|abstract[258]	new[257]|giv[258]	_	_
35-24	4664-4672	unigrams	abstract[257]|abstract[258]	new[257]|giv[258]	_	_
35-25	4673-4674	,	_	_	_	_
35-26	4675-4684	resulting	_	_	_	_
35-27	4685-4687	in	_	_	_	_
35-28	4688-4693	lower	abstract[259]	new[259]	_	_
35-29	4694-4703	Precision	abstract[259]	new[259]	_	_
35-30	4704-4705	.	_	_	_	_

#Text=On the other hand , models assigning low scores to unigrams ( such as a 4- or 5-gram model with the Stupid Backoff and backoff factor set as suggested by Brants et al. , and in particular the model applying modified Kneser-Ney smoothing ) are better at handling OoV words ( see Table 11 ) , but as a result of probability multiplication , in many cases they score unseen multi-word segments higher than a sequence of unigrams into which the given segment should be divided , hence yielding lower Recall .
36-1	4706-4708	On	_	_	_	_
36-2	4709-4712	the	_	_	_	_
36-3	4713-4718	other	_	_	_	_
36-4	4719-4723	hand	_	_	_	_
36-5	4724-4725	,	_	_	_	_
36-6	4726-4732	models	abstract[260]	giv[260]	_	_
36-7	4733-4742	assigning	abstract[260]	giv[260]	_	_
36-8	4743-4746	low	abstract[260]|abstract[261]	giv[260]|new[261]	_	_
36-9	4747-4753	scores	abstract[260]|abstract[261]	giv[260]|new[261]	_	_
36-10	4754-4756	to	abstract[260]	giv[260]	_	_
36-11	4757-4765	unigrams	abstract[260]|abstract	giv[260]|giv	coref	36-78[281_0]
36-12	4766-4767	(	_	_	_	_
36-13	4768-4772	such	_	_	_	_
36-14	4773-4775	as	_	_	_	_
36-15	4776-4777	a	abstract[263]|abstract[264]	giv[263]|giv[264]	coref|coref	36-15[264_263]|36-37[270_264]
36-16	4778-4780	4-	abstract[263]|abstract[264]	giv[263]|giv[264]	_	_
36-17	4781-4783	or	abstract[263]|abstract[264]	giv[263]|giv[264]	_	_
36-18	4784-4790	5-gram	abstract[263]|abstract[264]	giv[263]|giv[264]	_	_
36-19	4791-4796	model	abstract[263]|abstract[264]	giv[263]|giv[264]	_	_
36-20	4797-4801	with	abstract[263]|abstract[264]	giv[263]|giv[264]	_	_
36-21	4802-4805	the	abstract[263]|abstract[264]|abstract[266]	giv[263]|giv[264]|giv[266]	coref	36-25[0_266]
36-22	4806-4812	Stupid	abstract[263]|abstract[264]|person|abstract[266]	giv[263]|giv[264]|new|giv[266]	_	_
36-23	4813-4820	Backoff	abstract[263]|abstract[264]|abstract[266]	giv[263]|giv[264]|giv[266]	_	_
36-24	4821-4824	and	abstract[263]|abstract[264]	giv[263]|giv[264]	_	_
36-25	4825-4832	backoff	abstract[263]|abstract[264]|abstract|abstract[268]	giv[263]|giv[264]|giv|giv[268]	_	_
36-26	4833-4839	factor	abstract[263]|abstract[264]|abstract[268]	giv[263]|giv[264]|giv[268]	_	_
36-27	4840-4843	set	abstract[263]|abstract[264]|abstract[268]	giv[263]|giv[264]|giv[268]	_	_
36-28	4844-4846	as	abstract[264]	giv[264]	_	_
36-29	4847-4856	suggested	abstract[264]	giv[264]	_	_
36-30	4857-4859	by	abstract[264]	giv[264]	_	_
36-31	4860-4866	Brants	abstract[264]|person	giv[264]|giv	_	_
36-32	4867-4869	et	abstract[264]	giv[264]	_	_
36-33	4870-4873	al.	abstract[264]	giv[264]	_	_
36-34	4874-4875	,	abstract[264]	giv[264]	_	_
36-35	4876-4879	and	abstract[264]	giv[264]	_	_
36-36	4880-4882	in	abstract[264]	giv[264]	_	_
36-37	4883-4893	particular	abstract[264]|abstract[270]	giv[264]|giv[270]	_	_
36-38	4894-4897	the	abstract[264]|abstract[270]	giv[264]|giv[270]	_	_
36-39	4898-4903	model	abstract[264]|abstract[270]	giv[264]|giv[270]	_	_
36-40	4904-4912	applying	abstract[264]|abstract[270]	giv[264]|giv[270]	_	_
36-41	4913-4921	modified	abstract[264]|abstract[270]|abstract[272]	giv[264]|giv[270]|giv[272]	_	_
36-42	4922-4932	Kneser-Ney	abstract[264]|abstract[270]|person|abstract[272]	giv[264]|giv[270]|giv|giv[272]	_	_
36-43	4933-4942	smoothing	abstract[264]|abstract[270]|abstract[272]	giv[264]|giv[270]|giv[272]	_	_
36-44	4943-4944	)	_	_	_	_
36-45	4945-4948	are	_	_	_	_
36-46	4949-4955	better	_	_	_	_
36-47	4956-4958	at	_	_	_	_
36-48	4959-4967	handling	_	_	_	_
36-49	4968-4971	OoV	abstract[273]	giv[273]	ana	36-68[0_273]
36-50	4972-4977	words	abstract[273]	giv[273]	_	_
36-51	4978-4979	(	_	_	_	_
36-52	4980-4983	see	_	_	_	_
36-53	4984-4989	Table	abstract[274]	new[274]	_	_
36-54	4990-4992	11	abstract[274]	new[274]	_	_
36-55	4993-4994	)	_	_	_	_
36-56	4995-4996	,	_	_	_	_
36-57	4997-5000	but	_	_	_	_
36-58	5001-5003	as	_	_	_	_
36-59	5004-5005	a	_	_	_	_
36-60	5006-5012	result	_	_	_	_
36-61	5013-5015	of	_	_	_	_
36-62	5016-5027	probability	abstract|abstract[276]	new|new[276]	_	_
36-63	5028-5042	multiplication	abstract[276]	new[276]	_	_
36-64	5043-5044	,	_	_	_	_
36-65	5045-5047	in	_	_	_	_
36-66	5048-5052	many	abstract[277]	giv[277]	_	_
36-67	5053-5058	cases	abstract[277]	giv[277]	_	_
36-68	5059-5063	they	abstract	giv	_	_
36-69	5064-5069	score	_	_	_	_
36-70	5070-5076	unseen	abstract[279]	new[279]	_	_
36-71	5077-5087	multi-word	abstract[279]	new[279]	_	_
36-72	5088-5096	segments	abstract[279]	new[279]	_	_
36-73	5097-5103	higher	_	_	_	_
36-74	5104-5108	than	_	_	_	_
36-75	5109-5110	a	abstract[280]	new[280]	_	_
36-76	5111-5119	sequence	abstract[280]	new[280]	_	_
36-77	5120-5122	of	abstract[280]	new[280]	_	_
36-78	5123-5131	unigrams	abstract[280]|abstract[281]	new[280]|giv[281]	_	_
36-79	5132-5136	into	abstract[280]|abstract[281]	new[280]|giv[281]	_	_
36-80	5137-5142	which	abstract[280]|abstract[281]	new[280]|giv[281]	_	_
36-81	5143-5146	the	abstract[280]|abstract[281]|abstract[282]	new[280]|giv[281]|giv[282]	_	_
36-82	5147-5152	given	abstract[280]|abstract[281]|abstract[282]	new[280]|giv[281]|giv[282]	_	_
36-83	5153-5160	segment	abstract[280]|abstract[281]|abstract[282]	new[280]|giv[281]|giv[282]	_	_
36-84	5161-5167	should	abstract[280]|abstract[281]	new[280]|giv[281]	_	_
36-85	5168-5170	be	abstract[280]|abstract[281]	new[280]|giv[281]	_	_
36-86	5171-5178	divided	abstract[280]|abstract[281]	new[280]|giv[281]	_	_
36-87	5179-5180	,	_	_	_	_
36-88	5181-5186	hence	_	_	_	_
36-89	5187-5195	yielding	_	_	_	_
36-90	5196-5201	lower	abstract[283]	giv[283]	_	_
36-91	5202-5208	Recall	abstract[283]	giv[283]	_	_
36-92	5209-5210	.	_	_	_	_
