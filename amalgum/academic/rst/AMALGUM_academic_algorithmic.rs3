<rst>
<header>
	<relations>
			<rel name="purpose" type="rst"/>
			<rel name="manner" type="rst"/>
			<rel name="result" type="rst"/>
			<rel name="background" type="rst"/>
			<rel name="elaboration" type="rst"/>
			<rel name="circumstance" type="rst"/>
			<rel name="condition" type="rst"/>
			<rel name="preparation" type="rst"/>
			<rel name="concession" type="rst"/>
			<rel name="means" type="rst"/>
			<rel name="contrast" type="multinuc"/>
			<rel name="joint" type="multinuc"/>
			<rel name="same_unit" type="multinuc"/>
		</relations>
</header>
<body>
<segment id="1" parent="1001" relname="preparation">5. Ethical Concerns from Algorithmic Decision-Making in AVs</segment>
<segment id="2" parent="1003" relname="span">This Section explores ethical issues</segment>
<segment id="3" parent="2" relname="elaboration">associated with algorithmic decision-making in AVs ,</segment>
<segment id="4" parent="1004" relname="span">their implications for AV safety risks and discrimination and the steps</segment>
<segment id="5" parent="1005" relname="span">taken</segment>
<segment id="6" parent="5" relname="purpose">to tackle these issues .</segment>
<segment id="7" parent="1008" relname="span">Section 5.1 discusses the sources of bias in AVs ’ algorithms</segment>
<segment id="8" parent="7" relname="elaboration">that can yield discrimination</segment>
<segment id="9" parent="1008" relname="means">by disproportionately allocating more safety risks to some groups of individuals .</segment>
<segment id="10" parent="1010" relname="span">Next , Section 5.2 explores approaches</segment>
<segment id="11" parent="1011" relname="joint">to incorporate ethics into AV algorithms ’ decision-making</segment>
<segment id="12" parent="1011" relname="joint">and highlight their implications for AV safety and discrimination .</segment>
<segment id="13" parent="1013" relname="span">Lastly , Section 5.3 examines how the incentives of AV stakeholders shape AV algorithms ’ design and resulting decisions</segment>
<segment id="14" parent="13" relname="elaboration">that can introduce new safety risks and discrimination .</segment>
<segment id="15" parent="1015" relname="span">5.1. Bias</segment>
<segment id="16" parent="1017" relname="span">A system is considered biased</segment>
<segment id="17" parent="1018" relname="span">when it contains “ intended ” or “ unintended ” characteristics</segment>
<segment id="18" parent="17" relname="elaboration">that unfairly discriminate against certain individuals or groups of individuals in society .</segment>
<segment id="19" parent="1019" relname="span">In American anti-discrimination law , discrimination exists</segment>
<segment id="20" parent="1021" relname="span">when there is disparate treatment ,</segment>
<segment id="21" parent="20" relname="elaboration">which is the “ discriminatory intent or the formal application of different rules to people of different groups ” ,</segment>
<segment id="22" parent="1022" relname="span">and/or disparate impact ,</segment>
<segment id="23" parent="1023" relname="span">which is the result</segment>
<segment id="24" parent="23" relname="elaboration">that “ differ for different groups ” .</segment>
<segment id="25" parent="1025" relname="span">Bias can be introduced into AVs during the human designers ’ construction of the datasets , models , and the parameters of the algorithm ,</segment>
<segment id="26" parent="25" relname="elaboration">which potentially leads to unfair or discriminatory allocations of safety risks .</segment>
<segment id="27" parent="1027" relname="span">Firstly , statistical bias exists</segment>
<segment id="28" parent="27" relname="circumstance">when the input data are not statistically representative of the overall population .</segment>
<segment id="29" parent="1029" relname="span">For instance , training an AV</segment>
<segment id="30" parent="1030" relname="joint">using data from only one country could result in the AV learning localised patterns</segment>
<segment id="31" parent="1031" relname="span">and not accurately modelling driving behaviours</segment>
<segment id="32" parent="31" relname="elaboration">that apply in other countries or contexts .</segment>
<segment id="33" parent="1006" relname="joint">Thus , the under- or overrepresentation of certain groups in the data can lead to inaccurate classifications and biased outcomes .</segment>
<segment id="34" parent="1034" relname="span">Secondly , the algorithm can be biased relative to legal and moral standards</segment>
<segment id="35" parent="34" relname="condition">if it utilises sensitive input variables .</segment>
<segment id="36" parent="1039" relname="span">Individual-specific characteristics , such as a person ’s age and gender</segment>
<segment id="37" parent="36" relname="elaboration">that are used as decision-making criteria</segment>
<segment id="38" parent="1040" relname="span">can be penalised or privileged by the AVs ’ algorithms</segment>
<segment id="39" parent="1041" relname="contrast">to meet the algorithm ’s pre-defined preferences , such as prioritising the safety of children</segment>
<segment id="40" parent="1041" relname="contrast">or minimising the total quantity of harm ,</segment>
<segment id="41" parent="1042" relname="span">causing more safety risks to be allocated to individuals</segment>
<segment id="42" parent="41" relname="elaboration">that share the penalised characteristics .</segment>
<segment id="43" parent="1044" relname="span">These forms of bias can be introduced unintentionally or intentionally by algorithm designers and AV manufacturers</segment>
<segment id="44" parent="43" relname="purpose">to maximise profits ,</segment>
<segment id="45" parent="1046" relname="span">such as prioritising the safety of AV passengers</segment>
<segment id="46" parent="45" relname="purpose">to maximise profits ,</segment>
<segment id="47" parent="1047" relname="span">and this is exacerbated by the lack of legal frameworks</segment>
<segment id="48" parent="47" relname="purpose">to hold these stakeholders accountable .</segment>
<segment id="49" parent="1051" relname="span">Section 5.2 explores various types of ethical preferences</segment>
<segment id="50" parent="49" relname="elaboration">to which AVs may be programmed to follow and their implications of AV safety risks in greater detail ,</segment>
<segment id="51" parent="1052" relname="span">and Section 5.3 explores how perverse incentives influence the choice of preferences</segment>
<segment id="52" parent="51" relname="elaboration">that are programmed into AVs ’ algorithms .</segment>
<segment id="53" parent="1053" relname="span">Lessening bias in algorithms is therefore crucial</segment>
<segment id="54" parent="53" relname="purpose">to mitigate discriminatory outcomes from AVs .</segment>
<segment id="55" parent="1056" relname="span">In autonomous systems in general , scholars have recommended ways</segment>
<segment id="56" parent="55" relname="purpose">to detect and offset the effects of bias ,</segment>
<segment id="57" parent="1058" relname="span">such as modifying algorithmic outputs</segment>
<segment id="58" parent="57" relname="purpose">to balance the effects of bias between protected and unprotected groups ,</segment>
<segment id="59" parent="1060" relname="span">introducing minimally intrusive modification</segment>
<segment id="60" parent="59" relname="purpose">to remove bias from the data ,</segment>
<segment id="61" parent="1061" relname="span">incorporating individuals from potentially discriminated groups ,</segment>
<segment id="62" parent="1063" relname="span">testing techniques</segment>
<segment id="63" parent="62" relname="purpose">to measure discrimination</segment>
<segment id="64" parent="1064" relname="span">and identify groups of users</segment>
<segment id="65" parent="1065" relname="joint">significantly affected by bias in software</segment>
<segment id="66" parent="1066" relname="span">and creating algorithms</segment>
<segment id="67" parent="66" relname="elaboration">that certify the absence of data bias .</segment>
<segment id="68" parent="1068" relname="span">Apart from bias</segment>
<segment id="69" parent="68" relname="elaboration">originating from the data and selection of variables and criterion ,</segment>
<segment id="70" parent="1070" relname="span">Danks and London recommend clarifying ethical standards such as fairness</segment>
<segment id="71" parent="70" relname="purpose">to evaluate bias .</segment>
<segment id="72" parent="1073" relname="span">Furthermore , scholars recommend increasing transparency</segment>
<segment id="73" parent="72" relname="purpose">to identify biases ,</segment>
<segment id="74" parent="1075" relname="span">such as designing algorithms</segment>
<segment id="75" parent="1076" relname="span">whose original input variables can be traced throughout the system</segment>
<segment id="76" parent="75" relname="elaboration">( i.e. , traceability )</segment>
<segment id="77" parent="1078" relname="span">and auditing algorithms</segment>
<segment id="78" parent="77" relname="purpose">to enhance their interpretability</segment>
<segment id="79" parent="1079" relname="joint">so that biases can be detected</segment>
<segment id="80" parent="1079" relname="joint">and the system ’s outputs can be verified against safety requirements .</segment>
<segment id="81" parent="1081" relname="span">However , there are challenges</segment>
<segment id="82" parent="81" relname="elaboration">in identifying bias in algorithms and their discriminatory effects .</segment>
<segment id="83" parent="1084" relname="span">Firstly , many algorithms are designed</segment>
<segment id="84" parent="83" relname="purpose">to be highly complex for greater accuracy ,</segment>
<segment id="85" parent="1085" relname="span">but this renders the algorithm opaque and difficult to interpret even by the designers themselves ,</segment>
<segment id="86" parent="85" relname="elaboration">concealing the sources of bias .</segment>
<segment id="87" parent="1087" relname="same_unit">Secondly ,</segment>
<segment id="88" parent="1089" relname="span">as ML algorithms make decisions mainly based on the training data</segment>
<segment id="89" parent="88" relname="elaboration">that changes over time ,</segment>
<segment id="90" parent="1088" relname="span">it is difficult to predict potentially discriminatory effects in advance .</segment>
<segment id="91" parent="1080" relname="joint">Humans are also excessively trusting and insufficiently critical of algorithmic decisions</segment>
<segment id="92" parent="1093" relname="span">due to the popular perception of algorithms as objective and fair , a problem</segment>
<segment id="93" parent="92" relname="elaboration">referred to as “ automation bias ”</segment>
<segment id="94" parent="1095" relname="span">and the seemingly “ objective ” correlations</segment>
<segment id="95" parent="94" relname="elaboration">that the algorithm learns from the data</segment>
<segment id="96" parent="1094" relname="same_unit">makes it difficult to legally establish discriminatory intent in algorithms .</segment>
<segment id="97" parent="1098" relname="span">An emerging issue is the aggregation of individually biased outcomes</segment>
<segment id="98" parent="97" relname="circumstance">when AVs with similar preferences are deployed on a large-scale ,</segment>
<segment id="99" parent="1098" relname="elaboration">as doing so would centralise and replicate algorithmic preferences along with their individually biased risk allocation decisions .</segment>
<segment id="100" parent="1080" relname="joint">This could lead to the same groups of people being consistently allocated more safety risks</segment>
<segment id="101" parent="1100" relname="span">and perpetuate systemic discrimination ,</segment>
<segment id="102" parent="1101" relname="span">which is more difficult to detect</segment>
<segment id="103" parent="102" relname="manner">as it results from the accumulation of similar driving outcomes .</segment>
<group id="1000" type="span" />
<group id="1001" type="span" parent="1000" relname="span"/>
<group id="1002" type="multinuc" parent="1006" relname="background"/>
<group id="1003" type="span" parent="1002" relname="same_unit"/>
<group id="1004" type="span" parent="1002" relname="same_unit"/>
<group id="1005" type="span" parent="4" relname="elaboration"/>
<group id="1006" type="multinuc" parent="1001" relname="span"/>
<group id="1007" type="span" parent="1006" relname="joint"/>
<group id="1008" type="span" parent="1007" relname="span"/>
<group id="1010" type="span" parent="1006" relname="joint"/>
<group id="1011" type="multinuc" parent="10" relname="purpose"/>
<group id="1013" type="span" parent="1006" relname="joint"/>
<group id="1015" type="span" parent="1006" relname="joint"/>
<group id="1016" type="span" parent="15" relname="elaboration"/>
<group id="1017" type="span" parent="1016" relname="span"/>
<group id="1018" type="span" parent="16" relname="circumstance"/>
<group id="1019" type="span" parent="1017" relname="elaboration"/>
<group id="1020" type="multinuc" parent="19" relname="circumstance"/>
<group id="1021" type="span" parent="1020" relname="same_unit"/>
<group id="1022" type="span" parent="1020" relname="same_unit"/>
<group id="1023" type="span" parent="22" relname="elaboration"/>
<group id="1025" type="span" parent="1006" relname="joint"/>
<group id="1027" type="span" parent="1006" relname="joint"/>
<group id="1029" type="span" parent="1006" relname="joint"/>
<group id="1030" type="multinuc" parent="29" relname="elaboration"/>
<group id="1031" type="span" parent="1030" relname="joint"/>
<group id="1034" type="span" parent="1006" relname="joint"/>
<group id="1036" type="span" parent="1006" relname="joint"/>
<group id="1037" type="span" parent="1036" relname="span"/>
<group id="1038" type="multinuc" parent="1037" relname="span"/>
<group id="1039" type="span" parent="1038" relname="same_unit"/>
<group id="1040" type="span" parent="1038" relname="same_unit"/>
<group id="1041" type="multinuc" parent="38" relname="purpose"/>
<group id="1042" type="span" parent="1038" relname="result"/>
<group id="1043" type="multinuc" parent="1037" relname="elaboration"/>
<group id="1044" type="span" parent="1043" relname="same_unit"/>
<group id="1045" type="multinuc" parent="1043" relname="same_unit"/>
<group id="1046" type="span" parent="1045" relname="joint"/>
<group id="1047" type="span" parent="1045" relname="joint"/>
<group id="1049" type="span" parent="1006" relname="joint"/>
<group id="1050" type="multinuc" parent="1049" relname="span"/>
<group id="1051" type="span" parent="1050" relname="same_unit"/>
<group id="1052" type="span" parent="1050" relname="same_unit"/>
<group id="1053" type="span" parent="1050" relname="elaboration"/>
<group id="1055" type="multinuc" parent="1006" relname="joint"/>
<group id="1056" type="span" parent="1055" relname="same_unit"/>
<group id="1057" type="span" parent="1055" relname="same_unit"/>
<group id="1058" type="span" parent="1057" relname="span"/>
<group id="1059" type="multinuc" parent="1058" relname="means"/>
<group id="1060" type="span" parent="1059" relname="same_unit"/>
<group id="1061" type="span" parent="1059" relname="same_unit"/>
<group id="1062" type="multinuc" parent="61" relname="elaboration"/>
<group id="1063" type="span" parent="1062" relname="joint"/>
<group id="1064" type="span" parent="1062" relname="joint"/>
<group id="1065" type="multinuc" parent="64" relname="elaboration"/>
<group id="1066" type="span" parent="1065" relname="joint"/>
<group id="1067" type="span" parent="1006" relname="joint"/>
<group id="1068" type="span" parent="1069" relname="preparation"/>
<group id="1069" type="multinuc" parent="1067" relname="span"/>
<group id="1070" type="span" parent="1069" relname="joint"/>
<group id="1071" type="multinuc" parent="1069" relname="joint"/>
<group id="1072" type="multinuc" parent="1071" relname="contrast"/>
<group id="1073" type="span" parent="1072" relname="same_unit"/>
<group id="1074" type="multinuc" parent="1072" relname="same_unit"/>
<group id="1075" type="span" parent="1074" relname="joint"/>
<group id="1076" type="span" parent="74" relname="elaboration"/>
<group id="1077" type="span" parent="1074" relname="joint"/>
<group id="1078" type="span" parent="1077" relname="span"/>
<group id="1079" type="multinuc" parent="1078" relname="purpose"/>
<group id="1080" type="multinuc" parent="1071" relname="contrast"/>
<group id="1081" type="span" parent="1080" relname="joint"/>
<group id="1083" type="span" parent="1080" relname="joint"/>
<group id="1084" type="span" parent="1085" relname="concession"/>
<group id="1085" type="span" parent="1083" relname="span"/>
<group id="1087" type="multinuc" parent="1080" relname="joint"/>
<group id="1088" type="span" parent="1087" relname="same_unit"/>
<group id="1089" type="span" parent="90" relname="circumstance"/>
<group id="1093" type="span" parent="1080" relname="joint"/>
<group id="1094" type="multinuc" parent="1080" relname="joint"/>
<group id="1095" type="span" parent="1094" relname="same_unit"/>
<group id="1097" type="span" parent="1080" relname="joint"/>
<group id="1098" type="span" parent="1097" relname="span"/>
<group id="1100" type="span" parent="1080" relname="joint"/>
<group id="1101" type="span" parent="101" relname="elaboration"/>
	</body>
</rst>
