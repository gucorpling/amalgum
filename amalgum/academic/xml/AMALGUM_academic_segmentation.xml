<?xml version='1.0' encoding='utf8'?>
<text id="AMALGUM_academic_segmentation" title="MiNgMatch — A Fast N-gram Model for Word Segmentation of the Ainu Language" shortTitle="segmentation" author="Karol Nowakowski, Michal Ptaszynski, Fumito Masui" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2078-2489/10/10/317/htm" speakerList="none" speakerCount="0">
<head>
<s type="frag">
6.	LS	6.
Results	NNS	result
and	CC	and
Discussion	NN	discussion
</s>
</head>
<p>
<s type="decl">
The	DT	the
results	NNS	result
of	IN	of
the	DT	the
evaluation	NN	evaluation
experiments	NNS	experiment
with	IN	with
our	PRP$	our
algorithm	NN	algorithm
are	VBP	be
presented	VBN	present
in	IN	in
Table	NNP	Table
5	CD	5
.	.	.
</s>
<s type="decl">
The	DT	the
variant	NN	variant
without	IN	without
the	DT	the
limit	NN	limit
of	IN	of
n-grams	NNS	n-gram
per	IN	per
input	NN	input
segment	NN	segment
produces	VBZ	produce
unbalanced	JJ	unbalanced
results	NNS	result
(	-LRB-	(
especially	RB	especially
on	IN	on
SYOS	NNP	SYOS
)	-RRB-	)
,	,	,
with	IN	with
relatively	RB	relatively
low	JJ	low
Precision	NNP	Precision
.	.	.
</s>
<s type="decl">
After	IN	after
setting	VBG	set
the	DT	the
limit	NN	limit
to	IN	to
2	CD	2
,	,	,
Precision	NN	precision
improves	VBZ	improve
at	IN	at
the	DT	the
cost	NN	cost
of	IN	of
a	DT	a
drop	NN	drop
in	IN	in
Recall	NNP	recall
.	.	.
</s>
<s type="decl">
The	DT	the
F-score	NN	f-score
is	VBZ	be
better	JJR	good
for	IN	for
SYOS	NNP	SYOS
,	,	,
while	IN	while
on	IN	on
AKJ	NNP	AKJ
there	EX	there
is	VBZ	be
a	DT	a
very	RB	very
slight	JJ	slight
drop	NN	drop
.	.	.
</s>
</p>
<p>
<s type="decl">
Table	NNP	Table
6	CD	6
shows	VBZ	show
the	DT	the
results	NNS	result
of	IN	of
experiments	NNS	experiment
with	IN	with
the	DT	the
Stupid	JJ	Stupid
Backoff	NNP	Backoff
model	NN	model
.	.	.
</s>
<s type="decl">
When	WRB	when
no	DT	no
backoff	NN	backoff
factor	NN	factor
is	VBZ	be
applied	VBN	apply
,	,	,
results	NNS	result
for	IN	for
both	DT	both
test	NN	test
sets	NNS	set
are	VBP	be
similar	JJ	similar
to	IN	to
those	DT	this
from	IN	from
the	DT	the
MiNgMatch	NNP	MiNgMatch
Segmenter	NNP	Segmenter
without	IN	without
the	DT	the
limit	NN	limit
of	IN	of
n-grams	NNS	n-gram
per	IN	per
input	NN	input
segment	NN	segment
.	.	.
</s>
<s type="decl">
Setting	VBG	set
the	DT	the
backoff	NN	backoff
factor	NN	factor
to	IN	to
an	DT	a
appropriate	JJ	appropriate
value	NN	value
allows	VBZ	allow
for	IN	for
significant	JJ	significant
improvement	NN	improvement
in	IN	in
Precision	NNP	Precision
and	CC	and
F-score	NNP	F-score
(	-LRB-	(
and	CC	and
in	IN	in
some	DT	some
cases	NNS	case
also	RB	also
small	JJ	small
improvements	NNS	improvement
in	IN	in
Recall	NNP	recall
)	-RRB-	)
.	.	.
</s>
<s type="decl">
For	IN	for
the	DT	the
F-score	NNP	F-score
,	,	,
it	PRP	it
is	VBZ	be
better	JJR	good
to	TO	to
set	VB	set
a	DT	a
low	JJ	low
backoff	NN	backoff
factor	NN	factor
(	-LRB-	(
e.g.	FW	e.g.
,	,	,
0.09	CD	0.09
)	-RRB-	)
for	IN	for
1-grams	NNS	1-gram
only	RB	only
,	,	,
than	IN	than
to	TO	to
set	VB	set
it	PRP	it
to	IN	to
a	DT	a
fixed	VBN	fix
value	NN	value
for	IN	for
all	DT	all
backoff	NN	backoff
steps	NNS	step
(	-LRB-	(
e.g.	FW	e.g.
,	,	,
0.4	CD	0.4
,	,	,
as	IN	as
Brants	NNP	Brant
et	FW	et
al.	FW	al.
did	VBD	do
)	-RRB-	)
.	.	.
</s>
<s type="decl">
A	DT	a
backoff	NN	backoff
factor	NN	factor
of	IN	of
0.4	CD	0.4
gives	VBZ	give
significant	JJ	significant
improvement	NN	improvement
in	IN	in
Precision	NN	precision
with	IN	with
higher	JJR	high
order	NN	order
n-gram	NN	n-gram
models	NNS	model
,	,	,
but	CC	but
at	IN	at
the	DT	the
same	JJ	same
time	NN	time
Recall	NN	recall
drops	VBZ	drop
drastically	RB	drastically
and	CC	and
overall	JJ	overall
performance	NN	performance
deteriorates	VBZ	deteriorate
.	.	.
</s>
<s type="decl">
For	IN	for
models	NNS	model
with	IN	with
an	DT	a
n-gram	JJ	n-gram
order	NN	order
of	IN	of
3	CD	3
or	CC	or
higher	JJR	high
,	,	,
the	DT	the
backoff	NN	backoff
factor	NN	factor
has	VBZ	have
a	DT	a
bigger	JJR	big
impact	NN	impact
on	IN	on
the	DT	the
results	NNS	result
than	IN	than
further	RB	far
increasing	VBG	increase
the	DT	the
order	NN	order
of	IN	of
n-grams	NNS	n-gram
included	VBN	include
in	IN	in
the	DT	the
model	NN	model
.	.	.
</s>
<s type="decl">
A	DT	a
comparison	NN	comparison
with	IN	with
the	DT	the
results	NNS	result
yielded	VBN	yield
by	IN	by
MiNgMatch	NNP	MiNgMatch
shows	VBZ	show
that	IN	that
setting	VBG	set
the	DT	the
limit	NN	limit
of	IN	of
n-grams	NNS	n-gram
per	IN	per
input	NN	input
segment	NN	segment
is	VBZ	be
more	RBR	more
effective	JJ	effective
than	IN	than
Stupid	JJ	Stupid
Backoff	NNP	Backoff
as	IN	as
a	DT	a
method	NN	method
for	IN	for
improving	VBG	improve
precision	NN	precision
of	IN	of
the	DT	the
segmentation	NN	segmentation
process	NN	process
—	:	—
it	PRP	it
leads	VBZ	lead
to	IN	to
a	DT	a
much	RB	much
smaller	JJR	small
drop	NN	drop
in	IN	in
Recall	NNP	recall
.	.	.
</s>
</p>
<p>
<s type="decl">
The	DT	the
results	NNS	result
of	IN	of
the	DT	the
experiment	NN	experiment
with	IN	with
models	NNS	model
employing	VBG	employ
modified	VBN	modify
Kneser-Ney	NNP	Kneser-Ney
smoothing	NN	smoothing
are	VBP	be
shown	VBN	show
in	IN	in
Table	NNP	Table
7	CD	7
.	.	.
</s>
<s type="decl">
They	PRP	they
achieve	VBP	achieve
higher	JJR	high
Precision	NN	precision
than	IN	than
both	NN	both
the	DT	the
other	JJ	other
types	NNS	type
of	IN	of
n-gram	NN	n-gram
models	NNS	model
.	.	.
</s>
<s type="decl">
Nevertheless	RB	nevertheless
,	,	,
due	JJ	due
to	IN	to
very	RB	very
low	JJ	low
Recall	NN	recall
,	,	,
the	DT	the
overall	JJ	overall
results	NNS	result
are	VBP	be
low	JJ	low
.	.	.
</s>
</p>
<p>
<s type="decl">
The	DT	the
results	NNS	result
obtained	VBN	obtain
by	IN	by
the	DT	the
Universal	NNP	Universal
Segmenter	NNP	Segmenter
are	VBP	be
presented	VBN	present
in	IN	in
Table	NNP	Table
8	CD	8
.	.	.
</s>
<s type="decl">
The	DT	the
default	JJ	default
model	NN	model
(	-LRB-	(
regardless	RB	regardless
of	IN	of
what	WDT	what
kind	NN	kind
of	IN	of
character	NN	character
representations	NNS	representation
are	VBP	be
used	VBN	use
—	:	—
conventional	JJ	conventional
character	NN	character
embeddings	NNS	embedding
or	CC	or
concatenated	VBN	concatenate
n-gram	NN	n-gram
vectors	NNS	vector
)	-RRB-	)
learns	VBZ	learn
from	IN	from
the	DT	the
training	NN	training
data	NNS	datum
that	IN	that
the	DT	the
first	JJ	first
and	CC	and
the	DT	the
last	JJ	last
character	NN	character
of	IN	of
a	DT	a
word	NN	word
(	-LRB-	(
corresponding	VBG	correspond
to	IN	to
<tt>
B	NN	B
</tt>
,	,	,
<tt>
E	NNP	E
</tt>
and	CC	and
<tt>
S	NNP	S
</tt>
tags	NNS	tag
)	-RRB-	)
are	VBP	be
always	RB	always
adjacent	JJ	adjacent
either	CC	either
to	IN	to
the	DT	the
boundary	NN	boundary
of	IN	of
a	DT	a
space-delimited	JJ	space-delimited
segment	NN	segment
or	CC	or
to	IN	to
a	DT	a
punctuation	NN	punctuation
mark	NN	mark
.	.	.
</s>
<s type="decl">
As	IN	as
a	DT	a
result	NN	result
,	,	,
the	DT	the
model	NN	model
separates	VBZ	separate
punctuation	NN	punctuation
from	IN	from
alpha-numeric	JJ	alpha-numeric
strings	NNS	string
found	VBN	find
in	IN	in
the	DT	the
input	NN	input
,	,	,
but	CC	but
never	RB	never
applies	VBZ	apply
further	JJ	further
segmentation	NN	segmentation
to	IN	to
them	PRP	they
.	.	.
</s>
</p>
<p>
<s type="decl">
US-ISP	NNP	US-ISP
models	NNS	model
are	VBP	be
better	JJR	good
but	CC	but
still	RB	still
notably	RB	notably
worse	JJR	bad
than	IN	than
lexical	JJ	lexical
n-gram	NN	n-gram
models	NNS	model
(	-LRB-	(
especially	RB	especially
on	IN	on
SYOS	NNP	SYOS
)	-RRB-	)
.	.	.
</s>
<s type="decl">
Unlike	IN	unlike
with	IN	with
default	NN	default
settings	NNS	setting
,	,	,
the	DT	the
model	NN	model
trained	VBN	train
on	IN	on
data	NNS	datum
without	IN	without
whitespaces	NNS	whitespace
learns	VBZ	learn
to	TO	to
predict	VB	predict
word	NN	word
boundaries	NNS	boundary
within	IN	within
strings	NNS	string
of	IN	of
alpha-numeric	JJ	alpha-numeric
characters	NNS	character
.	.	.
</s>
<s type="decl">
However	RB	however
,	,	,
when	WRB	when
presented	VBN	present
with	IN	with
test	NN	test
data	NNS	datum
including	VBG	include
spaces	NNS	space
,	,	,
they	PRP	they
impede	VBP	impede
the	DT	the
segmentation	NN	segmentation
process	NN	process
rather	RB	rather
than	IN	than
supporting	VBG	support
it	PRP	it
.	.	.
</s>
<s type="decl">
As	IN	as
shown	VBN	show
in	IN	in
Table	NNP	Table
9	CD	9
,	,	,
if	IN	if
we	PRP	we
only	RB	only
take	VBP	take
into	IN	into
account	NN	account
the	DT	the
word	NN	word
boundaries	NNS	boundary
not	RB	not
already	RB	already
indicated	VBN	indicate
in	IN	in
the	DT	the
raw	JJ	raw
test	NN	test
set	NN	set
,	,	,
the	DT	the
model	NN	model
makes	VBZ	make
more	RBR	more
correct	JJ	correct
predictions	NNS	prediction
in	IN	in
data	NNS	datum
where	WRB	where
the	DT	the
whitespaces	NNS	whitespace
have	VBP	have
all	RB	all
been	VBN	be
removed	VBN	remove
.	.	.
</s>
</p>
<p>
<s type="decl">
Models	NNS	model
with	IN	with
multi-word	JJ	multi-word
tokens	NNS	token
achieve	VBP	achieve
significantly	RB	significantly
higher	JJR	high
results	NNS	result
.	.	.
</s>
<s type="decl">
Precision	NN	precision
of	IN	of
the	DT	the
US-MWTs	NNPS	US-MWTs
model	NN	model
is	VBZ	be
on	IN	on
par	NN	par
with	IN	with
the	DT	the
segmenter	NN	segmenter
applying	VBG	apply
Kneser-Ney	NNP	Kneser-Ney
smoothing	NN	smoothing
,	,	,
while	IN	while
maintaining	VBG	maintain
relatively	RB	relatively
high	JJ	high
Recall	NNP	recall
.	.	.
</s>
<s type="decl">
It	PRP	it
yields	VBZ	yield
lower	JJR	low
Recall	NNP	recall
than	IN	than
the	DT	the
model	NN	model
with	IN	with
randomly	RB	randomly
generated	VBN	generate
multi-word	JJ	multi-word
tokens	NNS	token
,	,	,
but	CC	but
the	DT	the
F-score	NNP	F-score
is	VBZ	be
higher	JJR	high
due	JJ	due
to	IN	to
better	JJR	good
Precision	NN	precision
.	.	.
</s>
</p>
<p>
<s type="decl">
With	IN	with
the	DT	the
exception	NN	exception
of	IN	of
the	DT	the
US-ISP	NNP	US-ISP
model	NN	model
on	IN	on
SYOS	NNP	SYOS
,	,	,
all	DT	all
variants	NNS	variant
of	IN	of
the	DT	the
neural	JJ	neural
segmenter	NN	segmenter
achieved	VBD	achieve
the	DT	the
best	JJS	good
performance	NN	performance
with	IN	with
concatenated	VBN	concatenate
9-gram	NN	9-gram
vectors	NNS	vector
.	.	.
</s>
<s type="decl">
This	DT	this
contrasts	VBZ	contrast
with	IN	with
the	DT	the
results	NNS	result
reported	VBN	report
by	IN	by
Shao	NNP	Shao
et	FW	et
al.	FW	al.
for	IN	for
Chinese	JJ	Chinese
,	,	,
where	WRB	where
in	IN	in
most	JJS	most
cases	NNS	case
there	EX	there
was	VBD	be
no	DT	no
further	JJR	further
improvement	NN	improvement
beyond	IN	beyond
3-grams	NNS	3-gram
.	.	.
</s>
<s type="decl">
This	DT	this
behavior	NN	behavior
is	VBZ	be
a	DT	a
consequence	NN	consequence
of	IN	of
differences	NNS	difference
between	IN	between
writing	VBG	write
systems	NNS	system
:	:	:
words	NNS	word
in	IN	in
Chinese	JJ	Chinese
are	VBP	be
on	IN	on
average	NN	average
composed	VBN	compose
of	IN	of
less	JJR	less
characters	NNS	character
than	IN	than
in	IN	in
languages	NNS	language
using	VBG	use
alphabetic	JJ	alphabetic
scripts	NNS	script
.	.	.
</s>
<s type="decl">
Due	JJ	due
to	IN	to
a	DT	a
much	RB	much
bigger	JJR	big
character	NN	character
set	NN	set
size	NN	size
,	,	,
<hi rend="italic">
hanzi	NN	hanzi
</hi>
characters	NNS	character
are	VBP	be
also	RB	also
more	RBR	more
informative	JJ	informative
to	IN	to
word	NN	word
segmentation	NN	segmentation
,	,	,
hence	RB	hence
better	JJR	good
performance	NN	performance
with	IN	with
models	NNS	model
using	VBG	use
shorter	JJR	short
context	NN	context
.	.	.
</s>
</p>
<head>
<s type="frag">
6.1.	CD	6.1.
</s>
<s type="frag">
General	NNP	General
Observations	NNS	observation
</s>
</head>
<p>
<s type="decl">
Due	JJ	due
to	IN	to
data	NN	datum
sparsity	NN	sparsity
,	,	,
n-gram	NN	n-gram
coverage	NN	coverage
in	IN	in
the	DT	the
test	NN	test
set	NN	set
(	-LRB-	(
the	DT	the
fraction	NN	fraction
of	IN	of
n-grams	NNS	n-gram
in	IN	in
the	DT	the
test	NN	test
data	NNS	datum
that	WDT	that
can	MD	can
be	VB	be
found	VBN	find
in	IN	in
the	DT	the
training	NN	training
set	NN	set
)	-RRB-	)
is	VBZ	be
low	JJ	low
(	-LRB-	(
see	VB	see
Table	NNP	Table
10	CD	10
)	-RRB-	)
.	.	.
</s>
<s type="decl">
It	PRP	it
means	VBZ	mean
that	IN	that
many	JJ	many
multi-word	JJ	multi-word
tokens	NNS	token
from	IN	from
the	DT	the
test	NN	test
set	NN	set
are	VBP	be
known	VBN	know
to	IN	to
n-gram	NN	n-gram
models	NNS	model
as	IN	as
separate	JJ	separate
unigrams	NNS	unigram
,	,	,
but	CC	but
not	RB	not
in	IN	in
the	DT	the
form	NN	form
of	IN	of
a	DT	a
single	JJ	single
n-gram	NN	n-gram
.	.	.
</s>
<s type="decl">
The	DT	the
Stupid	JJ	Stupid
Backoff	NNP	Backoff
model	NN	model
with	IN	with
a	DT	a
backoff	NN	backoff
factor	NN	factor
for	IN	for
unigrams	NNS	unigram
set	VBN	set
to	IN	to
a	DT	a
moderate	JJ	moderate
value	NN	value
(	-LRB-	(
such	JJ	such
as	IN	as
0.09	CD	0.09
)	-RRB-	)
is	VBZ	be
able	JJ	able
to	TO	to
segment	VB	segment
such	JJ	such
strings	NNS	string
correctly	RB	correctly
.	.	.
</s>
<s type="decl">
However	RB	however
,	,	,
it	PRP	it
also	RB	also
erroneously	RB	erroneously
segments	VBZ	segment
some	DT	some
OoV	NN	OoV
single-word	JJ	single-word
tokens	NNS	token
whose	WP$	whose
surface	NN	surface
forms	NNS	form
happen	VBP	happen
to	TO	to
be	VB	be
interpretable	JJ	interpretable
as	IN	as
a	DT	a
sequence	NN	sequence
of	IN	of
concatenated	VBN	concatenate
in-vocabulary	JJ	in-vocabulary
unigrams	NNS	unigram
,	,	,
resulting	VBG	result
in	IN	in
lower	JJR	low
Precision	NN	precision
.	.	.
</s>
<s type="decl">
On	IN	on
the	DT	the
other	JJ	other
hand	NN	hand
,	,	,
models	NNS	model
assigning	VBG	assign
low	JJ	low
scores	NNS	score
to	IN	to
unigrams	NNS	unigram
(	-LRB-	(
such	JJ	such
as	IN	as
a	DT	a
4-	NN	4-
or	CC	or
5-gram	NN	5-gram
model	NN	model
with	IN	with
the	DT	the
Stupid	NNP	Stupid
Backoff	NNP	Backoff
and	CC	and
backoff	NN	backoff
factor	NN	factor
set	VBN	set
as	IN	as
suggested	VBN	suggest
by	IN	by
Brants	NNP	Brant
et	FW	et
al.	FW	al.
,	,	,
and	CC	and
in	IN	in
particular	JJ	particular
the	DT	the
model	NN	model
applying	VBG	apply
modified	VBN	modify
Kneser-Ney	NNP	Kneser-Ney
smoothing	NN	smoothing
)	-RRB-	)
are	VBP	be
better	JJR	good
at	IN	at
handling	VBG	handle
OoV	JJ	OoV
words	NNS	word
(	-LRB-	(
see	VB	see
Table	NNP	Table
11	CD	11
)	-RRB-	)
,	,	,
but	CC	but
as	IN	as
a	DT	a
result	NN	result
of	IN	of
probability	NN	probability
multiplication	NN	multiplication
,	,	,
in	IN	in
many	JJ	many
cases	NNS	case
they	PRP	they
score	VBP	score
unseen	JJ	unseen
multi-word	JJ	multi-word
segments	NNS	segment
higher	JJR	high
than	IN	than
a	DT	a
sequence	NN	sequence
of	IN	of
unigrams	NNS	unigram
into	IN	into
which	WDT	which
the	DT	the
given	VBN	give
segment	NN	segment
should	MD	should
be	VB	be
divided	VBN	divide
,	,	,
hence	RB	hence
yielding	VBG	yield
lower	JJR	low
Recall	NNP	recall
.	.	.
</s>
</p>
</text>