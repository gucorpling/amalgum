<?xml version='1.0' encoding='utf8'?>
<text id="AMALGUM_academic_fourier" title="The Cauchy Conjugate Gradient Algorithm with Random Fourier Features" shortTitle="fourier" author="Xuewei Huang, Shiyuan Wang, Kui Xiong" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2073-8994/11/10/1323/htm" speakerList="none" speakerCount="0">
<head>
<s type="frag">
1.	LS	1.
Introduction	NN	introduction
</s>
</head>
<p>
<s type="decl">
Many	JJ	many
applications	NNS	application
in	IN	in
the	DT	the
real	JJ	real
world	NN	world
,	,	,
such	JJ	such
as	IN	as
system	NN	system
identification	NN	identification
,	,	,
regression	NN	regression
,	,	,
and	CC	and
online	JJ	online
kernel	NN	kernel
learning	NN	learning
(	-LRB-	(
OKL	NNP	OKL
)	-RRB-	)
,	,	,
require	VBP	require
complex	JJ	complex
nonlinear	JJ	nonlinear
models	NNS	model
.	.	.
</s>
<s type="decl">
The	DT	the
kernel	NN	kernel
method	NN	method
using	VBG	use
a	DT	a
Mercer	NNP	Mercer
kernel	NN	kernel
has	VBZ	have
attracted	VBN	attract
interests	NNS	interest
in	IN	in
tackling	VBG	tackle
these	DT	this
complex	JJ	complex
nonlinear	JJ	nonlinear
applications	NNS	application
,	,	,
which	WDT	which
transforms	VBZ	transform
nonlinear	JJ	nonlinear
applications	NNS	application
into	IN	into
linear	JJ	linear
ones	NNS	one
in	IN	in
the	DT	the
reproducing	VBG	reproduce
kernel	NN	kernel
Hilbert	NNP	Hilbert
space	NN	space
(	-LRB-	(
RKHS	NNP	RKHS
)	-RRB-	)
.	.	.
</s>
<s type="decl">
Developed	VBN	Develop
in	IN	in
RKHS	NNP	RKHS
,	,	,
a	DT	a
kernel	NN	kernel
adaptive	JJ	adaptive
filter	NN	filter
(	-LRB-	(
KAF	NNP	KAF
)	-RRB-	)
is	VBZ	be
the	DT	the
most	RBS	most
celebrated	VBN	celebrate
subfield	NN	subfield
of	IN	of
OKL	NNP	OKL
algorithms	NNS	algorithm
.	.	.
</s>
<s type="decl">
Using	VBG	use
the	DT	the
simplest	JJS	simple
stochastic	JJ	stochastic
gradient	NN	gradient
descent	NN	descent
(	-LRB-	(
SGD	NNP	SGD
)	-RRB-	)
method	NN	method
for	IN	for
learning	NN	learning
,	,	,
KAFs	NNS	kaf
including	VBG	include
the	DT	the
kernel	NN	kernel
least	NN	least
mean	NN	mean
square	NN	square
(	-LRB-	(
KLMS	NNP	KLMS
)	-RRB-	)
algorithm	NN	algorithm
,	,	,
kernel	NN	kernel
affine	NN	affine
projection	NN	projection
algorithm	NN	algorithm
(	-LRB-	(
KAPA	NNP	KAPA
)	-RRB-	)
,	,	,
and	CC	and
kernel	NN	kernel
recursive	JJ	recursive
least	NN	least
squares	NNS	square
(	-LRB-	(
KRLS	NNP	KRLS
)	-RRB-	)
algorithm	NN	algorithm
have	VBP	have
been	VBN	be
proposed	VBN	propose
.	.	.
</s>
</p>
<p>
<s type="decl">
However	RB	however
,	,	,
allocating	VBG	allocate
a	DT	a
new	JJ	new
kernel	NN	kernel
unit	NN	unit
as	IN	as
a	DT	a
radial	JJ	radial
basis	NN	basis
function	NN	function
(	-LRB-	(
RBF	NNP	RBF
)	-RRB-	)
center	NN	center
with	IN	with
the	DT	the
coming	NN	coming
of	IN	of
new	JJ	new
data	NNS	datum
,	,	,
the	DT	the
linearly	JJ	linearly
growing	VBG	grow
structure	NN	structure
(	-LRB-	(
called	VBN	call
“	``	''
dictionary	NN	dictionary
”	''	''
hereafter	RB	hereafter
)	-RRB-	)
will	MD	will
increase	VB	increase
the	DT	the
computational	JJ	computational
and	CC	and
memory	NN	memory
requirements	NNS	requirement
in	IN	in
KAFs	NNPS	KAF
.	.	.
</s>
<s type="decl">
To	TO	to
curb	VB	curb
the	DT	the
growth	NN	growth
of	IN	of
the	DT	the
dictionary	NN	dictionary
,	,	,
two	CD	2
categories	NNS	category
are	VBP	be
chosen	VBN	choose
for	IN	for
sparsification	NN	sparsification
.	.	.
</s>
<s type="decl">
The	DT	the
first	JJ	first
category	NN	category
accepts	VBZ	accept
only	RB	only
informative	JJ	informative
data	NNS	datum
as	IN	as
new	JJ	new
dictionary	NN	dictionary
centers	NNS	center
by	IN	by
using	VBG	use
a	DT	a
threshold	NN	threshold
,	,	,
including	VBG	include
the	DT	the
surprise	NN	surprise
criterion	NN	criterion
(	-LRB-	(
SC	NNP	SC
)	-RRB-	)
,	,	,
the	DT	the
coherence	NN	coherence
criterion	NN	criterion
(	-LRB-	(
CC	NNP	CC
)	-RRB-	)
,	,	,
and	CC	and
the	DT	the
vector	NN	vector
quantization	NN	quantization
(	-LRB-	(
VQ	NNP	VQ
)	-RRB-	)
.	.	.
</s>
<s type="decl">
However	RB	however
,	,	,
these	DT	this
methods	NNS	method
cannot	MD	cannot
fully	RB	fully
address	VB	address
the	DT	the
growing	VBG	grow
problem	NN	problem
and	CC	and
still	RB	still
introduce	VB	introduce
additional	JJ	additional
time	NN	time
consumption	NN	consumption
at	IN	at
each	DT	each
iteration	NN	iteration
.	.	.
</s>
<s type="decl">
The	DT	the
fixed	VBN	fix
points	NNS	point
methods	NNS	method
as	IN	as
the	DT	the
second	JJ	second
category	NN	category
,	,	,
including	VBG	include
the	DT	the
fixed-budget	NN	fixed-budget
(	-LRB-	(
FB	NNP	FB
)	-RRB-	)
,	,	,
the	DT	the
sliding	VBG	slide
window	NN	window
(	-LRB-	(
SW	NNP	SW
)	-RRB-	)
,	,	,
and	CC	and
the	DT	the
kernel	NN	kernel
approximation	NN	approximation
methods	NNS	method
(	-LRB-	(
e.g.	FW	e.g.
,	,	,
the	DT	the
Nystrm	NNP	Nystrm
method	NN	method
and	CC	and
random	JJ	random
Fourier	NNP	Fourier
features	NNS	feature
(	-LRB-	(
RFFs	NNS	RFF
)	-RRB-	)
method	NN	method
)	-RRB-	)
,	,	,
are	VBP	be
used	VBN	use
to	TO	to
overcome	VB	overcome
the	DT	the
sublinearly	RB	sublinearly
growing	VBG	grow
problem	NN	problem
.	.	.
</s>
<s type="decl">
However	RB	however
,	,	,
the	DT	the
FB	NNP	FB
method	NN	method
and	CC	and
the	DT	the
SW	NN	SW
method	NN	method
cannot	MD	cannot
guarantee	VB	guarantee
a	DT	a
good	JJ	good
performance	NN	performance
in	IN	in
specific	JJ	specific
environments	NNS	environment
with	IN	with
a	DT	a
small	JJ	small
amount	NN	amount
of	IN	of
time	NN	time
.	.	.
</s>
<s type="decl">
Compared	VBN	compare
with	IN	with
the	DT	the
Nystrm	NNP	Nystrm
method	NN	method
,	,	,
RFFs	NNS	RFF
are	VBP	be
drawn	VBN	draw
from	IN	from
a	DT	a
distribution	NN	distribution
that	WDT	that
is	VBZ	be
randomly	RB	randomly
independent	JJ	independent
from	IN	from
the	DT	the
training	NN	training
data	NNS	datum
.	.	.
</s>
<s type="decl">
Due	JJ	due
to	IN	to
a	DT	a
data-independent	JJ	data-independent
vector	NN	vector
representation	NN	representation
,	,	,
RFFs	NNS	RFF
can	MD	can
provide	VB	provide
a	DT	a
good	JJ	good
solution	NN	solution
to	IN	to
non-stationary	JJ	non-stationary
circumstances	NNS	circumstance
.	.	.
</s>
<s type="decl">
On	IN	on
the	DT	the
basis	NN	basis
of	IN	of
RFFs	NN	RFF
,	,	,
random	JJ	random
Fourier	NNP	Fourier
mapping	NN	mapping
(	-LRB-	(
RFM	NNP	RFM
)	-RRB-	)
is	VBZ	be
proposed	VBN	propose
by	IN	by
mapping	VBG	map
input	NN	input
data	NNS	datum
into	IN	into
a	DT	a
finite-dimensional	JJ	finite-dimensional
random	JJ	random
Fourier	NNP	Fourier
features	NNS	feature
space	NN	space
(	-LRB-	(
RFFS	NNP	RFFS
)	-RRB-	)
using	VBG	use
a	DT	a
randomized	JJ	randomized
feature	NN	feature
kernel	NN	kernel
’s	POS	's
Fourier	NNP	Fourier
transform	NN	transform
in	IN	in
a	DT	a
fixed	VBN	fix
network	NN	network
structure	NN	structure
.	.	.
</s>
<s type="decl">
The	DT	the
RFM	NNP	RFM
alleviates	VBZ	alleviate
the	DT	the
computational	JJ	computational
and	CC	and
storage	NN	storage
burdens	NNS	burden
of	IN	of
KAFs	NNS	kaf
,	,	,
and	CC	and
ensures	VBZ	ensure
a	DT	a
satisfactory	JJ	satisfactory
performance	NN	performance
under	IN	under
non-stationary	JJ	non-stationary
conditions	NNS	condition
.	.	.
</s>
<s type="decl">
The	DT	the
examples	NNS	example
for	IN	for
developing	VBG	develop
KAFs	NNS	kaf
with	IN	with
RFM	NNP	RFM
are	VBP	be
the	DT	the
random	JJ	random
Fourier	NNP	Fourier
features	NNS	feature
kernel	NN	kernel
least	NN	least
mean	NN	mean
square	NN	square
(	-LRB-	(
RFFKLMS	NNP	RFFKLMS
)	-RRB-	)
algorithm	NN	algorithm
,	,	,
random	JJ	random
Fourier	NNP	Fourier
features	VBN	feature
maximum	JJ	maximum
correntropy	NN	correntropy
(	-LRB-	(
RFFMC	NNP	RFFMC
)	-RRB-	)
algorithm	NN	algorithm
,	,	,
and	CC	and
random	JJ	random
Fourier	NNP	Fourier
features	VBN	feature
conjugate	NN	conjugate
gradient	NN	gradient
(	-LRB-	(
RFFCG	NNP	RFFCG
)	-RRB-	)
algorithm	NN	algorithm
.	.	.
</s>
</p>
<p>
<s type="decl">
For	IN	for
the	DT	the
loss	NN	loss
function	NN	function
,	,	,
due	JJ	due
to	IN	to
their	PRP$	their
simplicity	NN	simplicity
,	,	,
smoothness	NN	smoothness
,	,	,
and	CC	and
mathematical	JJ	mathematical
tractability	NN	tractability
,	,	,
the	DT	the
second-order	NN	second-order
statistical	JJ	statistical
measures	NNS	measure
(	-LRB-	(
e.g.	FW	e.g.
,	,	,
minimum	NN	minimum
mean	NN	mean
square	NN	square
error	NN	error
(	-LRB-	(
MMSE	NNP	MMSE
)	-RRB-	)
and	CC	and
least	NN	least
squares	NNS	square
)	-RRB-	)
are	VBP	be
widely	RB	widely
utilized	VBN	utilize
in	IN	in
KAFs	NNPS	KAF
.	.	.
</s>
<s type="decl">
However	RB	however
,	,	,
KAFs	NNPS	KAF
based	VBN	base
on	IN	on
the	DT	the
second-order	JJ	second-order
statistical	JJ	statistical
measures	NNS	measure
are	VBP	be
sensitive	JJ	sensitive
to	IN	to
non-Gaussian	JJ	non-Gaussian
noises	NNS	noise
including	VBG	include
the	DT	the
sub-Gaussian	JJ	sub-Gaussian
and	CC	and
super-Gaussian	JJ	super-Gaussian
noises	NNS	noise
,	,	,
which	WDT	which
means	VBZ	mean
that	IN	that
their	PRP$	their
performance	NN	performance
may	MD	may
be	VB	be
seriously	RB	seriously
degraded	VBN	degrade
if	IN	if
the	DT	the
training	NN	training
data	NNS	datum
are	VBP	be
contaminated	VBN	contaminate
by	IN	by
outliers	NNS	outlier
.	.	.
</s>
<s type="decl">
To	TO	to
handle	VB	handle
this	DT	this
issue	NN	issue
,	,	,
robust	JJ	robust
statistical	JJ	statistical
measures	NNS	measure
have	VBP	have
therefore	RB	therefore
gained	VBN	gain
more	JJR	more
attention	NN	attention
,	,	,
among	IN	among
which	WDT	which
the	DT	the
lower-order	NN	lower-order
error	NN	error
measure	NN	measure
and	CC	and
the	DT	the
higher-lower	JJR	higher-lower
error	NN	error
measure	NN	measure
are	VBP	be
two	CD	2
typical	JJ	typical
examples	NNS	example
.	.	.
</s>
<s type="decl">
However	RB	however
,	,	,
the	DT	the
higher-order	NN	higher-order
error	NN	error
measure	NN	measure
is	VBZ	be
not	RB	not
suitable	JJ	suitable
for	IN	for
the	DT	the
mixture	NN	mixture
of	IN	of
Gaussian	JJ	Gaussian
and	CC	and
super-Gaussian	JJ	super-Gaussian
noises	NNS	noise
(	-LRB-	(
Laplace	NNP	Laplace
,	,	,
-stable	NN	-stable
,	,	,
etc.	FW	etc.
)	-RRB-	)
with	IN	with
poor	JJ	poor
stability	NN	stability
and	CC	and
astringency	NN	astringency
,	,	,
and	CC	and
the	DT	the
lower-order	JJR	lower-order
measure	NN	measure
of	IN	of
error	NN	error
is	VBZ	be
usually	RB	usually
more	RBR	more
desirable	JJ	desirable
in	IN	in
these	DT	this
noise	NN	noise
environments	NNS	environment
with	IN	with
slow	JJ	slow
convergence	NN	convergence
rate	NN	rate
.	.	.
</s>
<s type="decl">
Recently	RB	recently
,	,	,
the	DT	the
information	NN	information
theoretic	JJ	theoretic
learning	NN	learning
(	-LRB-	(
ITL	NNP	ITL
)	-RRB-	)
similarity	NN	similarity
measures	NNS	measure
,	,	,
such	JJ	such
as	IN	as
the	DT	the
maximum	JJ	maximum
correntropy	NN	correntropy
criterion	NN	criterion
(	-LRB-	(
MCC	NNP	MCC
)	-RRB-	)
and	CC	and
minimum	JJ	minimum
error	NN	error
entropy	NN	entropy
criterion	NN	criterion
(	-LRB-	(
MEE	NNP	MEE
)	-RRB-	)
,	,	,
have	VBP	have
been	VBN	be
introduced	VBN	introduce
to	TO	to
implement	VB	implement
robust	JJ	robust
KAFs	NNS	kaf
.	.	.
</s>
<s type="decl">
The	DT	the
ITL	NNP	ITL
similarity	NN	similarity
measures	NNS	measure
have	VBP	have
been	VBN	be
shown	VBN	show
to	TO	to
have	VB	have
a	DT	a
strong	JJ	strong
robustness	NN	robustness
against	IN	against
non-Gaussian	JJ	non-Gaussian
noises	NNS	noise
at	IN	at
the	DT	the
expense	NN	expense
of	IN	of
increasing	VBG	increase
computational	JJ	computational
burden	NN	burden
in	IN	in
training	NN	training
processing	NN	processing
.	.	.
</s>
<s type="decl">
In	IN	in
addition	NN	addition
,	,	,
minimizing	VBG	minimize
the	DT	the
logarithmic	JJ	logarithmic
moments	NNS	moment
of	IN	of
the	DT	the
error	NN	error
,	,	,
the	DT	the
logarithmic	JJ	logarithmic
error	NN	error
measure	NN	measure
—	:	—
including	VBG	include
the	DT	the
Cauchy	NNP	Cauchy
loss	NN	loss
(	-LRB-	(
CL	NNP	CL
)	-RRB-	)
with	IN	with
low	JJ	low
computational	JJ	computational
complexity	NN	complexity
—	:	—
is	VBZ	be
an	DT	a
appropriate	JJ	appropriate
measure	NN	measure
of	IN	of
optimality	NN	optimality
.	.	.
</s>
<s type="decl">
Using	VBG	use
the	DT	the
Cauchy	NNP	Cauchy
loss	NN	loss
to	TO	to
penalize	VB	penalize
the	DT	the
noise	NN	noise
term	NN	term
,	,	,
some	DT	some
algorithms	NNS	algorithm
based	VBN	base
on	IN	on
the	DT	the
minimum	JJ	minimum
Cauchy	NNP	Cauchy
loss	NN	loss
(	-LRB-	(
MCL	NNP	MCL
)	-RRB-	)
criterion	NN	criterion
are	VBP	be
efficient	JJ	efficient
for	IN	for
combating	VBG	combat
non-Gaussian	JJ	non-Gaussian
noises	NNS	noise
,	,	,
especially	RB	especially
for	IN	for
heavy-tailed	JJ	heavy-tailed
-	:	-
stable	JJ	stable
noises	NNS	noise
.	.	.
</s>
</p>
<p>
<s type="decl">
From	IN	from
the	DT	the
aspect	NN	aspect
of	IN	of
the	DT	the
optimization	NN	optimization
method	NN	method
,	,	,
the	DT	the
stochastic	JJ	stochastic
gradient	NN	gradient
descent	NN	descent
(	-LRB-	(
SGD)-based	VBN	sGD)-base
algorithms	NNS	algorithm
cannot	MD	cannot
find	VB	find
the	DT	the
minimum	NN	minimum
using	VBG	use
the	DT	the
negative	JJ	negative
gradient	NN	gradient
in	IN	in
some	DT	some
loss	NN	loss
functions	NNS	function
.	.	.
</s>
<s type="decl">
Toward	IN	toward
this	DT	this
end	NN	end
,	,	,
recursive-based	JJ	recursive-based
algorithms	NNS	algorithm
address	VBP	address
these	DT	this
issues	NNS	issue
at	IN	at
the	DT	the
cost	NN	cost
of	IN	of
increasing	VBG	increase
computational	JJ	computational
cost	NN	cost
.	.	.
</s>
<s type="decl">
In	IN	in
comparison	NN	comparison
with	IN	with
the	DT	the
SGD	NNP	SGD
method	NN	method
and	CC	and
recursive	JJ	recursive
method	NN	method
,	,	,
the	DT	the
conjugate	NN	conjugate
gradient	NN	gradient
(	-LRB-	(
CG	NNP	CG
)	-RRB-	)
method	NN	method
and	CC	and
Newton	NNP	Newton
’s	POS	's
method	NN	method
as	IN	as
developments	NNS	development
of	IN	of
SGD	NNP	SGD
have	VBP	have
become	VBN	become
alternative	JJ	alternative
optimization	NN	optimization
methods	NNS	method
in	IN	in
KAFs	NNS	kaf
.	.	.
</s>
<s type="decl">
The	DT	the
inverse	NN	inverse
of	IN	of
matrix	NN	matrix
of	IN	of
Newton	NNP	Newton
’s	POS	's
method	NN	method
increases	VBZ	increase
the	DT	the
computation	NN	computation
and	CC	and
causes	VBZ	cause
the	DT	the
divergence	NN	divergence
of	IN	of
algorithms	NNS	algorithm
in	IN	in
some	DT	some
cases	NNS	case
.	.	.
</s>
<s type="decl">
However	RB	however
,	,	,
the	DT	the
CG	NNP	CG
method	NN	method
gives	VBZ	give
a	DT	a
trade-off	NN	trade-off
between	IN	between
convergence	NN	convergence
rate	NN	rate
and	CC	and
computational	JJ	computational
complexity	NN	complexity
without	IN	without
the	DT	the
inverse	JJ	inverse
computation	NN	computation
,	,	,
and	CC	and
has	VBZ	have
been	VBN	be
successfully	RB	successfully
applied	VBN	apply
in	IN	in
various	JJ	various
fields	NNS	field
,	,	,
including	VBG	include
compressed	VBN	compress
sensing	NN	sensing
,	,	,
neural	JJ	neural
networks	NNS	network
,	,	,
and	CC	and
large-scale	JJ	large-scale
optimization	NN	optimization
.	.	.
</s>
<s type="decl">
In	IN	in
addition	NN	addition
,	,	,
the	DT	the
kernel	NN	kernel
conjugate	NN	conjugate
gradient	NN	gradient
(	-LRB-	(
KCG	NNP	KCG
)	-RRB-	)
method	NN	method
is	VBZ	be
proposed	VBN	propose
for	IN	for
adaptive	JJ	adaptive
filtering	NN	filtering
.	.	.
</s>
<s type="decl">
KCG	NNP	KCG
with	IN	with
low	JJ	low
computational	JJ	computational
and	CC	and
space	NN	space
requirements	NNS	requirement
can	MD	can
produce	VB	produce
a	DT	a
better	JJR	good
solution	NN	solution
than	IN	than
KLMS	NNP	KLMS
,	,	,
and	CC	and
has	VBZ	have
comparable	JJ	comparable
accuracy	NN	accuracy
to	IN	to
KRLS	NNP	KRLS
.	.	.
</s>
</p>
</text>