<?xml version='1.0' encoding='utf8'?>
<text id="AMALGUM_academic_vector" title="Detection and Classification of Advanced Persistent Threats and Attacks Using the Support Vector Machine" shortTitle="vector" author="Wen-Lin Chu, Chih-Jer Lin, Ke-Neng Chang" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2076-3417/9/21/4579/htm" speakerList="none" speakerCount="0">
<head>
<s type="frag">
2.	LS	2.
Methods	NNS	method
</s>
</head>
<head>
<s type="frag">
2.1.	CD	2.1.
Materials	NNS	material
and	CC	and
Experimental	JJ	experimental
Setup	NN	setup
</s>
</head>
<p>
<s type="decl">
The	DT	the
analysis	NN	analysis
of	IN	of
APT	NNP	APT
network	NN	network
attack	NN	attack
packets	NNS	packet
is	VBZ	be
not	RB	not
new	JJ	new
technology	NN	technology
,	,	,
but	CC	but
it	PRP	it
has	VBZ	have
become	VBN	become
an	DT	a
essential	JJ	essential
part	NN	part
of	IN	of
network	NN	network
administrators	NNS	administrator
and	CC	and
information	NN	information
security	NN	security
and	CC	and
is	VBZ	be
used	VBN	use
to	TO	to
analyze	VB	analyze
regular	JJ	regular
activities	NNS	activity
.	.	.
</s>
<s type="decl">
In	IN	in
the	DT	the
past	NN	past
,	,	,
it	PRP	it
usually	RB	usually
applied	VBD	apply
to	IN	to
the	DT	the
analysis	NN	analysis
of	IN	of
network	NN	network
behavior	NN	behavior
or	CC	or
debugging	NN	debugging
of	IN	of
the	DT	the
network	NN	network
environment	NN	environment
.	.	.
</s>
<s type="decl">
In	IN	in
the	DT	the
current	JJ	current
network	NN	network
milieu	NN	milieu
,	,	,
where	WRB	where
information	NN	information
security	NN	security
incidents	NNS	incident
are	VBP	be
frequent	JJ	frequent
,	,	,
this	DT	this
investigation	NN	investigation
has	VBZ	have
become	VBN	become
regular	JJ	regular
and	CC	and
essential	JJ	essential
.	.	.
</s>
<s type="sub">
Side	NN	side
recording	NN	recording
of	IN	of
network	NN	network
packets	NNS	packet
from	IN	from
a	DT	a
target	NN	target
host	NN	host
can	MD	can
provide	VB	provide
information	NN	information
about	IN	about
events	NNS	event
that	WDT	that
enables	VBZ	enable
even	RB	even
more	JJR	more
information	NN	information
to	TO	to
be	VB	be
obtained	VBN	obtain
through	IN	through
analysis	NN	analysis
.	.	.
</s>
<s type="decl">
Therefore	RB	therefore
,	,	,
while	IN	while
facing	VBG	face
current	JJ	current
popular	JJ	popular
APT	NN	APT
attacks	NNS	attack
hidden	VBN	hide
behind	IN	behind
communication	NN	communication
behavior	NN	behavior
,	,	,
and	CC	and
even	RB	even
in	IN	in
the	DT	the
communication	NN	communication
content	NN	content
,	,	,
it	PRP	it
is	VBZ	be
possible	JJ	possible
to	TO	to
obtain	VB	obtain
key	JJ	key
information	NN	information
by	IN	by
using	VBG	use
network	NN	network
packet	NN	packet
analysis	NN	analysis
technology	NN	technology
.	.	.
</s>
<s type="decl">
In	IN	in
this	DT	this
study	NN	study
,	,	,
a	DT	a
comparison	NN	comparison
has	VBZ	have
been	VBN	be
made	VBN	make
between	IN	between
the	DT	the
correct	JJ	correct
rate	NN	rate
of	IN	of
APT	NNP	APT
network	NN	network
attack	NN	attack
detection	NN	detection
using	VBG	use
the	DT	the
NSL-KDD	NNP	NSL-KDD
data	NNS	datum
sets	NNS	set
and	CC	and
PCA	NN	PCA
dimensionality	NN	dimensionality
reduction	NN	reduction
technology	NN	technology
and	CC	and
four	CD	4
machine	NN	machine
learning	NN	learning
classification	NN	classification
algorithms	NNS	algorithm
:	:	:
SVM	NNP	SVM
,	,	,
naive	JJ	naive
Bayes	NNPS	Bayes
,	,	,
decision	NN	decision
tree	NN	tree
,	,	,
and	CC	and
the	DT	the
multi-layer	JJ	multi-layer
perceptron	NN	perceptron
neural	JJ	neural
network	NN	network
(	-LRB-	(
MLP	NNP	MLP
)	-RRB-	)
.	.	.
</s>
<s type="decl">
Most	RBS	most
relevant	JJ	relevant
work	NN	work
has	VBZ	have
been	VBN	be
done	VBN	do
using	VBG	use
the	DT	the
“	``	''
WEKA	NNP	WEKA
Spreadsheet	NNP	Spreadsheet
to	IN	to
ARFF	NNP	ARFF
”	''	''
service	NN	service
to	TO	to
convert	VB	convert
the	DT	the
NSL-KDD	NNP	NSL-KDD
data	NNS	datum
set	NN	set
format	NN	format
from	IN	from
files	NNS	file
with	IN	with
the	DT	the
csv	NN	csv
extension	NN	extension
to	IN	to
ARFF	NNP	ARFF
extension	NN	extension
format	NN	format
(	-LRB-	(
including	VBG	include
“	``	''
training	NN	training
data	NNS	datum
set	NN	set
(	-LRB-	(
KDDTrain+	FW	KDDTrain+
)	-RRB-	)
”	''	''
and	CC	and
“	``	''
test	NN	test
data	NN	datum
set	NN	set
(	-LRB-	(
KDDTest+	FW	KDDTest+
)	-RRB-	)
”	''	''
(	-LRB-	(
<ref target="https://github.com/jmnwong/NSL-KDD-Dataset">
https://github.com/jmnwong/NSL-KDD-Dataset	NNP	https://github.com/jmnwong/NSL-KDD-Dataset
</ref>
)	-RRB-	)
is	VBZ	be
the	DT	the
reference	NN	reference
URL	NNP	URL
.	.	.
</s>
<s type="decl">
Because	IN	because
the	DT	the
data	NNS	datum
has	VBZ	have
different	JJ	different
ranges	NNS	range
,	,	,
preprocessing	NN	preprocessing
needed	VBD	need
to	TO	to
be	VB	be
done	VBN	do
to	TO	to
round	VB	round
up	RP	up
all	RB	all
the	DT	the
features	NNS	feature
.	.	.
</s>
<s type="decl">
Two	CD	2
type	NN	type
classifiers	NNS	classifyer
were	VBD	be
used	VBN	use
,	,	,
normal	JJ	normal
,	,	,
and	CC	and
anomaly	NN	anomaly
.	.	.
</s>
<s type="decl">
The	DT	the
PCA	NNP	PCA
algorithm	NN	algorithm
was	VBD	be
then	RB	then
used	VBN	use
to	TO	to
reduce	VB	reduce
the	DT	the
size	NN	size
of	IN	of
the	DT	the
classified	VBN	classify
data	NNS	datum
set	NN	set
.	.	.
</s>
<s type="decl">
Finally	RB	finally
,	,	,
the	DT	the
pre-processed	JJ	pre-processed
training	NN	training
and	CC	and
test	NN	test
data	NNS	datum
sets	NNS	set
were	VBD	be
grouped	VBN	group
and	CC	and
tested	VBN	test
,	,	,
and	CC	and
experiments	NNS	experiment
with	IN	with
the	DT	the
four	CD	4
classification	NN	classification
algorithms	NNS	algorithm
were	VBD	be
carried	VBN	carry
out	RP	out
.	.	.
</s>
<s type="decl">
These	DT	this
were	VBD	be
SVM	NNP	SVM
,	,	,
naive	JJ	naive
Bayes	NNPS	Bayes
,	,	,
decision	NN	decision
tree	NN	tree
,	,	,
and	CC	and
MLP	NNP	MLP
and	CC	and
they	PRP	they
were	VBD	be
used	VBN	use
to	TO	to
train	VB	train
and	CC	and
test	VB	test
the	DT	the
data	NNS	datum
and	CC	and
compare	VB	compare
and	CC	and
analyze	VB	analyze
the	DT	the
results	NNS	result
.	.	.
</s>
<s type="decl">
Each	DT	each
record	NN	record
had	VBD	have
data	NNS	datum
with	IN	with
41	CD	41
different	JJ	different
feature	NN	feature
attributes	NNS	attribute
presenting	VBG	present
the	DT	the
content	NN	content
of	IN	of
the	DT	the
network	NN	network
packets	NNS	packet
.	.	.
</s>
<s type="decl">
There	EX	there
were	VBD	be
four	CD	4
categories	NNS	category
of	IN	of
anomalous	JJ	anomalous
attack	NN	attack
</s>
<s type="decl">
DoS	NNP	DoS
,	,	,
Probe	NNP	Probe
,	,	,
R2L	NNP	R2L
,	,	,
and	CC	and
U2R	NNP	U2R
and	CC	and
the	DT	the
definitions	NNS	definition
are	VBP	be
shown	VBN	show
in	IN	in
Table	NNP	Table
1	CD	1
.	.	.
</s>
</p>
<head>
<s type="frag">
2.2.	CD	2.2.
</s>
<s type="frag">
Method	NN	method
of	IN	of
Signal	NN	signal
Dimension	NN	dimension
Reduction	NN	reduction
</s>
</head>
<p>
<s type="decl">
PCA	NNP	PCA
is	VBZ	be
a	DT	a
statistical	JJ	statistical
technique	NN	technique
that	WDT	that
transforms	VBZ	transform
a	DT	a
set	NN	set
of	IN	of
possible	JJ	possible
correlation	NN	correlation
variables	NNS	variable
to	IN	to
a	DT	a
set	NN	set
of	IN	of
linearly	RB	linearly
uncorrelated	JJ	uncorrelated
variables	NNS	variable
by	IN	by
orthogonal	JJ	orthogonal
transformation	NN	transformation
.	.	.
</s>
<s type="decl">
The	DT	the
transformed	VBN	transform
set	NN	set
of	IN	of
variables	NNS	variable
is	VBZ	be
the	DT	the
principal	JJ	principal
component	NN	component
.	.	.
</s>
<s type="decl">
A	DT	a
set	NN	set
of	IN	of
related	JJ	related
features	NNS	feature
in	IN	in
high-dimensional	JJ	high-dimensional
data	NNS	datum
is	VBZ	be
converted	VBN	convert
to	IN	to
a	DT	a
smaller	JJR	small
subset	NN	subset
and	CC	and
named	VBN	name
as	IN	as
principal	JJ	principal
component	NN	component
.	.	.
</s>
<s type="decl">
High-dimensional	JJ	High-dimensional
data	NNS	datum
can	MD	can
be	VB	be
transformed	VBN	transform
to	IN	to
low-order	JJ	low-order
dimension	NN	dimension
data	NNS	datum
(	-LRB-	(
)	-RRB-	)
.	.	.
</s>
<s type="decl">
PCA	NNP	PCA
does	VBZ	do
this	DT	this
transformation	NN	transformation
by	IN	by
finding	VBG	find
a	DT	a
feature	NN	feature
vector	NN	vector
,	,	,
and	CC	and
projecting	VBG	project
the	DT	the
dimension	NN	dimension
data	NNS	datum
onto	IN	onto
that	DT	that
feature	NN	feature
vector	NN	vector
to	TO	to
minimize	VB	minimize
the	DT	the
overall	JJ	overall
projection	NN	projection
error	NN	error
.	.	.
</s>
<s type="decl">
PCA	NNP	PCA
can	MD	can
preserve	VB	preserve
around	RB	around
0.9	CD	0.9
variance	NN	variance
of	IN	of
the	DT	the
original	JJ	original
data	NNS	datum
set	NN	set
and	CC	and
significantly	RB	significantly
reduce	VB	reduce
the	DT	the
number	NN	number
of	IN	of
features	NNS	feature
as	RB	as
well	RB	well
as	IN	as
the	DT	the
dimensions	NNS	dimension
.	.	.
</s>
<s type="decl">
The	DT	the
original	JJ	original
high-dimensional	JJ	high-dimensional
data	NN	datum
set	NN	set
is	VBZ	be
projected	VBN	project
onto	IN	onto
a	DT	a
smaller	JJR	small
subspace	NN	subspace
while	IN	while
preserving	VBG	preserve
most	JJS	most
of	IN	of
the	DT	the
information	NN	information
contained	VBN	contain
in	IN	in
the	DT	the
original	JJ	original
data	NN	datum
set	NN	set
.	.	.
</s>
<s type="decl">
Assuming	VBG	assume
,	,	,
and	CC	and
,	,	,
the	DT	the
random	JJ	random
dimension	NN	dimension
with	IN	with
the	DT	the
mean	NN	mean
(	-LRB-	(
)	-RRB-	)
inputs	VBZ	input
the	DT	the
data	NNS	datum
recording	VBG	record
its	PRP$	its
definition	NN	definition
as	IN	as
(	-LRB-	(
1	CD	1
)	-RRB-	)
(	-LRB-	(
1	CD	1
)	-RRB-	)
</s>
</p>
<p>
<s type="frag">
The	DT	the
definition	NN	definition
<hi rend="italic">
f	IN	f
</hi>
the	DT	the
covariance	NN	covariance
matrix	NN	matrix
of	IN	of
is	VBZ	be
(	-LRB-	(
2	CD	2
)	-RRB-	)
:	:	:
(	-LRB-	(
2	CD	2
)	-RRB-	)
</s>
</p>
<p>
<s type="decl">
PCA	NNP	PCA
solves	VBZ	solve
the	DT	the
eigenvalues	NNS	eigenvalue
problem	NN	problem
of	IN	of
Covariance	NN	covariance
matrix	NN	matrix
(	-LRB-	(
3	CD	3
)	-RRB-	)
</s>
</p>
<p>
<s type="decl">
In	IN	in
Equation	NNP	Equation
(	-LRB-	(
3	CD	3
)	-RRB-	)
,	,	,
is	VBZ	be
the	DT	the
eigenvalue	NN	eigenvalue
and	CC	and
is	VBZ	be
the	DT	the
corresponding	VBG	correspond
eigenvector	NN	eigenvector
.	.	.
</s>
</p>
<p>
<s type="decl">
To	TO	to
represent	VB	represent
the	DT	the
data	NNS	datum
record	NN	record
with	IN	with
a	DT	a
low-dimensional	JJ	low-dimensional
vector	NN	vector
,	,	,
only	RB	only
pieces	NNS	piece
of	IN	of
eigenvector	NN	eigenvector
(	-LRB-	(
named	VBN	name
as	IN	as
the	DT	the
principal	JJ	principal
direction	NN	direction
)	-RRB-	)
are	VBP	be
needed	VBN	need
,	,	,
corresponding	VBG	correspond
to	IN	to
pieces	NNS	piece
of	IN	of
the	DT	the
largest	JJS	large
eigenvalue	NN	eigenvalue
(	-LRB-	(
)	-RRB-	)
,	,	,
and	CC	and
the	DT	the
variance	NN	variance
of	IN	of
the	DT	the
projection	NN	projection
of	IN	of
the	DT	the
input	NN	input
data	NNS	datum
in	IN	in
the	DT	the
principal	JJ	principal
direction	NN	direction
is	VBZ	be
greater	JJR	great
than	IN	than
the	DT	the
variance	NN	variance
in	IN	in
any	DT	any
other	JJ	other
direction	NN	direction
.	.	.
</s>
<s type="decl">
Hence	RB	hence
parameter	NN	parameter
is	VBZ	be
the	DT	the
approximate	JJ	approximate
precision	NN	precision
of	IN	of
the	DT	the
pieces	NNS	piece
of	IN	of
the	DT	the
largest	JJS	large
eigenvector	NN	eigenvector
,	,	,
so	RB	so
the	DT	the
following	VBG	follow
relationship	NN	relationship
(	-LRB-	(
4	CD	4
)	-RRB-	)
is	VBZ	be
obtained	VBN	obtain
(	-LRB-	(
4	CD	4
)	-RRB-	)
</s>
</p>
<p>
<s type="decl">
The	DT	the
purpose	NN	purpose
of	IN	of
PCA	NNP	PCA
is	VBZ	be
to	TO	to
maximize	VB	maximize
internal	JJ	internal
information	NN	information
and	CC	and
increase	VB	increase
calculation	NN	calculation
speed	NN	speed
after	IN	after
dimension	NN	dimension
reduction	NN	reduction
,	,	,
and	CC	and
to	TO	to
evaluate	VB	evaluate
the	DT	the
importance	NN	importance
of	IN	of
the	DT	the
direction	NN	direction
by	IN	by
the	DT	the
size	NN	size
of	IN	of
the	DT	the
data	NNS	datum
variance	NN	variance
in	IN	in
the	DT	the
projection	NN	projection
direction	NN	direction
.	.	.
</s>
</p>
</text>