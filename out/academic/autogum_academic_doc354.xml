<text id="autogum_academic_doc354" title="Positive and Negative Evidence Accumulation Clustering for Sensor Fusion: An Application to Heartbeat Clustering" shortTile="positive-negative" author="David  G. Márquez, Paulo Félix, Constantino  A. García, Javier Tejedor, Ana  L.N. Fred, Abraham Otero" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/1424-8220/19/21/4635/htm" speakerList="none" speakerCount="0">
<head> 6. Conclusions</head>
<p>
A new algorithm, named PN-EAC, for data and sensor fusion has been presented in this work. It is a clustering algorithm based on the evidence accumulation paradigm capable of combining and integrating information from multiple sources, while avoiding an increase in the dimensionality of the feature vector. PN-EAC extracts positive evidence from the different data sources to select those elements that should be grouped together in the final partition. Moreover, it combines that positive evidence with negative evidence, which has information about which elements should not be grouped together. The core idea of this concept is that, from certain data sources, it is more reliable to extract information on what elements should not be grouped together (negative evidence) instead of information on what elements should be grouped together (positive evidence). The final partition is obtained from the evidence matrix, which gathers both the positive and negative evidence. </p>

<p>PN-EAC was applied to the problem of heartbeat clustering, to separate the heartbeats according to their type. In the MIT-BIH Arrhythmia Database, three different strategies were applied, obtaining an average error of 1.44% as the best result. This result was obtained by separating the morphological information of the different leads to generate different partitions and deriving negative evidence from the information obtained from the distance between heartbeats. This strategy yielded a statistically significant improvement in performance when compared with the extraction of positive evidence from the distance between heartbeats. These results show the value of the novel concept of negative evidence. </p>

<p>As a general rule, any feature in which dissimilarity could be useful to distinguish between some clusters, but similarity is not useful to group in the same cluster may be considered to extract negative evidence. As we have argued, and shown in the validation over the MIT-BIH Arrhythmia Database, this is the case for the features extracted from the distances between heartbeats for arrhythmia identification. However, we recognize that we cannot provide a precise criterion to determine whether using a feature for the extraction of negative evidence would improve or not the clustering results, beyond the intuitive rule given in this paragraph. Like other steps in the data analysis process, such as determining exactly when a feature has too much noise and it being better to discard it than to include it in the analysis, there is certain subjectivity when making this assessment, and a trial and error process may be necessary as part of the data analysis. </p>

<p>The algorithm was also applied to the St. Petersburg Institute of Cardiological Technics 12-Lead Arrhythmia Database. Using this database, different tests were executed using from 2 to 12 leads. We obtained an average error ranging from 0.601% to 0.338% for 2 and 12 leads, respectively, with a statistically significant decrease in the percentage of error when the number of leads increases. This is due to the fact that each lead provides different nuances of the electrical activity of the myocardium (that is why, in the clinical routine, the 12 leads are usually employed for cardiological diagnosis). This also suggests that, in other ECG databases, like the MIT-BIH Arrhythmia Database, in case the 12 leads were available, PN-EAC would improve its results just by using more leads. These results support the utility of the PN-EAC algorithm as a tool to merge data from multiple data sources. Furthermore, contrary to what usually happens with machine learning techniques, the PN-EAC algorithm does not seem to worsen its performance when there are large amounts of features available, but it improves it. Therefore, this algorithm could be a possible workaround to avoid the problem of the curse of dimensionality in certain problems. It is also important to note that the algorithm is highly parallelizable, since the information of the different data sources can be used independently to generate partitions and it is only necessary to combine them at the end to extract the final partition. </p>

<p>Our experience using other algorithms besides K-means to generate the partitions has been mixed. On one hand, when using algorithms with a greater expressive power, like a mixture of models, the performance typically increases slightly. However, the execution time increased considerably. On the other hand, using K-means with a larger number of partitions also increases the performance slightly, at the cost of a small increase in the execution time. While we cannot completely rule out that the use of algorithms with more expressive power than K-means could yield better results in some problems, our experience suggests that it is better to generate a greater number of partitions, even though each of these individual partitions is of lower quality, than generating a smaller number of higher quality partitions with a more sophisticated clustering algorithm. </p>

<p>In the future, we would like to apply this algorithm to other fields and different problems that can benefit from combining different sources of information or data fusion. In addition, a dynamic version of the algorithm will be developed to apply it to long or non-ended data streams. This would enable its application to, for example, ECG real-time monitoring.
 </p>
</text>
