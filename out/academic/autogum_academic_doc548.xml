<text id="autogum_academic_doc548" title="Exposure Bracketing Techniques for Camera Document Image Enhancement" shortTile="exposure-bracketing" author="Tao Liu, Hao Liu, Yingying Wu, Bo Yin, Zhiqiang Wei" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2076-3417/9/21/4529/htm" speakerList="none" speakerCount="0">
<head> 3. HDR Document Image Generation</head><head> 3.1. Document Image Registration</head>
<p>
The image registration problem has been intensively studied in remote sensing images, medical images, and camera images and very rarely, research can be found for document image registration. The most popular registration in the context of exposure bracketing is from, where a translational geometric model was employed to account for the geometric disparity between two images. However, we found that the translational model is not suitable for general camera document images. </p>

<p><figure>Figure 4</figure> shows two pseudo color image patches that are composed of two LDR images that are already illustrated in <figure>Figure 1</figure>. The green band and blue band come from the corresponding bands in the well-exposed image while the red band is from the corresponding band in the over-exposed image. If there are no geometric disparities between these two images, the foreground (textual part) of the image should overlap. In <figure>Figure 4</figure>, we can clearly see that geometric difference exists as the foreground texts do not overlap. On top of it, we can clearly see that the global translational model cannot account for the geometric disparity between these two images. For example, <figure>Figure 4</figure>a is the left central image patch and we can see that the geometric difference between the well-exposed image and over-exposed image in this region is around 10 pixels (half the size of the lowercase letter “a”) in the vertical direction. However, in <figure>Figure 4</figure>b, the right central image patch, we can see that the geometric difference between the well-exposed image and over-exposed image in this region is around 20 pixels (the size of the lowercase letter “a”) in the vertical direction. This is obvious evidence that the geometric disparity between LDR images cannot be translational and that it must follow a more complicated geometric model. </p>

<p>Among all the geometric models, such as the affine model, translational model, rotation model, and so on, we ended up selecting the planar homograph model to represent the geometric disparity between LDR images. Under this model, points in two different images can be mapped as:

(1)

where points are represented by homogeneous coordinates and so point (<hi rend="italic">x</hi>, <hi rend="italic">y</hi>, <hi rend="italic">z</hi>) is the same as (<hi rend="italic">x</hi>/<hi rend="italic">z</hi>, <hi rend="italic">y</hi>/<hi rend="italic">z</hi>) in the inhomogeneous coordinate. We selected this model because during the bracketing stage, hand-shake is inevitably introduced, leading to different imaging angles for the same document object, and the planar homograph model is suitable for the situation where the imaging object is put on a planar surface and is captured from different view-angles. </p>

<p>When the planar homograph model is selected, we have to estimate this model’s eight parameters. Basically, there are two methods. The first method is called the area-based method. Using this method to estimate the planar homograph model involves two steps: in the first step, a moving window is defined in the reference image and the image patch within the window is regarded as the template. We used the template to search for a corresponding image patch in the sensed image (an image that was registered). The centers of matched image templates are used as control points (CPs). There are many ways of finding a matching template, and one of the most popular criteria is cross correlation. When multiple CPs are generated, we then use these CPs to estimate the planar homograph model. Area-based methods, however, are not employed due to two reasons: (1) the first reason is that this method is computationally heavy as it performs cross correlation on multiple image patches and (2) the second reason is that image patches under different exposure levels may display extremely different characteristics, which may fail cross the correlation method. </p>

<p>The second method to estimate the planar homograph transformation is called the feature-based method. Two critical steps in feature-based methods are feature extraction and feature matching. We expect that the extracted features will be consistent regardless of exposure levels and among all the feature extraction methods, we selected the Scale-invariant Feature Transform (SIFT) method because it improves detection stability in situations of illumination changes. In the meantime, it achieves almost real-time performance and the features that are detected are highly distinctive. SIFT does not only define the position of detected points, but also provides a description of the region around the feature point by means of a descriptor, which is then used to match SIFT feature points. Therefore, we have used the SIFT method to find CP pairs. <figure>Figure 5</figure> shows the extracted matched SIFT features for two LDR images. </p>
</text>
