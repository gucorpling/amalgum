<text id="autogum_academic_doc130" title="Algorithmic Decision-Making in AVs: Understanding Ethical and Technical Concerns for Smart Cities" shortTile="algorithmic-decisionmaking" author="Hazel  Si Min Lim, Araz Taeihagh" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2071-1050/11/20/5791/htm" speakerList="none" speakerCount="0">
<head> 5. Ethical Concerns from Algorithmic Decision-Making in AVs</head>
<p>
This Section explores ethical issues associated with algorithmic decision-making in AVs, their implications for AV safety risks and discrimination and the steps taken to tackle these issues. Section 5.1 discusses the sources of bias in AVs’ algorithms that can yield discrimination by disproportionately allocating more safety risks to some groups of individuals. Next, Section 5.2 explores approaches to incorporate ethics into AV algorithms’ decision-making and highlight their implications for AV safety and discrimination. Lastly, Section 5.3 examines how the incentives of AV stakeholders shape AV algorithms’ design and resulting decisions that can introduce new safety risks and discrimination. </p>

<head> 5.1. Bias</head>
<p>
A system is considered biased when it contains “intended” or “unintended” characteristics that unfairly discriminate against certain individuals or groups of individuals in society. In American anti-discrimination law, discrimination exists when there is disparate treatment, which is the “discriminatory intent or the formal application of different rules to people of different groups”, and/or disparate impact, which is the result that “differ for different groups”. Bias can be introduced into AVs during the human designers’ construction of the datasets, models, and the parameters of the algorithm, which potentially leads to unfair or discriminatory allocations of safety risks. Firstly, statistical bias exists when the input data are not statistically representative of the overall population. For instance, training an AV using data from only one country could result in the AV learning localised patterns and not accurately modelling driving behaviours that apply in other countries or contexts. Thus, the under- or overrepresentation of certain groups in the data can lead to inaccurate classifications and biased outcomes. Secondly, the algorithm can be biased relative to legal and moral standards if it utilises sensitive input variables. Individual-specific characteristics, such as a person’s age and gender that are used as decision-making criteria can be penalised or privileged by the AVs’ algorithms to meet the algorithm’s pre-defined preferences, such as prioritising the safety of children or minimising the total quantity of harm, causing more safety risks to be allocated to individuals that share the penalised characteristics. These forms of bias can be introduced unintentionally or intentionally by algorithm designers and AV manufacturers to maximise profits, such as prioritising the safety of AV passengers to maximise profits, and this is exacerbated by the lack of legal frameworks to hold these stakeholders accountable. Section 5.2 explores various types of ethical preferences to which AVs may be programmed to follow and their implications of AV safety risks in greater detail, and Section 5.3 explores how perverse incentives influence the choice of preferences that are programmed into AVs’ algorithms. </p>

<p>Lessening bias in algorithms is therefore crucial to mitigate discriminatory outcomes from AVs. In autonomous systems in general, scholars have recommended ways to detect and offset the effects of bias, such as modifying algorithmic outputs to balance the effects of bias between protected and unprotected groups, introducing minimally intrusive modification to remove bias from the data, incorporating individuals from potentially discriminated groups, testing techniques to measure discrimination and identify groups of users significantly affected by bias in software and creating algorithms that certify the absence of data bias. Apart from bias originating from the data and selection of variables and criterion, Danks and London recommend clarifying ethical standards such as fairness to evaluate bias. Furthermore, scholars recommend increasing transparency to identify biases, such as designing algorithms whose original input variables can be traced throughout the system (i.e., traceability) and auditing algorithms to enhance their interpretability so that biases can be detected and the system’s outputs can be verified against safety requirements.  </p>

<p>However, there are challenges in identifying bias in algorithms and their discriminatory effects. Firstly, many algorithms are designed to be highly complex for greater accuracy, but this renders the algorithm opaque and difficult to interpret even by the designers themselves, concealing the sources of bias. Secondly, as ML algorithms make decisions mainly based on the training data that changes over time, it is difficult to predict potentially discriminatory effects in advance. Humans are also excessively trusting and insufficiently critical of algorithmic decisions due to the popular perception of algorithms as objective and fair, a problem referred to as “automation bias” and the seemingly “objective” correlations that the algorithm learns from the data makes it difficult to legally establish discriminatory intent in algorithms. An emerging issue is the aggregation of individually biased outcomes when AVs with similar preferences are deployed on a large-scale, as doing so would centralise and replicate algorithmic preferences along with their individually biased risk allocation decisions. This could lead to the same groups of people being consistently allocated more safety risks and perpetuate systemic discrimination, which is more difficult to detect as it results from the accumulation of similar driving outcomes.  </p>
</text>
