<text id="autogum_academic_doc217" title="A Model-Based System for Real-Time Articulated Hand Tracking Using a Simple Data Glove and a Depth Camera" shortTile="modelbased-system" author="Linjun Jiang, Hailun Xia, Caili Guo" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/1424-8220/19/21/4680/htm" speakerList="none" speakerCount="0">
<head> 2. Related Works</head>
<p>
In this section, we put our focus on the relevant introduction of the two mainstream camera-based works, i.e., appearance-based methods and model-based methods. We also briefly introduce some multi-model methods. Dipietro et al. have done elaborate research for all kinds of data gloves and relevant applications. We refer the readers to for a detailed review of glove-based works. </p>

<head> 2.1. Appearance-Based Methods</head>
<p>
Appearance-based methods train a classifier or a regressor to map image features to hand poses. Nearest neighbor search and decision trees are widely used in early works. In recent years, convolutional neural network (CNN)-based discriminative methods are state-of-the-art which estimate 3D joint positions directly from depth images. Besides, kinematics and geometric constraints are considered to avoid joint estimations violating kinematic constraints. Malik et al. embedded a novel hand pose and shape layer inside CNN to produce not only 3D joint positions but also hand mesh information. For a more comprehensive analysis and investigation of the state-of-the-art along-with future challenges, we refer the readers to. The biggest limitation of appearance-based methods is the training data. Existing benchmarks are not perfect enough to ensure well generalize to unseen hand shapes. We refer to for a detailed analysis of the drawbacks of existing data-sets. Considering this limitation, our system follows the model-based approaches that do not rely on massive data-sets. </p>

<head> 2.2. Model-Based Methods</head>
<p>
Despite the considerable advance in learning-based hand tracking, systems that employ generative models of explicit hand kinematics and surface geometry and fit these models to depth data using local optimization have produced the most compelling results. The most common problems for model-based methods are a good enough initialization point, an expressive enough hand model and a discriminative object function that minimizes the error between the 3D hand model and the observed data. </p>

<head> 2.2.1. Initialization</head>
<p>
A good enough Initialization has been proven critical to the robustness, which enables faster converge and better resistant to local optima. There exist many initialization methods. Some works were initialized by the fingertip detection. Besides, Tagliasacchi et al. and Tkach et al. also detected a color wristband as a first alignment. The use of simple geometric heuristics for initialization can sometimes be impractical for those gestures which contain occlusions or difficult hand orientations. For this reason, most of the previous studies concentrated on exploiting the given image data with the train-based methods. Taylor et al. generated candidateâ€™s hand poses quickly by a retrieval forest. Taylor et al. trained a decision forest classifier on a synthetic training set to generate an initial pose estimate. Sanchez-Riera et al. trained a convolutional neural network for initialization with 243,000 tuples of images. Sharp et al. inferred a hierarchical distribution over hand pose with a layered discriminative model. However, initialization errors often occur due to imperfect training data-sets, mentioned in Section 2.1, which may cause tracking failure. In our system, it is more reliable and robust to provide an approximate initialization by a simple data glove. </p>

<head> 2.2.2. Hand Model</head>
<p>
The human hand model serves as the medium of computation and the presentation of algorithm results. A detailed and accurate generative model tends to deepen the good local minima and widen their basins of convergence. Many hand models have been proposed, see <figure>Figure 1</figure>. </p>

<p>Early works used the capsule mode made by two basic geometric primitives: a sphere and a cylinder. Qian et al. built the hand model using a number of spheres. Melax et al. used a union of convex bodies for hand tracking. Sridhar et al. modeled the volumetric extent of the hand as a 3D sum of an-isotropic Gaussian model. These approaches can model a broad spectrum of hand shape variations and enable fast evaluation of distances and a high degree of computational parallelism. However, they only roughly approximate hand shape even if Tkach et al. proposed the use of sphere-meshes as a novel geometric representation. An alternative is a triangulated mesh model with linear blend skinning (LBS) that is more realistic and fits image data better. But these triangulated meshes cost more computational effort and are hard to deal with the collision. There also exist some implicit templates except these explicit models. Schmidt et al. voxelized each shape-primitive and computed a signed distance function for the local coordinate frame. Taylor et al. constructed the hand as an articulated signed distance function that allows fast calculation of the distance to the hand surface. To explain the input data better and explicitly visualize the tracking result, our system uses an expressive triangular mesh hand model. </p>
</text>
