<text id="autogum_academic_doc248" title="Computational Analysis of Deep Visual Data for Quantifying Facial Expression Production" shortTile="computational-analysis" author="Marco Leo, Pierluigi CarcagnÃ¬, Cosimo Distante, Pier  Luigi Mazzeo, Paolo Spagnolo, Annalisa Levante, Serena Petrocchi, Flavia Lecciso" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2076-3417/9/21/4542/htm" speakerList="none" speakerCount="0">
<head> 5. Experimental Results</head>
<p>
This section reports experimental outcomes gathered by processing acquired videos by the algorithmic pipeline described in Section 3. In particular, a modeling window  s and an observation window  s were used. The observation window depends on the experimental setting. The interval between two consecutive requests has been set to 4 s by the clinicians. This means that the caregiver has to wait 4 s before moving to the following request for facial expression. The modeling window was consequently set to half of the observation window since lower values were experimentally proved to be not sufficient to model the neutral expression whereas higher values could include the offset of the previous facial expression. The experimental proofs were carried out in different phases. In the first phase, videos related to the TD children were processed and quantitative comparison with the annotations provided by professionals was then performed. In the second phase, the videos related to the ASD children were processed and outputs were subsequently compared with human annotations. As a final experimental phase, outcomes extracted on TD and ASD groups were put together to draw some conclusions from the different distribution of related numerical values. </p>

<head> 5.1. Assessment on TD Children</head>
<p>
In the first experimental phase, production scores on the group of TD children were computed and their graphic representations are reported in <figure>Figure 2</figure>, <figure>Figure 3</figure>, <figure>Figure 4</figure> and <figure>Figure 5</figure>. Please be aware that the highest scores were kept at a value of 1500 in order to increase graph readability. </p>

<p>It is worth to point out that scores come from negative logarithmic functions of likelihood values (see Equation (4)). When the likelihood values become very close to zero (in the case of a modification of action unit during a proper facial expression production) related logarithmic functions tend to very high values. The outcomes greater than 1500 are equivalent to probability values so small that can be considered as 0 (and their logarithms kept as a large constant) for the considered application purposes. In addition, the figures have a different scale on the axes since the gathered scores have a more uniform distribution when related to the upper face part than when related to the lower face part. This is not surprising since the use of the upper face part in emotion production is more difficult and then this can lead to man different levels of ability. For the lower face part, when children start reacting to the request usually their production level goes in saturation to the maximum allowed score. As a consequence, the <hi rend="italic">x</hi>-axis has a larger scale to point out that. </p>

<p>In figures, black circles refer to cases in which the team of psychologists labeled facial expressions as compliant with the supplied request (i.e., expressions correctly performed by the child), whereas red circles refer to cases in which the professionals labeled the related facial expressions as not compliant with the supplied requests (i.e., expressions not performed by the child). At first glance, it is quite clear that highest scores were properly associated with occurrences that professionals annotated as expression performed whereas lowest scores were associated with occurrences that professionals annotated as expressions not performed. Going into details, it is of interest to observe that, in correspondence of some requests of the happy face that psychologists annotated as performed, the automatic system gave low outcomes (either for lower or upper face part). This is the case, for example, of the two black spots that are close to the origin of the reference system in <figure>Figure 2</figure>. </p>

<p>This evident misalignment between manual annotations and automatic scores depended on a wrong positioning of facial landmarks due to occlusions of the mouth (and deformation of cheeks and consequently of eye regions) caused by the hands of the child touching his face. Concerning sad expression there were, once again, some misalignment occurred in case of mouth occlusion (resulting in low scores for lower face part in <figure>Figure 3</figure>) but, in addition, there were also some occurrences (manually annotated as performed) that experienced low scores only for lower face part (with very high scores for upper face part instead). These happened since, in correspondence to the requests of sad expression, some children occluded the mouth but without affecting the upper face part. </p>

<p>Similar conclusions can be drawn for some spots corresponding to requests of fear expressions (<figure>Figure 4</figure>) whereas this problem was never encountered during requests of anger expression (<figure>Figure 5</figure>). </p>
</text>
