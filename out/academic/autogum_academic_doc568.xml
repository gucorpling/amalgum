<text id="autogum_academic_doc568" title="The Cauchy Conjugate Gradient Algorithm with Random Fourier Features" shortTile="cauchy-conjugate" author="Xuewei Huang, Shiyuan Wang, Kui Xiong" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2073-8994/11/10/1323/htm" speakerList="none" speakerCount="0">
<head> 1. Introduction</head>
<p>
Many applications in the real world, such as system identification, regression, and online kernel learning (OKL), require complex nonlinear models. The kernel method using a Mercer kernel has attracted interests in tackling these complex nonlinear applications, which transforms nonlinear applications into linear ones in the reproducing kernel Hilbert space (RKHS). Developed in RKHS, a kernel adaptive filter (KAF) is the most celebrated subfield of OKL algorithms. Using the simplest stochastic gradient descent (SGD) method for learning, KAFs including the kernel least mean square (KLMS) algorithm, kernel affine projection algorithm (KAPA), and kernel recursive least squares (KRLS) algorithm have been proposed. </p>

<p>However, allocating a new kernel unit as a radial basis function (RBF) center with the coming of new data, the linearly growing structure (called “dictionary” hereafter) will increase the computational and memory requirements in KAFs. To curb the growth of the dictionary, two categories are chosen for sparsification. The first category accepts only informative data as new dictionary centers by using a threshold, including the surprise criterion (SC), the coherence criterion (CC), and the vector quantization (VQ). However, these methods cannot fully address the growing problem and still introduce additional time consumption at each iteration. The fixed points methods as the second category, including the fixed-budget (FB), the sliding window (SW), and the kernel approximation methods (e.g., the Nystrm method and random Fourier features (RFFs) method), are used to overcome the sublinearly growing problem. However, the FB method and the SW method cannot guarantee a good performance in specific environments with a small amount of time. Compared with the Nystrm method, RFFs are drawn from a distribution that is randomly independent from the training data. Due to a data-independent vector representation, RFFs can provide a good solution to non-stationary circumstances. On the basis of RFFs, random Fourier mapping (RFM) is proposed by mapping input data into a finite-dimensional random Fourier features space (RFFS) using a randomized feature kernel’s Fourier transform in a fixed network structure. The RFM alleviates the computational and storage burdens of KAFs, and ensures a satisfactory performance under non-stationary conditions. The examples for developing KAFs with RFM are the random Fourier features kernel least mean square (RFFKLMS) algorithm, random Fourier features maximum correntropy (RFFMC) algorithm, and random Fourier features conjugate gradient (RFFCG) algorithm. </p>

<p>For the loss function, due to their simplicity, smoothness, and mathematical tractability, the second-order statistical measures (e.g., minimum mean square error (MMSE) and least squares) are widely utilized in KAFs. However, KAFs based on the second-order statistical measures are sensitive to non-Gaussian noises including the sub-Gaussian and super-Gaussian noises, which means that their performance may be seriously degraded if the training data are contaminated by outliers. To handle this issue, robust statistical measures have therefore gained more attention, among which the lower-order error measure and the higher-lower error measure are two typical examples. However, the higher-order error measure is not suitable for the mixture of Gaussian and super-Gaussian noises (Laplace, -stable, etc.) with poor stability and astringency, and the lower-order measure of error is usually more desirable in these noise environments with slow convergence rate. Recently, the information theoretic learning (ITL) similarity measures, such as the maximum correntropy criterion (MCC) and minimum error entropy criterion (MEE), have been introduced to implement robust KAFs. The ITL similarity measures have been shown to have a strong robustness against non-Gaussian noises at the expense of increasing computational burden in training processing. In addition, minimizing the logarithmic moments of the error, the logarithmic error measure—including the Cauchy loss (CL) with low computational complexity—is an appropriate measure of optimality. Using the Cauchy loss to penalize the noise term, some algorithms based on the minimum Cauchy loss (MCL) criterion are efficient for combating non-Gaussian noises, especially for heavy-tailed - stable noises. </p>

<p>From the aspect of the optimization method, the stochastic gradient descent (SGD)-based algorithms cannot find the minimum using the negative gradient in some loss functions. Toward this end, recursive-based algorithms address these issues at the cost of increasing computational cost. In comparison with the SGD method and recursive method, the conjugate gradient (CG) method and Newton’s method as developments of SGD have become alternative optimization methods in KAFs. The inverse of matrix of Newton’s method increases the computation and causes the divergence of algorithms in some cases. However, the CG method gives a trade-off between convergence rate and computational complexity without the inverse computation, and has been successfully applied in various fields, including compressed sensing, neural networks, and large-scale optimization. In addition, the kernel conjugate gradient (KCG) method is proposed for adaptive filtering. KCG with low computational and space requirements can produce a better solution than KLMS, and has comparable accuracy to KRLS. </p>
</text>
