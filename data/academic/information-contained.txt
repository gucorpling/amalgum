 1. IntroductionThe concept of entropy was introduced by Clausius [1] in the nineteenth century into the evolving science of thermodynamics. In this context a change in entropy
S
 was defined as the amount of heat,

δ

Q

r
e
v



, reversibly exchanged between two macroscopic bodies maintained at an absolute temperature
T
:



Δ
S
=


δ

Q

r
e
v



T





(1)

Later, this macroscopically measurable quantity was explained in the context of statistical mechanics as molecular disorder contained in a macroscopic body. According to Boltzmann [2], entropy is related to the number
W
 of microscopic states of motion which are consistent with the limited macroscopic knowledge about the molecular system:




S
=

k
B

ln

(
W
)



,
 

k
B

=
8.62
∗


10

5



e
V

K






(2)

One of the most striking facts about entropy is its tendency to increase in an irreversible manner within an isolated system. This tendency was interpreted early on as arising from the statistics of motion of large numbers of particles rather than from any irreversibility in the underlying laws of motion of the individual molecules themselves. Considering the intrinsic reversibility of the basic mechanical laws it was argued by Maxwell [3] that a being “whose faculties are so sharpened that it can follow the course of individual molecules” should be able to operate a trap door, inserted into a vessel homogeneously filled with gas, to produce either temperature or pressure differences inside the gas that would allow mechanical work to be extracted from a single macroscopic heat reservoir, maintained at a finite temperature
T
. Such a gain in energy, clearly, would be in sharp contrast with the postulate imposed by the second law of thermodynamics and thus create a paradox. Since its invention in 1871, Maxwell’s demon paradox has fascinated researchers up to the present day. A good coverage of the historical developments can be found in the books of Rex and Leff [4,5].A decisive step forward in the analysis of Maxwell’s demons´ paradox was made by Leo Szilard in 1929 [6]. Considering the motion of a single atom, enclosed in a finite volume
V
 and in contact with a heat reservoir of temperature
T
, Szilard showed that knowledge about the location of the atom can be used to extract mechanical work from a single-molecule gas. This example showed that work extraction from a thermal reservoir is not for free but that it has to be paid for in terms of a new “currency”, called information. The other important achievement was that Szilard’s analysis established a connection between the physical concepts of entropy and abstract information, showing that 1 bit of abstract information corresponds to


k
B

ln

(
2
)


 units of entropy. This definition of a bit as the unit of information is generally regarded as the starting point of modern information theory [7,8]. A further important step in the analysis of Maxwell’s demon paradox was the realization that the information gained in a physical measurement is burdened with an energy cost that overwhelms the energy gained in operating a Maxwell’s demon device. Brillouin’s initial analysis [9], which claimed that this energy cost is associated with the measurement process itself was later challenged by arguments that reversible measurements might in principle be conceivable [4,5]. Even later it was argued that operating a Maxwell’s demon device entails storage of the information intermittently obtained, and that the energy cost of information ultimately needs to be paid when this information is erased again. It was argued that this latter step of erasure is unavoidable to arrive at a fully cyclic process of information gain and work extraction and that therefore the energy cost of information erasure is a more inescapable necessity than the energy cost of measurement and information gain. This latter resolution of Maxwell’s demons’ paradox became known as Landauer’s principle [10]. Brillouin and Landauer both agree in that ultimately an energy of at least

W
=

k
B

T

ln


(
2
)


 needs to be expended to gain one bit of information and to ultimately conserve the validity of the second law.Following discussions of Maxwell’s demon’s paradox, it can be observed that some authors express discomfort with the idea of thermodynamic information as the processes of gaining, receiving and maintaining information through measurement either explicitly or implicitly assume the involvement of intelligent beings [3,4,5,6]. It is argued that such an involvement introduces an element of subjectivity into the definition of entropy, which is unacceptable if entropy is to be considered as an element of physical reality that exists independent of the existence of human beings and of human observation [11,12]. In the present paper we consider the entropy of a molecular gas and interpret it as information carried by its molecular constituents. In §2 we consider the Sackur-Tetrode equation for the entropy of an ideal gas [13] consisting of elastic hard spheres of very small size, but without any internal structure. Transforming this equation to information units, the fact of ignorance about the molecular motion on the side of an outside observer is turned into a more positive statement about information that is carried with the mechanical motion of the gas molecules itself, but which remains unknown to the outside observer. With the help of the Sackur-Tetrode equation we arrive at the result that the individual molecules inside an ideal gas carry two kinds of information, the first one being related to the kinetic energy of molecules carried with their center-of-mass motion and the second to information related to the positions of gas molecules within their own mean-free volumes. With the Sackur-Tetrode equation being a result of equilibrium statistical mechanics, this equation provides a static and time-averaged picture of an ideal gas in which all molecules are treated alike, always moving with the same kinetic energy and always occupying the same effective space, independent of time. In §3 we therefore go beyond the limits of the Sackur-Tetrode equation and turn to a discussion of gas-kinetic collisions. There, it is argued that with gas-kinetic collisions taking place, the individual pieces of molecular information must depend on time with energy and initial localization changing discontinuously during gas-kinetic collisions and molecular localization changing continuously during times in between successive collisions. This latter effect is the most interesting one as it shows that the information about the particle location is initially determined by the energy transfers that have taken place during a first collision and that this kind of information deteriorates and finally gets lost as the molecules travel through their respective mean-free paths and as these become likely to undergo follow-on collisions. In §4, it is then argued that these repeated gains and losses in molecular information resemble measurement processes in which information is repeatedly gained, deteriorated, erased and finally updated again as gas-kinetic collisions occur. Considering two thought-experiments on a single-molecule gas, it is shown that the proposed continuous variation of molecular information is in principle experimentally accessible when the experiment is performed in a supervised manner but inaccessible otherwise. Realizing that in both experiments the basic processes of information gain and information erasure follow the same kind of physics, it is argued that both processes do not require the intervention of intelligent beings and that therefore entropy and thermodynamic information alike satisfy the requirement that physical entities should exist independent of the existence and observation of intelligent beings. In §5, we extend our considerations to molecules with a more complicated internal structure which allows them to store energy in the form of molecular rotations, vibrations and electronic excitations and to exchange such pieces of energy during gas-kinetic collisions. In §6, we summarize our results and provide an outlook towards possible future research.  2. Entropy of an Ideal GasIdeal gases are mental constructs which replace the complex molecules in real gases by simple elastic hard spheres of very small size, which collide from time to time and which thereby exchange energy and momentum. As is well known, this strongly simplified picture approximates the behavior of real gases in the limit of high temperature and low pressure. Such a situation is visualized in Figure 1 which assumes that
N
 molecules are contained in a box of volume

V
=

L
3


 which in turn is embedded in a heat reservoir of temperature
T
. As the molecules move inside this box they trace out, on average, a volume


V

m
o
l


=

(

V
/
N

)


 before suffering a gas-kinetic collision.The entropy of such a gas is experimentally accessible by measuring the amount of heat that needs to be fed into the volume
V
 as the gas is heated up from very low temperatures up to the reservoir temperature
T
:




S

g
a
s



(

T
,
V
,
N

)

=
V


∫

0
T




c
V


(
θ
)


θ

d
θ




(3)

In this equation


c
V


(
T
)


 is the heat capacity at constant volume for a gas containing


n

g
a
s


=

N
V


 molecules per unit volume. Once the reservoir temperature has been stabilized, the quantity


S

g
a
s



(

T
,
V
,
N

)


 has been fixed and from then on represents a quantitative measure of ignorance that exists on the side of the outside observer with regard to the details of the molecular motion inside the volume
V
. With the help of equilibrium statistical mechanics, the entropy


S

g
a
s



(

T
,
V
,
N

)


 can be related to those three parameters

N
,
V

 and
T
 that are known and under control of the outside observer:




S

g
a
s



(

T
,
V
,
N

)

=
N

{


5
2


k
B

+

k
B

l
n

[




V

m
o
l





V
Q


(

T
,
M

)




]


}





(4)

This result, which has originally been derived by Sakur and Tetrode [13] involves as a key parameter the quantum volume:




V
Q


(

T
,
M

)

=



(



2
π

ℏ
2



M

k
B

T



)




3
2







(5)


which up to a factor of the order of unity is the third power of the thermal de-Broglie wavelength of the moving particles:




λ

t
h



(

T
,
M

)

=

ℏ



3
M

k
B

T








(6)

Here,


p

t
h


=





3
M

k
B

T



 stands for the momentum of a molecule of mass
M
 moving with a kinetic energy of


E

t
h


=

3
2


k
B

T

 and
ℏ
 for the reduced Planck’s constant.The statement of ignorance, expressed by the function


S

g
a
s



(

T
,
V
,
N

)


, can be turned into more positive fashion by converting


S

g
a
s



(

T
,
V
,
N

)


 to information units:




I

g
a
s



(

T
,
V
,
N

)

=

1


k
B

l
n

(
2
)




S

g
a
s



(

T
,
V
,
N

)

.




(7)

In this latter form, the function


I

g
a
s



(

T
,
V
,
N

)


 represents an amount of information that is contained in the molecular motion of the gas and that remains unknown to the outside observer whose knowledge is confined to the three state parameters

T
,
V
,
N

. Further, considering that the entropy


S

g
a
s



(

T
,
V
,
N

)


 can be expressed in the form


S

g
a
s



(

T
,
V
,
N

)

=
N

s

m
o
l



(

T
,
V
,
N

)


, the molar entropies,


s

m
o
l



(

T
,
V
,
N

)


, can be interpreted as information that, on average, is missing to an outside observer with regard to each molecule inside the gas:




i

m
o
l



(

T
,
V
,
N

)

=

1

ln

(
2
)




{


5
2

+
l
n

[




V

m
o
l





V
Q


(

T
,
M

)




]


}





(8)

The latter equation shows that the molecular information principally consists of two parts, which can be interpreted as information,


i

c
c
m



(

T
,
V
,
N

)


, that is missing with regard to the center-of-mass motion of each molecule and


i

l
o
c



(

T
,
V
,
N

)


 as information that is missing with regard to its location within its own mean-free volume:





i

m
o
l



(

T
,
V
,
N

)

=

i

c
c
m



(

T
,
V
,
N

)

+



i

l
o
c



(

T
,
V
,
N

)

.





(9)

These latter assignments can be rationalized as follows: turning to


i

c
m
m



 first, we remember that in an ideal gas each particle, on average, carries an energy


E

a
v


=

1
2


k
B

T

 per degree of freedom
f
,


i

c
m
m



 therefore can be expressed as:




i

c
m
m



(

T
,
V
,
N

)

=

1

l
n

(
2
)





f

E

a
v





k
B

T






(10)

The number of

f
=
5

 degrees of freedom can be rationalized by considering that each particle enjoys 3 degrees of freedom regarding motion along the three orthogonal directions in space and two further degrees of freedom as each particle works against the constant-pressure background generated by the

N
−
1

 companion molecules inside the gas [13]. The interpretation of the second term,


i

l
o
c



, is straight-forward if one considers that


V
Q


(

T
,
M

)


 is the effective size of a molecule moving with its mean thermal energy through the gas. Localization of the molecule within its mean-free volume reduces the number
n
 of possibilities of finding it somewhere inside this volume from


n

i
n
i
t
i
a
l


=



V

m
o
l





V
Q


(

T
,
M

)




 to


n

f
i
n
a
l


=
1

. The associated gain in information is then:




i

l
o
c



(

T
,
V
,
N

)

=
l
o

g
2


[




V

m
o
l





V
Q


(

T
,
M

)




]

=

1

l
n

(
2
)



l
n

[




V

m
o
l





V
Q


(

T
,
M

)




]





(11)

 3. Gas-kinetic InteractionsBeing a result of equilibrium statistical mechanics, the Sackur-Tetrode equation (Equations (4) and (5)) provides a static and time-averaged picture of an ideal gas in which each molecule is treated alike. In particular, it suggests that each molecule is moving with the same kinetic energy and that each molecule occupies the same volume


V
Q


(

T
,
M

)


, independent of time. This static picture sharply contrasts with the situation in real gases: in a real gas under normal temperature-pressure (NTP) conditions, the average distance between molecules is in the order of


d

a
v


≈
5
×

10

−
5



 cm. Depending on the molecular mass, the molecules inside such a gas travel these distances with speeds around


v

t
h


≈
5
×

10
4


 cm/s and thus, on average, collide with each other after times


τ

c
o
l
l


≈


10


−
9



 s. In these interactions energy and momentum is exchanged between the collision partners and a Maxwellian velocity distribution is established. Allowing for gas-kinetic interactions, kinetic energy is constantly re-shuffled among gas molecules with an average rate of


ν

c
o
l
l


=
1
/

τ

c
o
l
l



 and collision partners become localized as energy is transferred between them. In such a situation the pieces of information relating to the different gas molecules become molecule-specific and time-dependent:





i

m
o
l



i
,
j



(
t
)

=


i

c
m
m



i
,
j


+


i

l
o
c



i
,
j



(
t
)





(12)

In this equation the index
i
 enumerates the individual molecules (

i
=
1
…
N

) and
j
 the particular collisions;
t
, finally, is the time between successive collisions
j
 and

j
+
1

. As before, the



i

c
m
m



i
,
j



 and



i

l
o
c



i
,
j



 take on the forms:





i

c
m
m



i
,
j


=

1

ln

(
2
)







E

k
i
n



i
,
j





k
B

T






(13)







i

l
o
c



i
,
j


=

1

l
n

(
2
)



l
n

[




V

m
o
l






V
Q


i
,
j





]

.




(14)

In this latter equation, the



V
Q


i
,
j



 are the quantum volumes of molecules moving with energies



E

k
i
n



i
,
j



. As de-Broglie wavelengths depend on particle momenta and as shown in more detail in the Appendix A, the



V
Q


i
,
j



 exhibit an energy dependence according to:





V
Q


i
,
j


=



(



2

π
2


ℏ
2



M


E

k
i
n



i
,
j





)



3
/
2






(15)

A particularly interesting aspect is that the



i

l
o
c



i
,
j



 also exhibit a continuous dependence on time during those phases in which the individual molecules move freely in between successive collisions:





i

l
o
c



i
,
j


=


i

l
o
c



i
,
j



(
t
)





(16)

This latter effect arises from the phenomenon of quantum-mechanical dispersion which causes the wave functions of the molecules to spread out in space as these are moving freely through empty space in between successive collisions. As shown in the Appendix A, the 3d molecular wave packets initially become localized up to the constraints imposed by the uncertainty relationships and then rapidly spread out to volumes



V
Q


i
,
j



 comparable to the average mean-free volume


V

m
o
l



 in the gas at times at which the molecules become likely to undergo follow-on collisions. As a consequence, all



i

l
o
c



i
,
j



, independent of kinetic energy, tend to zero immediately before follow-on collisions are likely to occur. This latter effect is illustrated in Figure 2 for functions



i

l
o
c



i
,
j



(
t
)


, appropriate to different values of



E

k
i
n



i
,
j



 gained in preceding collisions. In order to illustrate this energy-independence, all curves in Figure 2 have been plotted as functions of their respective reduced times





τ

r
e
d



(


E

k
i
n



)

=
t
/

τ

c
o
l
l
 



(


E

k
i
n



)





(17)


with


τ

c
o
l
l



(


E

k
i
n



)


 standing for the mean-free time of molecules moving with kinetic energy


E

k
i
n



 in between successive collisions:




τ

c
o
l
l



(


E

k
i
n



)

=

d

a
v





M

2

E

k
i
n










(18)

With this scaling applied, the results in Figure 2 show that, during gas-kinetic collisions, the individual molecules repeatedly gain and loose information with regard to their localization as these are moving on in between successive collisions.  4. Supervised and Unsupervised Measurement ProcessesInformation about physical phenomena is conventionally gained in measurements designed and performed by human beings. With this background in mind, the changes in


i

l
o
c



(


E

k
i
n


,
t

)


, displayed in Figure 2, can be regarded as measurement processes in which information is intermittently gained, deteriorated, lost and finally updated again as gas-kinetic collisions occur. At this point, it is relevant to note that we defined the functions


i

m
o
l



(

T
,
V
,
N

)


 (Equations (8) and (9)) and their time-dependent extrapolations (Equations (12)–(16)) as information carried by the individual molecules, which otherwise remains unknown to outside observers. Remembering this fact of unobservability, it is clear that these naturally occurring measurements proceed in an un-supervised manner, in sharp contrast to conventional ones. In this section, we consider two thought experiments, performed on single-molecule gases, which are carried out in a supervised and in an un-supervised mode. These considerations show that the pieces of information



i

m
o
l



i
,
j



(
t
)


 are principally accessible in the supervised mode but remain undetectable otherwise. In particular, we show that by performing many measurements in the supervised mode the Maxwellian velocity distribution can be re-established while in the unsupervised mode only learned guesses concerning the molecular motion are possible and that these lead back to the time-averaged Sackur-Tetrode result


i

m
o
l



(

T
,
V
,
N

)


 (Equation (8)).Both experiments are sketched in Figure 3. In both experiments the outside observer knows from their preparation that a single molecule of mass
M
 has been filled into a volume

V
=

L
3


 and that this volume is then maintained at a temperature
T
. Both experiments differ in the way that two of the walls in the supervised version (Figure 3a) consist of ultra-sensitive microphones which are able to detect wall-molecule and molecule-wall interactions, while in the unsupervised version (Figure 3b) all walls are simply macroscopic heat reservoirs without any sensory capabilities. Concerning the internal molecular motion, we assume that in both gases the molecule is initially adsorbed on the left-hand wall and that it takes off from the wall with a momentum


p

l
r


=


2
M

E

k
i
n
_
l
r





 with


E

k
i
n
_
l
r


≫

E

t
h


=

3
2


k
B

T

, thus enabling it to move horizontally towards the right-hand wall, which will be reached in a time


τ

l
r


=
L



M

2

E

k
i
n
_
l
r







. Both upon leaving the left-hand wall and arriving at the right-hand wall, signals will be generated which allows the outside observer to determine the transit time


τ

l
r



 and to calculate the kinetic energy and the linear momentum of the molecule. Further knowing that the molecule is adequately described by a 3d quantum-mechanical wave packet, its initial volume upon take-off and its dispersion during its travel to the right-hand wall can be ascertained. Similar arguments apply to the backward travel shown in the lower section of Figure 3a where the molecule might have started its journey back with a different momentum


p

r
l


=


2
M

E

k
i
n
_
r
l





. In brief, the dynamics displayed in Figure 3a can be reconstructed with a simple knowledge of basic mechanics and quantum mechanics. In case this experiment is repeated, the Maxwellian velocity distribution can be re-established. With the same kind of molecular motion, the situation in the un-supervised experiment (Figure 3b is a very different one: with the knowledge of the outside observer being limited to the three state parameters

T
,
V
,
N

, the outside observer can only guess that the molecule is moving with its most probable energy


E

t
h


=

3
2


k
B

T

. With this assumption, the proper guess for the quantum volume directly after a collision is


V
Q


(

T
,
M

)


 (Equation (5)). With the timing signals of the supervised experiment not being available, the outside observer cannot reconstruct the time evolution of the quantum volume and thus not reconstruct the function



i

m
o
l



i
,
j



(
t
)


 that would appropriately describe the molecule on its travel between both walls. With this timing information not being available, the outside observer therefore remains with the maximum uncertainty concerning the molecule’s position inside the box, which is given by


V
Q


(

T
,
M

)


 (Equation (5)). With this uncertainty in mind, the function


i

l
o
c



(

T
,
V
,
N

)


 (Equation (11)) is retained. With all these learned guesses, the outside observer finally arrives at:




i

m
o
l



(

T
,
V
,
N

)

=

1

ln

(
2
)




{


3
2

+
l
n

[




V

m
o
l





V
Q


(

T
,
M

)




]


}





(19)


which is exactly the Sackur-Tetrode equation (Equation (5)) when the correct limit to

N
→
1

 molecules is taken. Overall, both sketches show that the physics underlying supervised and unsupervised measurements is basically the same, involving particle-wall interactions, wave packet localization, wave dispersion and wave collapse during forward and backward travels. Not detecting any fundamental differences between supervised and un-supervised measurements, it is concluded that intelligent observers are not necessary to initiate and carry out processes of information gain and of information erasure. Thus, thermodynamic information, like entropy both qualify as physical quantities, independent of the existence and observation by human observers. 5. Gases with Internal Degrees of FreedomSo far, we have been dealing with ideal gases which are conceived as consisting of elastic hard spheres without any internal degrees of freedom. Arguing within the constraints of this model we have arrived at the conclusion that the information relating to the center-of-mass motion of a particular molecule is valued by the ratio of its kinetic energy


E

k
i
n



 relative to the average thermal energy


k
B

T

 inside the reservoir formed by the

N
−
1

 background molecules. In this section we show that the validity of Equation (13) is not limited to the kinetic energy of the center-of-mass motion alone, but that it is of more general validity, extending to other forms of energy as well. In addition to moving along the three orthogonal directions in geometrical space, real molecules also enjoy internal degrees of freedom. In gas-kinetic interactions the energy stored in such internal degrees of freedom can be exchanged with collision partners and re-appear as kinetic energy as the collision partners move away from their sites of impact. Alternatively, kinetic energy may become fully or partly transferred into internal degrees of freedom as gas-kinetic collisions occur. In statistical mechanics the entropies of molecular rotations, vibrations and electronic excitations can be obtained from the simplified models of rigid rotators, linear oscillators and electronic two-level systems [13] with the first two obeying Bose-Einstein and the latter Fermi-Dirac statistics. For the convenience of the reader these molecular entropies are listed in Table 1 and their temperature dependencies are displayed in Figure 4. From this latter figure it can be seen that for excitation energies

ε
>
2

k
B

T

 all entropies reasonably well follow temperature dependencies of the form.





S

i
n
t



(

ε
,
T

)

=

(


ε


k
B

T



)

exp

(

−

ε


k
B

T



)





(20)

In terms of information units these entropies correspond to information contents of:




i

i
n
t



(

ε
,
T

)

=

1

l
n

(
2
)




(


ε


k
B

T



)

e
x
p

(

−

ε


k
B

T



)





(21)

Equation (21) can be interpreted in the way that a two-level system actually excited with an energy

ε
>
2

k
B

T

 carries potential information in accordance with Equation (13). The additional exponential term arises from the fact that in
N
 gas-kinetic interactions only

n
≈
N
e
x
p

(

−

ε


k
B

T



)


 systems are expected to be encountered in their excited states. Significant differences in the statistical behavior of rotations, vibrations and electronic excitations only shine up when the mean thermal energy


k
B

T

 becomes large compared to the respective quantum energies (


k
B

T
≫
ε

). In electronic two-level systems equal numbers of excited and non-excited electronic systems will be encountered at very high temperatures. The information gain upon interaction therefore saturates at 1 bit per interaction, i.e., in the decision of a simple alternative. In contrast, increasing numbers of excitations can assemble in rotational and vibrational two-level systems as the mean thermal energy is raised beyond


k
B

T
>
ε

. The information gain upon interaction, therefore, can raise far beyond the Fermi-Dirac limit of 1 bit. 6. Conclusions and OutlookThe equivalence between information and entropy has been applied to the entropy of a molecular gas to interpret its entropy as information that is carried with its molecular constituents but missing to outside observers. The discussion has shown that the individual molecules inside the gas carry two kinds of information, which continually change as gas-kinetic collisions take place. The first part relates to energy that is stored in the external and internal excitations of the gas molecules, i.e., in their center-of-mass motion and in molecular rotations, vibrations and electronic excitations. All these forms of energies can be interchanged in gas-kinetic collisions and cause the wave functions of collision partners to become sharply localized to volumes much smaller than the average mean-free volume


V

m
o
l



 of the molecules inside the gas. As the resulting localization is determined by the energies
ε
, carried in the external and internal excitations, these first forms of information can be summarized under the name “potential information”:





i

p
o
t



(

ε
,
T

)

=

1

l
n

(
2
)




ε


k
B

T



 

ε
≫

k
B

T
.





(22)

The second form of information relates to the positions of gas molecules within their own mean-free volumes. Being the result of transferred energies, this second form of information can be called “realized information”:




i

r
e
a
l



(

ε
,
T

)

=

1

l
n

(
2
)




[




V

m
o
l





V
Q


(

ε
,
T

)




]





(23)

This second form of information contains as a key parameter the quantum volume


V
Q


(

ε
,
 
T

)


 which measures the extent of the molecular wave functions of the collision partners. As due to the effects of quantum-mechanical dispersion, the size of such wave packets changes as molecules move through their own mean-free volumes, the pieces of realized information are initially high directly after collisions and rapidly deteriorate and finally vanish shortly before follow-on collisions are likely to occur. It is proposed that these repeated changes in realized information in between successive collisions can be regarded as “measurement” processes in which information is continually generated, dispersed and erased without any human intervention. Extending the concept of “measurement” to naturally occurring physical interaction processes removes the need of “intelligence” in the processes of gaining and erasing information. In this sense it is further proposed that, entropy and thermodynamic information both fully qualify as objective physical entities, completely independent of human observation and interaction.Before concluding we want to point out that the concepts of potential and realized information also appear to be fruitful in considering supervised measurement processes involving human observers. As an example, we mention the process of particle detection in man-made detectors. In such technical devices the potential information carried with the particle is converted into realized information by dissipating the particle energy


E
p


 in a piece of macroscopic matter maintained at a temperature


T
d


. In order to be useful as information-generating devices, such devices are constructed in a way that during the process of energy dissipation a macroscopically observable output signal is created, which can be taken as proof that a particle has interacted with the device at a certain time


t
d


 and at the location


x
d


 of the detector device. Such output signals represent events localized at space-time coordinates


(


t
d

,

x
d


)


 which are endowed with a certain significance


I
d


(


E
p

,

T
d


)


 which depends on the signal-to-noise ratio,

S
N

, realized under the specific conditions of detection. We will show in a forthcoming paper [14] that the information gain in detection:




I
d


(


E
p

,

T
d


)

=

1

l
n

(
2
)



l
n

(

S
N

)

,




(24)


always remains smaller than the potential information, carried by the particle relative to the detector device:




I
d


(


E
p

,

T
d


)

<

I

p
o
t



(


E
p

,

T
d


)

=

1

l
n

(
2
)






E
p




k
B


T
d







(25)

This latter equation shows that a technical device only incompletely recovers the potential information that has been intrinsically carried with the particle.With an observable event having been generated, the possibility exists that this event might be observed by observers moving in different frames of reference. In these different frames both the particle energy


E
p


 and the temperature of the detector


T
d


 will look different to the different observers. As a consequence, the different observers will in general associate different levels of significance to one and the same event:




I
d


(


E
p

,

T
d


)

≠

I
d


(


E
p



′

,

T
d



′


)





(26)

The process of information gain thus ends up in the middle of the controversial subject of the correct relativistic transformation of temperatures [15,16,17,18,19,20]. An equality sign in Equation (25) only applies in case detector temperatures transform in the same manner as the particle energies. This equality, which assumes equal significances independent of the frame of reference, implies that moving objects appear to be hotter than stationary ones as predicted by the Ott transformation [16]. These final considerations suggest that measuring the significance of observable events in different frames of reference could provide experimental tests to shed more light on the controversial subject of relativistic temperature transformations.
