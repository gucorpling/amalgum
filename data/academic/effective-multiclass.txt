 1. IntroductionSupport vector machine (SVM) [1,2], as a computationally powerful tool for classification, have already applied in wide engineering problems [3,4,5,6,7,8]. The SVM has three elements that make it so successful, including structural risk minimization (SRM) principle, kernel trick and dual theory. However, SVM has to solve a large-sized quadratic programming problem (QPP), which greatly limits its applications. To improve learning complexity of SVM, Jayadeva et al. proposed twin SVM (TSVM) [9]. Unlike SVM that seeks an optimal separating hyperplane which maximizes the margin of two classes of samples, TSVM constructs two nonparallel proximal hyperplanes, each of which is close to the corresponding class as possible, and keeps as far away as possibly from the opposite class. The strategy makes TSVM only need to solve two smaller QPPs, instead of one larger QPP as in SVM. Due to its high learning speed, TSVM has attracted interest in recent years. Many improvements have also been proposed [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28], where the twin hypersphere SVM (THSVM) [25,26,27,28] is an excellent improvement of TSVM. Unlike TSVM, THSVM seeks a pair of hyperspheres, instead of two nonparallel hyperplanes, to depict two classes of samples. Compared with TSVM, THSVM has better classification performance.The SVM and TSVM can only solve the binary classification problems, however, many practical engineering problems are often related to multi-class classification in the real world. Currently, we usually use 1-vs-rest and 1-vs-1 strategy to resolve multi-class classification problems. In the 1-vs-1 SVM,

K
(
K
−
1
)
/
2

 binary SVM sub-classifiers are constructed. Each sub-classifier can be trained by using two classes of samples. Because only two classes are considered for each sub-classifier in the 1-vs-1 SVM and the rest samples are not involved, the 1-vs-1 SVM may get unfavorable classification results. The 1-vs-rest SVM needs to construct K binary SVM sub-classifiers. Each sub-classifier can be trained by using all the samples; thus, the 1-vs-rest SVM may lead to class imbalance problems. For above drawbacks of 1-vs-1 SVM and 1-vs-rest SVM, Angulo et al. proposed a new support vector classification-regression machine for K class (K-SVCR) [29].

K
(
K
−
1
)
/
2

 binary SVM sub-classifiers are constructed, each of which is trained with all the samples and evaluates each sample into 1-vs-1-vs-rest structure. K-SVCR avoids the class imbalance problems and information loss. Compared with 1-vs-1 SVM and 1-vs-rest SVM, K-SVCR achieves better generalization performance. Twin-KSVC [30,31], being an effective extension of K-SVCR, is based on TSVM and also evaluates each sample into 1-vs-1-vs-rest structure. Twin-KSVC achieves higher learning speed in comparison with K-SVCR. However, Twin-KSVC has the following disadvantages:Each pair of sub-classifiers has to calculate inverse matrices, which is extraordinarily time-consuming for the large-scale engineering problems.For nonlinear problems, each pair of sub-classifiers needs to construct a pair of additional primal problems, instead of directly applying the kernel trick to linear case as in SVM.For the disadvantages of Twin-KSVC, in this paper, we propose a Twin Hypersphere-KSVC, inspired by THSVM. The Twin Hypersphere-KSVC also evaluates each sample into 1-vs-1-vs-rest structure, as in Twin-KSVC. However, our Twin Hypersphere-KSVC does not seek two nonparallel hyperplanes in each pair of binary sub-classifiers as in Twin-KSVC, but a pair of hyperspheres. Compared with Twin-KSVC, Twin Hypersphere-KSVC avoids computing inverse matrices, and for nonlinear problems, can apply the kernel trick to linear case directly.This paper is outlined as follows. We briefly review the related multi-class classification algorithms in Section 2. In Section 3, the Twin Hypersphere-KSVC is proposed in detail. The experimental results on a set of benchmark datasets and several real engineering problems are presented in Section 4 and the conclusions are drawn in last section. 2. Related WorksIn this paper, we consider a multi-class classification problem with a training dataset

D
=
{

x
p
k

∈

R
d

|
k
=
1
,
⋯
,
K
,
p
=
1
,
⋯
,

n
k

}

, where K is the number of classes and

n
k

 is the number of the samples of the k-th class. The size of training dataset is

n
=

n
1

+
⋯
+

n
K


. Denote, for convenience, by

X
k

 the sets of samples of the k-th class, i.e.,


X
k

=

{

x
p
k

|
p
=
1
,
⋯
,

n
k

}


. 2.1. Review of K-SVCR Multi-ClassifierThe K-SVCR multi-classifier [29] is based on decomposition-reconstruction strategy.

K
(
K
−
1
)
/
2

 binary SVM sub-classifiers are constructed, each of which evaluates each sample into 1-vs-1-vs-rest structure. The classification result of K-SVCR is shown in Figure 1a intuitively.The sub-classifier


f

i
j



(
x
)


 for two focused classes i and j in K-SVCR seeks an optimal hyperplane





w

i
j


·
x
+

b

i
j


=
0
,




(1)


here


w

i
j


∈

R
d


 is the normal vector and


b

i
j


∈
R

 is the bias term. The optimal hyperplane can be obtained by resolving the following QPP:







min


1
2






w


i
j




2

+

c
1


(


∑

p
=
1


n
i




η

p


i
j


+


∑

p
=
1


n
j




η

p


i
j
*


)

+

c
2



∑

p
=
1


n


i
j

¯





(

ξ

p


i
j


+

ξ

p


i
j
*


)

,






s
.
t
.



w


i
j


·

x

p

i

+


b


i
j


≥
1
−

η

p


i
j


,

p
=
1
,
⋯
,

n
i

,











w


i
j


·

x

p

j

+


b


i
j


≤
−
1
+

η

p


i
j
*


,

p
=
1
,
⋯
,

n
j

,









−
ε
−

ξ

p


i
j
*


<


w


i
j


·

x

p



i
j

¯


+


b


i
j


≤
ε
+

ξ

p


i
j


,

p
=
1
,
⋯
,

n


i
j

¯


,










η

p


i
j


≥
0
,

p
=
1
,
⋯
,

n
i

,


η

p


i
j
*


≥
0
,

p
=
1
,
⋯
,

n
j

,










ξ

p


i
j


≥
0
,


ξ

p


i
j
*


≥
0
,

p
=
1
,
⋯
,

n


i
j

¯


,







(2)


where


x

p



i
j

¯


∈
D
−

X
i

−

X
j


,


n


i
j

¯


=
n
−

n
i

−

n
j


,

ξ

p


i
j


,



ξ

p


i
j
*



 and

η

p


i
j


,

η

p


i
j
*


 are slack variables, the parameter
ε
 is restricted to

[
0
,
1
)

.For a testing sample x, the sub-classifier



f


i
j



(
x
)

=


w


i
j


·
x
+


b


i
j



 determines its class by









F


i
j



(
x
)

=





−
1
,




i
f



f


i
j



(
x
)

<
ε
,






1
,




i
f



f


i
j



(
x
)

>
ε
,






0
,




e
l
s
e
.












(3)

For the testing sample x, the final label can be determined by vote rule. 2.2. Review of Twin-KSVC Multi-ClassifierTwin-KSVC [30,31] is an improvement of K-SVCR. The Twin-KSVC constructs

K
(
K
−
1
)
/
2

 pairs of binary TSVM sub-classifiers, which evaluates each sample into 1-vs-1-vs-rest structure. The classification result is intuitively presented in Figure 1b.The sub-classifiers


f
i


(
x
)


 and


f
j


(
x
)


 for two focused classes i and j in Twin-KSVC seek a pair of hyperplane





w
i

·
x
+

b
i

=
0


a
n
d



w
j

·
x
+

b
j

=
0




(4)


where


w

i
(
j
)


∈

R
d


 and


b

i
(
j
)


∈
R

 are the normal vector and the bias term of the corresponding hyperplane, respectively. The two hyperplanes can be obtained by resolving the QPPs as follows:







min

1
2



∑

p
=
1


n
i





(


w

i

·

x

p

i

+


b

i

)

2

+

c
1



∑

p
=
1


n
j




η

p

i

+

c
2



∑

p
=
1


n


i
j

¯





ξ

p

i

,






s
.
t
.

−

(


w

i

·

x

p

j

+


b

i

)

+

η

p

i

≥
1
,

p
=
1
,
⋯
,

n
j

,










−

(


w

i

·

x

p



i
j

¯


+


b

i

)

+

ξ

p

i

≥
1
−
ε
,

p
=
1
,
⋯
,

n


i
j

¯


,











η

p

i

≥
0
,

p
=
1
,
⋯
,

n
j

,











ξ

p

i

≥
0
,

p
=
1
,
⋯
,

n


i
j

¯


,







(5)








min

1
2



∑

p
=
1


n
j





(


w

j

·

x

p

j

+


b

j

)

2

+

c
3



∑

p
=
1


n
i




η

p

j

+

c
4



∑

p
=
1


n


i
j

¯





ξ

p

j

,






s
.
t
.


(


w

j

·

x

p

i

+


b

j

)

+

η

p

j

≥
1
,

p
=
1
,
⋯
,

n
i

,











(


w

j

·

x

p



i
j

¯


+


b

j

)

+

ξ

p

j

≥
1
−
ε
,

p
=
1
,
⋯
,

n


i
j

¯


,











η

p

j

≥
0
,

p
=
1
,
⋯
,

n
i

,











ξ

p

j

≥
0
,

p
=
1
,
⋯
,

n


i
j

¯


,







(6)


where

η

p


i
(
o
r

j
)


 and

ξ

p


i
(
o
r

j
)


 are slack variables.For a testing sample x, the sub-classifiers



f

i


(
x
)

=


x

T



w

i

+


b

i


 and



f

j


(
x
)

=


x

T



w

j

+


b

j


 assign its class by









F


i
j



(
x
)

=





−
1
,




i
f



f

j


(
x
)

<
1
−
ε
,






1
,




i
f



f

i


(
x
)

>
−
1
+
ε
,






0
,




e
l
s
e
.












(7)

For the testing sample x, the final label can be also determined by vote rule. 2.3. Review of THKSVM Multi-ClassifierTHKSVM (Twin Hypersphere Multiclass Support Vector Machine) [32] integrates THSVM and 1-vs-rest structure. THKSVM constructs K hyperspheres in the training stage, whose classification result is intuitively shown in Figure 1c.The sub-classifier for the focused classes i in THKSVM seeks a hypersphere







x
−

a
i



2

=

R
i
2





(8)


where


a
i

∈

R
d


 and


R
i

∈
R

 are the center and the radius of the corresponding hypersphere, respectively. The hypersphere can be constructed by resolving the following QPP:







min

1
2



∑

p
=
1


n

i
¯








x

p


i
¯


−

a
i



2

−

v
1


R

i

2

+

c
1



∑

p
=
1


n
i




η

p

i

,






s
.
t
.





x

p

i

−

a
i



2

≥

R

i

2

−

η

p

i

,










R

i

2

≥
0
,


η

p

i

≥
0
,

p
=
1
,
⋯
,

n
i

,







(9)


where


x

p


i
¯


∈
D
−

X
i


,


n

i
¯


=
n
−

n
i


 and


η

p

i

≥
0

 are slack variables.The class of a testing sample x can be determined by




C
l
a
s
s

k
=
arg

max

i
=
1
,
⋯
,
K







x
−

a
i



2


R

i

2


.




(10)

 3. Twin Hypersphere-KSVCTwin Hypersphere-KSVC, inspired by THSVM and 1-vs-1-vs-rest structure, constructs

K
(
K
−
1
)
/
2

 pairs of hyperspheres in the training stage. For two focused classes i and j, Twin Hypersphere-KSVC seeks a pair of hypersphere

(

a
i

,

R
i

)

 and

(

a
j

,

R
j

)

, where

a
i

 (

R
i

) and

a
j

 (

R
j

) are respectively the centers (radii) of the corresponding hyperspheres. Each hypersphere covers the corresponding class as many as possibly, keeps as far away as possibly from another class, contains the rest of samples as little as possibly, and the radius of the hypersphere is as small as possible. The Twin Hypersphere-KSVC are intuitively presented in Figure 1d. 3.1. Linear CaseFor the linear case, each pair of hyperspheres

(

a
i

,

R
i

)

 and

(

a
j

,

R
j

)

 for two focused classes i and j in Twin Hypersphere-KSVC is constructed by resolving the following QPPs:







min

R

i

2

−


v
1


n
j




∑

p
=
1


n
j







x

p

j

−

a
i



2

+


c
1


n
i




∑

p
=
1


n
i




η

p

i

+


c
2


n


i
j

¯





∑

p
=
1


n


i
j

¯





ξ

p

i

,






s
.
t
.





x

p

i

−

a
i



2

≤

R

i

2

+

η

p

i

,

p
=
1
,
⋯
,

n
i

,













x

p



i
j

¯


−

a
i



2

≥

R

i

2

−

ξ

p

i

,

p
=
1
,
⋯
,

n


i
j

¯


,










η

p

i

≥
0
,

p
=
1
,
⋯
,

n
i

,










ξ

p

i

≥
0
,

p
=
1
,
⋯
,

n


i
j

¯


,










R

i

2

≥
0
,







(11)








min

R

j

2

−


v
2


n
i




∑

p
=
1


n
i







x

p

i

−

a
j



2

+


c
3


n
j




∑

p
=
1


n
j




η

p

j

+


c
4


n


i
j

¯





∑

p
=
1


n


i
j

¯





ξ

p

j

,






s
.
t
.





x

p

j

−

a
j



2

≤

R

j

2

+

η

p

j

,

p
=
1
,
⋯
,

n
j

,













x

p



i
j

¯


−

a
j



2

≥

R

j

2

−

ξ

p

j

,

p
=
1
,
⋯
,

n


i
j

¯


,










η

p

j

≥
0
,

p
=
1
,
⋯
,

n
j

,










ξ

p

j

≥
0
,

p
=
1
,
⋯
,

n


i
j

¯


,










R

j

2

≥
0
.







(12)


where

η

p


i
(
j
)


 and

ξ

p


i
(
j
)


 are slack variables.The Lagrangian function L for the QPP (11) is given by:







L
=

R

i

2

−


v
1


n
j




∑

p
=
1


n
j







x

p

j

−

a
i



2

+


c
1


n
i




∑

p
=
1


n
i




η

p

i

+


c
2


n


i
j

¯





∑

p
=
1


n


i
j

¯





ξ

p

i

+


∑

p
=
1


n
i





α
p


(




x

p

i

−

a
i



2

−

R

i

2

−

η

p

i

)












−


∑

p
=
1


n


i
j

¯






β
p


(




x

p



i
j

¯


−

a
i



2

−

R

i

2

+

ξ

p

i

)


−


∑

p
=
1


n
i





s
p


η

p

i


−


∑

p
=
1


n


i
j

¯






q
p


ξ

p

i


−
λ

R

i

2

.







(13)

The Karush-Kuhn-Tucker (KKT) conditions are satisfied as follows:



2


v
1


n
j




∑

p
=
1


n
j




(

x

p

j

−

a
i

)

−
2


∑

p
=
1


n
i





α
p


(

x

p

i

−

a
i

)


+
2


∑

p
=
1


n


i
j

¯






β
p


(

x

p



i
j

¯


−

a
i

)


=
0
,




(14)





1
−


∑

p
=
1


n
i




α
p

+


∑

p
=
1


n


i
j

¯





β
p

−
λ
=
0
,




(15)







c
1


n
i


−

α
p

−

s
p

=
0
,

p
=
1
,
⋯
,

n
i

,




(16)







c
2


n


i
j

¯



−

β
p

−

q
p

=
0
,

p
=
1
,
⋯
,

n


i
j

¯


,




(17)






α
p


(




x

p

i

−

a
i



2

−

R

i

2

−

η

p

i

)

=
0
,

p
=
1
,
⋯
,

n
i

,




(18)






β
p


(




x

p



i
j

¯


−

a
i



2

−

R

i

2

+

ξ

p

i

)

=
0
,

p
=
1
,
⋯
,

n


i
j

¯


,




(19)






s
p


η

p

i

=
0
,

p
=
1
,
⋯
,

n
i

,




(20)






q
p


ξ

p

i

=
0
,

p
=
1
,
⋯
,

n


i
j

¯


,




(21)





λ

R

i

2

=
0
.




(22)

From (14), (15) and (22), we can obtain





a
i

=

1

1
−

v
1




(


∑

p
=
1


n
i





α
p


x

p

i


−


v
1


n
j




∑

p
=
1


n
j




x

p

j

−


∑

p
=
1


n


i
j

¯






β
p


x

p



i
j

¯



)





(23)

By denoting





v
i

=

1

1
−

v
1







(24)


and substituting (16)–(23) into (13), the dual optimal problem of (11) is obtained as follows:







max


∑

p
=
1


n
i





α
p


x

p

i

·

x

p

i


−


∑

p
=
1


n


i
j

¯






β
p


x

p



i
j

¯


·

x

p



i
j

¯



+
2

v
i



v
1


n
j




∑


p
1

=
1


n
j






∑


p
2

=
1


n
i





α

p
2



x


p
1


j

·

x


p
2


i



−
2

v
i



v
1


n
j




∑


p
1

=
1


n
j






∑


p
2

=
1


n


i
j

¯






β

p
2



x


p
1


j

·

x


p
2




i
j

¯














−

v
i



∑


p
1

=
1


n
i






∑


p
2

=
1


n
i





α

p
1



α

p
2



x


p
1


i

·

x


p
2


i



+
2

v
i



∑


p
1

=
1


n
i






∑


p
2

=
1


n


i
j

¯






α

p
1



β

p
2



x


p
1


i

·

x


p
2




i
j

¯




−

v
i



∑


p
1

=
1


n


i
j

¯







∑


p
2

=
1


n


i
j

¯






β

p
1



β

p
2



x


p
1




i
j

¯


·

x


p
2




i
j

¯










s
.
t
.

1
−


∑

p
=
1


n
i




α
p

+


∑

p
=
1


n


i
j

¯





β
p

=
0
,









0
≤

α
p

≤


c
1


n
i


,

p
=
1
,
⋯
,

n
i

,









0
≤

β
p

≤


c
2


n


i
j

¯



,

p
=
1
,
⋯
,

n


i
j

¯


.







(25)

By defining

α
=





α
1





⋮





α

n
i







 and

β
=





β
1





⋮





β

n


i
j

¯








, the optimal problem (25) can be reformed as







max
−

v
i


(





α

T





β

T




)







X

i

T


X
i





−

X

i

T


X


i
j

¯








−

X



i
j

¯


T


X
i






X



i
j

¯


T


X


i
j

¯











α




β




+







d
i
a
g

(

X

i

T


X
i

)

+
2

v
i



v
1


n
j



X

i

T


X
j


e
j







−
d
i
a
g

(

X



i
j

¯


T


X


i
j

¯


)

−
2

v
i



v
1


n
j



X



i
j

¯


T


X
j


e
j







T





α




β




,






s
.
t
.






e

i

T




−

e



i
j

¯


T










α




β




=
1
,










0

e
i

≤
α
≤


c
1


n
i



e
i

,










0

e


i
j

¯


≤
β
≤


c
2


n


i
j

¯




e


i
j

¯


.







(26)

According to the KKT conditions (16)–(21), we can obtain

R
i
2

 by the following formula:




R

i

2

=





x

*

−

a
i



2

,




(27)


where



x

*

∈

S

i
1


∪

S

i
2



,


S

i
1


=

{

x

p

i

|
0
<

α
p

<


c
1


n
i


}


 and


S

i
2


=

{

x

p



i
j

¯


|
0
<

β
p

<


c
2


n


i
j

¯



}


.By denoting


v
j

=

1

1
−

v
2




, the dual optimal problem of (12) can be obtained as follows:







max


∑

p
=
1


n
j





θ
p


x

p

j

·

x

p

j


−


∑

p
=
1


n


i
j

¯






γ
p


x

p



i
j

¯


·

x

p



i
j

¯



+
2

v
j



v
2


n
i




∑


p
1

=
1


n
i






∑


p
2

=
1


n
j





θ

p
2



x


p
1


i

·

x


p
2


j



−
2

v
j



v
2


n
i




∑


p
1

=
1


n
i






∑


p
2

=
1


n


i
j

¯






γ

p
2



x


p
1


i

·

x


p
2




i
j

¯














−

v
j



∑


p
1

=
1


n
j






∑


p
2

=
1


n
j





θ

p
1



θ

p
2



x


p
1


j

·

x


p
2


j



+
2

v
j



∑


p
1

=
1


n
j






∑


p
2

=
1


n


i
j

¯






θ

p
1



γ

p
2



x


p
1


j

·

x


p
2




i
j

¯




−

v
j



∑


p
1

=
1


n


i
j

¯







∑


p
2

=
1


n


i
j

¯






γ

p
1



γ

p
2



x


p
1




i
j

¯



x


p
2




i
j

¯




,






s
.
t
.

1
−


∑

p
=
1


n
j




θ
p

+


∑

p
=
1


n


i
j

¯





γ
p

=
0
,









0
≤

θ
p

≤


c
3


n
j


,

p
=
1
,
⋯
,

n
j

,









0
≤

γ
p

≤


c
4


n


i
j

¯



,

p
=
1
,
⋯
,

n


i
j

¯


.







(28)

By defining

θ
=





θ
1





⋮





θ

n
j







 and

γ
=





γ
1





⋮





γ

n


i
j

¯








, the (28) can be reformulated as







max
−

v
j


(





θ

T





γ

T




)







X

j

T


X
j





−

X

j

T


X


i
j

¯








−

X



i
j

¯


T


X
j






X



i
j

¯


T


X


i
j

¯











θ




γ




+







d
i
a
g

(

X

j

T


X
j

)

+
2

v
j



v
2


n
i



X

j

T


X
i


e
i







−
d
i
a
g

(

X



i
j

¯


T


X


i
j

¯


)

−
2

v
j



v
2


n
i



X



i
j

¯


T


X
i


e
i







T





θ




γ




,






s
.
t
.






e

j

T




−

e



i
j

¯


T










θ




γ




=
1
,










0

e
j

≤
θ
≤


c
3


n
j



e
j

,










0

e


i
j

¯


≤
γ
≤


c
4


n


i
j

¯




e


i
j

¯


.







(29)

We can compute

R
j
2

 by the following formula:




R

j

2

=





x

*

−

a
j



2

,




(30)


where



x

*

∈

S

j
1


∪

S

j
2



,


S

j
1


=

{

x

p

j

|
0
<

θ
p

<


c
3


n
j


}


 and


S

j
2


=

{

x

p



i
j

¯


|
0
<

γ
p

<


c
4


n


i
j

¯



}


. 3.2. Nonlinear CaseWe extend the linear Twin Hypersphere-KSVC to the nonlinear case by directly considering the nonlinear map

φ
:


R

d

→
H

 (H is a high-dimensional Hilbert space), instead of the kernel generated surfaces in Twin-KSVC.







min

R

i

2

−


v
1


n
j




∑

p
=
1


n
j






φ

(

x

p

j

)

−

a
i



2

+


c
1


n
i




∑

p
=
1


n
i




η

p

i

+


c
2


n


i
j

¯





∑

p
=
1


n


i
j

¯





ξ

p

i

,






s
.
t
.




φ

(

x

p

i

)

−

a
i



2

≤

R

i

2

+

η

p

i

,

p
=
1
,
⋯
,

n
i

,












φ

(

x

p



i
j

¯


)

−

a
i



2

≥

R

i

2

−

ξ

p

i

,

p
=
1
,
⋯
,

n


i
j

¯


,










η

p

i

≥
0
,

p
=
1
,
⋯
,

n
i

,










ξ

p

i

≥
0
,

p
=
1
,
⋯
,

n


i
j

¯


,










R

i

2

≥
0
,







(31)








min

R

j

2

−


v
2


n
i




∑

p
=
1


n
i






φ

(

x

p

i

)

−

a
j



2

+


c
3


n
j




∑

p
=
1


n
j




η

p

j

+


c
4


n


i
j

¯





∑

p
=
1


n


i
j

¯





ξ

p

j

,






s
.
t
.




φ

(

x

p

j

)

−

a
j



2

≤

R

j

2

+

η

p

j

,

p
=
1
,
⋯
,

n
j

,












φ

(

x

p



i
j

¯


)

−

a
j



2

≥

R

j

2

−

ξ

p

j

,

p
=
1
,
⋯
,

n


i
j

¯


,










η

p

j

≥
0
,

p
=
1
,
⋯
,

n
j

,










ξ

p

j

≥
0
,

p
=
1
,
⋯
,

n


i
j

¯


,










R

j

2

≥
0
.







(32)

According to the dual theory, one can get the dual optimal problems of (31) and (32) as follows:







max
−

v
i


(





α

T





β

T




)






K
(

X
i

,

X
i

)




−
K
(

X
i

,

X


i
j

¯


)






−
K
(

X


i
j

¯


,

X
i

)




K
(

X


i
j

¯


,

X


i
j

¯


)









α




β













+







d
i
a
g

(
K

(

X
i

,

X
i

)

)

+
2

v
i



v
1


n
j


K

(

X
i

,

X
j

)


e
j







−
d
i
a
g

(
K

(

X


i
j

¯


,

X


i
j

¯


)

)

−
2

v
i



v
1


n
j


K

(

X


i
j

¯


,

X
j

)


e
j







T





α




β




,






s
.
t
.






e

i

T




−

e



i
j

¯


T










α




β




=
1
,










0

e
i

≤
α
≤


c
1


n
i



e
i

,










0

e


i
j

¯


≤
β
≤


c
2


n


i
j

¯




e


i
j

¯


,







(33)








max
−

v
j


(





θ

T





γ

T




)






K
(

X
j

,

X
j

)




−
K
(

X
j

,

X


i
j

¯


)






−
K
(

X


i
j

¯


,

X
j

)




K
(

X


i
j

¯


,

X


i
j

¯


)









θ




γ













+







d
i
a
g

(
K

(

X
j

,

X
j

)

)

+
2

v
j



v
2


n
i


K

(

X
j

,

X
i

)


e
i







−
d
i
a
g

(
K

(

X


i
j

¯


,

X


i
j

¯


)

)

−
2

v
j



v
2


n
i


K

(

X


i
j

¯


,

X
i

)


e
i







T





θ




γ




,






s
.
t
.






e

j

T




−

e



i
j

¯


T










θ




γ




=
1
,










0

e
j

≤
θ
≤


c
3


n
j



e
j

,










0

e


i
j

¯


≤
γ
≤


c
4


n


i
j

¯




e


i
j

¯


,







(34)


where

K
(
.
,
.
)

 is a kernel matrix. 3.3. Decision RuleFor a testing sample x, each pair of sub-classifiers determines its label by





F

i
j



(
x
)

=




1






φ

(
x
)

−

a
i



2

/

R

i

2

≤



φ

(
x
)

−

a
j



2

/

R

j

2







−
1







φ

(
x
)

−

a
i



2

/

R

i

2

>



φ

(
x
)

−

a
j



2

/

R

j

2










(35)


wher










φ

(
x
)

−

a
i



2

=
K

(
x
,
x
)

+
2

v
i


(


∑

p
=
1


n
i





α
p

K

(

x

p

i

,
x
)


−


v
1


n
j




∑

p
=
1


n
j




K
(

x

p

j

,
x
)

−


∑

p
=
1


n


i
j

¯






β
p

K

(

x

p



i
j

¯


,
x
)


)









+

v

i

2


(



∑


p
1

=
1


n
i






∑


p
2

=
1


n
i





α

p
1



α

p
2


K

(

x


p
1


i

,

x


p
2


i

)



+


(


v
1


n
j


)

2



∑


p
1

=
1


n
j






∑


p
2

=
1


n
j




K

(

x


p
1


j

,

x


p
2


j

)

+


∑


p
1

=
1


n


i
j

¯







∑


p
2

=
1


n


i
j

¯






β

p
1



β

p
2



K
(


x


p
1




i
j

¯


,

x


p
2




i
j

¯






)










−


2

v
1



n
j




∑


p
1

=
1


n
i






∑


p
2

=
1


n
j





α

p
1


K

(

x


p
1


i

,

x


p
2


j

)



−
2


∑


p
1

=
1


n
i






∑


p
2

=
1


n


i
j

¯






α

p
1



β

p
2


K

(

x


p
1


i

,

x


p
2




i
j

¯


)



+


2

v
1



n
j




∑


p
1

=
1


n
j






∑


p
2

=
1


n


i
j

¯






β

p
2


K

(

x


p
1


i

,

x


p
2




i
j

¯


)




)








(36)


and










φ

(
x
)

−

a
j



2

=
K

(
x
,
x
)

+
2

v
j


(


∑

p
=
1


n
j





θ
p

K

(

x

p

j

,
x
)


−


v
2


n
i




∑

p
=
1


n
i




K
(

x

p

i

,
x
)

−


∑

p
=
1


n


i
j

¯






γ
p

K

(

x

p



i
j

¯


,
x
)


)











+

v

j

2


(



∑


p
1

=
1


n
j






∑


p
2

=
1


n
j





θ

p
1



θ

p
2


K

(

x


p
1


j

,

x


p
2


j

)



+


(


v
2


n
i


)

2



∑


p
1

=
1


n
i






∑


p
2

=
1


n
i




K

(

x


p
1


i

,

x


p
2


i

)

+


∑


p
1

=
1


n


i
j

¯







∑


p
2

=
1


n


i
j

¯






γ

p
1



γ

p
2



K
(


x


p
1




i
j

¯


,

x


p
2




i
j

¯






)












−


2

v
2



n
i




∑


p
1

=
1


n
j






∑


p
2

=
1


n
i





θ

p
1


K

(

x


p
1


j

,

x


p
2


i

)




−
2


∑


p
1

=
1


n
j






∑


p
2

=
1


n


i
j

¯






θ

p
1



γ

p
2


K

(

x


p
1


j

,

x


p
2




i
j

¯


)



+


2

v
2



n
i




∑


p
1

=
1


n
i






∑


p
2

=
1


n


i
j

¯






γ

p
2


K

(

x


p
1


i

,

x


p
2




i
j

¯


)




)
.








(37)

For the testing sample x, the final label can be also determined by vote rule. 3.4. Analysis of Learning ComplexityNext, the learning complexity of the proposed Twin Hypersphere-KSVC will be discussed. We take the 4-class classification as an example, suppose samples of 4 classes are approximately equal, and present the learning complexity of K-SVCR, Twin-KSVC and Twin Hypersphere-KSVC in Table 1. The main calculating burden in Twin-KSVC includes solving QPPs and calculating inverse matrices. Therefore the learning complexities of linear and nonlinear Twin-KSVC are respectively

K
(
K
−
1
)

(

O
(


d

3

)

+

O
(


(

3
4

n
)

3

)

) and

K
(
K
−
1
)

(

O
(


n

3

)

+

O
(


(

3
4

n
)

3

)

). However, K-SVCR and Twin Hypersphere-KSVC avoid computing inverse matrices, the learning complexities of linear and nonlinear K-SVCR are both



K
(
K
−
1
)

2

O

(


(

3
2

n
)

3

)


 while the learning complexities of linear and nonlinear Twin Hypersphere-KSVC are both

K
(
K
−
1
)



O
(


(

3
4

n
)

3

)

. From the above analysis, we can see that our Twin Hypersphere-KSVC requires less learning time. 4. ExperimentsIn this section, we investigate classification performance of our Twin Hypersphere-KSVC, ITKSVC [31], THKSVM, Twin-KSVC and K-SVCR on a set of benchmark datasets from the UCI repository and several real engineering problems. The above algorithms are implemented in Matlab R2012a, and we use the “quadprog.m” function to solve the QPP and the “inv.m” function to calculate matrix inversion.The parameter selection directly affects the classification performance of the above algorithms. We use the most popular exhaustive search to determine the parameters in this section. The K-SVCR includes two penalty parameters


c
i


(
i
=
1
,
2
)


 and bandwidth parameter
ε
. The Twin-KSVC has five parameters, including four penalty parameters


c
i


(
i
=
1
,
2
,
3
,
4
)


 and bandwidth parameter
ε
. The THKSVM holds the penalty parameters

c
1

 and

v
1

. The ITKSVC contains seven parameters which are six penalty parameters


c
i


(
i
=
1
,
2
,
3
,
4
,
5
,
6
)


 and bandwidth parameter
ε
. There exist six penalty parameters


c
i


(
i
=
1
,
2
,
3
,
4
)


 and


v
i


(
i
=
1
,
2
)


 in our Twin Hypersphere-KSVC. The optimal values of penalty parameters

c
i

 are searched from set

{


2


−
7


,
⋯
,


2

7

}

, penalty parameters

v
i

 from

{
0.1
,
⋯
,
0.9
}

 and bandwidth parameter
ε
 from set

{
0
,
⋯
,
0.5
}

. 4.1. Benchmark DatasetsWe compare Twin Hypersphere-KSVC with ITKSVC, THKSVM, Twin-KSVC and K-SVCR on a set of benchmark datasets from the UCI repository in this subsection. The benchmark datasets are presented in Table 2. The 5-fold cross validation is used to estimate the testing accuracy and we use radial basis function

K

(
x
,
y
)

=


e


−



x
−
y


2

/


σ

2




 in this subsection, where the parameter
σ
 are selected from

{


2


−
7


,
⋯
,


2

7

}

.The predicting accuracy and learning time of five algorithms on UCI benchmark data sets are respectively shown in Table 3 and Table 4. By observing Table 3 and Table 4, one can come to the following conclusions.
(1)The Twin Hypersphere-KSVC, ITKSVC, Twin-KSVC and K-SVCR obtain better test accuracy than THKSVM. This is mainly because the sub-classifiers in Twin Hypersphere-KSVC, ITKSVC, Twin-KSVC and K-SVCR avoid the class imbalance problem appearing in THKSVM.(2)The Twin Hypersphere-KSVC works faster than Twin-KSVC, K-SVCR and ITKSVC. It is mainly because, compared with Twin-KSVC, the sub-classifiers in Twin Hypersphere-KSVC avoid calculating inverse matrices which appear in Twin-KSVC, while compared with K-SVCR and ITKSVC, each pair of sub-classifiers of our Twin Hypersphere-KSVC only needs to resolve two smaller QPPs.(3)For predicting accuracy of Twin-KSVC, K-SVCR, ITKSVC and Twin Hypersphere-KSVC, we can observe that not any method is superior to others for all data sets. We can apply Friedman test to analyze the test accuracy of these classifiers statistically [33,34]. The ranks of five algorithms for all data set in the light of test accuracy are presented in Table 5. The Friedman statistic

χ
F
2

 can be calculated by





χ
F
2

=


12

m
2




m
1


(

m
1

+
1
)




[


∑


k
1

=
1


m
1




r
a
n

k


k
1


2


−



m
1



(

m
1

+
1
)

2


4

]

,




(38)


where

r
a
n

k

k
1


=

1

m
2




∑


k
2

=
1


m
2




r


k
1



k
2



,

r


k
1



k
2


 denotes the rank of the

k
1

-th of

m
1

 classifiers on the

k
2

-th of

m
2

 data sets. Because

χ
F
2

 is undesirably conservative, we can use the other statistic





F
F

=



(

m
2

−
1
)


χ
F
2




m
2


(

m
1

−
1
)

−

χ
F
2



,




(39)


which is distributed by the

F
(

m
1

−
1
,

(

m
1

−
1
)


(

m
2

−
1
)

)

.We can calculate the statistic


F
F

=
10.41

, where


F
F

∼
F

(
4
,
28
)


. For the level of significance

α
=
0.05

,

F
(
4
,
28
)
=
2.95

 is smaller than

F
F

, which means there are significant differences among these classifiers. We can see from Table 5 that the average rank of Twin Hypersphere-KSVC is lower than ITKSVC, and is higher than Twin-KSVC and K-SVCR. It implies that classification accuracy of Twin Hypersphere-KSVC is slightly lower than ITKSVC, however, is much higher than Twin-KSVC and K-SVCR. 4.2. Handwritten Digits RecognitionWe use Twin Hypersphere-KSVC to recognize handwritten digits. The USPS database is used to compare our Twin Hypersphere-KSVC with Twin-KSVC, THKSVM, ITKSVC and K-SVCR. The USPS dataset consists of 8-bit grayscale images of handwritten digits from 0 to 9, as presented in Figure 2. We choose 55 images for each handwritten digit in the USPS database, 550 images in total. We only consider linear kernel function

K

(
x
,
y
)

=


x

T

y

, and also use 5-fold cross validation to estimate the testing accuracy.The handwritten digits recognition results of five algorithms are presented in Table 6. From Table 6, we can observe that, the proposed Twin Hypersphere-KSVC obtains better accuracy among all algorithms. In terms of learning time, our Twin Hypersphere-KSVC costs shorter learning time, compared with ITKSVC, Twin-KSVC and K-SVCR. 4.3. Text ClassificationWe evaluate our Twin Hypersphere-KSVC to text classification in this subsection and compare it with the other algorithms on the Reuters21578 dataset. We choose 6 classes from the Reuters21578 dataset, 708 documents in total, which are presented in Table 7. We also consider linear kernel function in this subsection.The experimental results of five algorithms for text classification are presented in Table 8. By observing Table 8, we can notice that, our Twin Hypersphere-KSVC can get better accuracy among all multi-classifiers. The proposed Twin Hypersphere-KSVC runs faster in comparison with ITKSVC, Twin-KSVC and K-SVCR. 5. ConclusionsIn this paper, we propose a novel multi-class classification algorithm, named Twin Hypersphere-KSVC. The Twin Hypersphere-KSVC evaluates each training sample into 1-vs-1-vs-rest structure, as in Twin-KSVC and K-SVCR, and constructs two hyperspheres in each pair of sub-classifiers, instead of two nonparallel hyperplanes as in Twin-KSVC. Compared with Twin-KSVC, the sub-classifiers in Twin Hypersphere-KSVC avoid computing inverse matrices, and for nonlinear problems, can apply the kernel trick to linear case directly. The classification results on a set of benchmark datasets from UCI repository, handwritten digits recognition and text classification, show that the Twin Hypersphere-KSVC gets better classification performance in comparison with the other classical multi-classifiers.
