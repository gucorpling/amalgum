 1. IntroductionThe concept of precision viticulture [1] has rapidly developed and extended to a variety of unmanned aerial vehicle (UAV) applications in recent years. These include digital 3D vineyard structure reconstruction from UAV imagery for precise row monitoring [2,3] and crop status quantification, such as evaluating growth conditions with hyperspectral sensors [4], RGB, multispectral, thermal sensors combined together [5], and utilizing machine-learning techniques for multispectral sensors to schedule the irrigation [6]. All of the mentioned quantification works have used optical sensors, which heavily rely upon the accurate derivation of canopy reflectance.In viticulture, this demand faces two unique geometric problems. First, the canopy converges to a narrower shape after pruning, limiting the retrievable information from nadir orthophotos. Second, although the former problem can be improved by tilting the camera’s view angle to observe the side of the canopies, the enlarged sun-target-sensor angle in the sloped areas will further interfere with the target reflectance anisotropy. The resulting consequences have been demonstrated in other scenarios. One study on wheat [7] showed that SCOPE-modeled directional reflectance was largely different from the UAV hyperspectral spectrometer measurement in low-tilt angles. Another hyperspectral study [8] highlighted the influence of viewing geometry on the Normalized Differential Vegetation Index (NDVI) values, demanding the necessary compensation. The angular reflectance dependency has been intensively studied within the framework of the bidirectional reflectance distribution function (BRDF) [9]. For vegetation canopies, the anisotropic reflectance can be dependent on the canopy structure (surface curvature, leaf layers, shadow proportion, leaf orientations, etc.) and visible background at different view geometries. This has resulted in a series of modeling tools. One of the commonly used models was described by Walthall et al. [10]. It was then improved in a detailed layout in Nilson and Kuusk’s work [11], which was illustrated by Beisl et al. [12]. Another commonly used model was proposed by Rahman et al. [13]. These models have been used to semi-empirically establish the relation between canopy directional reflectance and geometry. Further modeling with BRDF simulation tools, such as PROSAIL [14,15], can also be found in application reviews [16]. Early works have used ground goniometers to study the angular reflectance dependency [17,18]. But by paralleling the goniometer measured ground truth and UAV measurements, studies have already shown that the UAV-mounted cameras can obtain bidirectional reflectance factors without ground measurements. For example, a correspondence was found between goniometer and UAV commercial camera on a smooth snow surface [19]. A low standard deviation between goniometer and UAV directional reflectance on reference panels was found in another study [20]. By involving a sun sensor, UAV directional reflectance can also be derived without a reference panel [7,21,22]. Such a method was tested and evaluated in a very recent work, where high correlation coefficients were found between UAV measurement and literature goniometer values on a spectralon white reference panel [23]. A range of dedicated UAV reflectance anisotropy studies have been conducted on different types of vegetation. Aside from the wheat study mentioned before [7], another hyperspectral study on wheat and barley [24] found that the scattering parameter in the Rahman–Pinty–Verstraete model differed by the crop heading stage. It also found a strong backscattering reduction in potato after the full coverage of canopies [25], indicating the potential of crop-growth stage monitoring by angular observation. Taking the bidirectional effect into consideration, the radiometric correction via geometric block adjustment method [26,27] was developed to significantly improve the uniformity of spectral mosaics. Another assessment study of BRDF in UAV near-infrared (NIR) imagery has illustrated the compensation method for angular radiometric disturbances [28]. As part of a UAV-monitoring project on a sloped vineyard, this paper focuses on delivering a feasible approach to sample, model, and correct the angular dependency for the vine canopy reflectance from UAV imagery without field measurements. In the study, the UAV carried a tilted RedEdge-M sensor to take the images from a sloped vineyard. The reflectance anisotropy was studied on these images by combining the RedEdge Python package [29] and the 3D-reconstruction software Agisoft© Metashape [30] applications. Then, both the corrected and uncorrected reflectance images of the red and NIR bands were used to compute their side-view orthomosaics and corresponding NDVI maps from different directions. The goal of this study is to deliver a 3D NDVI surface that is independent of the angular effects. The hypothesis is that the corrected NDVI values from different views should be significantly lower than the uncorrected ones.  2. Materials and Methods  2.1. Study Area, UAV Trials, and MaterialsThe study area is located at Bernkastel-Kues, Germany, a Riesling vineyard on the Mosel valley (Figure 1). The vineyard slope angle is around 19°, facing southwest. The average row distance is 1.3m, and the average canopy distance is 1.2m. The soils between rows were slightly weeded. For the BRDF study in this paper, the particular imagery on 13 July 2018 was used, just 2 weeks after pruning. The time point was between fruit set and veraison.The UAV is an RC-Upgrade© BlackSnapper-L-pro quad helicopter frame cored with a DJI© A3 flight control system. The MicaSense© RedEdge-M is a 5-discrete-narrowband frame multispectral sensor that is commonly used in remote-sensing studies and precision agriculture. The band information is summarized in Table 1. It contains a downwelling light sensor (DLS) module that measures the ambient light for each spectral band during flight [31]. It is also configured with a 3DR© GPS (accuracy, 2–3 m) and a 3-axis magnetometer module that records the UAV’s exterior orientation for each capture. This information is used to geometrically trace back the illumination on each specific field of view (FOV) for direct reflectance computation. The flight lasted around 12 minutes (from 11:39 to 11:51), during which the camera was configured 20 degrees zenith from nadir, always toward the northwest. There was no wind during the flight. The flight altitude was kept at around 242 m above sea level (30–50m relative to the slope). The nadir pixel resolution was around 2.6 cm.The 3D processing was conducted in Agisoft© Metashape 1.5.1. After 3D processing, the report showed that GPS errors ranged from 0.8~3 m. Image-wise geometric analysis and correction were processed in Python, with certain original codes from RedEdge-M packages modified. Sampling points allocation, canopy mask building, NDVI computation, and results visualization were conducted in ESRI© ArcGIS [32]. 2.2. BRDF Sampling, Modeling, and Correction As illustrated in the schema (Figure 2), the whole procedure can be divided into three major modules: BRDF variables retrieval, sampling, and correction. The variables retrieval can be further divided into two sub-modules: the direct reflectance (R) retrieval and the geometry (θi, θv, and φ) retrieval.This study utilized the Walthall model [10,11,12] in Equation (1), which was developed for homogenous spherical canopies such as wheat and soybeans. For computation, the variables consist of two major compartments: (1) the dependent variable reflectance factor R and (2) the independent geometric variables.




R

(


θ
i




,
θ


v


,
φ


)




 
=
 
a
θ


i
2


θ
v
2


 
+
 
b


(


θ
i
2




 
+
 
θ


v
2


)




 
+
 
c
θ


i


θ
v


cos
φ
 
+
 
d





(1)


where:R—the observed directional reflectance factor; θi—the solar incident zenith; θv—the view zenith; φ—the relative azimuth; a, b, c, and d—coefficients to be empirically determined.  2.2.1. BRDF Variables Retrieval For the first sub-module of variables retrieval, the dependent variable (R) is calculated within the framework of RedEdge radiometric calibration applications. This is the bidirectional reflectance factor, defined as the ratio of the radiant flux reflected by a target surface to the radiant flux from an ideal diffusive surface of the identical geometry [9]. Illustrated by Figure 3, the reflectance factor (R) of an infinitesimal surface (A) under incoming radiance (Etarget) from any view direction (

V
→

) is expressed by Equation (2). For reading convenience, the proof of involving RedEdge computed at-sensor-radiance (L) and at target irradiance (Etarget) in this equation is attached in Appendix A.




R

(


θ
i




,
θ


v


,
φ


)


 
=
 
π


L


E

target







 
=
 
π
f


r


(
θ
i
,
 
φ
i
,
 
θ
v
,
 
φ
v
)





(2)


where:R—the observed reflectance factor; L—at-sensor-radiance (W/m2); Etarget—the irradiance (W/m2) at the target surface A;


f
r


(
θ
i
,
 
φ
i
,
 
θ
v
,
 
φ
v
)


—the BRDF that takes the parameters of incoming zenith and azimuth θi, φi, and view zenith and azimuth θv, φv. To reduce one degree of freedom, the function is normally written as


f
r


(
θ
i
,
 
θ
v
,
 
φ
)


, where φ is the relative azimuth of φv counterclockwise rotated from φi. Here, this function is the Walthall model in Equation (1). The second sub-module retrieves the geometric variables at the fine optical level. Within an FOV as illustrated by Figure 4a, an individual sun-target-sensor geometry can be established for each instantaneous field of view (IFOV), or pixel. This pixel depicts a “fine” surface section from a canopy. Corresponding to a directional reflectance value, the geometry contains three vectors: a solar incident vector


I
→


, a surface normal vector


N
→


 that describes the “fine” surface, and a view vector


V
→


 that describes the viewing angle of the pixel. Given the three vectors, the geometric parameters required by the Walthall model can be easily decomposed as illustrated by the workflow in Figure 4b. As the first element of the three vectors, unit solar incident (


I
→


) data cubes are computed. When provided with GPS and timestamp of one image, a unit solar incident can be computed in the north-east-down (NED) coordinate system by PySolar packages [33]. The vector is then rotated back to the RedEdge image perspective coordinates by the image exterior orientations latter computed from Metashape. For each pixel within an image,

I
→

 is the same.For the second element, the 3D reconstruction workflow of Metashape was implemented to compute a surface model. A typical workflow generally consists of the following steps:
A robust scale-invariant feature and match detection on image pairs. In this case, the direct reflectance images were used, and the RedEdge band was used as the master-band, due to its vegetation sensitivity.A sparse point cloud generation that computes the robust matching features in the 3D coordinates and aligns the camera extrinsic via structure from motion (SfM) methods.Depth maps generation on fine elements (on a downscaling of four pixels) by stereo pairs.A dense point cloud generation that computes the fine elements on the 3D coordinates based on their depth.Since the dense points do not have the image resolution, the mesh generation triangular irregular network (TIN) interpolates the dense points to the pixel level in the 3D coordinates to fill the gaps. In Figure 5, this mesh is visually described by the TIN faces (the violet 3D surface map on the left and the colorful micro triangle surfaces on the right) and numerically described by the unit surface normal vectors.Rendered by the FOVs from the mesh, the

N
→

 data cubes for all the images are thus reached.Illustrated in Figure 6, the remaining view vector

V
→

 from a pixel (x, y) to the focal point (O) within the RedEdge image perspective coordinates is defined by Equation (3), which needs to be scaled to a unit vector afterward. The view vector is computed from an undistorted image, where

V
→

 travels in a straight line. For one camera (i.e., red or NIR), the

V
→

 data cube is always the same in the image perspective coordinates, which is determined by the camera intrinsics.





V
→

=
(
(

x
0

−
x
)
p
,
 
 
 
(

y
0

−
y
)
p
,
−
f
)




(3)


where:(x, y)—the location of the pixel in RedEdge image perspective coordinates;(x0, y0)—the location of the principal point in RedEdge image perspective coordinates computed from Metashape;p—the pixel size;f—the focal length of each camera. When computing all the vectors of interest under the RedEdge image perspective coordinates, the procedure favored the optical parameters (e.g., principal point location and distortion parameters) and camera exterior orientation computed by Metashape. The reason is that vector geometries between 3D and 2D should be as aligned as possible, where optimized intrinsic and extrinsic are preferred over the original ones. Also, since the gimbal did not carry an inertial measurement unit (IMU), camera exterior orientations had to be computed by Metashape.  2.2.2. BRDF SamplingAfter computing the necessary variables, every single pixel became a possible BRDF study object. Nine hundred and seventy-two camera shots in one flight made over one billion pixels (972 shots * 960 rows * 1280 columns) available for anisotropy study, which was a very large dataset. For sampling, this study narrowed down to red and near-infrared (NIR) bands for NDVI analysis. Large quantities of points were manually allocated to the reflectance–normal image pairs (Figure 7). With the assistance of normal maps, the manual sampling has the following visual criteria: (1) a point falls on a vine canopy; (2) that canopy has not been affected by disease or drought or is severely shadowed (most representative for the study area canopy majority); (3) the geometries of points should be as diverse as possible (to ensure the anisotropy variety).Six images were selected for both red and NIR bands as the training dataset. Then, 4856 points were sampled for red, and 3386 points were sampled for NIR. Three images were selected as the validation dataset, with 1390 points sampled for red, and 1373 for NIR. The difference in point number was due to the manual points allocation, where “confident sampling” differed from the human judgment on the individual image of each band. To sample the directional reflectance, a 3 x 3 pixel window was center-located at a point and took the average value from this window. To sample the geometric variables, the points directly took the pixel values that were decomposed from the three-image vector-data cubes (

I
→

,

N
→

, and

V
→

) via the method described in Figure 4b. The reason for this sampling method is that the computed normal map is spatially continuous, while the direct reflectance image representing the real world is not; thus, it needs to be kernel smoothed.The training and validation images were evenly distributed during the flight (Figure 8), without a sudden change of sunlight.  2.2.3. BRDF Modeling and CorrectionAfter sampling, the three geometric variables θi, θv, and φ, along with the R from the training dataset, were imported into the Walthall model in Equation (1), as multilinear regression, to empirically determine the four coefficients. To correct the angular dependency, the observed direct reflectance was pixel-wise multiplied by the anisotropy correction factor



anc



(


θ
i




,
θ


v


,
φ


)




, which is defined by the nadir reflectance to the predicted reflectance under the same incident zenith in Equation (4) [26,34]:




R

corrected





 
=
 
R



observed



(




R


(


θ
i


,
0
,
0


)






R


(


θ
i




,
θ


v


,
φ


)






)




 
=
 
R



observed


·


anc



(


θ
i




,
θ


v


,
φ


)







(4)

 2.3. Result Analysis 2.3.1. Sample and Model VisualizationFirst, the results of BRDF sampling and modeling were illustrated in the polar plots of six incident zenith classes. The anisotropy was classified into 1296 angle classes (6 incident zenith x 24 relative azimuth x 9 view zenith). The values that fell in every incident zenith range class were displayed on a single polar plot, where the incident light comes from the azimuth 0° (right) and an incident zenith range is shown on the subtitle. On this plot, the view zenith increases outward from the center (each circle is 10°), and the relative azimuth increases counterclockwise from the right (each grid is 15°). Due to the fine angular resolution, a grid displays only the median of the corresponding values within an angle class.The sampling was illustrated by the reflectance, while the modeling result was illustrated by the anisotropy factor anif =

 



1
/
anc




(


θ
i




,
θ


v


,
φ


)




 for literature comparison. 2.3.2. Prediction Assessment on Validation PointsAfter the polar visualization of the trained models, the prediction accuracy was assessed by the validation points. Due to the vegetation sensitivity difference in red and NIR bands, the vine canopy reflectance varied in significantly different ranges (red 0.01~0.04, NIR 0.6~0.8). To analyze the accuracy in a similar scope, root-relative square errors (RRSEs) were first computed between the predicted and observed reflectance, along with RMSEs on the validation points. A result close to 0 indicates good performance. Then the observed and predicted reflectance was scatter plotted for the validation points. A converged shape of points on slope = 1 indicates an ideal prediction. Due to the large number of values and the canopy complexity, a certain level of errors was unavoidable. Therefore, reflectance error threshold lines were arranged beside the slope = 1. They were set as ±0.01 for the red band and ±0.1 for the NIR, which make the common spectral error range for vegetation remote sensing. 2.3.3. Correction Assessment on Validation PointsAfter angular correction, the performance was first assessed on the validation points. Based on the results of modeling, the backscattering effect dominated the vine canopy directional reflectance, where the reflectance increases by the decreasing view-incident angle. To illustrate this angular dependency and the independency after correction, reflectance and view-incident angles were scatter plotted for the validation points, without considering the surface normal (for 2D plot illustration simplification). Again, due to the large quantities of points, a single p-value from reflectance-angle linear regression always tends to be significant. Therefore, a p-series analysis was conducted. An increasing number of points (once every +10 points) were randomly pair-selected from both corrected and uncorrected validation points, where a p-value for the reflectance-angle regression was separately computed. Then the p-values were plotted against the number of the points involved in the regression. In this pair, the later the low p-value appears in the correction series, the more successfully the angular correction performs.  2.3.4. Correction Assessment on NDVI OrthomosaicsBoth corrected and uncorrected orthomosaics were computed from two different view directions. Since the angular correction was uniformly performed on all pixels, all non-vine pixels were “corrected” in the final orthomosaics. To focus on the vine canopies, a supervised maximum likelihood classification in ArcGIS was implemented on the uncorrected orthomosaic to form an effective canopy mask first. The classification steps are: (1) major thematic class mask selection (vine, soil, shadow, car, etc.) on an orthomosaic in Figure 9a; (2) inputting the 5 bands’ spectral signature of these masks to the algorithm for classifier training; (3) applying the classifier on the whole orthomosaic to form the class-map in Figure 9b; (4) converting the class map to a binary mask for vine and non-vine canopy in Figure 9c.The method yielded an acceptable canopy mask. For instance, in Figure 9c,d, the two user-defined references concluded that 2976 out of 3046 pixels were correct in a vine reference, and 1173 out of 1342 pixels were correct in a no-vine reference. The kappa statistics was therefore 86.9%, yielding satisfying accuracy. In Figure 9d’s direct illustration, the only false classified targets are the dense grasses that stand very close to the vine canopy bottom. On each view direction, the canopy mask retrieved the red and NIR data cube from both the corrected and uncorrected orthomosaics, resulting in 4 NDVI maps (uncorrected vs corrected on 2 different angles). Four boxes were put on four canopy rows to extract values from these NDVI maps. The median value differentials between the two directions were computed on each box for both the uncorrected and corrected. This was also computed for all the pixels in the NDVI maps. The median differential from the corrected should be smaller than the uncorrected, due to angular independence.  3. Results 3.1. Sample and Model VisualizationFigure 10 shows that the sampled reflectance of three incident range classes for both the red and NIR bands. Generally, the angular dependency has similar patterns for red and NIR reflectance, with brighter backscattering reflectance and darker forward-scattering. The brightness is most pronounced in the small incident zenith backscattering direction (Figure 10a,b). As the incident angle increased, the number of reflectance samples in the forward-scattering (the left part in a polar plot) decreased. Figure 11 illustrates the anisotropy factor development from low to high sun incident zenith for red and NIR bands. Both bands showed backscattering effects, with the increasing anisotropy factor toward the solar incident direction. The red anisotropy factor spread wider than NIR in the large incident zenith.  3.2. Prediction Assessment on Validation PointsAs illustrated by Table 2 and Table 3, although the NIR RMSE was larger than the red, the RRSE suggests that NIR model prediction was comparatively better than the red model, when values were standardized by the original observations.Illustrated by Figure 12a, 47.63% of the red points were accurately predicted within ±0.01 prediction errors. In Figure 12b, 58.27% of the NIR points were accurate within ±0.1 prediction errors. This proved the RRSE statement before. The R2 of prediction to observation upon slope = 1 is –1.01 for red and –0.36 for NIR, expressing no linearity in both cases. Excluding the points that are outside of the error ranges, the R2 is increased to 0.41 for red and to 0.52 for NIR. Judged from the observation–prediction point distribution, underestimations are observed in both red and NIR predictions.  3.3. Correction Assessment on Validation PointsIn Figure 13, the reflectance-angle slope is flattened after angular correction. The p-series analysis suggests that the red reflectance is completely angular independent for the corrected validation points.Similarly, in Figure 14, the reflectance-angle slope has also been flattened, but there is still a very low-level linear relation. Also, after involving more than 210 validation points, the significance of the angular dependency showed up again. This indicates that the angular effect was not completely removed from the NIR band.The anisotropy correction factor


anc

(


θ
i

,

θ
v

,
φ

)



 in Figure 15c illustrates the spatial distribution of correction factors when the model was image-wise applied to all the canopy pixels.For the canopy reflectance in this image, 15.9% of the pixels are reduced by more than 10%, while 13.3% of them are increased by more than 10%, and the majority of correction factors are around 1 (no change). The decreased reflectance pixels (purple) locate in the upper and bottom edges; the increased (red) are mainly in the right bottom corner, and the unchanged (cyan) dominate the central left down. 3.4. Correction Assessment on NDVI OrthomosaicsFigure 16 and Figure 17 illustrate the corrected NDVI orthomosaics from two different viewing directions in locally defined coordinate systems, with the canopy row boxes displayed on the zoomed-in images. Compared with the first direction, the second direction was closer to nadir. Aside from the shape, the depicting canopy section sizes (pixel number) are also different. Canopy-viewing sections from the first direction are larger than those in the second. The medians of the directional NDVI differentials are summarized in Table 4. The uncorrected directional NDVIs were actually smaller in Box 3, Box4, and also the whole NDVI map.  4. Discussion The proposed procedure was evaluated by the sample- and model-result visualization, the prediction and correction assessment on validation points, and finally the correction performance assessment on NDVI orthomosaics. In Figure 10c–f, no forward-scattering reflectance could be sampled when incident zenith was large. This was limited by the actual flight scenario. For instance, when the high sun elevation and steep canopy surface formed a large incident angle on the fine surface (3) illustrated in Figure 3, the forward directions were downward; thus, they could not be captured by the UAV mounted camera.The model visualization in Figure 11 has yielded expected results. Figure 11c has a similar pattern as illustrated by an RPV model simulated red reflectance on black spruce [9] under incident sun zenith 30°. Figure 11e,f also shares similar patterns with the Walthall model derived anisotropy on winter wheat, under incident sun zenith 39.8° [26]. This proves the procedure’s functionality at the methodology level.As for the prediction, this study reached 0.64 and 0.52 (red R2 = 0.41 and NIR R2 = 0.52) for vine canopy, when excluding the points out of the accepting error ranges. Compared with the high correlation coefficients (0.87~0.94 for red, 0.83~0.93 for NIR) on a spectralon [23] obtained under a clear sky, there is a limitation. Nevertheless, given the complexity of a canopy and the amounts of the points, the results showed a reasonable level of prediction accuracy. For both the red and NIR bands, a certain level of underestimation was observed. The most likely explanation is the micro shadow variation involved in the training procedure. One of the major advantages of high-resolution UAV images is the canopy structure details, but this also enlarges the observable heterogeneity where the tiny shadow variations are visible on the image. Therefore, the training data could include directional reflectance that was affected by the shadows. In the red band, the major absorption in the wavelength reduced the vegetation red reflectance to a very low scope, leading to the confusion between real vegetation reflectance and shadows. Meanwhile, this problem was further complicated by the high reflectance-vegetation sensitivity in the NIR. The hidden layer beneath the canopy surface that increased NIR reflectance, which could not be distinguished by a feature-detection-based 3D computation, was further combined with this micro-shadow effect. In short, when variant reflectance was corresponding to the same geometries, errors were produced. Indeed, the vegetation detail features, such as LAI, can have major impacts on the canopy bidirectional effect, as other studies have shown [22,23]. Another possible limitation is from the 3D surface model, which causes the inaccuracy of inputting geometric variables. Although this can be improved by computing the dense points of higher resolution, the time cost will be increased. Also, the detail of the computable dense points is limited by the detectable contrast in an image, which is namely linked with the vegetation sensitivity of the sensor itself.Accordingly, it was also proved on the validation points that the angular dependency is removed from red and partially removed from NIR.However, when magnifying from points to map level, the hypothesis could not be completely proved on the NDVI orthomosaics. It was originally assumed that the procedure could correct the angular dependency at fine resolution level purely from the canopy out-surface structures, yet the changing view directions also corresponded to the changing of relative canopy features to the sensor, such as hidden layer thickness and leaf orientation. The variation of these unknown features, combined together with micro shadows, could have frustrated the results. There are several points in this procedure that could be proved. The first is the sampling procedure. Instead of the human selection on the individual bands, a highly accurate band-alignment algorithm could help the retrieval of sampling points from the same real-world location for each band. Also, an NDVI mask could be created on this alignment to assist the point sampling, avoiding the micro shadows to a certain scope. Further image-based methods to derive more detailed vine canopy features (LAI, leaf orientation, etc.) with other BRDF modeling tools are expected to improve the functionalities.  5. ConclusionsThis paper has illustrated an example workflow for a tilted MicaSense© RedEdge-M multispectral sensor to sample, model, and correct the vine canopy angular reflectance in a sloped area. This method utilized the sensor’s own radiometric package to derive the directional reflectance, computed the canopy out-surface geometries by Agisoft© Metashape, and empirically established the Walthall BRDF model. The study showed that such methods could sample and model the angular dependency without ground measurements. The validation points showed that an empirically established model could achieve prediction accuracy to RRSE 1.42 in the red band and to RRSE 1.17 in the NIR band. This means the procedure can be useful for modeling bidirectional reflectance captured by this sensor under any other circumstances.The correction showed certain effectiveness on validation points, but could not on NDVI orthomosaics. This indicates that the proposed procedure can function on a coarser resolution, but cannot be applied on a fine-resolution orthomosaics for inhomogeneous vegetation such as vine canopies.
