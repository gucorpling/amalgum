 1. IntroductionRelative reduction [1] of covering information system refers to reducing extra covering within a family of covering while keeping the characterization ability of knowledge. Knowledge reduction [2] is one of the most important research subjects for both theoretical development and application. It has been widely used in the field of artificial intelligence [3], pattern recognition [4] and machine learning [5].Rough set [6], an important mathematical tool of granular computing [7], provides an effective method of knowledge discovery [8] and knowledge reduction. Covering rough set [9] and multigranulation rough set are two special models dealing with the real data sets when overlapping and multiple knowledge are involved. Many researchers have studied the two models, especially their hybrid model, covering multigranulation rough set [10]. Qian et al. [11] proposed multigranulation rough sets (MGRS) based on multiple equivalence relation. Liu and Miao [12] introduced four types of covering MGRS models in the covering approximation space. Xu et al. further weakened the equivalence relation, and proposed covering MGRS based on order relation [13], generalized relation [14] and fuzzy compatibility relation [15]. In real life, different MGRS models will be generated according to the needs of different data sets, for example, variable-precision MGRS model. Dou et al. [16] first proposed the variable-precision MGRS model and explored its properties. Ju et al. [17] introduced the model of the variable-precision MGRS, and presented a heuristic algorithm for computing reduction of variable-precision MGRS. Feng et al. [18] proposed Type-1 variable-precision multigranulation decision-theoretic fuzzy rough set based on three-way decisions.The Dempser-Shafer evidence theory [19,20] is based on a basic probability distribution, i.e., a mass function, and uses the belief and plausibility functions derived from the mass function to describe the uncertainty of evidence. There is a strong connection between rough set and evidence theory. Many scholars combined rough set with evidence theory to investigate the uncertainty measures and knowledge representation. Yao et al. [21] indicated that the belief and plausibility functions can be derived by the lower and upper approximation operators in rough set theory. Wu et al. [22] combined the belief structure with the rough approximation space and investigated knowledge reductions of rough sets based on evidence theory. Xu et al. [23] employed belief and plausibility functions to describe the attribute reductions of ordered information systems. Chen et al. [24] associated evidence theory with neighborhood-covering rough set, and discussed the connection between a pair of covering approximation operator and belief and plausibility functions. They did not consider covering rough set with MGRSs. Zhang et al. [25] explored the attribute reductions of neighborhood-covering rough set in the covering decision information systems. Tan et al. [26] employed the evidence theory to discuss the numerical characterization of multigranulation rough sets in incomplete information system, and developed an attribute reduction algorithm based on evidence theory. They did not consider the relation under general covering. Che et al. [27] used evidence theory to characterize the numerical characterization of multigranulation rough sets in a multi-source covering information system.However, covering MGRS have been rarely considered in the reduction theory by using evidence theory. This brings limitations for the applications of rough set theory in dealing with the data which are usually formalized to multiple coverings. To address this issue, we in this paper aim to measure the approximations of covering MGRS and characterize the reductions of covering MGRS by the belief and plausibility functions. Based on these studies, the relationship between covering MGRS and evidence theory is established, and fusion methods are generated for uncertainty measurement in information systems.In this paper, the relative reduction of neighborhood-covering pessimistic multigranulation rough set is investigated by using belief and plausibility function based on evidence theory. First, the lower and upper approximations of multigranulation rough set in neighborhood-covering information systems are introduced. Second, the belief and plausibility functions from evidence theory are employed to characterize the approximations of neighborhood-covering multigranulation rough set. The relative reduction of neighborhood-covering information system is then investigated. Finally, an algorithm for computing a relative reduction of neighborhood-covering pessimistic multigranulation rough set is proposed according to the significance of coverings defined by the belief function, and its validity is examined by a practical example. 2. PreliminariesIn this section, we review some basic concepts related to covering rough sets, multigranulation rough sets and evidence theory. More details can be found in [9,11,19,20,28,29]. 2.1. Covering Rough Set Based on NeighborhoodDefinition 1.  [9] Let U be a universe and C be a family of subsets of U. If no subsets in C are empty and

⋃
C
=
U

, then C is called a covering of U. The ordered pair

(
U
,
C
)

 is called a covering approximation space.One can see that a partition of U is certainly a covering of U.Definition 2. (Neighborhood). [9] Let U be a universe and C be a covering of U. For

x
∈
U

, denote



(
x
)

C

=
⋂

{
K
∈
C
∣
x
∈
K
}


 as the neighborhood of x regarding C.Definition 3. (neighborhood-covering information systems). [28] Let

(
U
,
C
)

 be a covering approximation space. For

X
⊆
U

,

N
=
{


(
x
)

C

∣
x
∈
U
}

, we call

(
U
,
N
)

 the neighborhood-covering information systems induced by

(
U
,
C
)

.Definition 4.  [9] Let

(
U
,
C
)

 be a covering approximation space. A pair of approximation operators

(

C
¯

,

C
̲

)

 is defined as: for

X
⊆
U

,





C
̲


(
X
)

=

{
x
∈
U
∣


(
x
)

C

⊆
X
}

,





(1)







C
¯


(
X
)

=

{
x
∈
U
∣


(
x
)

C

∩
X
≠
∅
}

.





(2)

 2.2. Multigranulation ApproximationsIn this subsection, we introduce the multigranulation approximations of MGRS described in [11].Definition 5.  [11] Let

(
U
,
A
)

 be an information system, U be a universe and A be a set of attributes,


A
1

,

A
2

,
…
,

A
m

⊆
A

 and

X
⊆
U

. The optimistic multigranulation lower and upper approximations of X regarding


A
1

,

A
2

,
…
,

A
m


 are denoted by




∑

i
=
1

m


A

i

O


̲


(
X
)


 and




∑

i
=
1

m


A

i

O


¯


(
X
)


, respectively, where







∑

i
=
1

m


A

i

O


̲


(
X
)

=

{
x
∈
U
∣


[
x
]


A
1


⊆
X
∨


[
x
]


A
2


⊆
X
∨
…
∨


[
x
]


A
m


⊆
X
}

,





(3)









∑

i
=
1

m


A

i

O


¯


(
X
)

=

{
x
∈
U
∣


[
x
]


A
1


⋂
X
≠
∅
∧


[
x
]


A
2


⋂
X
≠
∅
∧
…
∧


[
x
]


A
m


⋂
X
≠
∅
}

,





(4)

It is easy to see that the optimistic multigranulation lower and upper approximations are dual, i.e.,




∑

i
=
1

m


A

i

O


¯


(
X
)

=
∼



∑

i
=
1

m


A

i

O


̲


(
∼
X
)


, where ∼X is the complement of X.Definition 6.  [11] Let

(
U
,
A
)

 be an information system, U be a universe and A be a set of attributes,


A
1

,

A
2

,
…
,

A
m

⊆
A

 and

X
⊆
U

. The pessimistic multigranulation lower and upper approximations of X regarding


A
1

,

A
2

,
…
,

A
m


 are denoted by




∑

i
=
1

m


A

i

P


̲


(
X
)


 and




∑

i
=
1

m


A

i

P


¯


(
X
)


, respectively, where







∑

i
=
1

m


A

i

P


̲


(
X
)

=

{
x
∈
U
∣


[
x
]


A
1


⊆
X
∧


[
x
]


A
2


⊆
X
∧
…
∧


[
x
]


A
m


⊆
X
}

,





(5)









∑

i
=
1

m


A

i

P


¯


(
X
)

=

{
x
∈
U
∣


[
x
]


A
1


⋂
X
≠
∅
∨


[
x
]


A
2


⋂
X
≠
∅
∨
…
∨


[
x
]


A
m


⋂
X
≠
∅
}

,





(6)

It is easy to see that the pessimistic multigranulation lower and upper approximations are dual, i.e.,




∑

i
=
1

m


A

i

P


¯


(
X
)

=
∼



∑

i
=
1

m


A

i

P


̲


(
∼
X
)


, where ∼X is the complement of X. 2.3. Evidence TheoryFirst, we review some basic concepts from Dempster-Shafer evidence theory. More details can be found in [19,20].Definition 7. (mass function). [19,20] Let U be a universe. A set function

m
:

2
U

→

[
0
,
1
]


 (

2
U

 is the power set of U) in the following is referred to as a basic probability assignment (mass function) if it satisfies the following conditions:(1)

m
(
∅
)
=
0

,(2)


∑

X
⊆
U


m

(
X
)

=
1

.

m
(
X
)

 denotes the trust values of the evidence in X. If

m
(
X
)
≠
0

, we call X a focal element of m. Let M be the union of all focal elements, and M is called the core. The pair

(
M
,
m
)

 is called a belief structure on U. The belief and plausibility functions can be derived based on the belief structure.Definition 8.  [19,20] Let U be a universe and

m
:

2
U

→

[
0
,
1
]


 be a basic probability assignment. We can export the following:A set function

B
e
l
:

2
U

→

[
0
,
1
]


 is referred to as a belief function on U if




B
e
l

(
X
)

=

∑


X


′


⊆
X


m

(

X


′


)

,
∀
X
⊆

2
U

.





(7)

A set function is referred to as a plausibility function on if




P
l

(
X
)

=

∑


X


′


⋂
X
≠
∅


m

(

X


′


)

,
∀
X
⊆

2
U

.





(8)

It is easy to see that the belief function and the plausibility function are dual, i.e.,

B
e
l
(
X
)
=
∼
P
l
(
∼
X
)

, where ∼X is the complement of X.In addition, a belief function also satisfies the following:(1)

B
e
l
(
∅
)
=
0

,(2)

B
e
l
(
U
)
=
1

,(3) ∀


X
1

,

X
2

,
…
,

X
m

⊆
U
,
B
e
l

(

⋃

i
=
1

m


X
i

)

≥

∑

J
⊆
{
1
,
2
,
…
,
m
}




(
−
1
)


|
J
|
+
1


B
e
l

(

⋂

i
∈
J



X
i

)

.

Next, we review some basic concepts from Smets evidence theory. More details can be found in [29].Definition 9. (mass function). [29] The function

m
:

2
U

→

[
0
,
1
]


 (

2
U

 is the power set of U) is called a basic belief assignment (bba) and the m values are called the basic belief mass (bbm), with:





∑

X
⊆
U


m

(
X
)

=
1
.





(9)

Definition 10.  [29] Based on the bbm, the functiona bel(X) and pl(X) are defined for

X
⊆
U

 by:




B
e
l

(
X
)

=

∑

∅
≠

X


′


⊆
X


m

(

X


′


)

,





(10)






P
l

(
X
)

=

∑


X


′


⋂
X
≠
∅


m

(

X


′


)

.





(11)

Hence

p
l

(
X
)

=
b
e
l

(
U
)

−
b
e
l

(

X
¯

)


,

b
e
l
(
U
)
=
p
l
(
U
)
=
1
−
m
(
∅
)
≤
1

, and

m
(
∅
)
>
0

.In this article, the mass function is used to measure the mass of a set. The set we want to measure is the power set of U. The set for measures, ordered from the smallest to the largest, is from the empty set to the universal set. To do this, we picked numbers from zero to one to measure from the empty set to the universal set. Therefore, Dempster’s evidence theory is relatively suitable for the measurement of sets in this paper. 3. Neighborhood-Covering Multigranulation Rough SetLet the pair

(
U
,
C
)

 denote a covering information system, where

U
=
{

x
1

,

x
2

,
…
,

x
n

}

 is a nonempty, finite set of objects called the universe of discourse, and

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 is a family of coverings of U.Definition 11.  [10] Let

(
U
,
C
)

 be a covering information system, U be a universe and

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 is a family of covering on U. For

X
⊆
U

, the neighborhood-covering optimistic multigranulation lower and upper approximations of X regarding


C
1

,

C
2

,
…
,

C
m


 are denoted by




∑

i
=
1

m


C

i

O


̲


(
X
)


 and




∑

i
=
1

m


C

i

O


¯


(
X
)


, respectively, where




∑

i
=
1

m


C

i

O


̲


(
X
)

=

{
x
∈
U
∣


(
x
)


C
1


⊆
X
∨


(
x
)


C
2


⊆
X
∨
…
∨


(
x
)


C
m


⊆
X
}


,





∑

i
=
1

m


C

i

O


¯


(
X
)

=

{
x
∈
U
∣


(
x
)


C
1


⋂
X
≠
∅
∧


(
x
)


C
2


⋂
X
≠
∅
∧
…
∧


(
x
)


C
m


⋂
X
≠
∅
}



It is easy to see that the neighborhood-covering optimistic multigranulation lower and upper approximations are dual, i.e.,




∑

i
=
1

m


C

i

O


¯


(
X
)

=
∼



∑

i
=
1

m


C

i

O


̲


(
∼
X
)


, where ∼X is the complement of X.Definition 12.  [10] Let

(
U
,
C
)

 be a covering information system, U be a universe and

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 is a family of covering on U. For

X
⊆
U

, the neighborhood-covering pessimistic multigranulation lower and upper approximations of X regarding


C
1

,

C
2

,
…
,

C
m


 are denoted by




∑

i
=
1

m


C

i

P


̲


(
X
)


 and




∑

i
=
1

m


C

i

P


¯


(
X
)


, respectively, where





∑

i
=
1

m


C

i

P


̲


(
X
)

=

{
x
∈
U
∣


(
x
)


C
1


⊆
X
∧


(
x
)


C
2


⊆
X
∧
…
∧


(
x
)


C
m


⊆
X
}

,








∑

i
=
1

m


C

i

P


¯


(
X
)

=

{
x
∈
U
∣


(
x
)


C
1


⋂
X
≠
∅
∨


(
x
)


C
2


⋂
X
≠
∅
∨
…
∨


(
x
)


C
m


⋂
X
≠
∅
}

,


It is easy to see that the neighborhood-covering pessimistic multigranulation lower and upper approximations are dual, i.e.,




∑

i
=
1

m


C

i

P


¯


(
X
)

=
∼



∑

i
=
1

m


C

i

P


̲


(
∼
X
)


, where ∼X is the complement of X.Lemma 1.  [10] Let

(
U
,
C
)

 be a covering information system, U be a universe and

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 be a family of coverings of U. The following property holds, for

X
,

Y
⊆
U

,(1)




∑

i
=
1

m


C

i

O


̲


(
X
)

⊆
X
⊆



∑

i
=
1

m


C

i

O


¯


(
X
)


,




∑

i
=
1

m


C

i

P


̲


(
X
)

⊆
X
⊆



∑

i
=
1

m


C

i

P


¯


(
X
)


;(2) If

X
⊆
Y

, then




∑

i
=
1

m


C

i

O


¯


(
X
)

⊆



∑

i
=
1

m


C

i

O


¯


(
Y
)


,




∑

i
=
1

m


C

i

O


̲


(
X
)

⊆



∑

i
=
1

m


C

i

O


̲


(
Y
)


;(3) If

X
⊆
Y

, then




∑

i
=
1

m


C

i

P


¯


(
X
)

⊆



∑

i
=
1

m


C

i

P


¯


(
Y
)


,




∑

i
=
1

m


C

i

P


̲


(
X
)

⊆



∑

i
=
1

m


C

i

P


̲


(
Y
)


;(4)




∑

i
=
1

m


C

i

O


̲


(
X
⋂
Y
)

⊆



∑

i
=
1

m


C

i

O


̲


(
X
)

⋂



∑

i
=
1

m


C

i

O


̲


(
Y
)


,




∑

i
=
1

m


C

i

O


¯


(
X
⋃
Y
)

⊇



∑

i
=
1

m


C

i

O


¯


(
X
)

⋃



∑

i
=
1

m


C

i

O


¯


(
Y
)


;(5)




∑

i
=
1

m


C

i

P


̲


(
X
⋂
Y
)

⊆



∑

i
=
1

m


C

i

P


̲


(
X
)

⋂



∑

i
=
1

m


C

i

P


̲


(
Y
)


,




∑

i
=
1

m


C

i

P


¯


(
X
⋃
Y
)

⊇



∑

i
=
1

m


C

i

P


¯


(
X
)

⋃



∑

i
=
1

m


C

i

P


¯


(
Y
)


;(6)




∑

i
=
1

m


C

i

O


̲


(
X
⋃
Y
)

⊇



∑

i
=
1

m


C

i

O


̲


(
X
)

⋃



∑

i
=
1

m


C

i

O


̲


(
Y
)


,




∑

i
=
1

m


C

i

O


¯


(
X
⋂
Y
)

⊆



∑

i
=
1

m


C

i

O


¯


(
X
)

⋂



∑

i
=
1

m


C

i

O


¯


(
Y
)


;(7)




∑

i
=
1

m


C

i

P


̲


(
X
⋃
Y
)

⊇



∑

i
=
1

m


C

i

P


̲


(
X
)

⋃



∑

i
=
1

m


C

i

P


̲


(
Y
)


,




∑

i
=
1

m


C

i

P


¯


(
X
⋂
Y
)

⊆



∑

i
=
1

m


C

i

P


¯


(
X
)

⋂



∑

i
=
1

m


C

i

P


¯


(
Y
)


.Example 1. Let

(
U
,
C
)

 be a covering information system,

U
=
{

x
1

,

x
2

,

x
3

,

x
4

}

 be a universe and

C
=
{

C
1

,

C
2

}

 be a family of coverings of U. For

X
⊆
Y

,

X
=
{

x
1

,

x
2

,

x
3

}

, and





C
1

=

{

{

x
1

,

x
2

}

,

{

x
3

}

,

{

x
4

}

}

,


C
2

=

{

{

x
1

,

x
2

}

,

{

x
1

,

x
3

,

x
4

}

,

{

x
3

}

}

.






According to Definition 2.2, we can calculate



(

x
1

)


C
1


=


(

x
2

)


C
1


=

{

x
1

,

x
2

}

,



(

x
3

)


C
1


=

{

x
3

}

,



(

x
4

)


C
1


=

{

x
4

}


;



(

x
1

)


C
2


=

{

x
1

}

,



(

x
2

)


C
2


=

{

x
2

}

,



(

x
3

)


C
2


=

{

x
3

}

,



(

x
4

)


C
2


=

{

x
1

,

x
3

,

x
4

}


.According to Definitions 3.1, we can calculate the neighborhood-covering optimistic multigranulation lower and upper approximations of X regarding


C
1

,

C
2


 as the following:







∑

i
=
1

2


C

i

O


̲


(
X
)

=

{

x
1

,

x
2

,

x
3

}

,




∑

i
=
1

2


C

i

O


¯


(
X
)

=

{

x
1

,

x
2

,

x
3

}


.






According to Definitions 3.2, we can calculate the neighborhood-covering pessimistic multigranulation lower and upper approximations of X regarding


C
1

,

C
2


 as the following:







∑

i
=
1

2


C

i

P


̲


(
X
)

=

{

x
1

,

x
2

,

x
3

}

,




∑

i
=
1

2


C

i

P


¯


(
X
)

=
U

.






 4. The Belief Structure of Neighborhood-Covering Multigranulation Rough SetNext, we use the belief and plausibility function to analyze the belief structure of the neighborhood-covering multigranulation rough set. Tan et al. [26] pointed out that only the pessimistic multigranulation rough sets have the belief structure. Therefore, we only discuss the belief structure of neighborhood-covering pessimistic multigranulation rough sets.Let P be an average probability distribution, i.e.,

P

(
X
)

=


|
X
|


|
U
|



 for

X
⊆
U

, where

∣
·
∣

 denotes the cardinality of a set.Chen et al. [24] stated that the neighborhood-covering single-granularity rough sets have the belief structure. If the covering C is single-granularity covering in the model of neighborhood-covering multigranulation rough set, then the neighborhood-covering multigranulation rough sets are reduced to neighborhood-covering single-granularity rough sets. This is a special case in the model of neighborhood-covering multigranulation rough set, where the belief and plausibility function can be employed to characterize the belief structure.Theorem 1.  [24] Let

(
U
,
C
)

 be a covering information system. If the covering C is single-granularity covering, then there is a belief structure such that for any

X
⊆
U

,




B
e
l

(
X
)

=
P

(


C

i

P

̲


(
X
)

)

,
P
l

(
X
)

=
P

(


C

i

P

¯


(
X
)

)

.





(12)

Then

B
e
l
(
X
)

 is a belief function on U, and

P
l
(
X
)

 is a plausibility function on U.Corollary 1.  [24] Let

(
U
,
C
)

 be a covering information system. If the covering C is single-granularity covering, then there is a belief structure such that for any

X
⊆
U

,




B
e
l

(
X
)

=



|



C

i

P

̲



(
X
)

|



|
U
|


,
P
l

(
X
)

=



|



C

i

P

¯



(
X
)

|



|
U
|


.





(13)

Then

B
e
l
(
X
)

 is a belief function on U, and

P
l
(
X
)

 is a plausibility function on U.However, whether the belief structure exists in the general case of neighborhood-covering multigranulation rough set needs to be further discussed. First, we use the union of sets and transform the neighborhood-covering pessimistic multigranulation rough set to the neighborhood-covering single-granulation rough set. Then we use the relationship partition function to establish the relationship between neighborhood-covering and partition, and transform the neighborhood-covering single-granulation rough set into the single-granulation classic rough set. Finally, we obtain the relationship between the evidence theory and neighborhood-covering multigranulation rough set.We use the following definition to transform the neighborhood-covering pessimistic multigranulation rough set to the neighborhood-covering single-granulation rough set.Definition 13. Let

(
U
,
C
)

 be a covering information system and

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 be a family of coverings of U. For

x
∈
U

,

▽

(
x
)

=
⋃
{


(
x
)


C
i


|
x
∈
U
}

 denotes a covering based on the covering family C w.r.t the neighborhood of x.The definition of covering in Definition 13 is the single-grain covering of U, therefore the pessimistic multigranulation rough set is transformed into the single-grain rough set. Next, we will define the relationship partitioning function, establish the relationship between covering and partition, and transform the covering rough set into the classic rough set.Theorem 2. Let U be a universe and C be a covering of U. For

x
∈
U

, we define the relationship partition function

f
:
C
→
U

,

f
(
x
)
=
{
x
∈
U
∣
X
=
▽
(
X
)
}

. Then

f
(
X
)

 is a partition of U.Proof of Theorem 2. First, we prove that

f

(
X
)

⋂
f

(

X
′

)

=
∅

, for

∀
X
,

X
′

⊆
U
,
X
≠

X
′


.Suppose there exists

x
∈
U

 such that

x
∈
f

(
X
)

⋂
f

(

X
′

)


. Then,

▽

(
x
)

=
X
=

X
′


, which contradicts with

X
≠

X
′


. Thus,

f

(
X
)

⋂
f

(

X
′

)

=
∅

, for

∀
X
,

X
′

⊆
U
,
X
≠

X
′


.Second, we prove that


⋃

X
⊆
U


f

(
X
)

=
U

. For any

x
∈
U

, we have

x
∈
f
(
▽
(
X
)
)

 and

▽
(
X
)
≠
∅
,
▽
(
X
)
⊆
U

. Hence,


⋃

X
⊆
U


f

(
X
)

=
U

.Therefore,

f
(
X
)

 is a partition of U. □Because of Theorem 2, we transform the covering rough set into the classic rough set. Yao et al. [21] showed that the belief and plausibility functions can be derived by the lower and upper approximation operators in rough set theory. So, the following theorem holds.Theorem 3. Let

(
U
,
C
)

 be a covering information system,

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 be a family of coverings of U. For any

X
⊆
U
,
x
∈
U

, a probability assignment function is

m
:

2
U

→

[
0
,
1
]


, and its definition is as follows:




m

(
X
)

=





P
(
f
(
x
)
)
,




X
=
▽
(
x
)
,






0
,




o
t
h
e
r
,










(14)

then the belief and plausibility function on U are




B
e
l

(
X
)

=
P
(



∑

i
=
1

m


C

i

P


̲


(
X
)

)
,





(15)






P
l

(
X
)

=
P
(



∑

i
=
1

m


C

i

P


¯


(
X
)

)
.





(16)

Proof of Theorem 3. By Theorem 2, we have


∑

X
⊆
U


m

(
X
)

=

∑

X
⊆
U


P

(
f

(
X
)

)

=

∑

X
⊆
U




|
f
(
X
)
|


|
U
|


=



∑

X
⊆
U



|
f

(
X
)

|



|
U
|


=



⋃

X
⊆
U



|
f

(
X
)

|



|
U
|


=


|
U
|


|
U
|


=
1

.The following proves

B
e
l

(
X
)

=
P
(

▽
̲


(
X
)

)

.We have:

B
e
l

(
X
)

=

∑


X
′

⊆
X


m

(

X
′

)

=

∑


X
′

⊆
X


P

(
f

(

X
′

)

)

=

∑


X
′

⊆
X




|
f
(

X
′

)
|


|
U
|


=



∑


X
′

⊆
X



|
f

(

X
′

)

|



|
U
|


=



⋃


X
′

⊆
X



|
f

(

X
′

)

|



|
U
|



.In addition,

f

(

X
′

)

=

{
x
∈
U
∣

X
′

=
▽

(
x
)

}


, and thus

x
∈

⋃


X
‘

⊆
X


f

(
X
)


 if and only if



(
x
)

c

⊆
X

.Furthermore, we can easily conclude that

x
∈

⋃


X
‘

⊆
X


f

(
X
)


⇔

x
∈



∑

i
=
1

m


C

i

P


̲


(
X
)


.Hence,


B
e
l

(
X
)

=



|


⋃


X
′

⊆
X


f

(

X
′

)


|



|
U
|


=



|




∑

i
=
1

m


C

i

P


̲



(
X
)

|



|
U
|


=
P

(



∑

i
=
1

m


C

i

P


̲


(
X
)

)



The proof of

P
l

(
X
)

=
P
(



∑

i
=
1

m


C

i

P


¯


(
X
)

)

 is similar. Thus, we can assert this conclusion. □Next, we will give a counterexample to illustrate that neighborhood-covering optimistic multigranulation rough set approximation cannot be characterized by evidence theory.Example 2. Let

(
U
,
C
)

 be a covering information system,

U
=
{

x
1

,

x
2

,

x
3

,

x
4

}

,

C
=
{

C
1

,

C
2

,

C
3

}

 be a family of coverings of U.


C
1

=

{

{

x
1

,

x
2

,

x
3

}

,

{

x
1

,

x
2

,

x
4

}

}


,


C
2

=

{

{

x
2

,

x
3

,

x
4

}

,

{

x
1

,
,

x
2

,

x
3

}

}


,


C
3

=

{

{

x
1

,

x
2

,

x
4

}

,

{

x
2

,

x
3

,

x
4

}

}


. Let


X
1

=

{

x
1

,

x
2

}


,


X
2

=

{

x
2

,

x
3

}


.We can calculate:




∑

i
=
1

3


C

i

O


̲


(

X
1

⋃

X
2

)

=

{

x
1

,

x
2

,

x
3

}


,




∑

i
=
1

3


C

i

O


̲


(

X
1

)

=

{

x
1

,

x
2

}


,




∑

i
=
1

3


C

i

O


̲


(

X
2

)

=

{

x
2

,

x
3

}


,




∑

i
=
1

3


C

i

O


̲


(

X
1

∩

X
2

)

=
∅

,Then


∣



∑

i
=
1

3


C

i

O


̲


(

X
1

⋃

X
2

)

∣
=
3
,





∣



∑

i
=
1

3


C

i

O


̲


(

X
1

)

∣
=
2
,





∣



∑

i
=
1

3


C

i

O


̲


(

X
2

)

∣
=
2
,





∣



∑

i
=
1

3


C

i

O


̲


(

X
1

∩

X
2

)

∣
=
0
.


We assume that

B
e
l

(
X
)

=

1
4

∣



∑

i
=
1

3


C

i

O


̲


(
X
)

∣

 for all

X
⊆
U

.Therefore,

B
e
l

(

X
1

⋃

X
2

)

=

3
4


,

B
e
l

(

X
1

)

=

2
4


,

B
e
l

(

X
2

)

=

2
4


,

B
e
l
(

X
1

∩

X
2

)
=
0

.As we know, a belief function satisfies the following:∀


X
1

,

X
2

,
…
,

X
m

⊆
U
,
B
e
l

(

⋃

i
=
1

m


X
i

)

≥

∑

J
⊆
{
1
,
2
,
…
,
m
}




(
−
1
)


|
J
|
+
1


B
e
l

(

⋂

i
∈
J



X
i

)

.

However, in this example, we have

B
e
l

(

X
1

⋃

X
2

)

<
B
e
l

(

X
1

)

+
B
e
l

(

X
2

)

−
B
e
l

(

X
1

∩

X
2

)


.Thus, neighborhood-covering optimistic multigranulation rough set approximation cannot be characterized by belief and plausibility functions. 5. Relative Reduction of Neighborhood-Covering Pessimistic Multigranulation Rough SetThe relative reduction of neighborhood-covering pessimistic multigranularity rough set is discussed below. First, we give the definition of relative reduction of neighborhood-covering pessimistic multigranulation rough set.A covering decision information system is a triple

(
U
,
C
,
D
)

, where

U
=
{

x
1

,

x
2

,
…
,

x
n

}

 is a nonempty, finite set of objects called the universe of discourse,

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 is a family of coverings of U and

D
=
{

D
1

,

D
2

,
…
,

D
l

}

 is a decision partition of U.Definition 14. [11] Let

(
U
,
C
,
D
)

 be a covering decision information system,

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 be a family of coverings of U, and

D
=
{

D
1

,

D
2

,
…
,

D
l

}

 be a decision partition of U. We have the following definition.






C
P

̲


(
d
)

=

(



∑

i
=
1

m


C

i

P


̲


(

D
1

)

,



∑

i
=
1

m


C

i

P


̲


(

D
2

)

,
…
,



∑

i
=
1

m


C

i

P


̲


(

D
l

)

)

,





(17)








C
P

¯


(
d
)

=

(



∑

i
=
1

m


C

i

P


¯


(

D
1

)

,



∑

i
=
1

m


C

i

P


¯


(

D
2

)

,
…
,



∑

i
=
1

m


C

i

P


¯


(

D
l

)

)

.





(18)

(1) If

B
⊆
C

 and



B
P

̲


(
d
)

=


C
P

̲


(
d
)


, but



B



′

P


̲


(
d
)

≠


C
P

̲


(
d
)


, for


B


′


⊆
B

, then B is a d reduction of neighborhood-covering pessimistic multigranularity lower approximation w.r.t C;(2) If

B
⊆
C

 and



B
P

¯


(
d
)

=


C
P

¯


(
d
)


, but



B



′

P


¯


(
d
)

≠


C
P

¯


(
d
)


, for


B


′


⊆
B

, then B is a d reduction of neighborhood-covering pessimistic multigranularity upper approximation w.r.t C;(3) If

B
⊆
C

 and



B
O

̲


(
d
)

=


C
O

̲


(
d
)


, but



B



′

O


̲


(
d
)

≠


C
O

̲


(
d
)


, for


B


′


⊆
B

, then B is a d reduction of neighborhood-covering optimistic multigranularity lower approximation w.r.t C;(4) If

B
⊆
C

 and



B
O

¯


(
d
)

=


C
O

¯


(
d
)


, but



B



′

O


¯


(
d
)

≠


C
O

¯


(
d
)


, for


B


′


⊆
B

, then B is a d reduction of neighborhood-covering optimistic multigranularity upper approximation w.r.t C.Next, let

(
U
,
C
,
D
)

 be a covering decision information system,

C
=
{

C
1

,

C
2

,
…
,

C
m

}

 be a family of coverings of U and

D
=
{

D
1

,

D
2

,
…
,

D
l

}

 be a decision partition of U.In Algorithm 1, computing the neighborhood of all the objects can be done in

O
(
|
U

|
2

|
C
|
)

, and the time complex for computing



C
P

̲


(
d
)


 is

O
(
|
U
|
|
C
|
|
D
|
)

. Since

|
D
|
<
|
U
|

, the time complexity of the first step is

O
(
|
U

|
2

|
C
|
)

. In Step 2-3, the time complex is

O
(
|
U
|
|
C

|
2

|
D
|
)

. In sum, the total time complexity of Algorithm 1 does not exceed

O
(
|
U

|
2

|
C

|
2

)

. Next, we give an example to calculate the relative reduction of the pessimistic multigranularity covering lower approximation.Algorithm 1 Relative reduction algorithm of neighborhood-covering pessimistic multigranularity lower approximationInput: a covering decision information system

(
U
,
C
,
D
)

.Output: relative reduction set B of neighborhood-covering pessimistic multigranularity lower approximation.1: Compute



C
P

̲


(
d
)


;2: Remove a covering

C
k

, let

B
=
C
−
{

C
k

}

, if



B
P

̲


(
d
)

=


C
P

̲


(
d
)


;3: Remove a covering in B again and get

B


′


. If



B



′

P


̲


(
d
)

≠


C
P

̲


(
d
)


, return B; else, go to Step 2;4: Repeat the Steps 2 and 3 for each covering in C to get all the relative reduce of the covering family.Example 3. Consider a house evaluation problem. Let

U
=
{

x
1

,

x
2

,
…
,

x
6

}

 be a set of six houses,

A
=
{
e
q
u
a
l
l
y

s
h
a
r
e
d

a
r
e
a
,

c
o
l
o
r
,

p
r
i
c
e
,

s
u
r
r
o
u
n
d
i
n
g
s
}

 be a set of attribute, and

B
=
{
p
u
r
c
h
a
s
e

o
p
i
n
i
o
n
s
}

 be a set of decision. The values of equally shared area could be

{
l
a
r
g
e
,

o
r
d
i
n
a
r
y
,

s
m
a
l
l
}

. The values of color could be

{
e
x
c
e
l
l
e
n
t
,

g
o
o
d
,

b
a
d
}

. The values of price¡ could be

{
h
i
g
h
,

m
i
d
d
l
e
,

l
o
w
}

. The values of surroundings could be

{
q
u
i
e
t
,

n
o
i
s
y
,

v
e
r
y

n
o
i
s
y
}

. The decision values of purchase opinions could be

{
s
u
p
p
o
r
t
,

o
p
p
o
s
e
}

, which is randomly chosen from experts. The evaluation results are shown in Table 1.From the attribute set A, we can get a family of coverings

C
=
{

C
1

,

C
2

,

C
3

,

C
4

}

, and a decision class

D
=
{

{

x
2

,

x
3

,

x
6

}

,

{

x
1

,

x
4

,

x
5

}

}

. The coverings are as follows.


C
1

=

{

{

x
2

,

x
3

,

x
4

,

x
5

}

,

{

x
4

,

x
5

,

x
6

}

,

{

x
1

,

x
2

,

x
3

,

x
6

}

}


;


C
2

=

{

{

x
2

,

x
3

,

x
6

}

,

{

x
1

,

x
3

,

x
6

}

,

{

x
4

,

x
5

}

}


;


C
3

=

{

{

x
1

,

x
4

,

x
5

,

x
6

}

,

{

x
2

,

x
3

,

x
4

,

x
5

}

,

{

x
2

,

x
3

,

x
6

}

}


;


C
4

=

{

{

x
2

,

x
6

}

,

{

x
2

,

x
3

,

x
4

,

x
6

}

,

{

x
1

,

x
4

,

x
5

}

}


;It is easy to calculate that



(

x
1

)


C
1


=

{

x
1

,

x
2

,

x
3

,

x
6

}


;



(

x
2

)


C
1


=


(

x
3

)


C
1


=

{

x
2

,

x
3

}


;



(

x
4

)


C
1


=


(

x
5

)


C
1


=

{

x
4

,

x
5

}


;



(

x
6

)


C
1


=

{

x
6

}


;



(

x
1

)


C
2


=

{

x
1

,

x
3

,

x
6

}


;



(

x
2

)


C
2


=

{

x
2

,

x
3

,

x
6

}


;



(

x
3

)


C
2


=


(

x
6

)


C
2


=

{

x
3

,

x
6

}


;



(

x
4

)


C
2


=


(

x
5

)


C
2


=

{

x
4

,

x
5

}


;



(

x
1

)


C
3


=

{

x
1

,

x
4

,

x
5

,

x
6

}


;



(

x
2

)


C
1


=


(

x
3

)


C
1


=

{

x
2

,

x
3

}


;



(

x
4

)


C
1


=


(

x
5

)


C
1


=

{

x
4

,

x
5

}


;



(

x
6

)


C
1


=

{

x
6

}


;



(

x
1

)


C
4


=


(

x
5

)


C
4


=

{

x
1

,

x
4

,

x
5

}


;



(

x
2

)


C
4


=


(

x
6

)


C
4


=

{

x
2

,

x
6

}


;



(

x
3

)


C
4


=

{

x
2

,

x
3

,

x
4

,

x
6

}


;



(

x
4

)


C
4


=

{

x
4

}


;According to Algorithm 1, for the first step,



C
P

̲


(
d
)

=

(

{

x
2

,

x
6

}

,

{

x
4

,

x
5

}

)


.For the second step,Let


B
1

=
C
−

C
1


, then



B

1

P

̲


(
d
)

=

(

{

x
2

,

x
6

}

,

{

x
4

,

x
5

}

)


.Let


B
2

=

B
1

−

C
2


, then



B

2

P

̲


(
d
)

=

(

{

x
2

,

x
6

}

,

{

x
4

,

x
5

}

)


.Finally,let

B
=
{

C
3

,

C
4

}

, by removing any covering on B, we can get

B


′


, and



B
P

̲


(
d
)

≠


B



′

P


̲


(
d
)


.Therefore B is a d reduction of neighborhood-covering pessimistic multigranularity lower approximation w.r.t C.Theorem 4. Let

(
U
,
C
,
D
)

 be a covering decision information system and

D
=
{

D
1

,

D
2

,
…
,

D
l

}

 be a decision partition of U. Let


∑

j
=
1

l

B
e

l
C


(

D
j

)

=
M

, then

B
⊆
C

 is a neighborhood-covering pessimistic multigranularity lower approximation relative reduction of C iff


∑

j
=
1

l

B
e

l
B


(

D
j

)

=
M

, and for any subset


B


′


⊆
B

,


∑

j
=
1

l

B
e

l

B


′




(

D
j

)

>
M

.Proof of Theorem 4. Sufficiency.If

B
⊆
C

 is a relative reduction of neighborhood-covering pessimistic multigranularity lower approximation w.r.t C,

∀
j
∈
{
1
,
2
,
…
,
l
}

, we have




∑

i
=
1

m


C

i

P


̲


(

D
j

)

=



∑

i
=
1

m


B

i

P


̲


(

D
j

)


. By Definition 14, we can see that

∀
j
∈
{
1
,
2
,
…
,
l
}

,

B
e

l
C


(

D
j

)

=
B
e

l
B


(

D
j

)


, then


∑

j
=
1

l

B
e

l
B


(

D
j

)

=
M

, and for any


B


′


⊆
B

,


∑

j
=
1

l

B
e

l

B


′




(

D
j

)

>
M

.Necessity. Since

B
⊆
C

, we have

∀
j
∈
{
1
,
2
,
…
,
l
}

,

B
e

l
C


(

D
j

)

≤
B
e

l
B


(

D
j

)


. Since


∑

j
=
1

l

B
e

l
B


(

D
j

)

=
M

, and for any


B


′


⊆
B

,


∑

j
=
1

l

B
e

l

B


′




(

D
j

)

>
M

, then for

∀
j
∈
{
1
,
2
,
…
,
l
}

,

B
e

l
C


(

D
j

)

=
B
e

l
B


(

D
j

)


,

B
e

l
C


(

D
j

)

≠
B
e

l

B
‘



(

D
j

)


. By Definition 14, we have

∀
j
∈
{
1
,
2
,
…
,
l
}

,




∑

i
=
1

m


C

i

P


̲


(

D
j

)

=



∑

i
=
1

m


B

i

P


̲


(

D
j

)


,



C
P

̲


(
d
)

=


B
P

̲


(
d
)


, and for any


B


′


⊆
B

,



C
P

̲


(
d
)

≠


B



′

P


̲


(
d
)


. Therefore,

B
⊆
C

 is a reduction of neighborhood-covering pessimistic multigranularity lower approximation w.r.t C. □Definition 15. Let

(
U
,
C
,
D
)

 be a covering information system and

D
=
{

D
1

,

D
2

,
…
,

D
l

}

 be a decision partition of U. For

B
⊆
C

,


C
i

∉
B

, the significance of

C
i

 w.r.t B is defined as:

S
i
g

(

C
i

,
B
)

=

∑

j
=
1

l


(
B
e

l
B


(

D
j

)

−
B
e

l

B
⋃
{

C
j

}



(

D
i

)

)


.In Definition 15, if

S
i
g
(

C
i

,
C
−

{

C
i

}

)
>
0

, then

C
i

 is called a core.Through the definition of the core, we can get the relative reduction algorithm of neighborhood- covering pessimistic multigranularity lower approximation.The mechanism of Algorithm 2 can be described as follows. In Step 2, computing the significance of all covering can be done in

O
(
|
U

|
2

|
C

|
2

)

, and the time complexity of Step 2 and Step 3 is

O
(
|
U

|
2

|
C

|
2

)

. In Step 5, Comparing the maximum significance of all covering requires


|
C
|
(
|
C
|
−
1
)

2

 times at worst, and the time complexity of 4–6 is

O
(
|
U
|
|
C
|
)

. Above all, the time complexity of Algorithm 2 is

O
(
|
U

|
2

|
C

|
3

)

.Algorithm 2 Relative reduction algorithm of neighborhood-covering pessimistic multigranularity lower approximation based on evidence theoryInput: a covering decision information system

(
U
,
C
,
D
)

.Output: relative reduction set B of neighborhood-covering pessimistic multigranularity lower approximation.1: Let

B
=
∅

;2: For any


C
i

∈
C

, calculate

S
i
g
(

C
i

,
C
−

{

C
i

}

)

;3: If

S
i
g
(

C
i

,
C
−

{

C
i

}

)
>
0

, let

B
=
B
⋃

C
i


, if


∑

j
=
1

l

B
e

l
B


(

D
j

)

=

∑

j
=
1

l

B
e

l
C


(

D
j

)


, then return B; Otherwise, go to Steps 4–6;4: For any


C
0

∈
C
−
B

, calculate

S
i
g
(

C
0

,
B
}
)

;5: For


C
0

∈
C
−
B

,

S
i
g

(

C
0

,
B
}


)
=
m
a
x


{
S
i
g

(

C
k

,
B
)

|

C
k

∈
C
−
B
}


. Let

B
=
B
⋃
{

C
0

}

 and

C
=
C
−
{

C
0

}

;6: If


∑

j
=
1

l

B
e

l
B


(

D
j

)

=

∑

j
=
1

l

B
e

l
C


(

D
j

)


, return B; else, go to Step 4.Example 4. We use Algorithm 2 to carry out the relative reduction of the decision system in Example 3.First let

B
=
∅

. For the second step,




S
i
g
(

C
1

,
C
−

{

C
1

}

)
=
0
;

S
i
g
(

C
2

,
C
−

{

C
2

}

)
=
0

;











S
i
g
(

C
3

,
C
−

{

C
3

}

)
=
0
;

S
i
g

(

C
4

,
C
−

{

C
4

}

)

=

1
6

>
0
.







Let

B
=
{

C
4

}

,


∑

j
=
1

2

B
e

l
B


(

D
j

)

=

5
6

≠

4
6

=

∑

j
=
1

2

B
e

l
C


(

D
j

)


. Then




S
i
g

(

C
1

,
B
)

=
S
i
g

(

C
2

,
B
)

=
S
i
g

(

C
3

,
B
)

=

1
6

.






For


B
1

=

{

C
1

,

C
4

}


,


B
2

=

{

C
2

,

C
4

}


,


B
3

=

{

C
3

,

C
4

}


, we have





∑

j
=
1

2

B
e

l

B
1



(

D
j

)

=

∑

j
=
1

2

B
e

l

B
2



(

D
j

)

=

∑

j
=
1

2

B
e

l

B
3



(

D
j

)

=

∑

j
=
1

2

B
e

l
C


(

D
j

)

.






Therefore,


B
1

=

{

C
1

,

C
4

}


,


B
2

=

{

C
2

,

C
4

}


,


B
3

=

{

C
3

,

C
4

}


 are all relative reduction set of neighborhood-covering pessimistic multigranularity lower approximation w.r.t C.Algorithm 1 in this paper is the original algorithm for the relative reduction of neighborhood-covering pessimistic multigranulation rough set. Its time complexity is relatively low, but it has more approximate data, which is troublesome to compare, and only one reduction can be obtained. Algorithm 2 is proposed by combining the neighborhood-covering pessimistic multigranulation rough set with evidence theory. Algorithm 2 employs the belief function from evidence theory to measure the quality of the lower approximation of the model. After the data is simplified, the comparison of the data is relatively concise, and all the reduction can be obtained.The relative reduction in this paper is the reduction that keeps the upper and lower approximations unchanged, and the belief and plausibility functions are used to calculate the mass of the upper and lower approximations. The upper and lower approximation not changing is equivalent to the mass function of the upper and lower approximation not changing. This algorithm can be widely used in neighborhood-covering pessimistic multigranulation rough set model to solve the relative reduction that keeps the upper and lower approximations unchanged.The algorithm in this paper is investigated by using belief and plausibility function based on evidence theory. Since the neighborhood-covering optimistic multigranulation rough set approximation cannot be characterized by belief and plausibility functions, the proposed algorithm is not applicable to the neighborhood-covering optimistic multigranulation rough set, but only suitable for computing the relative reduction of neighborhood-covering pessimistic multigranularity rough set. 6. ConclusionsIn this paper, the relative reduction of neighborhood-covering multigranulation rough set is explored by using evidence theory. We introduce the lower and upper approximations of multigranulation rough set in neighborhood-covering information systems based on the concept of neighborhood of objects. The approximations of neighborhood-covering multigranulation rough set are characterized by the belief and plausibility functions from evidence theory. Moreover, according to the significance of coverings defined by the belief function, the algorithm for computing a relative reduction of neighborhood-covering information systems is proposed, and its validity is examined by a practical example. This paper does not only enrich the relative reduction theory of multigranulation rough set, but also provide a new idea for relative reduction of data sets based on the covering decision information system. In the future, the relative reduction theory of covering decision information system under other covering multigranulation rough set approximation operators will be further considered, and the reduction theory and results under different covering multigranulation rough set approximation operators will be compared.
