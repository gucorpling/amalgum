#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=1. Introduction
1-1	0-2	1.	_	_	_	_
1-2	3-15	Introduction	abstract	new	_	_

#Text=Nowadays , position information has become key information in people ’s daily lives .
2-1	16-24	Nowadays	_	_	_	_
2-2	25-26	,	_	_	_	_
2-3	27-35	position	abstract|abstract[3]	new|new[3]	coref|coref|coref|coref	4-4[11_0]|18-10[92_3]|4-4[11_0]|18-10[92_3]
2-4	36-47	information	abstract[3]	new[3]	_	_
2-5	48-51	has	_	_	_	_
2-6	52-58	become	_	_	_	_
2-7	59-62	key	_	_	_	_
2-8	63-74	information	_	_	_	_
2-9	75-77	in	_	_	_	_
2-10	78-84	people	person[4]|abstract[5]	new[4]|new[5]	ana|coref|ana|coref	3-1[0_5]|6-7[0_4]|3-1[0_5]|6-7[0_4]
2-11	85-87	’s	person[4]|abstract[5]	new[4]|new[5]	_	_
2-12	88-93	daily	abstract[5]	new[5]	_	_
2-13	94-99	lives	abstract[5]	new[5]	_	_
2-14	100-101	.	_	_	_	_

#Text=This has inspired position-based services , which aim to provide personalized services to mobile users whose positions are changing .
3-1	102-106	This	abstract	giv	_	_
3-2	107-110	has	_	_	_	_
3-3	111-119	inspired	_	_	_	_
3-4	120-134	position-based	abstract[7]	new[7]	coref	3-11[8_7]
3-5	135-143	services	abstract[7]	new[7]	_	_
3-6	144-145	,	_	_	_	_
3-7	146-151	which	_	_	_	_
3-8	152-155	aim	_	_	_	_
3-9	156-158	to	_	_	_	_
3-10	159-166	provide	_	_	_	_
3-11	167-179	personalized	abstract[8]	giv[8]	coref	4-11[13_8]
3-12	180-188	services	abstract[8]	giv[8]	_	_
3-13	189-191	to	_	_	_	_
3-14	192-198	mobile	person[9]	new[9]	_	_
3-15	199-204	users	person[9]	new[9]	_	_
3-16	205-210	whose	abstract[10]	new[10]	_	_
3-17	211-220	positions	abstract[10]	new[10]	_	_
3-18	221-224	are	_	_	_	_
3-19	225-233	changing	_	_	_	_
3-20	234-235	.	_	_	_	_

#Text=Therefore , obtaining a precise position is a prerequisite for these services .
4-1	236-245	Therefore	_	_	_	_
4-2	246-247	,	_	_	_	_
4-3	248-257	obtaining	_	_	_	_
4-4	258-259	a	abstract[11]	giv[11]	coref	19-23[0_11]
4-5	260-267	precise	abstract[11]	giv[11]	_	_
4-6	268-276	position	abstract[11]	giv[11]	_	_
4-7	277-279	is	_	_	_	_
4-8	280-281	a	abstract[12]	new[12]	_	_
4-9	282-294	prerequisite	abstract[12]	new[12]	_	_
4-10	295-298	for	abstract[12]	new[12]	_	_
4-11	299-304	these	abstract[12]|abstract[13]	new[12]|giv[13]	_	_
4-12	305-313	services	abstract[12]|abstract[13]	new[12]|giv[13]	_	_
4-13	314-315	.	_	_	_	_

#Text=The most commonly used positioning method in the outdoor environment is the Global Navigation Satellite System ( GNSS ) .
5-1	316-319	The	abstract[15]	new[15]	coref	5-12[19_15]
5-2	320-324	most	abstract[15]	new[15]	_	_
5-3	325-333	commonly	abstract[15]	new[15]	_	_
5-4	334-338	used	abstract[15]	new[15]	_	_
5-5	339-350	positioning	abstract|abstract[15]	new|new[15]	coref	7-3[27_0]
5-6	351-357	method	abstract[15]	new[15]	_	_
5-7	358-360	in	abstract[15]	new[15]	_	_
5-8	361-364	the	abstract[15]|place[16]	new[15]|new[16]	_	_
5-9	365-372	outdoor	abstract[15]|place[16]	new[15]|new[16]	_	_
5-10	373-384	environment	abstract[15]|place[16]	new[15]|new[16]	_	_
5-11	385-387	is	_	_	_	_
5-12	388-391	the	abstract[19]	giv[19]	coref	23-29[138_19]
5-13	392-398	Global	abstract[19]	giv[19]	_	_
5-14	399-409	Navigation	abstract|abstract[19]	new|giv[19]	_	_
5-15	410-419	Satellite	organization|abstract[19]	new|giv[19]	_	_
5-16	420-426	System	abstract[19]	giv[19]	_	_
5-17	427-428	(	_	_	_	_
5-18	429-433	GNSS	place	new	coref	8-2
5-19	434-435	)	_	_	_	_
5-20	436-437	.	_	_	_	_

#Text=In most cases , however , people spend more than 70 % of their time indoors .
6-1	438-440	In	_	_	_	_
6-2	441-445	most	abstract[21]	new[21]	_	_
6-3	446-451	cases	abstract[21]	new[21]	_	_
6-4	452-453	,	_	_	_	_
6-5	454-461	however	_	_	_	_
6-6	462-463	,	_	_	_	_
6-7	464-470	people	person	giv	ana	6-14
6-8	471-476	spend	_	_	_	_
6-9	477-481	more	quantity[23]	new[23]	_	_
6-10	482-486	than	quantity[23]	new[23]	_	_
6-11	487-489	70	quantity[23]	new[23]	_	_
6-12	490-491	%	quantity[23]	new[23]	_	_
6-13	492-494	of	quantity[23]	new[23]	_	_
6-14	495-500	their	quantity[23]|person|time[25]	new[23]|giv|new[25]	coref|coref	9-4|9-4
6-15	501-505	time	quantity[23]|time[25]	new[23]|new[25]	_	_
6-16	506-513	indoors	_	_	_	_
6-17	514-515	.	_	_	_	_

#Text=Therefore , accurate indoor positioning has important practical significance .
7-1	516-525	Therefore	_	_	_	_
7-2	526-527	,	_	_	_	_
7-3	528-536	accurate	abstract[27]	giv[27]	coref	13-14[63_27]
7-4	537-543	indoor	place|abstract[27]	new|giv[27]	coref	8-23
7-5	544-555	positioning	abstract[27]	giv[27]	_	_
7-6	556-559	has	_	_	_	_
7-7	560-569	important	abstract[28]	new[28]	_	_
7-8	570-579	practical	abstract[28]	new[28]	_	_
7-9	580-592	significance	abstract[28]	new[28]	_	_
7-10	593-594	.	_	_	_	_

#Text=Although GNSS is a good choice for outdoor positioning , due to signal occlusion and attenuations , it is often useless in indoor environments .
8-1	595-603	Although	_	_	_	_
8-2	604-608	GNSS	abstract	giv	coref	8-4[30_0]
8-3	609-611	is	_	_	_	_
8-4	612-613	a	abstract[30]	giv[30]	_	_
8-5	614-618	good	abstract[30]	giv[30]	_	_
8-6	619-625	choice	abstract[30]	giv[30]	_	_
8-7	626-629	for	abstract[30]	giv[30]	_	_
8-8	630-637	outdoor	abstract[30]|abstract[31]	giv[30]|new[31]	_	_
8-9	638-649	positioning	abstract[30]|abstract[31]	giv[30]|new[31]	_	_
8-10	650-651	,	_	_	_	_
8-11	652-655	due	_	_	_	_
8-12	656-658	to	_	_	_	_
8-13	659-665	signal	_	_	_	_
8-14	666-675	occlusion	abstract	new	ana	8-18
8-15	676-679	and	_	_	_	_
8-16	680-692	attenuations	abstract	new	_	_
8-17	693-694	,	_	_	_	_
8-18	695-697	it	abstract	giv	_	_
8-19	698-700	is	_	_	_	_
8-20	701-706	often	_	_	_	_
8-21	707-714	useless	_	_	_	_
8-22	715-717	in	_	_	_	_
8-23	718-724	indoor	abstract|place[36]	giv|new[36]	coref|coref	9-7|9-7
8-24	725-737	environments	place[36]	new[36]	_	_
8-25	738-739	.	_	_	_	_

#Text=Thus , positioning people accurately in indoor scenes remains a challenge and it has stimulated a large number of indoor-positioning methods in recent years .
9-1	740-744	Thus	_	_	_	_
9-2	745-746	,	_	_	_	_
9-3	747-758	positioning	_	_	_	_
9-4	759-765	people	person	giv	_	_
9-5	766-776	accurately	_	_	_	_
9-6	777-779	in	_	_	_	_
9-7	780-786	indoor	place|abstract[39]	giv|new[39]	coref|coref	13-15|13-15
9-8	787-793	scenes	abstract[39]	new[39]	_	_
9-9	794-801	remains	_	_	_	_
9-10	802-803	a	abstract[40]	new[40]	ana	9-13[0_40]
9-11	804-813	challenge	abstract[40]	new[40]	_	_
9-12	814-817	and	_	_	_	_
9-13	818-820	it	abstract	giv	_	_
9-14	821-824	has	_	_	_	_
9-15	825-835	stimulated	_	_	_	_
9-16	836-837	a	abstract[42]	new[42]	_	_
9-17	838-843	large	abstract[42]	new[42]	_	_
9-18	844-850	number	abstract[42]	new[42]	_	_
9-19	851-853	of	abstract[42]	new[42]	_	_
9-20	854-872	indoor-positioning	abstract[42]|abstract[43]	new[42]|new[43]	coref	10-2[45_43]
9-21	873-880	methods	abstract[42]|abstract[43]	new[42]|new[43]	_	_
9-22	881-883	in	_	_	_	_
9-23	884-890	recent	time[44]	new[44]	coref	15-11[73_44]
9-24	891-896	years	time[44]	new[44]	_	_
9-25	897-898	.	_	_	_	_

#Text=Among these methods , fingerprint-based algorithms are widely used .
10-1	899-904	Among	_	_	_	_
10-2	905-910	these	abstract[45]	giv[45]	coref	12-2[54_45]
10-3	911-918	methods	abstract[45]	giv[45]	_	_
10-4	919-920	,	_	_	_	_
10-5	921-938	fingerprint-based	abstract[46]	new[46]	ana	11-1[0_46]
10-6	939-949	algorithms	abstract[46]	new[46]	_	_
10-7	950-953	are	_	_	_	_
10-8	954-960	widely	_	_	_	_
10-9	961-965	used	_	_	_	_
10-10	966-967	.	_	_	_	_

#Text=Their fingerprint databases include Wi-Fi , Bluetooth , and magnetic field strengths .
11-1	968-973	Their	abstract|object[49]	giv|new[49]	coref|coref	26-4[171_0]|26-4[171_0]
11-2	974-985	fingerprint	object|object[49]	new|new[49]	coref	12-12
11-3	986-995	databases	object[49]	new[49]	_	_
11-4	996-1003	include	_	_	_	_
11-5	1004-1009	Wi-Fi	object	new	_	_
11-6	1010-1011	,	_	_	_	_
11-7	1012-1021	Bluetooth	person	new	_	_
11-8	1022-1023	,	_	_	_	_
11-9	1024-1027	and	_	_	_	_
11-10	1028-1036	magnetic	abstract[53]	new[53]	_	_
11-11	1037-1042	field	abstract|abstract[53]	new|new[53]	_	_
11-12	1043-1052	strengths	abstract[53]	new[53]	_	_
11-13	1053-1054	.	_	_	_	_

#Text=Although these methods are easy to implement , construction of a fingerprint database is usually labor-intensive and time-consuming .
12-1	1055-1063	Although	_	_	_	_
12-2	1064-1069	these	abstract[54]	giv[54]	ana	13-7[0_54]
12-3	1070-1077	methods	abstract[54]	giv[54]	_	_
12-4	1078-1081	are	_	_	_	_
12-5	1082-1086	easy	_	_	_	_
12-6	1087-1089	to	_	_	_	_
12-7	1090-1099	implement	_	_	_	_
12-8	1100-1101	,	_	_	_	_
12-9	1102-1114	construction	event[55]	new[55]	ana	13-3[0_55]
12-10	1115-1117	of	event[55]	new[55]	_	_
12-11	1118-1119	a	event[55]|object[57]	new[55]|new[57]	coref	18-4[91_57]
12-12	1120-1131	fingerprint	event[55]|object|object[57]	new[55]|giv|new[57]	_	_
12-13	1132-1140	database	event[55]|object[57]	new[55]|new[57]	_	_
12-14	1141-1143	is	_	_	_	_
12-15	1144-1151	usually	_	_	_	_
12-16	1152-1167	labor-intensive	_	_	_	_
12-17	1168-1171	and	_	_	_	_
12-18	1172-1186	time-consuming	_	_	_	_
12-19	1187-1188	.	_	_	_	_

#Text=Moreover , it is difficult for their results to meet the needs of high-accuracy indoor positioning .
13-1	1189-1197	Moreover	_	_	_	_
13-2	1198-1199	,	_	_	_	_
13-3	1200-1202	it	event	giv	_	_
13-4	1203-1205	is	_	_	_	_
13-5	1206-1215	difficult	_	_	_	_
13-6	1216-1219	for	_	_	_	_
13-7	1220-1225	their	abstract|abstract[60]	giv|new[60]	coref|coref	15-1[72_0]|15-1[72_0]
13-8	1226-1233	results	abstract[60]	new[60]	_	_
13-9	1234-1236	to	_	_	_	_
13-10	1237-1241	meet	_	_	_	_
13-11	1242-1245	the	abstract[61]	new[61]	_	_
13-12	1246-1251	needs	abstract[61]	new[61]	_	_
13-13	1252-1254	of	abstract[61]	new[61]	_	_
13-14	1255-1268	high-accuracy	abstract[61]|abstract[63]	new[61]|giv[63]	ana	14-18[0_63]
13-15	1269-1275	indoor	abstract[61]|place|abstract[63]	new[61]|giv|giv[63]	coref	32-7
13-16	1276-1287	positioning	abstract[61]|abstract[63]	new[61]|giv[63]	_	_
13-17	1288-1289	.	_	_	_	_

#Text=Given that humans use their eyes to see where they are , mobile platforms can also do this with cameras .
14-1	1290-1295	Given	_	_	_	_
14-2	1296-1300	that	_	_	_	_
14-3	1301-1307	humans	person	new	ana	14-5
14-4	1308-1311	use	_	_	_	_
14-5	1312-1317	their	person|object[66]	giv|new[66]	ana|ana	14-10|14-10
14-6	1318-1322	eyes	object[66]	new[66]	_	_
14-7	1323-1325	to	_	_	_	_
14-8	1326-1329	see	_	_	_	_
14-9	1330-1335	where	_	_	_	_
14-10	1336-1340	they	person	giv	_	_
14-11	1341-1344	are	_	_	_	_
14-12	1345-1346	,	_	_	_	_
14-13	1347-1353	mobile	object[68]	new[68]	_	_
14-14	1354-1363	platforms	object[68]	new[68]	_	_
14-15	1364-1367	can	_	_	_	_
14-16	1368-1372	also	_	_	_	_
14-17	1373-1375	do	_	_	_	_
14-18	1376-1380	this	abstract	giv	coref	15-5
14-19	1381-1385	with	_	_	_	_
14-20	1386-1393	cameras	object	new	_	_
14-21	1394-1395	.	_	_	_	_

#Text=A number of visual positioning methods have been proposed in recent years .
15-1	1396-1397	A	abstract[72]	giv[72]	coref	16-1[75_72]
15-2	1398-1404	number	abstract[72]	giv[72]	_	_
15-3	1405-1407	of	abstract[72]	giv[72]	_	_
15-4	1408-1414	visual	abstract[72]	giv[72]	_	_
15-5	1415-1426	positioning	abstract|abstract[72]	giv|giv[72]	coref	16-2
15-6	1427-1434	methods	abstract[72]	giv[72]	_	_
15-7	1435-1439	have	_	_	_	_
15-8	1440-1444	been	_	_	_	_
15-9	1445-1453	proposed	_	_	_	_
15-10	1454-1456	in	_	_	_	_
15-11	1457-1463	recent	time[73]	giv[73]	_	_
15-12	1464-1469	years	time[73]	giv[73]	_	_
15-13	1470-1471	.	_	_	_	_

#Text=These positioning methods are divided into three categories : image retrieval based methods , visual landmarks-based methods , and learning-based methods .
16-1	1472-1477	These	abstract[75]	giv[75]	coref	16-13[79_75]
16-2	1478-1489	positioning	abstract|abstract[75]	giv|giv[75]	coref	17-7
16-3	1490-1497	methods	abstract[75]	giv[75]	_	_
16-4	1498-1501	are	_	_	_	_
16-5	1502-1509	divided	_	_	_	_
16-6	1510-1514	into	_	_	_	_
16-7	1515-1520	three	abstract[76]	new[76]	appos	16-10[78_76]
16-8	1521-1531	categories	abstract[76]	new[76]	_	_
16-9	1532-1533	:	_	_	_	_
16-10	1534-1539	image	abstract|abstract[78]	new|giv[78]	coref|coref|coref|coref	17-1|17-1[82_78]|17-1|17-1[82_78]
16-11	1540-1549	retrieval	abstract[78]	giv[78]	_	_
16-12	1550-1555	based	_	_	_	_
16-13	1556-1563	methods	abstract[79]	giv[79]	coref	16-20[80_79]
16-14	1564-1565	,	abstract[79]	giv[79]	_	_
16-15	1566-1572	visual	abstract[79]	giv[79]	_	_
16-16	1573-1588	landmarks-based	abstract[79]	giv[79]	_	_
16-17	1589-1596	methods	abstract[79]	giv[79]	_	_
16-18	1597-1598	,	_	_	_	_
16-19	1599-1602	and	_	_	_	_
16-20	1603-1617	learning-based	abstract[80]	giv[80]	coref	17-4[0_80]
16-21	1618-1625	methods	abstract[80]	giv[80]	_	_
16-22	1626-1627	.	_	_	_	_

#Text=Image retrieval based methods treat the positioning task as an image retrieval or recognition process .
17-1	1628-1633	Image	abstract|abstract[82]	giv|giv[82]	coref|coref|coref|coref	17-12[0_82]|18-14[93_0]|17-12[0_82]|18-14[93_0]
17-2	1634-1643	retrieval	abstract[82]	giv[82]	_	_
17-3	1644-1649	based	_	_	_	_
17-4	1650-1657	methods	abstract	giv	coref	19-1[96_0]
17-5	1658-1663	treat	_	_	_	_
17-6	1664-1667	the	abstract[85]	new[85]	_	_
17-7	1668-1679	positioning	abstract|abstract[85]	giv|new[85]	coref	22-2
17-8	1680-1684	task	abstract[85]	new[85]	_	_
17-9	1685-1687	as	_	_	_	_
17-10	1688-1690	an	_	_	_	_
17-11	1691-1696	image	_	_	_	_
17-12	1697-1706	retrieval	abstract|abstract[87]	giv|new[87]	ana|coref|ana|coref	18-1[0_87]|22-17[129_0]|18-1[0_87]|22-17[129_0]
17-13	1707-1709	or	abstract[87]	new[87]	_	_
17-14	1710-1721	recognition	abstract[87]|abstract|abstract[89]	new[87]|new|new[89]	coref|coref	20-6[111_89]|20-6[111_89]
17-15	1722-1729	process	abstract[87]|abstract[89]	new[87]|new[89]	_	_
17-16	1730-1731	.	_	_	_	_

#Text=They usually have a database that are augmented with geospatial information , and every image in the database is described through the same specific features .
18-1	1732-1736	They	abstract	giv	_	_
18-2	1737-1744	usually	_	_	_	_
18-3	1745-1749	have	_	_	_	_
18-4	1750-1751	a	object[91]	giv[91]	coref	18-17[94_91]
18-5	1752-1760	database	object[91]	giv[91]	_	_
18-6	1761-1765	that	_	_	_	_
18-7	1766-1769	are	_	_	_	_
18-8	1770-1779	augmented	_	_	_	_
18-9	1780-1784	with	_	_	_	_
18-10	1785-1795	geospatial	abstract[92]	giv[92]	coref	19-21[103_92]
18-11	1796-1807	information	abstract[92]	giv[92]	_	_
18-12	1808-1809	,	_	_	_	_
18-13	1810-1813	and	_	_	_	_
18-14	1814-1819	every	abstract[93]	giv[93]	coref	19-26[105_93]
18-15	1820-1825	image	abstract[93]	giv[93]	_	_
18-16	1826-1828	in	abstract[93]	giv[93]	_	_
18-17	1829-1832	the	abstract[93]|object[94]	giv[93]|giv[94]	coref	19-12[99_94]
18-18	1833-1841	database	abstract[93]|object[94]	giv[93]|giv[94]	_	_
18-19	1842-1844	is	_	_	_	_
18-20	1845-1854	described	_	_	_	_
18-21	1855-1862	through	_	_	_	_
18-22	1863-1866	the	abstract[95]	new[95]	coref	23-16[133_95]
18-23	1867-1871	same	abstract[95]	new[95]	_	_
18-24	1872-1880	specific	abstract[95]	new[95]	_	_
18-25	1881-1889	features	abstract[95]	new[95]	_	_
18-26	1890-1891	.	_	_	_	_

#Text=These methods perform a first step to retrieve candidate images from the database according to a similarity search , and the coarse position information of the query image is then obtained based on the geospatial information of these candidate images .
19-1	1892-1897	These	abstract[96]	giv[96]	coref	22-1[121_96]
19-2	1898-1905	methods	abstract[96]	giv[96]	_	_
19-3	1906-1913	perform	_	_	_	_
19-4	1914-1915	a	event[97]	new[97]	coref	20-2[109_97]
19-5	1916-1921	first	event[97]	new[97]	_	_
19-6	1922-1926	step	event[97]	new[97]	_	_
19-7	1927-1929	to	_	_	_	_
19-8	1930-1938	retrieve	_	_	_	_
19-9	1939-1948	candidate	object[98]	new[98]	coref	19-38[108_98]
19-10	1949-1955	images	object[98]	new[98]	_	_
19-11	1956-1960	from	_	_	_	_
19-12	1961-1964	the	object[99]	giv[99]	coref	23-11[132_99]
19-13	1965-1973	database	object[99]	giv[99]	_	_
19-14	1974-1983	according	_	_	_	_
19-15	1984-1986	to	_	_	_	_
19-16	1987-1988	a	event[101]	new[101]	coref	21-19[119_101]
19-17	1989-1999	similarity	abstract|event[101]	new|new[101]	coref	21-19
19-18	2000-2006	search	event[101]	new[101]	_	_
19-19	2007-2008	,	_	_	_	_
19-20	2009-2012	and	_	_	_	_
19-21	2013-2016	the	abstract[103]	giv[103]	_	_
19-22	2017-2023	coarse	abstract[103]	giv[103]	_	_
19-23	2024-2032	position	abstract|abstract[103]	giv|giv[103]	coref	29-11
19-24	2033-2044	information	abstract[103]	giv[103]	_	_
19-25	2045-2047	of	abstract[103]	giv[103]	_	_
19-26	2048-2051	the	abstract[103]|abstract[105]	giv[103]|giv[105]	coref	20-6[110_105]
19-27	2052-2057	query	abstract[103]|abstract|abstract[105]	giv[103]|new|giv[105]	coref	26-21
19-28	2058-2063	image	abstract[103]|abstract[105]	giv[103]|giv[105]	_	_
19-29	2064-2066	is	_	_	_	_
19-30	2067-2071	then	_	_	_	_
19-31	2072-2080	obtained	_	_	_	_
19-32	2081-2086	based	_	_	_	_
19-33	2087-2089	on	_	_	_	_
19-34	2090-2093	the	abstract[106]	new[106]	_	_
19-35	2094-2104	geospatial	abstract[106]	new[106]	_	_
19-36	2105-2116	information	abstract[106]	new[106]	_	_
19-37	2117-2119	of	abstract[106]	new[106]	_	_
19-38	2120-2125	these	abstract[106]|object[108]	new[106]|giv[108]	coref	23-8[131_108]
19-39	2126-2135	candidate	abstract[106]|person|object[108]	new[106]|new|giv[108]	coref	29-26
19-40	2136-2142	images	abstract[106]|object[108]	new[106]|giv[108]	_	_
19-41	2143-2144	.	_	_	_	_

#Text=So the first step , similar image retrieval process , is critical .
20-1	2145-2147	So	_	_	_	_
20-2	2148-2151	the	event[109]	giv[109]	_	_
20-3	2152-2157	first	event[109]	giv[109]	_	_
20-4	2158-2162	step	event[109]	giv[109]	_	_
20-5	2163-2164	,	_	_	_	_
20-6	2165-2172	similar	abstract[110]|abstract[111]	giv[110]|giv[111]	coref|coref|coref|coref	22-14[127_111]|22-17[0_110]|22-14[127_111]|22-17[0_110]
20-7	2173-2178	image	abstract[110]|abstract[111]	giv[110]|giv[111]	_	_
20-8	2179-2188	retrieval	abstract[111]	giv[111]	_	_
20-9	2189-2196	process	abstract[111]	giv[111]	_	_
20-10	2197-2198	,	_	_	_	_
20-11	2199-2201	is	_	_	_	_
20-12	2202-2210	critical	_	_	_	_
20-13	2211-2212	.	_	_	_	_

#Text=The brute-force approach , which is a distance comparison between feature descriptor vectors , is often used for similarity search .
21-1	2213-2216	The	abstract[112]	new[112]	_	_
21-2	2217-2228	brute-force	abstract[112]	new[112]	_	_
21-3	2229-2237	approach	abstract[112]	new[112]	_	_
21-4	2238-2239	,	_	_	_	_
21-5	2240-2245	which	_	_	_	_
21-6	2246-2248	is	_	_	_	_
21-7	2249-2250	a	abstract[114]	new[114]	coref	22-9[124_114]
21-8	2251-2259	distance	place|abstract[114]	new|new[114]	_	_
21-9	2260-2270	comparison	abstract[114]	new[114]	_	_
21-10	2271-2278	between	abstract[114]	new[114]	_	_
21-11	2279-2286	feature	abstract[114]|abstract|object[117]	new[114]|new|new[117]	coref|coref|coref|coref	22-6|25-29[167_117]|22-6|25-29[167_117]
21-12	2287-2297	descriptor	abstract[114]|person|object[117]	new[114]|new|new[117]	_	_
21-13	2298-2305	vectors	abstract[114]|object[117]	new[114]|new[117]	_	_
21-14	2306-2307	,	_	_	_	_
21-15	2308-2310	is	_	_	_	_
21-16	2311-2316	often	_	_	_	_
21-17	2317-2321	used	_	_	_	_
21-18	2322-2325	for	_	_	_	_
21-19	2326-2336	similarity	abstract|event[119]	giv|giv[119]	coref|coref|coref|coref	22-12[125_0]|22-14[0_119]|22-12[125_0]|22-14[0_119]
21-20	2337-2343	search	event[119]	giv[119]	_	_
21-21	2344-2345	.	_	_	_	_

#Text=Some positioning methods based on feature descriptors adopt brute-force comparison for the similarity search process of image retrieval .
22-1	2346-2350	Some	abstract[121]	giv[121]	coref	27-12[183_121]
22-2	2351-2362	positioning	abstract|abstract[121]	giv|giv[121]	coref	30-28[215_0]
22-3	2363-2370	methods	abstract[121]	giv[121]	_	_
22-4	2371-2376	based	_	_	_	_
22-5	2377-2379	on	_	_	_	_
22-6	2380-2387	feature	abstract|abstract[123]	giv|new[123]	coref|coref|coref|coref	23-49|25-32[0_123]|23-49|25-32[0_123]
22-7	2388-2399	descriptors	abstract[123]	new[123]	_	_
22-8	2400-2405	adopt	_	_	_	_
22-9	2406-2417	brute-force	abstract[124]	giv[124]	_	_
22-10	2418-2428	comparison	abstract[124]	giv[124]	_	_
22-11	2429-2432	for	abstract[124]	giv[124]	_	_
22-12	2433-2436	the	abstract[124]|abstract[125]	giv[124]|giv[125]	coref	25-10[0_125]
22-13	2437-2447	similarity	abstract[124]|abstract[125]	giv[124]|giv[125]	_	_
22-14	2448-2454	search	abstract[124]|abstract[125]|event|abstract[127]	giv[124]|giv[125]|giv|giv[127]	coref|coref	23-36|23-36
22-15	2455-2462	process	abstract[124]|abstract[125]|abstract[127]	giv[124]|giv[125]|giv[127]	_	_
22-16	2463-2465	of	abstract[124]|abstract[125]|abstract[127]	giv[124]|giv[125]|giv[127]	_	_
22-17	2466-2471	image	abstract[124]|abstract[125]|abstract[127]|abstract|abstract[129]	giv[124]|giv[125]|giv[127]|giv|giv[129]	ana|coref|ana|coref	23-3[0_129]|26-21[177_0]|23-3[0_129]|26-21[177_0]
22-18	2472-2481	retrieval	abstract[124]|abstract[125]|abstract[127]|abstract[129]	giv[124]|giv[125]|giv[127]|giv[129]	_	_
22-19	2482-2483	.	_	_	_	_

#Text=However , it is computationally intensive when the images of a database are described with high-dimensional features , limiting its scope of applications . Azzi et al. use a global feature-based system to reduce the search space and find candidate images in the database , then the local feature scale-invariant feature transform ( SIFT ) is adopted for points matching in pose estimation .
23-1	2484-2491	However	_	_	_	_
23-2	2492-2493	,	_	_	_	_
23-3	2494-2496	it	abstract	giv	ana	23-20
23-4	2497-2499	is	_	_	_	_
23-5	2500-2515	computationally	_	_	_	_
23-6	2516-2525	intensive	_	_	_	_
23-7	2526-2530	when	_	_	_	_
23-8	2531-2534	the	object[131]	giv[131]	coref	23-40[141_131]
23-9	2535-2541	images	object[131]	giv[131]	_	_
23-10	2542-2544	of	object[131]	giv[131]	_	_
23-11	2545-2546	a	object[131]|object[132]	giv[131]|giv[132]	coref	23-43[142_132]
23-12	2547-2555	database	object[131]|object[132]	giv[131]|giv[132]	_	_
23-13	2556-2559	are	_	_	_	_
23-14	2560-2569	described	_	_	_	_
23-15	2570-2574	with	_	_	_	_
23-16	2575-2591	high-dimensional	abstract[133]	giv[133]	coref	27-15[185_133]
23-17	2592-2600	features	abstract[133]	giv[133]	_	_
23-18	2601-2602	,	_	_	_	_
23-19	2603-2611	limiting	_	_	_	_
23-20	2612-2615	its	abstract|abstract[135]	giv|new[135]	coref|coref	27-30[189_0]|27-30[189_0]
23-21	2616-2621	scope	abstract[135]	new[135]	_	_
23-22	2622-2624	of	abstract[135]	new[135]	_	_
23-23	2625-2637	applications	abstract[135]|abstract	new[135]|new	_	_
23-24	2638-2639	.	_	_	_	_
23-25	2640-2644	Azzi	person	new	_	_
23-26	2645-2647	et	_	_	_	_
23-27	2648-2651	al.	_	_	_	_
23-28	2652-2655	use	_	_	_	_
23-29	2656-2657	a	abstract[138]	giv[138]	_	_
23-30	2658-2664	global	abstract[138]	giv[138]	_	_
23-31	2665-2678	feature-based	abstract[138]	giv[138]	_	_
23-32	2679-2685	system	abstract[138]	giv[138]	_	_
23-33	2686-2688	to	_	_	_	_
23-34	2689-2695	reduce	_	_	_	_
23-35	2696-2699	the	abstract[140]	new[140]	_	_
23-36	2700-2706	search	event|abstract[140]	giv|new[140]	coref	24-11[153_0]
23-37	2707-2712	space	abstract[140]	new[140]	_	_
23-38	2713-2716	and	_	_	_	_
23-39	2717-2721	find	_	_	_	_
23-40	2722-2731	candidate	object[141]	giv[141]	coref	26-24[179_141]
23-41	2732-2738	images	object[141]	giv[141]	_	_
23-42	2739-2741	in	_	_	_	_
23-43	2742-2745	the	object[142]	giv[142]	coref	26-24[0_142]
23-44	2746-2754	database	object[142]	giv[142]	_	_
23-45	2755-2756	,	_	_	_	_
23-46	2757-2761	then	_	_	_	_
23-47	2762-2765	the	abstract[145]	new[145]	_	_
23-48	2766-2771	local	abstract[145]	new[145]	_	_
23-49	2772-2779	feature	abstract|abstract[145]	giv|new[145]	coref	23-51
23-50	2780-2795	scale-invariant	abstract[145]	new[145]	_	_
23-51	2796-2803	feature	abstract|abstract[145]	giv|new[145]	coref	25-29
23-52	2804-2813	transform	abstract[145]	new[145]	_	_
23-53	2814-2815	(	_	_	_	_
23-54	2816-2820	SIFT	_	_	_	_
23-55	2821-2822	)	_	_	_	_
23-56	2823-2825	is	_	_	_	_
23-57	2826-2833	adopted	_	_	_	_
23-58	2834-2837	for	_	_	_	_
23-59	2838-2844	points	abstract	new	_	_
23-60	2845-2853	matching	_	_	_	_
23-61	2854-2856	in	_	_	_	_
23-62	2857-2861	pose	abstract|abstract[148]	new|new[148]	coref|coref	31-16[221_0]|31-16[221_0]
23-63	2862-2872	estimation	abstract[148]	new[148]	_	_
23-64	2873-2874	.	_	_	_	_

#Text=Some researchers try to trade accuracy for rapidity by using approximate nearest neighbor search , such as quantization and vocabulary tree .
24-1	2875-2879	Some	person[149]	new[149]	_	_
24-2	2880-2891	researchers	person[149]	new[149]	_	_
24-3	2892-2895	try	_	_	_	_
24-4	2896-2898	to	_	_	_	_
24-5	2899-2904	trade	_	_	_	_
24-6	2905-2913	accuracy	abstract	new	coref	36-9[249_0]
24-7	2914-2917	for	_	_	_	_
24-8	2918-2926	rapidity	abstract	new	_	_
24-9	2927-2929	by	_	_	_	_
24-10	2930-2935	using	_	_	_	_
24-11	2936-2947	approximate	event[153]	giv[153]	coref	25-10[161_153]
24-12	2948-2955	nearest	event[153]	giv[153]	_	_
24-13	2956-2964	neighbor	person|event[153]	new|giv[153]	_	_
24-14	2965-2971	search	event[153]	giv[153]	_	_
24-15	2972-2973	,	event[153]	giv[153]	_	_
24-16	2974-2978	such	event[153]	giv[153]	_	_
24-17	2979-2981	as	event[153]	giv[153]	_	_
24-18	2982-2994	quantization	event[153]|abstract|plant[156]	giv[153]|new|new[156]	_	_
24-19	2995-2998	and	event[153]|plant[156]	giv[153]|new[156]	_	_
24-20	2999-3009	vocabulary	event[153]|abstract|plant[156]	giv[153]|new|new[156]	_	_
24-21	3010-3014	tree	event[153]|plant[156]	giv[153]|new[156]	_	_
24-22	3015-3016	.	_	_	_	_

#Text=Another common way to save time and memory of similarity search is principal component analysis ( PCA ) , which has been used to reduce the size of feature vectors and descriptors .
25-1	3017-3024	Another	abstract[157]	new[157]	coref	25-13[163_157]
25-2	3025-3031	common	abstract[157]	new[157]	_	_
25-3	3032-3035	way	abstract[157]	new[157]	_	_
25-4	3036-3038	to	_	_	_	_
25-5	3039-3043	save	_	_	_	_
25-6	3044-3048	time	time	new	coref	36-14[251_0]
25-7	3049-3052	and	_	_	_	_
25-8	3053-3059	memory	abstract[159]	new[159]	_	_
25-9	3060-3062	of	abstract[159]	new[159]	_	_
25-10	3063-3073	similarity	abstract[159]|abstract|event[161]	new[159]|giv|giv[161]	coref|coref	26-18[175_0]|26-18[175_0]
25-11	3074-3080	search	abstract[159]|event[161]	new[159]|giv[161]	_	_
25-12	3081-3083	is	_	_	_	_
25-13	3084-3093	principal	abstract[163]	giv[163]	appos	25-17[0_163]
25-14	3094-3103	component	abstract|abstract[163]	new|giv[163]	_	_
25-15	3104-3112	analysis	abstract[163]	giv[163]	_	_
25-16	3113-3114	(	_	_	_	_
25-17	3115-3118	PCA	abstract	giv	_	_
25-18	3119-3120	)	_	_	_	_
25-19	3121-3122	,	_	_	_	_
25-20	3123-3128	which	_	_	_	_
25-21	3129-3132	has	_	_	_	_
25-22	3133-3137	been	_	_	_	_
25-23	3138-3142	used	_	_	_	_
25-24	3143-3145	to	_	_	_	_
25-25	3146-3152	reduce	_	_	_	_
25-26	3153-3156	the	abstract[165]	new[165]	_	_
25-27	3157-3161	size	abstract[165]	new[165]	_	_
25-28	3162-3164	of	abstract[165]	new[165]	_	_
25-29	3165-3172	feature	abstract[165]|abstract|object[167]	new[165]|giv|giv[167]	coref|coref	33-15|33-15
25-30	3173-3180	vectors	abstract[165]|object[167]	new[165]|giv[167]	_	_
25-31	3181-3184	and	abstract[165]|object[167]	new[165]|giv[167]	_	_
25-32	3185-3196	descriptors	abstract[165]|object[167]|abstract	new[165]|giv[167]|giv	coref	28-10[196_0]
25-33	3197-3198	.	_	_	_	_

#Text=Some works use correlation algorithms , such as sum of absolute difference ( SAD ) , for computing similarity between query image and database images .
26-1	3199-3203	Some	abstract[169]	new[169]	_	_
26-2	3204-3209	works	abstract[169]	new[169]	_	_
26-3	3210-3213	use	_	_	_	_
26-4	3214-3225	correlation	abstract|abstract[171]	new|giv[171]	coref|coref	27-5[181_171]|27-5[181_171]
26-5	3226-3236	algorithms	abstract[171]	giv[171]	_	_
26-6	3237-3238	,	abstract[171]	giv[171]	_	_
26-7	3239-3243	such	abstract[171]	giv[171]	_	_
26-8	3244-3246	as	abstract[171]	giv[171]	_	_
26-9	3247-3250	sum	abstract[171]|abstract[172]	giv[171]|new[172]	_	_
26-10	3251-3253	of	abstract[171]|abstract[172]	giv[171]|new[172]	_	_
26-11	3254-3262	absolute	abstract[171]|abstract[172]|abstract[173]	giv[171]|new[172]|new[173]	_	_
26-12	3263-3273	difference	abstract[171]|abstract[172]|abstract[173]	giv[171]|new[172]|new[173]	_	_
26-13	3274-3275	(	abstract[171]	giv[171]	_	_
26-14	3276-3279	SAD	abstract[171]	giv[171]	_	_
26-15	3280-3281	)	abstract[171]	giv[171]	_	_
26-16	3282-3283	,	abstract[171]	giv[171]	_	_
26-17	3284-3287	for	abstract[171]	giv[171]	_	_
26-18	3288-3297	computing	abstract[171]|abstract|abstract[175]	giv[171]|new|giv[175]	_	_
26-19	3298-3308	similarity	abstract[171]|abstract[175]	giv[171]|giv[175]	_	_
26-20	3309-3316	between	abstract[171]|abstract[175]	giv[171]|giv[175]	_	_
26-21	3317-3322	query	abstract[171]|abstract[175]|abstract|abstract[177]	giv[171]|giv[175]|giv|giv[177]	coref|coref|coref|coref	27-26[0_177]|29-15|27-26[0_177]|29-15
26-22	3323-3328	image	abstract[171]|abstract[175]|abstract[177]	giv[171]|giv[175]|giv[177]	_	_
26-23	3329-3332	and	abstract[171]|abstract[175]	giv[171]|giv[175]	_	_
26-24	3333-3341	database	abstract[171]|abstract[175]|object|object[179]	giv[171]|giv[175]|giv|giv[179]	coref|coref|coref|coref	29-2[199_179]|33-6[231_0]|29-2[199_179]|33-6[231_0]
26-25	3342-3348	images	abstract[171]|abstract[175]|object[179]	giv[171]|giv[175]|giv[179]	_	_
26-26	3349-3350	.	_	_	_	_

#Text=In recent studies , deep learning-based algorithms are an alternative to aforementioned methods . Razavian et al. use features extracted from a network as an image representation for image retrieval in a diverse set of datasets . Yandex et al.
27-1	3351-3353	In	_	_	_	_
27-2	3354-3360	recent	abstract[180]	new[180]	_	_
27-3	3361-3368	studies	abstract[180]	new[180]	_	_
27-4	3369-3370	,	_	_	_	_
27-5	3371-3375	deep	abstract[181]	giv[181]	coref	27-9[182_181]
27-6	3376-3390	learning-based	abstract[181]	giv[181]	_	_
27-7	3391-3401	algorithms	abstract[181]	giv[181]	_	_
27-8	3402-3405	are	_	_	_	_
27-9	3406-3408	an	abstract[182]	giv[182]	_	_
27-10	3409-3420	alternative	abstract[182]	giv[182]	_	_
27-11	3421-3423	to	abstract[182]	giv[182]	_	_
27-12	3424-3438	aforementioned	abstract[182]|abstract[183]	giv[182]|giv[183]	coref	31-1[217_183]
27-13	3439-3446	methods	abstract[182]|abstract[183]	giv[182]|giv[183]	_	_
27-14	3447-3448	.	abstract[182]|abstract[183]	giv[182]|giv[183]	_	_
27-15	3449-3457	Razavian	abstract[182]|abstract[183]|abstract[185]	giv[182]|giv[183]|giv[185]	coref	28-6[194_185]
27-16	3458-3460	et	abstract[182]|abstract[183]|abstract[185]	giv[182]|giv[183]|giv[185]	_	_
27-17	3461-3464	al.	abstract[182]|abstract[183]|abstract[185]	giv[182]|giv[183]|giv[185]	_	_
27-18	3465-3468	use	abstract[182]|abstract[183]|abstract|abstract[185]	giv[182]|giv[183]|new|giv[185]	_	_
27-19	3469-3477	features	abstract[182]|abstract[183]|abstract[185]	giv[182]|giv[183]|giv[185]	_	_
27-20	3478-3487	extracted	_	_	_	_
27-21	3488-3492	from	_	_	_	_
27-22	3493-3494	a	abstract[186]	new[186]	_	_
27-23	3495-3502	network	abstract[186]	new[186]	_	_
27-24	3503-3505	as	_	_	_	_
27-25	3506-3508	an	_	_	_	_
27-26	3509-3514	image	abstract	giv	coref	27-29
27-27	3515-3529	representation	_	_	_	_
27-28	3530-3533	for	_	_	_	_
27-29	3534-3539	image	abstract	giv	coref	28-13
27-30	3540-3549	retrieval	abstract[189]	giv[189]	coref	28-13[198_189]
27-31	3550-3552	in	abstract[189]	giv[189]	_	_
27-32	3553-3554	a	abstract[189]|object[190]	giv[189]|new[190]	_	_
27-33	3555-3562	diverse	abstract[189]|object[190]	giv[189]|new[190]	_	_
27-34	3563-3566	set	abstract[189]|object[190]	giv[189]|new[190]	_	_
27-35	3567-3569	of	abstract[189]|object[190]	giv[189]|new[190]	_	_
27-36	3570-3578	datasets	abstract[189]|object[190]|abstract	giv[189]|new[190]|new	_	_
27-37	3579-3580	.	_	_	_	_
27-38	3581-3587	Yandex	person	new	_	_
27-39	3588-3590	et	_	_	_	_
27-40	3591-3594	al.	_	_	_	_

#Text=propose a method that aggregates local deep features to product descriptors for image retrieval .
28-1	3595-3602	propose	_	_	_	_
28-2	3603-3604	a	abstract[193]	new[193]	coref	38-7[269_193]
28-3	3605-3611	method	abstract[193]	new[193]	_	_
28-4	3612-3616	that	_	_	_	_
28-5	3617-3627	aggregates	_	_	_	_
28-6	3628-3633	local	abstract[194]	giv[194]	coref	36-19[252_194]
28-7	3634-3638	deep	abstract[194]	giv[194]	_	_
28-8	3639-3647	features	abstract[194]	giv[194]	_	_
28-9	3648-3650	to	_	_	_	_
28-10	3651-3658	product	abstract|abstract[196]	new|giv[196]	coref|coref	33-15[233_196]|33-15[233_196]
28-11	3659-3670	descriptors	abstract[196]	giv[196]	_	_
28-12	3671-3674	for	abstract[196]	giv[196]	_	_
28-13	3675-3680	image	abstract[196]|abstract|abstract[198]	giv[196]|giv|giv[198]	coref|coref	29-14[203_0]|29-14[203_0]
28-14	3681-3690	retrieval	abstract[196]|abstract[198]	giv[196]|giv[198]	_	_
28-15	3691-3692	.	_	_	_	_

#Text=After a set of candidate images are retrieved , the position information of the query image is calculated according to the geospatial information of these candidate images through a weighting scheme or linear combination .
29-1	3693-3698	After	_	_	_	_
29-2	3699-3700	a	object[199]	giv[199]	coref	29-25[206_199]
29-3	3701-3704	set	object[199]	giv[199]	_	_
29-4	3705-3707	of	object[199]	giv[199]	_	_
29-5	3708-3717	candidate	object[199]	giv[199]	_	_
29-6	3718-3724	images	object[199]	giv[199]	_	_
29-7	3725-3728	are	_	_	_	_
29-8	3729-3738	retrieved	_	_	_	_
29-9	3739-3740	,	_	_	_	_
29-10	3741-3744	the	abstract[201]	new[201]	_	_
29-11	3745-3753	position	abstract|abstract[201]	giv|new[201]	coref	30-5
29-12	3754-3765	information	abstract[201]	new[201]	_	_
29-13	3766-3768	of	abstract[201]	new[201]	_	_
29-14	3769-3772	the	abstract[201]|abstract[203]	new[201]|giv[203]	coref	31-18[223_203]
29-15	3773-3778	query	abstract[201]|abstract|abstract[203]	new[201]|giv|giv[203]	coref	31-19
29-16	3779-3784	image	abstract[201]|abstract[203]	new[201]|giv[203]	_	_
29-17	3785-3787	is	_	_	_	_
29-18	3788-3798	calculated	_	_	_	_
29-19	3799-3808	according	_	_	_	_
29-20	3809-3811	to	_	_	_	_
29-21	3812-3815	the	abstract[204]	new[204]	_	_
29-22	3816-3826	geospatial	abstract[204]	new[204]	_	_
29-23	3827-3838	information	abstract[204]	new[204]	_	_
29-24	3839-3841	of	abstract[204]	new[204]	_	_
29-25	3842-3847	these	abstract[204]|object[206]	new[204]|giv[206]	coref	33-18[234_206]
29-26	3848-3857	candidate	abstract[204]|person|object[206]	new[204]|giv|giv[206]	_	_
29-27	3858-3864	images	abstract[204]|object[206]	new[204]|giv[206]	_	_
29-28	3865-3872	through	abstract[204]|object[206]	new[204]|giv[206]	_	_
29-29	3873-3874	a	abstract[204]|object[206]|abstract[207]	new[204]|giv[206]|new[207]	_	_
29-30	3875-3884	weighting	abstract[204]|object[206]|abstract[207]	new[204]|giv[206]|new[207]	_	_
29-31	3885-3891	scheme	abstract[204]|object[206]|abstract[207]	new[204]|giv[206]|new[207]	_	_
29-32	3892-3894	or	abstract[204]|object[206]	new[204]|giv[206]	_	_
29-33	3895-3901	linear	abstract[204]|object[206]|abstract[208]	new[204]|giv[206]|new[208]	_	_
29-34	3902-3913	combination	abstract[204]|object[206]|abstract[208]	new[204]|giv[206]|new[208]	_	_
29-35	3914-3915	.	_	_	_	_

#Text=However , because this position result is not calculated by strict geometric relations , it is rough in most cases and difficult to meet the requirement of high-accuracy positioning .
30-1	3916-3923	However	_	_	_	_
30-2	3924-3925	,	_	_	_	_
30-3	3926-3933	because	_	_	_	_
30-4	3934-3938	this	abstract[210]	new[210]	ana	30-15[0_210]
30-5	3939-3947	position	abstract|abstract[210]	giv|new[210]	_	_
30-6	3948-3954	result	abstract[210]	new[210]	_	_
30-7	3955-3957	is	_	_	_	_
30-8	3958-3961	not	_	_	_	_
30-9	3962-3972	calculated	_	_	_	_
30-10	3973-3975	by	_	_	_	_
30-11	3976-3982	strict	abstract[211]	new[211]	_	_
30-12	3983-3992	geometric	abstract[211]	new[211]	_	_
30-13	3993-4002	relations	abstract[211]	new[211]	_	_
30-14	4003-4004	,	_	_	_	_
30-15	4005-4007	it	abstract	giv	_	_
30-16	4008-4010	is	_	_	_	_
30-17	4011-4016	rough	_	_	_	_
30-18	4017-4019	in	_	_	_	_
30-19	4020-4024	most	abstract[213]	new[213]	_	_
30-20	4025-4030	cases	abstract[213]	new[213]	_	_
30-21	4031-4034	and	_	_	_	_
30-22	4035-4044	difficult	_	_	_	_
30-23	4045-4047	to	_	_	_	_
30-24	4048-4052	meet	_	_	_	_
30-25	4053-4056	the	abstract[214]	new[214]	_	_
30-26	4057-4068	requirement	abstract[214]	new[214]	_	_
30-27	4069-4071	of	abstract[214]	new[214]	_	_
30-28	4072-4085	high-accuracy	abstract[214]|abstract[215]	new[214]|giv[215]	coref	31-3[0_215]
30-29	4086-4097	positioning	abstract[214]|abstract[215]	new[214]|giv[215]	_	_
30-30	4098-4099	.	_	_	_	_

#Text=Visual landmarks-based positioning methods aim to provide a six degrees of freedom ( DoF ) pose of the query image .
31-1	4100-4106	Visual	abstract[217]	giv[217]	coref	36-5[248_217]
31-2	4107-4122	landmarks-based	abstract[217]	giv[217]	_	_
31-3	4123-4134	positioning	abstract|abstract[217]	giv|giv[217]	coref	38-15
31-4	4135-4142	methods	abstract[217]	giv[217]	_	_
31-5	4143-4146	aim	_	_	_	_
31-6	4147-4149	to	_	_	_	_
31-7	4150-4157	provide	_	_	_	_
31-8	4158-4159	a	abstract[218]	new[218]	_	_
31-9	4160-4163	six	abstract[218]	new[218]	_	_
31-10	4164-4171	degrees	abstract[218]	new[218]	_	_
31-11	4172-4174	of	abstract[218]	new[218]	_	_
31-12	4175-4182	freedom	abstract[218]|abstract	new[218]|new	appos	31-14
31-13	4183-4184	(	_	_	_	_
31-14	4185-4188	DoF	abstract	giv	_	_
31-15	4189-4190	)	_	_	_	_
31-16	4191-4195	pose	abstract[221]	giv[221]	_	_
31-17	4196-4198	of	abstract[221]	giv[221]	_	_
31-18	4199-4202	the	abstract[221]|abstract[223]	giv[221]|giv[223]	coref	35-5[243_223]
31-19	4203-4208	query	abstract[221]|abstract|abstract[223]	giv[221]|giv|giv[223]	coref	35-5
31-20	4209-4214	image	abstract[221]|abstract[223]	giv[221]|giv[223]	_	_
31-21	4215-4216	.	_	_	_	_

#Text=Generally , visual landmarks in the indoor environments includes natural landmarks and artificial landmarks .
32-1	4217-4226	Generally	_	_	_	_
32-2	4227-4228	,	_	_	_	_
32-3	4229-4235	visual	object[224]	new[224]	coref	33-1[229_224]
32-4	4236-4245	landmarks	object[224]	new[224]	_	_
32-5	4246-4248	in	object[224]	new[224]	_	_
32-6	4249-4252	the	object[224]|place[226]	new[224]|new[226]	coref	40-8[278_226]
32-7	4253-4259	indoor	object[224]|abstract|place[226]	new[224]|giv|new[226]	coref	36-32
32-8	4260-4272	environments	object[224]|place[226]	new[224]|new[226]	_	_
32-9	4273-4281	includes	_	_	_	_
32-10	4282-4289	natural	abstract[227]	new[227]	_	_
32-11	4290-4299	landmarks	abstract[227]	new[227]	_	_
32-12	4300-4303	and	_	_	_	_
32-13	4304-4314	artificial	object[228]	new[228]	coref	37-14[261_228]
32-14	4315-4324	landmarks	object[228]	new[228]	_	_
32-15	4325-4326	.	_	_	_	_

#Text=The natural landmarks refer to the geo-tagged 3D database , which is represented by feature descriptors or images with poses .
33-1	4327-4330	The	object[229]	giv[229]	coref	37-4[259_229]
33-2	4331-4338	natural	object[229]	giv[229]	_	_
33-3	4339-4348	landmarks	object[229]	giv[229]	_	_
33-4	4349-4354	refer	_	_	_	_
33-5	4355-4357	to	_	_	_	_
33-6	4358-4361	the	object[231]	giv[231]	coref	34-1[236_231]
33-7	4362-4372	geo-tagged	object[231]	giv[231]	_	_
33-8	4373-4375	3D	abstract|object[231]	new|giv[231]	coref	36-26
33-9	4376-4384	database	object[231]	giv[231]	_	_
33-10	4385-4386	,	_	_	_	_
33-11	4387-4392	which	_	_	_	_
33-12	4393-4395	is	_	_	_	_
33-13	4396-4407	represented	_	_	_	_
33-14	4408-4410	by	_	_	_	_
33-15	4411-4418	feature	abstract|abstract[233]	giv|giv[233]	_	_
33-16	4419-4430	descriptors	abstract[233]	giv[233]	_	_
33-17	4431-4433	or	abstract[233]	giv[233]	_	_
33-18	4434-4440	images	abstract[233]|object[234]	giv[233]|giv[234]	_	_
33-19	4441-4445	with	abstract[233]|object[234]	giv[233]|giv[234]	_	_
33-20	4446-4451	poses	abstract[233]|object[234]|abstract	giv[233]|giv[234]|new	_	_
33-21	4452-4453	.	_	_	_	_

#Text=This database could have been built thanks to the mapping module of simultaneous localization and mapping ( SLAM ) .
34-1	4454-4458	This	object[236]	giv[236]	coref	36-25[256_236]
34-2	4459-4467	database	object[236]	giv[236]	_	_
34-3	4468-4473	could	_	_	_	_
34-4	4474-4478	have	_	_	_	_
34-5	4479-4483	been	_	_	_	_
34-6	4484-4489	built	_	_	_	_
34-7	4490-4496	thanks	_	_	_	_
34-8	4497-4499	to	_	_	_	_
34-9	4500-4503	the	abstract[238]	new[238]	coref	35-12[245_238]
34-10	4504-4511	mapping	object|abstract[238]	new|new[238]	_	_
34-11	4512-4518	module	abstract[238]	new[238]	_	_
34-12	4519-4521	of	abstract[238]	new[238]	_	_
34-13	4522-4534	simultaneous	abstract[238]|abstract[239]	new[238]|new[239]	_	_
34-14	4535-4547	localization	abstract[238]|abstract[239]	new[238]|new[239]	_	_
34-15	4548-4551	and	abstract[238]	new[238]	_	_
34-16	4552-4559	mapping	abstract[238]|abstract	new[238]|new	_	_
34-17	4560-4561	(	_	_	_	_
34-18	4562-4566	SLAM	_	_	_	_
34-19	4567-4568	)	_	_	_	_
34-20	4569-4570	.	_	_	_	_

#Text=Then the pose of query image is estimated by means of re-localization module and feature correspondence .
35-1	4571-4575	Then	_	_	_	_
35-2	4576-4579	the	abstract[241]	new[241]	_	_
35-3	4580-4584	pose	abstract[241]	new[241]	_	_
35-4	4585-4587	of	abstract[241]	new[241]	_	_
35-5	4588-4593	query	abstract[241]|abstract|abstract[243]	new[241]|giv|giv[243]	coref|coref|coref|coref	36-22|36-22[254_243]|36-22|36-22[254_243]
35-6	4594-4599	image	abstract[241]|abstract[243]	new[241]|giv[243]	_	_
35-7	4600-4602	is	_	_	_	_
35-8	4603-4612	estimated	_	_	_	_
35-9	4613-4615	by	_	_	_	_
35-10	4616-4621	means	_	_	_	_
35-11	4622-4624	of	_	_	_	_
35-12	4625-4640	re-localization	abstract|abstract[245]	new|giv[245]	_	_
35-13	4641-4647	module	abstract[245]	giv[245]	_	_
35-14	4648-4651	and	_	_	_	_
35-15	4652-4659	feature	_	_	_	_
35-16	4660-4674	correspondence	abstract	new	ana	36-12
35-17	4675-4676	.	_	_	_	_

#Text=Although the results of these methods are of good accuracy , it takes a long time to match the features of query image with geo-tagged 3D database , especially when the indoor scenes are large .
36-1	4677-4685	Although	_	_	_	_
36-2	4686-4689	the	abstract[247]	new[247]	_	_
36-3	4690-4697	results	abstract[247]	new[247]	_	_
36-4	4698-4700	of	abstract[247]	new[247]	_	_
36-5	4701-4706	these	abstract[247]|abstract[248]	new[247]|giv[248]	coref	37-11[0_248]
36-6	4707-4714	methods	abstract[247]|abstract[248]	new[247]|giv[248]	_	_
36-7	4715-4718	are	_	_	_	_
36-8	4719-4721	of	_	_	_	_
36-9	4722-4726	good	abstract[249]	giv[249]	_	_
36-10	4727-4735	accuracy	abstract[249]	giv[249]	_	_
36-11	4736-4737	,	_	_	_	_
36-12	4738-4740	it	abstract	giv	_	_
36-13	4741-4746	takes	_	_	_	_
36-14	4747-4748	a	time[251]	giv[251]	_	_
36-15	4749-4753	long	time[251]	giv[251]	_	_
36-16	4754-4758	time	time[251]	giv[251]	_	_
36-17	4759-4761	to	_	_	_	_
36-18	4762-4767	match	_	_	_	_
36-19	4768-4771	the	abstract[252]	giv[252]	_	_
36-20	4772-4780	features	abstract[252]	giv[252]	_	_
36-21	4781-4783	of	abstract[252]	giv[252]	_	_
36-22	4784-4789	query	abstract[252]|abstract|abstract[254]	giv[252]|giv|giv[254]	_	_
36-23	4790-4795	image	abstract[252]|abstract[254]	giv[252]|giv[254]	_	_
36-24	4796-4800	with	abstract[252]|abstract[254]	giv[252]|giv[254]	_	_
36-25	4801-4811	geo-tagged	abstract[252]|abstract[254]|object[256]	giv[252]|giv[254]|giv[256]	_	_
36-26	4812-4814	3D	abstract[252]|abstract[254]|abstract|object[256]	giv[252]|giv[254]|giv|giv[256]	_	_
36-27	4815-4823	database	abstract[252]|abstract[254]|object[256]	giv[252]|giv[254]|giv[256]	_	_
36-28	4824-4825	,	_	_	_	_
36-29	4826-4836	especially	_	_	_	_
36-30	4837-4841	when	_	_	_	_
36-31	4842-4845	the	abstract[258]	new[258]	_	_
36-32	4846-4852	indoor	place|abstract[258]	giv|new[258]	_	_
36-33	4853-4859	scenes	abstract[258]	new[258]	_	_
36-34	4860-4863	are	_	_	_	_
36-35	4864-4869	large	_	_	_	_
36-36	4870-4871	.	_	_	_	_

#Text=In addition to natural landmarks , there are also positioning methods based on artificial landmarks , e. g. , Degol et al. proposed a fiducial marker and detection algorithm .
37-1	4872-4874	In	_	_	_	_
37-2	4875-4883	addition	_	_	_	_
37-3	4884-4886	to	_	_	_	_
37-4	4887-4894	natural	object[259]	giv[259]	_	_
37-5	4895-4904	landmarks	object[259]	giv[259]	_	_
37-6	4905-4906	,	_	_	_	_
37-7	4907-4912	there	_	_	_	_
37-8	4913-4916	are	_	_	_	_
37-9	4917-4921	also	_	_	_	_
37-10	4922-4933	positioning	_	_	_	_
37-11	4934-4941	methods	abstract	giv	coref	40-2[276_0]
37-12	4942-4947	based	_	_	_	_
37-13	4948-4950	on	_	_	_	_
37-14	4951-4961	artificial	object[261]	giv[261]	_	_
37-15	4962-4971	landmarks	object[261]	giv[261]	_	_
37-16	4972-4973	,	_	_	_	_
37-17	4974-4976	e.	plant[262]	new[262]	_	_
37-18	4977-4979	g.	plant[262]	new[262]	_	_
37-19	4980-4981	,	_	_	_	_
37-20	4982-4987	Degol	person	new	_	_
37-21	4988-4990	et	_	_	_	_
37-22	4991-4994	al.	_	_	_	_
37-23	4995-5003	proposed	_	_	_	_
37-24	5004-5005	a	abstract[264]	new[264]	coref	39-5[273_264]
37-25	5006-5014	fiducial	abstract[264]	new[264]	_	_
37-26	5015-5021	marker	abstract[264]	new[264]	_	_
37-27	5022-5025	and	_	_	_	_
37-28	5026-5035	detection	abstract|abstract[266]	new|new[266]	_	_
37-29	5036-5045	algorithm	abstract[266]	new[266]	_	_
37-30	5046-5047	.	_	_	_	_

#Text=In reference , the authors proposed a method to simultaneously solve the problems of positioning from a set of squared planar markers .
38-1	5048-5050	In	_	_	_	_
38-2	5051-5060	reference	abstract	new	_	_
38-3	5061-5062	,	_	_	_	_
38-4	5063-5066	the	person[268]	new[268]	_	_
38-5	5067-5074	authors	person[268]	new[268]	_	_
38-6	5075-5083	proposed	_	_	_	_
38-7	5084-5085	a	abstract[269]	giv[269]	_	_
38-8	5086-5092	method	abstract[269]	giv[269]	_	_
38-9	5093-5095	to	_	_	_	_
38-10	5096-5110	simultaneously	_	_	_	_
38-11	5111-5116	solve	_	_	_	_
38-12	5117-5120	the	abstract[270]	new[270]	_	_
38-13	5121-5129	problems	abstract[270]	new[270]	_	_
38-14	5130-5132	of	abstract[270]	new[270]	_	_
38-15	5133-5144	positioning	abstract[270]|abstract	new[270]|giv	_	_
38-16	5145-5149	from	_	_	_	_
38-17	5150-5151	a	object[272]	new[272]	coref	40-6[0_272]
38-18	5152-5155	set	object[272]	new[272]	_	_
38-19	5156-5158	of	object[272]	new[272]	_	_
38-20	5159-5166	squared	object[272]	new[272]	_	_
38-21	5167-5173	planar	object[272]	new[272]	_	_
38-22	5174-5181	markers	object[272]	new[272]	_	_
38-23	5182-5183	.	_	_	_	_

#Text=However , positioning from a planar marker suffers from the ambiguity problem .
39-1	5184-5191	However	_	_	_	_
39-2	5192-5193	,	_	_	_	_
39-3	5194-5205	positioning	_	_	_	_
39-4	5206-5210	from	_	_	_	_
39-5	5211-5212	a	abstract[273]	giv[273]	_	_
39-6	5213-5219	planar	abstract[273]	giv[273]	_	_
39-7	5220-5226	marker	abstract[273]	giv[273]	_	_
39-8	5227-5234	suffers	_	_	_	_
39-9	5235-5239	from	_	_	_	_
39-10	5240-5243	the	abstract[275]	new[275]	_	_
39-11	5244-5253	ambiguity	abstract|abstract[275]	new|new[275]	_	_
39-12	5254-5261	problem	abstract[275]	new[275]	_	_
39-13	5262-5263	.	_	_	_	_

#Text=Since these methods require posting markers in the environments , they are not suitable for places such as shopping malls that maintain a clean appearance .
40-1	5264-5269	Since	_	_	_	_
40-2	5270-5275	these	abstract[276]	giv[276]	ana	40-11[0_276]
40-3	5276-5283	methods	abstract[276]	giv[276]	_	_
40-4	5284-5291	require	_	_	_	_
40-5	5292-5299	posting	_	_	_	_
40-6	5300-5307	markers	object	giv	_	_
40-7	5308-5310	in	_	_	_	_
40-8	5311-5314	the	place[278]	giv[278]	_	_
40-9	5315-5327	environments	place[278]	giv[278]	_	_
40-10	5328-5329	,	_	_	_	_
40-11	5330-5334	they	abstract	giv	_	_
40-12	5335-5338	are	_	_	_	_
40-13	5339-5342	not	_	_	_	_
40-14	5343-5351	suitable	_	_	_	_
40-15	5352-5355	for	_	_	_	_
40-16	5356-5362	places	place[280]	new[280]	_	_
40-17	5363-5367	such	place[280]	new[280]	_	_
40-18	5368-5370	as	place[280]	new[280]	_	_
40-19	5371-5379	shopping	place[280]|place[281]	new[280]|new[281]	_	_
40-20	5380-5385	malls	place[280]|place[281]	new[280]|new[281]	_	_
40-21	5386-5390	that	_	_	_	_
40-22	5391-5399	maintain	_	_	_	_
40-23	5400-5401	a	abstract[282]	new[282]	_	_
40-24	5402-5407	clean	abstract[282]	new[282]	_	_
40-25	5408-5418	appearance	abstract[282]	new[282]	_	_
40-26	5419-5420	.	_	_	_	_
