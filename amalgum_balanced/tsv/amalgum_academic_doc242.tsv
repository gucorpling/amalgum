#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=2. Gradient Descent Learning of Stochastic Neural Networks
1-1	0-2	2.	_	_	_	_
1-2	3-11	Gradient	substance[1]	new[1]	_	_
1-3	12-19	Descent	substance[1]|abstract	new[1]|new	_	_
1-4	20-28	Learning	abstract[3]	new[3]	coref	7-5[0_3]
1-5	29-31	of	abstract[3]	new[3]	_	_
1-6	32-42	Stochastic	abstract[3]|abstract[5]	new[3]|new[5]	coref	3-1[7_5]
1-7	43-49	Neural	abstract[3]|abstract|abstract[5]	new[3]|new|new[5]	_	_
1-8	50-58	Networks	abstract[3]|abstract[5]	new[3]|new[5]	_	_

#Text=2.1 .
2-1	59-62	2.1	abstract	new	_	_
2-2	63-64	.	_	_	_	_

#Text=Stochastic Neural Networks
3-1	65-75	Stochastic	abstract[7]	giv[7]	_	_
3-2	76-82	Neural	abstract[7]	giv[7]	_	_
3-3	83-91	Networks	abstract[7]	giv[7]	_	_

#Text=Since the natural gradient is derived from stochastic neural network models , let us start from the brief description of the two popular stochastic models .
4-1	92-97	Since	_	_	_	_
4-2	98-101	the	abstract[8]	new[8]	coref	19-1[125_8]
4-3	102-109	natural	abstract[8]	new[8]	_	_
4-4	110-118	gradient	abstract[8]	new[8]	_	_
4-5	119-121	is	_	_	_	_
4-6	122-129	derived	_	_	_	_
4-7	130-134	from	_	_	_	_
4-8	135-145	stochastic	abstract[10]	new[10]	coref	4-21[13_10]
4-9	146-152	neural	abstract[10]	new[10]	_	_
4-10	153-160	network	abstract|abstract[10]	new|new[10]	coref	13-6
4-11	161-167	models	abstract[10]	new[10]	_	_
4-12	168-169	,	_	_	_	_
4-13	170-173	let	_	_	_	_
4-14	174-176	us	person	acc	ana	12-2
4-15	177-182	start	_	_	_	_
4-16	183-187	from	_	_	_	_
4-17	188-191	the	abstract[12]	new[12]	_	_
4-18	192-197	brief	abstract[12]	new[12]	_	_
4-19	198-209	description	abstract[12]	new[12]	_	_
4-20	210-212	of	abstract[12]	new[12]	_	_
4-21	213-216	the	abstract[12]|abstract[13]	new[12]|giv[13]	coref	18-16[118_13]
4-22	217-220	two	abstract[12]|abstract[13]	new[12]|giv[13]	_	_
4-23	221-228	popular	abstract[12]|abstract[13]	new[12]|giv[13]	_	_
4-24	229-239	stochastic	abstract[12]|abstract[13]	new[12]|giv[13]	_	_
4-25	240-246	models	abstract[12]|abstract[13]	new[12]|giv[13]	_	_
4-26	247-248	.	_	_	_	_

#Text=Although typical neural networks , including deep networks , are defined as deterministic input-output mapping functions of input , output , and parameter , the observed data for training the networks always have inevitable noises and thus the input-output relation can be described in the stochastic manner .
5-1	249-257	Although	_	_	_	_
5-2	258-265	typical	place[14]	new[14]	coref	10-7[49_14]
5-3	266-272	neural	place[14]	new[14]	_	_
5-4	273-281	networks	place[14]	new[14]	_	_
5-5	282-283	,	place[14]	new[14]	_	_
5-6	284-293	including	place[14]	new[14]	_	_
5-7	294-298	deep	place[14]|place[15]	new[14]|new[15]	coref	5-30[21_15]
5-8	299-307	networks	place[14]|place[15]	new[14]|new[15]	_	_
5-9	308-309	,	_	_	_	_
5-10	310-313	are	_	_	_	_
5-11	314-321	defined	_	_	_	_
5-12	322-324	as	_	_	_	_
5-13	325-338	deterministic	_	_	_	_
5-14	339-351	input-output	_	_	_	_
5-15	352-359	mapping	object	new	_	_
5-16	360-369	functions	_	_	_	_
5-17	370-372	of	_	_	_	_
5-18	373-378	input	abstract	new	_	_
5-19	379-380	,	_	_	_	_
5-20	381-387	output	abstract	new	coref	6-5[26_0]
5-21	388-389	,	_	_	_	_
5-22	390-393	and	_	_	_	_
5-23	394-403	parameter	abstract	new	coref	7-13
5-24	404-405	,	_	_	_	_
5-25	406-409	the	abstract[20]	new[20]	_	_
5-26	410-418	observed	abstract[20]	new[20]	_	_
5-27	419-423	data	abstract[20]	new[20]	_	_
5-28	424-427	for	_	_	_	_
5-29	428-436	training	_	_	_	_
5-30	437-440	the	place[21]	giv[21]	_	_
5-31	441-449	networks	place[21]	giv[21]	_	_
5-32	450-456	always	_	_	_	_
5-33	457-461	have	_	_	_	_
5-34	462-472	inevitable	abstract[22]	new[22]	_	_
5-35	473-479	noises	abstract[22]	new[22]	_	_
5-36	480-483	and	_	_	_	_
5-37	484-488	thus	_	_	_	_
5-38	489-492	the	abstract[23]	new[23]	_	_
5-39	493-505	input-output	abstract[23]	new[23]	_	_
5-40	506-514	relation	abstract[23]	new[23]	_	_
5-41	515-518	can	_	_	_	_
5-42	519-521	be	_	_	_	_
5-43	522-531	described	_	_	_	_
5-44	532-534	in	_	_	_	_
5-45	535-538	the	abstract[24]	new[24]	_	_
5-46	539-549	stochastic	abstract[24]	new[24]	_	_
5-47	550-556	manner	abstract[24]	new[24]	_	_
5-48	557-558	.	_	_	_	_

#Text=In other words , the observed output can be regarded as a random vector that is dependent on the deterministic function and some additional stochastic process that is described by the conditional probability .
6-1	559-561	In	_	_	_	_
6-2	562-567	other	abstract[25]	new[25]	_	_
6-3	568-573	words	abstract[25]	new[25]	_	_
6-4	574-575	,	_	_	_	_
6-5	576-579	the	abstract[26]	giv[26]	coref	11-6[52_26]
6-6	580-588	observed	abstract[26]	giv[26]	_	_
6-7	589-595	output	abstract[26]	giv[26]	_	_
6-8	596-599	can	_	_	_	_
6-9	600-602	be	_	_	_	_
6-10	603-611	regarded	_	_	_	_
6-11	612-614	as	_	_	_	_
6-12	615-616	a	_	_	_	_
6-13	617-623	random	_	_	_	_
6-14	624-630	vector	_	_	_	_
6-15	631-635	that	_	_	_	_
6-16	636-638	is	_	_	_	_
6-17	639-648	dependent	_	_	_	_
6-18	649-651	on	_	_	_	_
6-19	652-655	the	abstract[27]	new[27]	coref	7-16[35_27]
6-20	656-669	deterministic	abstract[27]	new[27]	_	_
6-21	670-678	function	abstract[27]	new[27]	_	_
6-22	679-682	and	_	_	_	_
6-23	683-687	some	abstract[28]	new[28]	_	_
6-24	688-698	additional	abstract[28]	new[28]	_	_
6-25	699-709	stochastic	abstract[28]	new[28]	_	_
6-26	710-717	process	abstract[28]	new[28]	_	_
6-27	718-722	that	_	_	_	_
6-28	723-725	is	_	_	_	_
6-29	726-735	described	_	_	_	_
6-30	736-738	by	_	_	_	_
6-31	739-742	the	abstract[29]	new[29]	coref	10-22[0_29]
6-32	743-754	conditional	abstract[29]	new[29]	_	_
6-33	755-766	probability	abstract[29]	new[29]	_	_
6-34	767-768	.	_	_	_	_

#Text=Then the goal of learning is to find an optimal value of parameter that minimizes the loss function defined as negative log likelihood of given input-output sample .
7-1	769-773	Then	_	_	_	_
7-2	774-777	the	event[30]	new[30]	_	_
7-3	778-782	goal	event[30]	new[30]	_	_
7-4	783-785	of	event[30]	new[30]	_	_
7-5	786-794	learning	event[30]|abstract	new[30]|giv	coref	18-42[124_0]
7-6	795-797	is	_	_	_	_
7-7	798-800	to	_	_	_	_
7-8	801-805	find	_	_	_	_
7-9	806-808	an	abstract[32]	new[32]	_	_
7-10	809-816	optimal	abstract[32]	new[32]	_	_
7-11	817-822	value	abstract[32]	new[32]	_	_
7-12	823-825	of	abstract[32]	new[32]	_	_
7-13	826-835	parameter	abstract[32]|abstract	new[32]|giv	coref	8-13[40_0]
7-14	836-840	that	_	_	_	_
7-15	841-850	minimizes	abstract	new	coref|none	28-33[205_0]|7-15[0_205]
7-16	851-854	the	abstract[35]	giv[35]	coref	8-1[39_35]
7-17	855-859	loss	abstract|abstract[35]	new|giv[35]	coref	8-2
7-18	860-868	function	abstract[35]	giv[35]	_	_
7-19	869-876	defined	_	_	_	_
7-20	877-879	as	_	_	_	_
7-21	880-888	negative	_	_	_	_
7-22	889-892	log	object	new	coref	16-14
7-23	893-903	likelihood	_	_	_	_
7-24	904-906	of	_	_	_	_
7-25	907-912	given	abstract[37]	new[37]	_	_
7-26	913-925	input-output	abstract[37]	new[37]	_	_
7-27	926-932	sample	abstract[37]	new[37]	_	_
7-28	933-934	.	_	_	_	_

#Text=The loss function can then be written as ( 1 ) and the optimal parameter is described as ( 2 )
8-1	935-938	The	abstract[39]	giv[39]	coref	9-20[46_39]
8-2	939-943	loss	abstract|abstract[39]	giv|giv[39]	coref	12-29
8-3	944-952	function	abstract[39]	giv[39]	_	_
8-4	953-956	can	_	_	_	_
8-5	957-961	then	_	_	_	_
8-6	962-964	be	_	_	_	_
8-7	965-972	written	_	_	_	_
8-8	973-975	as	_	_	_	_
8-9	976-977	(	_	_	_	_
8-10	978-979	1	_	_	_	_
8-11	980-981	)	_	_	_	_
8-12	982-985	and	_	_	_	_
8-13	986-989	the	abstract[40]	giv[40]	coref	9-14[0_40]
8-14	990-997	optimal	abstract[40]	giv[40]	_	_
8-15	998-1007	parameter	abstract[40]	giv[40]	_	_
8-16	1008-1010	is	_	_	_	_
8-17	1011-1020	described	_	_	_	_
8-18	1021-1023	as	_	_	_	_
8-19	1024-1025	(	_	_	_	_
8-20	1026-1027	2	_	_	_	_
8-21	1028-1029	)	_	_	_	_

#Text=Note that the last term in equation ( 1 ) is independent of parameter and can be ignored in the objective function for optimization .
9-1	1030-1034	Note	_	_	_	_
9-2	1035-1039	that	_	_	_	_
9-3	1040-1043	the	abstract[41]	new[41]	_	_
9-4	1044-1048	last	abstract[41]	new[41]	_	_
9-5	1049-1053	term	abstract[41]	new[41]	_	_
9-6	1054-1056	in	abstract[41]	new[41]	_	_
9-7	1057-1065	equation	abstract[41]|abstract	new[41]|new	coref	16-17[104_0]
9-8	1066-1067	(	_	_	_	_
9-9	1068-1069	1	quantity	new	_	_
9-10	1070-1071	)	_	_	_	_
9-11	1072-1074	is	_	_	_	_
9-12	1075-1086	independent	_	_	_	_
9-13	1087-1089	of	_	_	_	_
9-14	1090-1099	parameter	abstract	giv	coref	27-27
9-15	1100-1103	and	_	_	_	_
9-16	1104-1107	can	_	_	_	_
9-17	1108-1110	be	_	_	_	_
9-18	1111-1118	ignored	_	_	_	_
9-19	1119-1121	in	_	_	_	_
9-20	1122-1125	the	abstract[46]	giv[46]	coref	11-14[56_46]
9-21	1126-1135	objective	abstract|abstract[46]	new|giv[46]	coref	28-26[203_0]
9-22	1136-1144	function	abstract[46]	giv[46]	_	_
9-23	1145-1148	for	abstract[46]	giv[46]	_	_
9-24	1149-1161	optimization	abstract[46]|abstract	giv[46]|new	_	_
9-25	1162-1163	.	_	_	_	_

#Text=Based on the general definition , the conventional neural networks can be regarded as a special case with a specific conditional probability distribution , .
10-1	1164-1169	Based	_	_	_	_
10-2	1170-1172	on	_	_	_	_
10-3	1173-1176	the	abstract[48]	new[48]	_	_
10-4	1177-1184	general	abstract[48]	new[48]	_	_
10-5	1185-1195	definition	abstract[48]	new[48]	_	_
10-6	1196-1197	,	_	_	_	_
10-7	1198-1201	the	place[49]	giv[49]	coref	11-17[0_49]
10-8	1202-1214	conventional	place[49]	giv[49]	_	_
10-9	1215-1221	neural	place[49]	giv[49]	_	_
10-10	1222-1230	networks	place[49]	giv[49]	_	_
10-11	1231-1234	can	_	_	_	_
10-12	1235-1237	be	_	_	_	_
10-13	1238-1246	regarded	_	_	_	_
10-14	1247-1249	as	_	_	_	_
10-15	1250-1251	a	_	_	_	_
10-16	1252-1259	special	_	_	_	_
10-17	1260-1264	case	_	_	_	_
10-18	1265-1269	with	_	_	_	_
10-19	1270-1271	a	abstract[51]	new[51]	coref	12-11[62_51]
10-20	1272-1280	specific	abstract[51]	new[51]	_	_
10-21	1281-1292	conditional	abstract[51]	new[51]	_	_
10-22	1293-1304	probability	abstract|abstract[51]	giv|new[51]	coref	12-17
10-23	1305-1317	distribution	abstract[51]	new[51]	_	_
10-24	1318-1319	,	_	_	_	_
10-25	1320-1321	.	_	_	_	_

#Text=For example , consider that the output is observed with additive noise to the deterministic neural networks function such as ( 3 ) where is a random noise vector .
11-1	1322-1325	For	_	_	_	_
11-2	1326-1333	example	_	_	_	_
11-3	1334-1335	,	_	_	_	_
11-4	1336-1344	consider	_	_	_	_
11-5	1345-1349	that	_	_	_	_
11-6	1350-1353	the	abstract[52]	giv[52]	coref	14-9[82_52]
11-7	1354-1360	output	abstract[52]	giv[52]	_	_
11-8	1361-1363	is	_	_	_	_
11-9	1364-1372	observed	_	_	_	_
11-10	1373-1377	with	_	_	_	_
11-11	1378-1386	additive	person|object[54]	new|new[54]	coref|coref	11-28[0_54]|20-23[140_0]
11-12	1387-1392	noise	object[54]	new[54]	_	_
11-13	1393-1395	to	object[54]	new[54]	_	_
11-14	1396-1399	the	object[54]|abstract[56]	new[54]|giv[56]	coref	12-28[69_56]
11-15	1400-1413	deterministic	object[54]|abstract[56]	new[54]|giv[56]	_	_
11-16	1414-1420	neural	object[54]|abstract[56]	new[54]|giv[56]	_	_
11-17	1421-1429	networks	object[54]|place|abstract[56]	new[54]|giv|giv[56]	coref	17-21[110_0]
11-18	1430-1438	function	object[54]|abstract[56]	new[54]|giv[56]	_	_
11-19	1439-1443	such	_	_	_	_
11-20	1444-1446	as	_	_	_	_
11-21	1447-1448	(	_	_	_	_
11-22	1449-1450	3	quantity	new	_	_
11-23	1451-1452	)	_	_	_	_
11-24	1453-1458	where	_	_	_	_
11-25	1459-1461	is	_	_	_	_
11-26	1462-1463	a	abstract[59]	new[59]	coref	12-5[61_59]
11-27	1464-1470	random	abstract[59]	new[59]	_	_
11-28	1471-1476	noise	object|abstract[59]	giv|new[59]	coref	13-9[74_0]
11-29	1477-1483	vector	abstract[59]	new[59]	_	_
11-30	1484-1485	.	_	_	_	_

#Text=When we assume that the random vector is subject to the standard Gaussian distribution , its probability density is defined as the normal distribution function , and its loss function becomes the well-known squared error function , which can be written as ( 4 )
12-1	1486-1490	When	_	_	_	_
12-2	1491-1493	we	person	giv	ana	17-10
12-3	1494-1500	assume	_	_	_	_
12-4	1501-1505	that	_	_	_	_
12-5	1506-1509	the	abstract[61]	giv[61]	_	_
12-6	1510-1516	random	abstract[61]	giv[61]	_	_
12-7	1517-1523	vector	abstract[61]	giv[61]	_	_
12-8	1524-1526	is	_	_	_	_
12-9	1527-1534	subject	_	_	_	_
12-10	1535-1537	to	_	_	_	_
12-11	1538-1541	the	abstract[62]	giv[62]	ana	12-16[0_62]
12-12	1542-1550	standard	abstract[62]	giv[62]	_	_
12-13	1551-1559	Gaussian	abstract[62]	giv[62]	_	_
12-14	1560-1572	distribution	abstract[62]	giv[62]	_	_
12-15	1573-1574	,	_	_	_	_
12-16	1575-1578	its	abstract|abstract[65]	giv|new[65]	coref|coref	12-24|19-15[0_65]
12-17	1579-1590	probability	abstract|abstract[65]	giv|new[65]	coref	14-18
12-18	1591-1598	density	abstract[65]	new[65]	_	_
12-19	1599-1601	is	_	_	_	_
12-20	1602-1609	defined	_	_	_	_
12-21	1610-1612	as	_	_	_	_
12-22	1613-1616	the	_	_	_	_
12-23	1617-1623	normal	_	_	_	_
12-24	1624-1636	distribution	abstract	giv	ana	12-28
12-25	1637-1645	function	_	_	_	_
12-26	1646-1647	,	_	_	_	_
12-27	1648-1651	and	_	_	_	_
12-28	1652-1655	its	abstract|abstract[69]	giv|giv[69]	coref|coref	12-32[71_69]|14-16[85_0]
12-29	1656-1660	loss	abstract|abstract[69]	giv|giv[69]	coref	16-3
12-30	1661-1669	function	abstract[69]	giv[69]	_	_
12-31	1670-1677	becomes	_	_	_	_
12-32	1678-1681	the	abstract[71]	giv[71]	coref	13-22[78_71]
12-33	1682-1692	well-known	abstract[71]	giv[71]	_	_
12-34	1693-1700	squared	abstract[71]	giv[71]	_	_
12-35	1701-1706	error	abstract|abstract[71]	new|giv[71]	coref	13-23
12-36	1707-1715	function	abstract[71]	giv[71]	_	_
12-37	1716-1717	,	_	_	_	_
12-38	1718-1723	which	_	_	_	_
12-39	1724-1727	can	_	_	_	_
12-40	1728-1730	be	_	_	_	_
12-41	1731-1738	written	_	_	_	_
12-42	1739-1741	as	_	_	_	_
12-43	1742-1743	(	_	_	_	_
12-44	1744-1745	4	_	_	_	_
12-45	1746-1747	)	_	_	_	_

#Text=Therefore , the stochastic neural network model with additive Gaussian noise is equivalent to the typical neural network model trained with squared error function , which is widely used for regression task .
13-1	1748-1757	Therefore	_	_	_	_
13-2	1758-1759	,	_	_	_	_
13-3	1760-1763	the	abstract[73]	new[73]	_	_
13-4	1764-1774	stochastic	abstract[73]	new[73]	_	_
13-5	1775-1781	neural	abstract[73]	new[73]	_	_
13-6	1782-1789	network	abstract|abstract[73]	giv|new[73]	coref	13-18
13-7	1790-1795	model	abstract[73]	new[73]	_	_
13-8	1796-1800	with	abstract[73]	new[73]	_	_
13-9	1801-1809	additive	abstract[73]|object[74]	new[73]|giv[74]	coref	20-26[0_74]
13-10	1810-1818	Gaussian	abstract[73]|object[74]	new[73]|giv[74]	_	_
13-11	1819-1824	noise	abstract[73]|object[74]	new[73]|giv[74]	_	_
13-12	1825-1827	is	_	_	_	_
13-13	1828-1838	equivalent	_	_	_	_
13-14	1839-1841	to	_	_	_	_
13-15	1842-1845	the	abstract[76]	new[76]	coref	14-25[86_76]
13-16	1846-1853	typical	abstract[76]	new[76]	_	_
13-17	1854-1860	neural	abstract[76]	new[76]	_	_
13-18	1861-1868	network	abstract|abstract[76]	giv|new[76]	coref	18-17
13-19	1869-1874	model	abstract[76]	new[76]	_	_
13-20	1875-1882	trained	_	_	_	_
13-21	1883-1887	with	_	_	_	_
13-22	1888-1895	squared	abstract[78]	giv[78]	coref	16-1[100_78]
13-23	1896-1901	error	abstract|abstract[78]	giv|giv[78]	_	_
13-24	1902-1910	function	abstract[78]	giv[78]	_	_
13-25	1911-1912	,	_	_	_	_
13-26	1913-1918	which	_	_	_	_
13-27	1919-1921	is	_	_	_	_
13-28	1922-1928	widely	_	_	_	_
13-29	1929-1933	used	_	_	_	_
13-30	1934-1937	for	_	_	_	_
13-31	1938-1948	regression	abstract|abstract[80]	new|new[80]	coref|coref	17-16|17-34[113_80]
13-32	1949-1953	task	abstract[80]	new[80]	_	_
13-33	1954-1955	.	_	_	_	_

#Text=On the other hand , in case that the output is a binary vector , the corresponding probability distribution can be defined by using a logistic model , such as ( 5 ) where and are the i -th component of L -dimensional vector and , respectively .
14-1	1956-1958	On	_	_	_	_
14-2	1959-1962	the	_	_	_	_
14-3	1963-1968	other	_	_	_	_
14-4	1969-1973	hand	_	_	_	_
14-5	1974-1975	,	_	_	_	_
14-6	1976-1978	in	_	_	_	_
14-7	1979-1983	case	abstract[81]	new[81]	_	_
14-8	1984-1988	that	abstract[81]	new[81]	_	_
14-9	1989-1992	the	abstract[81]|abstract[82]	new[81]|giv[82]	coref	14-12[83_82]
14-10	1993-1999	output	abstract[81]|abstract[82]	new[81]|giv[82]	_	_
14-11	2000-2002	is	abstract[81]	new[81]	_	_
14-12	2003-2004	a	abstract[81]|abstract[83]	new[81]|giv[83]	coref	14-42[89_83]
14-13	2005-2011	binary	abstract[81]|abstract[83]	new[81]|giv[83]	_	_
14-14	2012-2018	vector	abstract[81]|abstract[83]	new[81]|giv[83]	_	_
14-15	2019-2020	,	_	_	_	_
14-16	2021-2024	the	abstract[85]	giv[85]	_	_
14-17	2025-2038	corresponding	abstract[85]	giv[85]	_	_
14-18	2039-2050	probability	abstract|abstract[85]	giv|giv[85]	coref	19-14
14-19	2051-2063	distribution	abstract[85]	giv[85]	_	_
14-20	2064-2067	can	_	_	_	_
14-21	2068-2070	be	_	_	_	_
14-22	2071-2078	defined	_	_	_	_
14-23	2079-2081	by	_	_	_	_
14-24	2082-2087	using	_	_	_	_
14-25	2088-2089	a	abstract[86]	giv[86]	ana	14-37[87_86]
14-26	2090-2098	logistic	abstract[86]	giv[86]	_	_
14-27	2099-2104	model	abstract[86]	giv[86]	_	_
14-28	2105-2106	,	_	_	_	_
14-29	2107-2111	such	_	_	_	_
14-30	2112-2114	as	_	_	_	_
14-31	2115-2116	(	_	_	_	_
14-32	2117-2118	5	_	_	_	_
14-33	2119-2120	)	_	_	_	_
14-34	2121-2126	where	_	_	_	_
14-35	2127-2130	and	_	_	_	_
14-36	2131-2134	are	_	_	_	_
14-37	2135-2138	the	abstract[87]	giv[87]	coref	15-14[95_87]
14-38	2139-2140	i	abstract[87]	giv[87]	_	_
14-39	2141-2144	-th	_	_	_	_
14-40	2145-2154	component	abstract[88]	new[88]	_	_
14-41	2155-2157	of	abstract[88]	new[88]	_	_
14-42	2158-2159	L	abstract[88]|abstract[89]	new[88]|giv[89]	coref	15-6[92_89]
14-43	2160-2172	-dimensional	abstract[88]|abstract[89]	new[88]|giv[89]	_	_
14-44	2173-2179	vector	abstract[88]|abstract[89]	new[88]|giv[89]	_	_
14-45	2180-2183	and	_	_	_	_
14-46	2184-2185	,	_	_	_	_
14-47	2186-2198	respectively	_	_	_	_
14-48	2199-2200	.	_	_	_	_

#Text=Since the typical problems with binary target output vector is pattern classification , the logistic model is appropriate for L -class classification tasks .
15-1	2201-2206	Since	_	_	_	_
15-2	2207-2210	the	abstract[90]	new[90]	coref	15-11[94_90]
15-3	2211-2218	typical	abstract[90]	new[90]	_	_
15-4	2219-2227	problems	abstract[90]	new[90]	_	_
15-5	2228-2232	with	abstract[90]	new[90]	_	_
15-6	2233-2239	binary	abstract[90]|abstract[92]	new[90]|giv[92]	coref	27-26[196_92]
15-7	2240-2246	target	abstract[90]|abstract|abstract[92]	new[90]|new|giv[92]	_	_
15-8	2247-2253	output	abstract[90]|abstract[92]	new[90]|giv[92]	_	_
15-9	2254-2260	vector	abstract[90]|abstract[92]	new[90]|giv[92]	_	_
15-10	2261-2263	is	_	_	_	_
15-11	2264-2271	pattern	abstract|abstract[94]	new|giv[94]	coref	18-35[121_94]
15-12	2272-2286	classification	abstract[94]	giv[94]	_	_
15-13	2287-2288	,	_	_	_	_
15-14	2289-2292	the	abstract[95]	giv[95]	coref	16-6[101_95]
15-15	2293-2301	logistic	abstract[95]	giv[95]	_	_
15-16	2302-2307	model	abstract[95]	giv[95]	_	_
15-17	2308-2310	is	_	_	_	_
15-18	2311-2322	appropriate	_	_	_	_
15-19	2323-2326	for	_	_	_	_
15-20	2327-2328	L	person|abstract[98]	new|new[98]	_	_
15-21	2329-2335	-class	abstract[98]	new[98]	_	_
15-22	2336-2350	classification	abstract|abstract[98]	new|new[98]	coref	17-34
15-23	2351-2356	tasks	abstract[98]	new[98]	_	_
15-24	2357-2358	.	_	_	_	_

#Text=The corresponding loss function of the logistic model is obtained by taking negative log likelihood of Equation ( 5 ) , which can be written as , ( 6 )
16-1	2359-2362	The	abstract[100]	giv[100]	_	_
16-2	2363-2376	corresponding	abstract[100]	giv[100]	_	_
16-3	2377-2381	loss	abstract|abstract[100]	giv|giv[100]	coref	23-12
16-4	2382-2390	function	abstract[100]	giv[100]	_	_
16-5	2391-2393	of	abstract[100]	giv[100]	_	_
16-6	2394-2397	the	abstract[100]|abstract[101]	giv[100]|giv[101]	coref	17-14[109_101]
16-7	2398-2406	logistic	abstract[100]|abstract[101]	giv[100]|giv[101]	_	_
16-8	2407-2412	model	abstract[100]|abstract[101]	giv[100]|giv[101]	_	_
16-9	2413-2415	is	_	_	_	_
16-10	2416-2424	obtained	_	_	_	_
16-11	2425-2427	by	_	_	_	_
16-12	2428-2434	taking	_	_	_	_
16-13	2435-2443	negative	abstract[103]	new[103]	_	_
16-14	2444-2447	log	object|abstract[103]	giv|new[103]	_	_
16-15	2448-2458	likelihood	abstract[103]	new[103]	_	_
16-16	2459-2461	of	abstract[103]	new[103]	_	_
16-17	2462-2470	Equation	abstract[103]|abstract[104]	new[103]|giv[104]	ana	17-3[0_104]
16-18	2471-2472	(	abstract[103]|abstract[104]	new[103]|giv[104]	_	_
16-19	2473-2474	5	abstract[103]|abstract[104]	new[103]|giv[104]	_	_
16-20	2475-2476	)	abstract[103]|abstract[104]	new[103]|giv[104]	_	_
16-21	2477-2478	,	_	_	_	_
16-22	2479-2484	which	_	_	_	_
16-23	2485-2488	can	_	_	_	_
16-24	2489-2491	be	_	_	_	_
16-25	2492-2499	written	_	_	_	_
16-26	2500-2502	as	_	_	_	_
16-27	2503-2504	,	_	_	_	_
16-28	2505-2506	(	_	_	_	_
16-29	2507-2508	6	_	_	_	_
16-30	2509-2510	)	_	_	_	_

#Text=Noting that this is the well-known cross-entropy error , we can say that the logistic regression model is equivalent to the typical neural networks with cross-entropy error , which is widely used for classification task .
17-1	2511-2517	Noting	_	_	_	_
17-2	2518-2522	that	_	_	_	_
17-3	2523-2527	this	abstract	giv	coref	17-5[106_0]
17-4	2528-2530	is	_	_	_	_
17-5	2531-2534	the	abstract[106]	giv[106]	coref	17-26[111_106]
17-6	2535-2545	well-known	abstract[106]	giv[106]	_	_
17-7	2546-2559	cross-entropy	abstract[106]	giv[106]	_	_
17-8	2560-2565	error	abstract[106]	giv[106]	_	_
17-9	2566-2567	,	_	_	_	_
17-10	2568-2570	we	person	giv	ana	18-10
17-11	2571-2574	can	_	_	_	_
17-12	2575-2578	say	_	_	_	_
17-13	2579-2583	that	_	_	_	_
17-14	2584-2587	the	abstract[109]	giv[109]	coref	18-5[114_109]
17-15	2588-2596	logistic	abstract[109]	giv[109]	_	_
17-16	2597-2607	regression	abstract|abstract[109]	giv|giv[109]	coref	20-31
17-17	2608-2613	model	abstract[109]	giv[109]	_	_
17-18	2614-2616	is	_	_	_	_
17-19	2617-2627	equivalent	_	_	_	_
17-20	2628-2630	to	_	_	_	_
17-21	2631-2634	the	place[110]	giv[110]	coref	19-18[131_110]
17-22	2635-2642	typical	place[110]	giv[110]	_	_
17-23	2643-2649	neural	place[110]	giv[110]	_	_
17-24	2650-2658	networks	place[110]	giv[110]	_	_
17-25	2659-2663	with	place[110]	giv[110]	_	_
17-26	2664-2677	cross-entropy	place[110]|abstract[111]	giv[110]|giv[111]	coref	28-1[197_111]
17-27	2678-2683	error	place[110]|abstract[111]	giv[110]|giv[111]	_	_
17-28	2684-2685	,	_	_	_	_
17-29	2686-2691	which	_	_	_	_
17-30	2692-2694	is	_	_	_	_
17-31	2695-2701	widely	_	_	_	_
17-32	2702-2706	used	_	_	_	_
17-33	2707-2710	for	_	_	_	_
17-34	2711-2725	classification	abstract|abstract[113]	giv|giv[113]	coref	18-23[119_113]
17-35	2726-2730	task	abstract[113]	giv[113]	_	_
17-36	2731-2732	.	_	_	_	_

#Text=Likewise , by defining a proper stochastic model , we can derive various types of neural network models , which can explain the given task more adequately and get a new insight to solve many unresolved problems in the field of neural network learning .
18-1	2733-2741	Likewise	_	_	_	_
18-2	2742-2743	,	_	_	_	_
18-3	2744-2746	by	_	_	_	_
18-4	2747-2755	defining	_	_	_	_
18-5	2756-2757	a	abstract[114]	giv[114]	coref	20-25[142_114]
18-6	2758-2764	proper	abstract[114]	giv[114]	_	_
18-7	2765-2775	stochastic	abstract[114]	giv[114]	_	_
18-8	2776-2781	model	abstract[114]	giv[114]	_	_
18-9	2782-2783	,	_	_	_	_
18-10	2784-2786	we	person	giv	ana	20-5
18-11	2787-2790	can	_	_	_	_
18-12	2791-2797	derive	_	_	_	_
18-13	2798-2805	various	abstract[116]	new[116]	_	_
18-14	2806-2811	types	abstract[116]	new[116]	_	_
18-15	2812-2814	of	abstract[116]	new[116]	_	_
18-16	2815-2821	neural	abstract[116]|abstract[118]	new[116]|giv[118]	coref	20-16[139_118]
18-17	2822-2829	network	abstract[116]|abstract|abstract[118]	new[116]|giv|giv[118]	coref	18-43
18-18	2830-2836	models	abstract[116]|abstract[118]	new[116]|giv[118]	_	_
18-19	2837-2838	,	_	_	_	_
18-20	2839-2844	which	_	_	_	_
18-21	2845-2848	can	_	_	_	_
18-22	2849-2856	explain	_	_	_	_
18-23	2857-2860	the	abstract[119]	giv[119]	_	_
18-24	2861-2866	given	abstract[119]	giv[119]	_	_
18-25	2867-2871	task	abstract[119]	giv[119]	_	_
18-26	2872-2876	more	_	_	_	_
18-27	2877-2887	adequately	_	_	_	_
18-28	2888-2891	and	_	_	_	_
18-29	2892-2895	get	_	_	_	_
18-30	2896-2897	a	abstract[120]	new[120]	_	_
18-31	2898-2901	new	abstract[120]	new[120]	_	_
18-32	2902-2909	insight	abstract[120]	new[120]	_	_
18-33	2910-2912	to	_	_	_	_
18-34	2913-2918	solve	_	_	_	_
18-35	2919-2923	many	abstract[121]	giv[121]	_	_
18-36	2924-2934	unresolved	abstract[121]	giv[121]	_	_
18-37	2935-2943	problems	abstract[121]	giv[121]	_	_
18-38	2944-2946	in	_	_	_	_
18-39	2947-2950	the	abstract[122]	new[122]	_	_
18-40	2951-2956	field	abstract[122]	new[122]	_	_
18-41	2957-2959	of	abstract[122]	new[122]	_	_
18-42	2960-2966	neural	abstract[122]|abstract[124]	new[122]|giv[124]	coref	20-13[0_124]
18-43	2967-2974	network	abstract[122]|abstract|abstract[124]	new[122]|giv|giv[124]	coref	20-20
18-44	2975-2983	learning	abstract[122]|abstract[124]	new[122]|giv[124]	_	_
18-45	2984-2985	.	_	_	_	_

#Text=Natural gradient is also derived from a new metric for the space of probability density function of stochastic neural networks .
19-1	2986-2993	Natural	abstract[125]	giv[125]	coref	20-12[0_125]
19-2	2994-3002	gradient	abstract[125]	giv[125]	_	_
19-3	3003-3005	is	_	_	_	_
19-4	3006-3010	also	_	_	_	_
19-5	3011-3018	derived	_	_	_	_
19-6	3019-3023	from	_	_	_	_
19-7	3024-3025	a	abstract[126]	new[126]	_	_
19-8	3026-3029	new	abstract[126]	new[126]	_	_
19-9	3030-3036	metric	abstract[126]	new[126]	_	_
19-10	3037-3040	for	abstract[126]	new[126]	_	_
19-11	3041-3044	the	abstract[126]|abstract[127]	new[126]|new[127]	_	_
19-12	3045-3050	space	abstract[126]|abstract[127]	new[126]|new[127]	_	_
19-13	3051-3053	of	abstract[126]|abstract[127]	new[126]|new[127]	_	_
19-14	3054-3065	probability	abstract[126]|abstract[127]|abstract|abstract[130]	new[126]|new[127]|giv|new[130]	coref	23-10[153_130]
19-15	3066-3073	density	abstract[126]|abstract[127]|abstract|abstract[130]	new[126]|new[127]|giv|new[130]	_	_
19-16	3074-3082	function	abstract[126]|abstract[127]|abstract[130]	new[126]|new[127]|new[130]	_	_
19-17	3083-3085	of	abstract[126]|abstract[127]|abstract[130]	new[126]|new[127]|new[130]	_	_
19-18	3086-3096	stochastic	abstract[126]|abstract[127]|abstract[130]|place[131]	new[126]|new[127]|new[130]|giv[131]	coref	23-6[151_131]
19-19	3097-3103	neural	abstract[126]|abstract[127]|abstract[130]|place[131]	new[126]|new[127]|new[130]|giv[131]	_	_
19-20	3104-3112	networks	abstract[126]|abstract[127]|abstract[130]|place[131]	new[126]|new[127]|new[130]|giv[131]	_	_
19-21	3113-3114	.	_	_	_	_

#Text=In this paper , we present explicit algorithms of adaptive natural gradient learning method for two representative stochastic neural network models : The additive Gaussian noise model and the logistic regression model .
20-1	3115-3117	In	_	_	_	_
20-2	3118-3122	this	object[132]	new[132]	_	_
20-3	3123-3128	paper	object[132]	new[132]	_	_
20-4	3129-3130	,	_	_	_	_
20-5	3131-3133	we	person	giv	ana	26-16
20-6	3134-3141	present	_	_	_	_
20-7	3142-3150	explicit	abstract[134]	new[134]	_	_
20-8	3151-3161	algorithms	abstract[134]	new[134]	_	_
20-9	3162-3164	of	abstract[134]	new[134]	_	_
20-10	3165-3173	adaptive	abstract[134]|abstract[137]	new[134]|new[137]	coref	23-23[158_137]
20-11	3174-3181	natural	abstract[134]|abstract[137]	new[134]|new[137]	_	_
20-12	3182-3190	gradient	abstract[134]|abstract|abstract[137]	new[134]|giv|new[137]	coref	22-1
20-13	3191-3199	learning	abstract[134]|abstract|abstract[137]	new[134]|giv|new[137]	coref	22-1[149_0]
20-14	3200-3206	method	abstract[134]|abstract[137]	new[134]|new[137]	_	_
20-15	3207-3210	for	abstract[134]	new[134]	_	_
20-16	3211-3214	two	abstract[134]|abstract[139]	new[134]|giv[139]	_	_
20-17	3215-3229	representative	abstract[134]|abstract[139]	new[134]|giv[139]	_	_
20-18	3230-3240	stochastic	abstract[134]|abstract[139]	new[134]|giv[139]	_	_
20-19	3241-3247	neural	abstract[134]|abstract[139]	new[134]|giv[139]	_	_
20-20	3248-3255	network	abstract[134]|abstract|abstract[139]	new[134]|giv|giv[139]	coref	27-10[192_0]
20-21	3256-3262	models	abstract[134]|abstract[139]	new[134]|giv[139]	_	_
20-22	3263-3264	:	_	_	_	_
20-23	3265-3268	The	person[140]	giv[140]	appos	20-25[143_140]
20-24	3269-3277	additive	person[140]	giv[140]	_	_
20-25	3278-3286	Gaussian	abstract[142]|person[143]	giv[142]|giv[143]	coref	20-29[145_142]
20-26	3287-3292	noise	object|abstract[142]|person[143]	giv|giv[142]|giv[143]	_	_
20-27	3293-3298	model	abstract[142]|person[143]	giv[142]|giv[143]	_	_
20-28	3299-3302	and	person[143]	giv[143]	_	_
20-29	3303-3306	the	person[143]|abstract[145]	giv[143]|giv[145]	coref	23-2[150_145]
20-30	3307-3315	logistic	person[143]|abstract[145]	giv[143]|giv[145]	_	_
20-31	3316-3326	regression	person[143]|abstract|abstract[145]	giv[143]|giv|giv[145]	_	_
20-32	3327-3332	model	person[143]|abstract[145]	giv[143]|giv[145]	_	_
20-33	3333-3334	.	_	_	_	_

#Text=2.2 .
21-1	3335-3338	2.2	abstract	new	_	_
21-2	3339-3340	.	_	_	_	_

#Text=Gradient Descent Learning
22-1	3341-3349	Gradient	substance|abstract[148]|abstract[149]	giv|new[148]|giv[149]	coref|coref|coref	23-23|23-24[0_148]|24-12[0_149]
22-2	3350-3357	Descent	abstract[148]|abstract[149]	new[148]|giv[149]	_	_
22-3	3358-3366	Learning	abstract[149]	giv[149]	_	_

#Text=Once a specific model of stochastic neural networks and its corresponding loss function are determined , the weight parameters are optimized by gradient descent method .
23-1	3367-3371	Once	_	_	_	_
23-2	3372-3373	a	abstract[150]	giv[150]	_	_
23-3	3374-3382	specific	abstract[150]	giv[150]	_	_
23-4	3383-3388	model	abstract[150]	giv[150]	_	_
23-5	3389-3391	of	abstract[150]	giv[150]	_	_
23-6	3392-3402	stochastic	abstract[150]|place[151]	giv[150]|giv[151]	_	_
23-7	3403-3409	neural	abstract[150]|place[151]	giv[150]|giv[151]	_	_
23-8	3410-3418	networks	abstract[150]|place[151]	giv[150]|giv[151]	_	_
23-9	3419-3422	and	abstract[150]|place[151]	giv[150]|giv[151]	_	_
23-10	3423-3426	its	abstract[150]|place[151]|abstract[153]	giv[150]|giv[151]|giv[153]	coref	28-17[202_153]
23-11	3427-3440	corresponding	abstract[150]|place[151]|abstract[153]	giv[150]|giv[151]|giv[153]	_	_
23-12	3441-3445	loss	abstract[150]|place[151]|abstract|abstract[153]	giv[150]|giv[151]|giv|giv[153]	coref	28-18
23-13	3446-3454	function	abstract[150]|place[151]|abstract[153]	giv[150]|giv[151]|giv[153]	_	_
23-14	3455-3458	are	_	_	_	_
23-15	3459-3469	determined	_	_	_	_
23-16	3470-3471	,	_	_	_	_
23-17	3472-3475	the	abstract[155]	new[155]	coref	34-17[0_155]
23-18	3476-3482	weight	abstract|abstract[155]	new|new[155]	coref	27-26
23-19	3483-3493	parameters	abstract[155]	new[155]	_	_
23-20	3494-3497	are	_	_	_	_
23-21	3498-3507	optimized	_	_	_	_
23-22	3508-3510	by	_	_	_	_
23-23	3511-3519	gradient	abstract|abstract[158]	giv|giv[158]	coref|coref	24-10|24-11[165_158]
23-24	3520-3527	descent	abstract|abstract[158]	giv|giv[158]	coref	24-11
23-25	3528-3534	method	abstract[158]	giv[158]	_	_
23-26	3535-3536	.	_	_	_	_

#Text=The well-known error-backpropagation algorithm is the standard type of gradient descent learning method .
24-1	3537-3540	The	abstract[160]	new[160]	coref	24-6[161_160]
24-2	3541-3551	well-known	abstract[160]	new[160]	_	_
24-3	3552-3573	error-backpropagation	abstract|abstract[160]	new|new[160]	_	_
24-4	3574-3583	algorithm	abstract[160]	new[160]	_	_
24-5	3584-3586	is	_	_	_	_
24-6	3587-3590	the	abstract[161]	giv[161]	_	_
24-7	3591-3599	standard	abstract[161]	giv[161]	_	_
24-8	3600-3604	type	abstract[161]	giv[161]	_	_
24-9	3605-3607	of	abstract[161]	giv[161]	_	_
24-10	3608-3616	gradient	abstract[161]|abstract	giv[161]|giv	coref	25-9
24-11	3617-3624	descent	abstract[161]|abstract|abstract[165]	giv[161]|giv|giv[165]	coref|coref	25-10|25-7[169_165]
24-12	3625-3633	learning	abstract[161]|abstract|abstract[165]	giv[161]|giv|giv[165]	coref	25-22
24-13	3634-3640	method	abstract[161]|abstract[165]	giv[161]|giv[165]	_	_
24-14	3641-3642	.	_	_	_	_

#Text=There have been numerous variations of the standard gradient descent method , including second-order methods , momentum method , and adaptive learning rate methods .
25-1	3643-3648	There	_	_	_	_
25-2	3649-3653	have	_	_	_	_
25-3	3654-3658	been	_	_	_	_
25-4	3659-3667	numerous	abstract[166]	new[166]	_	_
25-5	3668-3678	variations	abstract[166]	new[166]	_	_
25-6	3679-3681	of	abstract[166]	new[166]	_	_
25-7	3682-3685	the	abstract[166]|abstract[169]	new[166]|giv[169]	coref	26-2[177_169]
25-8	3686-3694	standard	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-9	3695-3703	gradient	abstract[166]|abstract|abstract[169]	new[166]|giv|giv[169]	coref	26-4
25-10	3704-3711	descent	abstract[166]|abstract|abstract[169]	new[166]|giv|giv[169]	coref	26-13
25-11	3712-3718	method	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-12	3719-3720	,	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-13	3721-3730	including	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-14	3731-3743	second-order	abstract[166]|abstract[169]|abstract[170]	new[166]|giv[169]|new[170]	_	_
25-15	3744-3751	methods	abstract[166]|abstract[169]|abstract[170]	new[166]|giv[169]|new[170]	_	_
25-16	3752-3753	,	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-17	3754-3762	momentum	abstract[166]|abstract[169]|abstract	new[166]|giv[169]|new	_	_
25-18	3763-3769	method	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-19	3770-3771	,	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-20	3772-3775	and	abstract[166]|abstract[169]	new[166]|giv[169]	_	_
25-21	3776-3784	adaptive	abstract[166]|abstract[169]|abstract[174]	new[166]|giv[169]|new[174]	_	_
25-22	3785-3793	learning	abstract[166]|abstract[169]|abstract|abstract[174]	new[166]|giv[169]|giv|new[174]	coref	26-5
25-23	3794-3798	rate	abstract[166]|abstract[169]|abstract|abstract[174]	new[166]|giv[169]|new|new[174]	_	_
25-24	3799-3806	methods	abstract[166]|abstract[169]|abstract[174]	new[166]|giv[169]|new[174]	_	_
25-25	3807-3808	.	_	_	_	_

#Text=Since the natural gradient learning method is also based on the gradient descent method , we describe the basic formula of gradient descent learning and its online version that is called stochastic gradient descent method .
26-1	3809-3814	Since	_	_	_	_
26-2	3815-3818	the	abstract[177]	giv[177]	coref	26-11[180_177]
26-3	3819-3826	natural	abstract[177]	giv[177]	_	_
26-4	3827-3835	gradient	abstract|abstract[177]	giv|giv[177]	coref	26-12
26-5	3836-3844	learning	abstract|abstract[177]	giv|giv[177]	coref	26-23[185_0]
26-6	3845-3851	method	abstract[177]	giv[177]	_	_
26-7	3852-3854	is	_	_	_	_
26-8	3855-3859	also	_	_	_	_
26-9	3860-3865	based	_	_	_	_
26-10	3866-3868	on	_	_	_	_
26-11	3869-3872	the	abstract[180]	giv[180]	coref	26-32[189_180]
26-12	3873-3881	gradient	abstract|abstract[180]	giv|giv[180]	coref	26-22
26-13	3882-3889	descent	abstract|abstract[180]	giv|giv[180]	coref	26-23
26-14	3890-3896	method	abstract[180]	giv[180]	_	_
26-15	3897-3898	,	_	_	_	_
26-16	3899-3901	we	person	giv	_	_
26-17	3902-3910	describe	_	_	_	_
26-18	3911-3914	the	abstract[182]	new[182]	_	_
26-19	3915-3920	basic	abstract[182]	new[182]	_	_
26-20	3921-3928	formula	abstract[182]	new[182]	_	_
26-21	3929-3931	of	abstract[182]	new[182]	_	_
26-22	3932-3940	gradient	abstract[182]|abstract	new[182]|giv	coref	26-33
26-23	3941-3948	descent	abstract[182]|abstract|abstract[185]	new[182]|giv|giv[185]	coref|coref	26-34|28-29[0_185]
26-24	3949-3957	learning	abstract[182]|abstract[185]	new[182]|giv[185]	_	_
26-25	3958-3961	and	abstract[182]	new[182]	_	_
26-26	3962-3965	its	abstract[182]|abstract[186]	new[182]|new[186]	_	_
26-27	3966-3972	online	abstract[182]|abstract[186]	new[182]|new[186]	_	_
26-28	3973-3980	version	abstract[182]|abstract[186]	new[182]|new[186]	_	_
26-29	3981-3985	that	_	_	_	_
26-30	3986-3988	is	_	_	_	_
26-31	3989-3995	called	_	_	_	_
26-32	3996-4006	stochastic	abstract[189]	giv[189]	_	_
26-33	4007-4015	gradient	abstract|abstract[189]	giv|giv[189]	coref	29-23[211_0]
26-34	4016-4023	descent	abstract|abstract[189]	giv|giv[189]	coref	31-10
26-35	4024-4030	method	abstract[189]	giv[189]	_	_
26-36	4031-4032	.	_	_	_	_

#Text=When a set of training data is given , a neural network is trained in order to find an input-output mapping that is specified with weight parameter vector .
27-1	4033-4037	When	_	_	_	_
27-2	4038-4039	a	abstract[191]	new[191]	_	_
27-3	4040-4043	set	abstract[191]	new[191]	_	_
27-4	4044-4046	of	abstract[191]	new[191]	_	_
27-5	4047-4055	training	abstract|abstract[191]	new|new[191]	_	_
27-6	4056-4060	data	abstract[191]	new[191]	_	_
27-7	4061-4063	is	_	_	_	_
27-8	4064-4069	given	_	_	_	_
27-9	4070-4071	,	_	_	_	_
27-10	4072-4073	a	abstract[192]	giv[192]	coref	28-4[198_192]
27-11	4074-4080	neural	abstract[192]	giv[192]	_	_
27-12	4081-4088	network	abstract[192]	giv[192]	_	_
27-13	4089-4091	is	_	_	_	_
27-14	4092-4099	trained	_	_	_	_
27-15	4100-4102	in	_	_	_	_
27-16	4103-4108	order	_	_	_	_
27-17	4109-4111	to	_	_	_	_
27-18	4112-4116	find	_	_	_	_
27-19	4117-4119	an	abstract[193]	new[193]	_	_
27-20	4120-4132	input-output	abstract[193]	new[193]	_	_
27-21	4133-4140	mapping	abstract[193]	new[193]	_	_
27-22	4141-4145	that	_	_	_	_
27-23	4146-4148	is	_	_	_	_
27-24	4149-4158	specified	_	_	_	_
27-25	4159-4163	with	_	_	_	_
27-26	4164-4170	weight	abstract|abstract[196]	giv|giv[196]	coref	29-7
27-27	4171-4180	parameter	abstract|abstract[196]	giv|giv[196]	coref	29-6[208_0]
27-28	4181-4187	vector	abstract[196]	giv[196]	_	_
27-29	4188-4189	.	_	_	_	_

#Text=The error of neural network for the whole data set can then be defined by using a loss function such as ( 7 ) and the goal of learning is to get the optimal minimizing .
28-1	4190-4193	The	abstract[197]	giv[197]	_	_
28-2	4194-4199	error	abstract[197]	giv[197]	_	_
28-3	4200-4202	of	abstract[197]	giv[197]	_	_
28-4	4203-4209	neural	abstract[197]|abstract[198]	giv[197]|giv[198]	_	_
28-5	4210-4217	network	abstract[197]|abstract[198]	giv[197]|giv[198]	_	_
28-6	4218-4221	for	abstract[197]|abstract[198]	giv[197]|giv[198]	_	_
28-7	4222-4225	the	abstract[197]|abstract[198]|abstract[200]	giv[197]|giv[198]|new[200]	coref	30-17[219_200]
28-8	4226-4231	whole	abstract[197]|abstract[198]|abstract[200]	giv[197]|giv[198]|new[200]	_	_
28-9	4232-4236	data	abstract[197]|abstract[198]|abstract|abstract[200]	giv[197]|giv[198]|new|new[200]	coref	33-8
28-10	4237-4240	set	abstract[197]|abstract[198]|abstract[200]	giv[197]|giv[198]|new[200]	_	_
28-11	4241-4244	can	_	_	_	_
28-12	4245-4249	then	_	_	_	_
28-13	4250-4252	be	_	_	_	_
28-14	4253-4260	defined	_	_	_	_
28-15	4261-4263	by	_	_	_	_
28-16	4264-4269	using	_	_	_	_
28-17	4270-4271	a	abstract[202]	giv[202]	coref	34-28[245_202]
28-18	4272-4276	loss	abstract|abstract[202]	giv|giv[202]	coref	34-28
28-19	4277-4285	function	abstract[202]	giv[202]	_	_
28-20	4286-4290	such	abstract[202]	giv[202]	_	_
28-21	4291-4293	as	abstract[202]	giv[202]	_	_
28-22	4294-4295	(	abstract[202]	giv[202]	_	_
28-23	4296-4297	7	abstract[202]	giv[202]	_	_
28-24	4298-4299	)	abstract[202]	giv[202]	_	_
28-25	4300-4303	and	_	_	_	_
28-26	4304-4307	the	abstract[203]	giv[203]	coref	29-3[206_203]
28-27	4308-4312	goal	abstract[203]	giv[203]	_	_
28-28	4313-4315	of	abstract[203]	giv[203]	_	_
28-29	4316-4324	learning	abstract[203]|abstract	giv[203]|giv	coref	30-7
28-30	4325-4327	is	_	_	_	_
28-31	4328-4330	to	_	_	_	_
28-32	4331-4334	get	_	_	_	_
28-33	4335-4338	the	abstract[205]	new[205]	_	_
28-34	4339-4346	optimal	abstract[205]	new[205]	_	_
28-35	4347-4357	minimizing	abstract[205]	new[205]	_	_
28-36	4358-4359	.	_	_	_	_

#Text=To achieve the goal , the weight parameter is updated starting from the current position , according to the opposite direction of the gradient of , which can be written as ( 8 )
29-1	4360-4362	To	_	_	_	_
29-2	4363-4370	achieve	_	_	_	_
29-3	4371-4374	the	abstract[206]	giv[206]	_	_
29-4	4375-4379	goal	abstract[206]	giv[206]	_	_
29-5	4380-4381	,	_	_	_	_
29-6	4382-4385	the	abstract[208]	giv[208]	_	_
29-7	4386-4392	weight	abstract|abstract[208]	giv|giv[208]	_	_
29-8	4393-4402	parameter	abstract[208]	giv[208]	_	_
29-9	4403-4405	is	_	_	_	_
29-10	4406-4413	updated	_	_	_	_
29-11	4414-4422	starting	_	_	_	_
29-12	4423-4427	from	_	_	_	_
29-13	4428-4431	the	place[209]	new[209]	coref	31-14[225_209]
29-14	4432-4439	current	place[209]	new[209]	_	_
29-15	4440-4448	position	place[209]	new[209]	_	_
29-16	4449-4450	,	_	_	_	_
29-17	4451-4460	according	_	_	_	_
29-18	4461-4463	to	_	_	_	_
29-19	4464-4467	the	abstract[210]	new[210]	_	_
29-20	4468-4476	opposite	abstract[210]	new[210]	_	_
29-21	4477-4486	direction	abstract[210]	new[210]	_	_
29-22	4487-4489	of	abstract[210]	new[210]	_	_
29-23	4490-4493	the	abstract[210]|abstract[211]	new[210]|giv[211]	coref	34-7[238_211]
29-24	4494-4502	gradient	abstract[210]|abstract[211]	new[210]|giv[211]	_	_
29-25	4503-4505	of	abstract[210]|abstract[211]	new[210]|giv[211]	_	_
29-26	4506-4507	,	_	_	_	_
29-27	4508-4513	which	_	_	_	_
29-28	4514-4517	can	_	_	_	_
29-29	4518-4520	be	_	_	_	_
29-30	4521-4528	written	_	_	_	_
29-31	4529-4531	as	_	_	_	_
29-32	4532-4533	(	_	_	_	_
29-33	4534-4535	8	_	_	_	_
29-34	4536-4537	)	_	_	_	_

#Text=This update rule is called batch learning mode , meaning that an update is done for the whole batch set .
30-1	4538-4542	This	abstract[213]	new[213]	_	_
30-2	4543-4549	update	event|abstract[213]	new|new[213]	coref	30-12[217_0]
30-3	4550-4554	rule	abstract[213]	new[213]	_	_
30-4	4555-4557	is	_	_	_	_
30-5	4558-4564	called	_	_	_	_
30-6	4565-4570	batch	quantity|abstract[216]	new|new[216]	coref|coref	30-19|31-5[0_216]
30-7	4571-4579	learning	abstract|abstract[216]	giv|new[216]	coref	31-3[222_0]
30-8	4580-4584	mode	abstract[216]	new[216]	_	_
30-9	4585-4586	,	_	_	_	_
30-10	4587-4594	meaning	_	_	_	_
30-11	4595-4599	that	_	_	_	_
30-12	4600-4602	an	event[217]	giv[217]	coref	33-19[235_217]
30-13	4603-4609	update	event[217]	giv[217]	_	_
30-14	4610-4612	is	_	_	_	_
30-15	4613-4617	done	_	_	_	_
30-16	4618-4621	for	_	_	_	_
30-17	4622-4625	the	abstract[219]	giv[219]	_	_
30-18	4626-4631	whole	abstract[219]	giv[219]	_	_
30-19	4632-4637	batch	quantity|abstract[219]	giv|giv[219]	coref	31-4
30-20	4638-4641	set	abstract[219]	giv[219]	_	_
30-21	4642-4643	.	_	_	_	_

#Text=Theoretically , the batch mode learning gives the steepest descent direction of at the current position of , but it has two practical drawbacks .
31-1	4644-4657	Theoretically	_	_	_	_
31-2	4658-4659	,	_	_	_	_
31-3	4660-4663	the	abstract[222]	giv[222]	_	_
31-4	4664-4669	batch	quantity|abstract[222]	giv|giv[222]	_	_
31-5	4670-4674	mode	abstract|abstract[222]	giv|giv[222]	_	_
31-6	4675-4683	learning	abstract[222]	giv[222]	_	_
31-7	4684-4689	gives	_	_	_	_
31-8	4690-4693	the	abstract[224]	new[224]	_	_
31-9	4694-4702	steepest	abstract[224]	new[224]	_	_
31-10	4703-4710	descent	abstract|abstract[224]	giv|new[224]	_	_
31-11	4711-4720	direction	abstract[224]	new[224]	_	_
31-12	4721-4723	of	abstract[224]	new[224]	_	_
31-13	4724-4726	at	abstract[224]	new[224]	_	_
31-14	4727-4730	the	abstract[224]|abstract[225]	new[224]|giv[225]	ana	31-20[0_225]
31-15	4731-4738	current	abstract[224]|abstract[225]	new[224]|giv[225]	_	_
31-16	4739-4747	position	abstract[224]|abstract[225]	new[224]|giv[225]	_	_
31-17	4748-4750	of	_	_	_	_
31-18	4751-4752	,	_	_	_	_
31-19	4753-4756	but	_	_	_	_
31-20	4757-4759	it	place	giv	ana	32-3
31-21	4760-4763	has	_	_	_	_
31-22	4764-4767	two	abstract[227]	new[227]	_	_
31-23	4768-4777	practical	abstract[227]	new[227]	_	_
31-24	4778-4787	drawbacks	abstract[227]	new[227]	_	_
31-25	4788-4789	.	_	_	_	_

#Text=First , it is too stable to be easily trapped in undesirable local minima .
32-1	4790-4795	First	_	_	_	_
32-2	4796-4797	,	_	_	_	_
32-3	4798-4800	it	abstract	giv	_	_
32-4	4801-4803	is	_	_	_	_
32-5	4804-4807	too	_	_	_	_
32-6	4808-4814	stable	_	_	_	_
32-7	4815-4817	to	_	_	_	_
32-8	4818-4820	be	_	_	_	_
32-9	4821-4827	easily	_	_	_	_
32-10	4828-4835	trapped	_	_	_	_
32-11	4836-4838	in	_	_	_	_
32-12	4839-4850	undesirable	abstract[229]	new[229]	_	_
32-13	4851-4856	local	abstract[229]	new[229]	_	_
32-14	4857-4863	minima	abstract[229]	new[229]	_	_
32-15	4864-4865	.	_	_	_	_

#Text=In addition , when the number of data is large , it needs large amounts of computation for just a single update , making the learning process slow .
33-1	4866-4868	In	_	_	_	_
33-2	4869-4877	addition	_	_	_	_
33-3	4878-4879	,	_	_	_	_
33-4	4880-4884	when	_	_	_	_
33-5	4885-4888	the	quantity[230]	new[230]	_	_
33-6	4889-4895	number	quantity[230]	new[230]	_	_
33-7	4896-4898	of	quantity[230]	new[230]	_	_
33-8	4899-4903	data	quantity[230]|abstract	new[230]|giv	ana	33-12
33-9	4904-4906	is	_	_	_	_
33-10	4907-4912	large	_	_	_	_
33-11	4913-4914	,	_	_	_	_
33-12	4915-4917	it	abstract	giv	coref	34-22
33-13	4918-4923	needs	_	_	_	_
33-14	4924-4929	large	quantity[233]	new[233]	_	_
33-15	4930-4937	amounts	quantity[233]	new[233]	_	_
33-16	4938-4940	of	quantity[233]	new[233]	_	_
33-17	4941-4952	computation	quantity[233]|abstract[234]	new[233]|new[234]	_	_
33-18	4953-4956	for	quantity[233]|abstract[234]	new[233]|new[234]	_	_
33-19	4957-4961	just	quantity[233]|abstract[234]|event[235]	new[233]|new[234]|giv[235]	_	_
33-20	4962-4963	a	quantity[233]|abstract[234]|event[235]	new[233]|new[234]|giv[235]	_	_
33-21	4964-4970	single	quantity[233]|abstract[234]|event[235]	new[233]|new[234]|giv[235]	_	_
33-22	4971-4977	update	quantity[233]|abstract[234]|event[235]	new[233]|new[234]|giv[235]	_	_
33-23	4978-4979	,	_	_	_	_
33-24	4980-4986	making	_	_	_	_
33-25	4987-4990	the	abstract[236]	new[236]	_	_
33-26	4991-4999	learning	abstract[236]	new[236]	_	_
33-27	5000-5007	process	abstract[236]	new[236]	_	_
33-28	5008-5012	slow	_	_	_	_
33-29	5013-5014	.	_	_	_	_

#Text=To overcome this practical inefficiency , online stochastic gradient decent learning is proposed , in which parameters are updated for each data sample by using gradient of loss function defined with a single data pair , such as ( 9 )
34-1	5015-5017	To	_	_	_	_
34-2	5018-5026	overcome	_	_	_	_
34-3	5027-5031	this	abstract[237]	new[237]	_	_
34-4	5032-5041	practical	abstract[237]	new[237]	_	_
34-5	5042-5054	inefficiency	abstract[237]	new[237]	_	_
34-6	5055-5056	,	_	_	_	_
34-7	5057-5063	online	abstract[238]	giv[238]	appos	34-10[239_238]
34-8	5064-5074	stochastic	abstract[238]	giv[238]	_	_
34-9	5075-5083	gradient	abstract[238]	giv[238]	_	_
34-10	5084-5090	decent	abstract[239]	giv[239]	_	_
34-11	5091-5099	learning	abstract[239]	giv[239]	_	_
34-12	5100-5102	is	_	_	_	_
34-13	5103-5111	proposed	_	_	_	_
34-14	5112-5113	,	_	_	_	_
34-15	5114-5116	in	_	_	_	_
34-16	5117-5122	which	_	_	_	_
34-17	5123-5133	parameters	abstract	giv	_	_
34-18	5134-5137	are	_	_	_	_
34-19	5138-5145	updated	_	_	_	_
34-20	5146-5149	for	_	_	_	_
34-21	5150-5154	each	abstract[242]	new[242]	_	_
34-22	5155-5159	data	abstract|abstract[242]	giv|new[242]	coref	34-34
34-23	5160-5166	sample	abstract[242]	new[242]	_	_
34-24	5167-5169	by	_	_	_	_
34-25	5170-5175	using	_	_	_	_
34-26	5176-5184	gradient	abstract[243]	new[243]	_	_
34-27	5185-5187	of	abstract[243]	new[243]	_	_
34-28	5188-5192	loss	abstract[243]|abstract|abstract[245]	new[243]|giv|giv[245]	_	_
34-29	5193-5201	function	abstract[243]|abstract[245]	new[243]|giv[245]	_	_
34-30	5202-5209	defined	_	_	_	_
34-31	5210-5214	with	_	_	_	_
34-32	5215-5216	a	plant[247]	new[247]	_	_
34-33	5217-5223	single	plant[247]	new[247]	_	_
34-34	5224-5228	data	abstract|plant[247]	giv|new[247]	_	_
34-35	5229-5233	pair	plant[247]	new[247]	_	_
34-36	5234-5235	,	plant[247]	new[247]	_	_
34-37	5236-5240	such	plant[247]	new[247]	_	_
34-38	5241-5243	as	plant[247]	new[247]	_	_
34-39	5244-5245	(	plant[247]	new[247]	_	_
34-40	5246-5247	9	plant[247]	new[247]	_	_
34-41	5248-5249	)	plant[247]	new[247]	_	_
