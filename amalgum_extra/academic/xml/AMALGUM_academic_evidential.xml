<?xml version='1.0' encoding='utf8'?>
<text id="AMALGUM_academic_evidential" title="Augmenting Deep Learning Performance in an Evidential Multiple Classifier System" shortTitle="evidential" author="Jennifer Vandoni, Sylvie  Le Hégarat-Mascle, Emanuel Aldea" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/1424-8220/19/21/4664/htm" speakerList="none" speakerCount="0">
<head>
<s type="frag">
1.	LS	1.
Introduction	NN	introduction
</s>
</head>
<p>
<s type="decl">
Even	RB	even
though	IN	though
deep	JJ	deep
learning	NN	learning
solutions	NNS	solution
tend	VBP	tend
to	TO	to
outperform	VB	outperform
the	DT	the
other	JJ	other
supervised	VBN	supervise
learning	NN	learning
techniques	NNS	technique
when	WRB	when
trained	VBN	train
on	IN	on
large	JJ	large
amounts	NNS	amount
of	IN	of
data	NNS	datum
,	,	,
applying	VBG	apply
them	PRP	they
effectively	RB	effectively
in	IN	in
presence	NN	presence
of	IN	of
few	JJ	few
labeled	VBN	label
data	NNS	datum
is	VBZ	be
nowadays	RB	nowadays
an	DT	a
open	JJ	open
issue	NN	issue
.	.	.
</s>
<s type="decl">
Most	JJS	most
of	IN	of
the	DT	the
existing	VBG	exist
works	NNS	work
are	VBP	be
devoted	VBN	devote
to	IN	to
finding	VBG	find
the	DT	the
best	JJS	good
network	NN	network
for	IN	for
applications	NNS	application
for	IN	for
which	WDT	which
huge	JJ	huge
datasets	NNS	dataset
exist	VBP	exist
,	,	,
but	CC	but
few	JJ	few
attention	NN	attention
is	VBZ	be
given	VBN	give
to	IN	to
specific	JJ	specific
real-setting	NN	real-setting
problems	NNS	problem
where	WRB	where
training	NN	training
data	NNS	datum
are	VBP	be
hard	JJ	hard
to	TO	to
obtain	VB	obtain
and	CC	and
therefore	RB	therefore
out-of-the-box	JJ	out-of-the-box
networks	NNS	network
may	MD	may
be	VB	be
impossible	JJ	impossible
to	TO	to
be	VB	be
trained	VBN	train
.	.	.
</s>
<s type="decl">
Nonetheless	RB	nonetheless
,	,	,
in	IN	in
recent	JJ	recent
years	NNS	year
,	,	,
many	JJ	many
regularization	NN	regularization
techniques	NNS	technique
have	VBP	have
been	VBN	be
proposed	VBN	propose
to	TO	to
tackle	VB	tackle
the	DT	the
problem	NN	problem
of	IN	of
overfitting	NN	overfitting
,	,	,
from	IN	from
data	NNS	datum
augmentation	NN	augmentation
to	IN	to
early	JJ	early
stopping	NN	stopping
and	CC	and
dropout	NN	dropout
,	,	,
besides	IN	besides
the	DT	the
traditional	JJ	traditional
weight	NN	weight
decay	NN	decay
.	.	.
</s>
<s type="sub">
These	DT	this
techniques	NNS	technique
used	VBN	use
together	RB	together
could	MD	could
help	VB	help
in	IN	in
applying	VBG	apply
deep	JJ	deep
learning	NN	learning
techniques	NNS	technique
in	IN	in
the	DT	the
presence	NN	presence
of	IN	of
small	JJ	small
datasets	NNS	dataset
.	.	.
</s>
<s type="sub">
In	IN	in
addition	NN	addition
to	IN	to
these	DT	this
techniques	NNS	technique
,	,	,
fusion	NN	fusion
with	IN	with
another	DT	another
strong	JJ	strong
classifier	NN	classifier
may	MD	may
be	VB	be
considered	VBN	consider
.	.	.
</s>
</p>
<p>
<s type="decl">
Simultaneously	RB	simultaneously
,	,	,
a	DT	a
criticism	NN	criticism
that	WDT	that
is	VBZ	be
often	RB	often
made	VBN	make
of	IN	of
deep	JJ	deep
learning	NN	learning
methods	NNS	method
is	VBZ	be
the	DT	the
fact	NN	fact
that	IN	that
they	PRP	they
act	VBP	act
like	IN	like
“	``	''
black-boxes	NNS	black-box
”	''	''
,	,	,
making	VBG	make
it	PRP	it
hard	JJ	hard
for	IN	for
their	PRP$	their
users	NNS	user
to	TO	to
interpret	VB	interpret
the	DT	the
obtained	VBN	obtain
results	NNS	result
.	.	.
</s>
<s type="decl">
This	DT	this
limitation	NN	limitation
is	VBZ	be
highly	RB	highly
relevant	JJ	relevant
when	WRB	when
learning	VBG	learn
from	IN	from
small	JJ	small
amounts	NNS	amount
of	IN	of
data	NNS	datum
,	,	,
where	WRB	where
a	DT	a
measure	NN	measure
of	IN	of
model	NN	model
uncertainty	NN	uncertainty
would	MD	would
be	VB	be
particularly	RB	particularly
important	JJ	important
.	.	.
</s>
<s type="decl">
To	IN	to
this	DT	this
extent	NN	extent
Bayesian	NNP	Bayesian
Neural	NNP	Neural
Networks	NNPS	Network
(	-LRB-	(
BNNs	NNS	bnn
,	,	,
Bayesian	NNP	Bayesian
NNs	NNP	NNs
)	-RRB-	)
offer	VBP	offer
a	DT	a
probabilistic	JJ	probabilistic
interpretation	NN	interpretation
of	IN	of
deep	JJ	deep
learning	NN	learning
models	NNS	model
by	IN	by
inferring	VBG	infer
distributions	NNS	distribution
over	IN	over
the	DT	the
models	NNS	model
’	POS	's
weights	NNS	weight
,	,	,
allowing	VBG	allow
to	TO	to
measure	VB	measure
model	NN	model
uncertainty	NN	uncertainty
,	,	,
but	CC	but
they	PRP	they
are	VBP	be
usually	RB	usually
practically	RB	practically
limited	VBN	limit
.	.	.
</s>
<s type="decl">
Recently	RB	recently
,	,	,
an	DT	a
ensemble-based	JJ	ensemble-based
method	NN	method
relying	VBG	rely
on	IN	on
the	DT	the
use	NN	use
of	IN	of
dropout	NN	dropout
at	IN	at
inference	NN	inference
time	NN	time
has	VBZ	have
been	VBN	be
proposed	VBN	propose
in	IN	in
(	-LRB-	(
Monte	NNP	Monte
Carlo	NNP	Carlo
dropout	NN	dropout
)	-RRB-	)
,	,	,
allowing	VBG	allow
to	TO	to
obtain	VB	obtain
several	JJ	several
realizations	NNS	realization
sampled	VBN	sample
from	IN	from
the	DT	the
same	JJ	same
network	NN	network
with	IN	with
randomly	RB	randomly
dropped-out	VBN	dropped-out
units	NNS	unit
at	IN	at
test	NN	test
time	NN	time
,	,	,
from	IN	from
which	WDT	which
a	DT	a
confidence	NN	confidence
measure	NN	measure
on	IN	on
the	DT	the
prediction	NN	prediction
can	MD	can
be	VB	be
derived	VBN	derive
.	.	.
</s>
</p>
<p>
<s type="decl">
Following	VBG	follow
this	DT	this
line	NN	line
of	IN	of
work	NN	work
,	,	,
we	PRP	we
intend	VBP	intend
to	TO	to
investigate	VB	investigate
the	DT	the
use	NN	use
of	IN	of
deep	JJ	deep
learning	NN	learning
techniques	NNS	technique
in	IN	in
presence	NN	presence
of	IN	of
small	JJ	small
training	NN	training
datasets	NNS	dataset
for	IN	for
specific	JJ	specific
applications	NNS	application
(	-LRB-	(
in	IN	in
our	PRP$	our
case	NN	case
high-density	JJ	high-density
crowd	NN	crowd
pedestrian	NN	pedestrian
detection	NN	detection
)	-RRB-	)
.	.	.
</s>
<s type="decl">
A	DT	a
solution	NN	solution
proposed	VBN	propose
for	IN	for
instance	NN	instance
by	IN	by
in	IN	in
the	DT	the
case	NN	case
of	IN	of
hyperspectral	NN	hyperspectral
data	NNS	datum
is	VBZ	be
to	TO	to
reduce	VB	reduce
the	DT	the
number	NN	number
of	IN	of
weight	NN	weight
parameters	NNS	parameter
required	VBN	require
to	TO	to
train	VB	train
the	DT	the
model	NN	model
by	IN	by
considering	VBG	consider
some	DT	some
constraints	NNS	constraint
related	JJ	related
to	IN	to
the	DT	the
physical	JJ	physical
interpretation	NN	interpretation
of	IN	of
the	DT	the
weights	NNS	weight
.	.	.
</s>
<s type="decl">
In	IN	in
this	DT	this
work	NN	work
,	,	,
the	DT	the
type	NN	type
of	IN	of
the	DT	the
data	NNS	datum
(	-LRB-	(
grayscale	JJ	grayscale
images	NNS	image
)	-RRB-	)
is	VBZ	be
not	RB	not
suitable	JJ	suitable
for	IN	for
such	JJ	such
prior	JJ	prior
constraints	NNS	constraint
,	,	,
we	PRP	we
propose	VBP	propose
the	DT	the
use	NN	use
of	IN	of
an	DT	a
ensemble	NN	ensemble
method	NN	method
that	WDT	that
is	VBZ	be
justified	VBN	justify
according	VBG	accord
to	IN	to
two	CD	2
different	JJ	different
reasons	NNS	reason
.	.	.
</s>
<s type="decl">
Firstly	RB	firstly
,	,	,
it	PRP	it
acts	VBZ	act
as	IN	as
another	DT	another
regularization	NN	regularization
technique	NN	technique
to	TO	to
mitigate	VB	mitigate
the	DT	the
risk	NN	risk
of	IN	of
overfitting	NN	overfitting
;	:	;
</s>
<s type="decl">
secondly	RB	secondly
,	,	,
it	PRP	it
allows	VBZ	allow
us	PRP	we
to	TO	to
measure	VB	measure
the	DT	the
model	NN	model
confidence	NN	confidence
about	IN	about
each	DT	each
prediction	NN	prediction
.	.	.
</s>
<s type="decl">
To	IN	to
this	DT	this
extent	NN	extent
,	,	,
we	PRP	we
propose	VBP	propose
to	TO	to
work	VB	work
in	IN	in
the	DT	the
context	NN	context
of	IN	of
the	DT	the
Belief	NNP	Belief
Function	NNP	Function
Theory	NNP	Theory
(	-LRB-	(
BFT	NNP	BFT
)	-RRB-	)
to	TO	to
better	RBR	well
leverage	VB	leverage
the	DT	the
classifier	NN	classifier
’s	POS	's
unique	JJ	unique
properties	NNS	property
.	.	.
</s>
<s type="decl">
The	DT	the
evidential	JJ	evidential
framework	NN	framework
is	VBZ	be
indeed	RB	indeed
able	JJ	able
to	TO	to
naturally	RB	naturally
model	VB	model
the	DT	the
concept	NN	concept
of	IN	of
<hi rend="italic">
imprecision	NN	imprecision
</hi>
in	IN	in
addition	NN	addition
to	IN	to
the	DT	the
uncertainty	NN	uncertainty
value	NN	value
provided	VBN	provide
by	IN	by
the	DT	the
classifiers	NNS	classifyer
.	.	.
</s>
</p>
<p>
<s type="decl">
We	PRP	we
thus	RB	thus
propose	VBP	propose
an	DT	a
evidential	JJ	evidential
Multiple	NNP	Multiple
Classifier	NNP	Classifier
System	NNP	system
(	-LRB-	(
MCS	NNP	MCS
)	-RRB-	)
,	,	,
which	WDT	which
is	VBZ	be
in	IN	in
turn	NN	turn
composed	VBN	compose
by	IN	by
two	CD	2
ensembles	NNS	ensemble
of	IN	of
classifiers	NNS	classifyer
.	.	.
</s>
<s type="decl">
The	DT	the
first	JJ	first
one	CD	one
,	,	,
called	VBN	call
CNN-ensemble	NNP	CNN-ensemble
,	,	,
is	VBZ	be
an	DT	a
ensemble	NN	ensemble
of	IN	of
convolutional	JJ	convolutional
neural	JJ	neural
networks	NNS	network
(	-LRB-	(
CNNs	NNS	CNN
)	-RRB-	)
derived	VBN	derive
using	VBG	use
the	DT	the
Monte	NNP	Monte
Carlo	NNP	Carlo
dropout	NN	dropout
technique	NN	technique
.	.	.
</s>
<s type="decl">
The	DT	the
second	JJ	second
one	CD	one
,	,	,
called	VBN	call
SVM-ensemble	NNP	SVM-ensemble
,	,	,
is	VBZ	be
an	DT	a
ensemble	NN	ensemble
of	IN	of
Support	NNP	Support
Vector	NNP	Vector
Machine	NNP	Machine
(	-LRB-	(
SVM	NNP	SVM
)	-RRB-	)
classifiers	NNS	classifyer
trained	VBN	train
with	IN	with
different	JJ	different
descriptors	NNS	descriptor
in	IN	in
an	DT	a
active	JJ	active
learning	NN	learning
(	-LRB-	(
AL	NNP	AL
)	-RRB-	)
Query-by-Committee	NNP	Query-by-Committee
fashion	NN	fashion
previously	RB	previously
proposed	VBN	propose
in	IN	in
.	.	.
</s>
<s type="decl">
Specifically	RB	specifically
,	,	,
starting	VBG	start
from	IN	from
a	DT	a
single	JJ	single
sensor	NN	sensor
input	NN	input
,	,	,
i.e.	FW	i.e.
,	,	,
an	DT	a
image	NN	image
lattice	NN	lattice
,	,	,
we	PRP	we
derive	VBP	derive
two	CD	2
different	JJ	different
ensembles	NNS	ensemble
based	VBN	base
on	IN	on
complementary	JJ	complementary
classifiers	NNS	classifyer
.	.	.
</s>
<s type="decl">
These	DT	this
two	CD	2
ensembles	NNS	ensemble
are	VBP	be
then	RB	then
considered	VBN	consider
as	IN	as
different	JJ	different
information	NN	information
sources	NNS	source
,	,	,
like	IN	like
virtual	JJ	virtual
sensors	NNS	sensor
.	.	.
</s>
</p>
<p>
<s type="decl">
We	PRP	we
apply	VBP	apply
the	DT	the
proposed	VBN	propose
Evidential	NNP	Evidential
MCS	NNP	MCS
to	IN	to
the	DT	the
difficult	JJ	difficult
application	NN	application
of	IN	of
high-density	JJ	high-density
crowd	NN	crowd
pedestrian	NN	pedestrian
detection	NN	detection
for	IN	for
multiple	JJ	multiple
reasons	NNS	reason
.	.	.
</s>
<s type="sub">
Indeed	RB	indeed
,	,	,
although	IN	although
in	IN	in
the	DT	the
last	JJ	last
years	NNS	year
,	,	,
many	JJ	many
efforts	NNS	effort
have	VBP	have
been	VBN	be
devoted	VBN	devote
to	TO	to
improve	VB	improve
the	DT	the
performance	NN	performance
of	IN	of
pedestrian	NN	pedestrian
detection	NN	detection
,	,	,
baseline	NN	baseline
methods	NNS	method
cannot	MD	cannot
be	VB	be
always	RB	always
applied	VBN	apply
in	IN	in
crowded	JJ	crowded
contexts	NNS	context
because	IN	because
of	IN	of
scarce	JJ	scarce
labeled	VBN	label
data	NNS	datum
,	,	,
and	CC	and
intrinsic	JJ	intrinsic
differences	NNS	difference
with	IN	with
respect	NN	respect
to	IN	to
the	DT	the
sparse	JJ	sparse
case	NN	case
which	WDT	which
may	MD	may
be	VB	be
cause	NN	cause
of	IN	of
imprecision	NN	imprecision
in	IN	in
the	DT	the
final	JJ	final
detection	NN	detection
results	NNS	result
.	.	.
</s>
</p>
<p>
<s type="decl">
Pedestrian	JJ	Pedestrian
detection	NN	detection
by	IN	by
itself	PRP	itself
is	VBZ	be
noticeably	RB	noticeably
one	CD	one
of	IN	of
the	DT	the
most	RBS	most
challenging	JJ	challenging
categories	NNS	category
of	IN	of
object	NN	object
detection	NN	detection
.	.	.
</s>
<s type="decl">
There	EX	there
exists	VBZ	exist
indeed	RB	indeed
a	DT	a
large	JJ	large
variability	NN	variability
in	IN	in
the	DT	the
local	JJ	local
and	CC	and
global	JJ	global
pedestrians	NNS	pedestrian
’	POS	's
appearance	NN	appearance
,	,	,
due	JJ	due
to	IN	to
the	DT	the
variety	NN	variety
of	IN	of
possible	JJ	possible
body	NN	body
shapes	NNS	shape
,	,	,
or	CC	or
different	JJ	different
styles	NNS	style
and	CC	and
types	NNS	type
of	IN	of
clothes	NNS	clothes
and	CC	and
accessories	NNS	accessory
which	WDT	which
may	MD	may
alter	VB	alter
the	DT	the
silhouettes	NNS	silhouette
of	IN	of
the	DT	the
individuals	NNS	individual
.	.	.
</s>
<s type="decl">
Besides	RB	besides
,	,	,
in	IN	in
real-world	JJ	real-world
scenarios	NNS	scenario
several	JJ	several
people	NNS	person
can	MD	can
occupy	VB	occupy
the	DT	the
same	JJ	same
region	NN	region
,	,	,
partially	RB	partially
occluding	VBG	occlude
each	DT	each
other	JJ	other
,	,	,
and	CC	and
this	DT	this
phenomenon	NN	phenomenon
becomes	VBZ	become
more	RBR	more
prevalent	JJ	prevalent
as	IN	as
the	DT	the
crowd	NN	crowd
density	NN	density
increases	VBZ	increase
.	.	.
</s>
</p>
</text>