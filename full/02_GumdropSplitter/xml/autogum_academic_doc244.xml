<?xml version="1.0" ?><text author="Hyeyoung Park, Kwanyong Lee" dateCollected="2019-11-03" id="autogum_academic_doc244" shortTile="adaptive-natural" sourceURL="https://www.mdpi.com/2076-3417/9/21/4568/htm" speakerCount="0" speakerList="none" title="Adaptive Natural Gradient Method for Learning of Stochastic Neural Networks in Mini-Batch Mode" type="academic">
<head>
<s>
2
.
</s>
<s>
Gradient
Descent
Learning
of
Stochastic
Neural
Networks
</s>
</head>
<head>
<s>
2.1
.
</s>
<s>
Stochastic
Neural
Networks
</s>
</head>
<p>
<s>
Since
the
natural
gradient
is
derived
from
stochastic
neural
network
models
,
let
us
start
from
the
brief
description
of
the
two
popular
stochastic
models
.
</s>
<s>
Although
typical
neural
networks
,
including
deep
networks
,
are
defined
as
deterministic
input-output
mapping
functions
of
input
,
output
,
and
parameter
,
the
observed
data
for
training
the
networks
always
have
inevitable
noises
and
thus
the
input-output
relation
can
be
described
in
the
stochastic
manner
.
</s>
<s>
In
other
words
,
the
observed
output
can
be
regarded
as
a
random
vector
that
is
dependent
on
the
deterministic
function
and
some
additional
stochastic
process
that
is
described
by
the
conditional
probability
.
</s>
</p>
<p>
<s>
Then
the
goal
of
learning
is
to
find
an
optimal
value
of
parameter
that
minimizes
the
loss
function
defined
as
negative
log
likelihood
of
given
input-output
sample
.
</s>
<s>
The
loss
function
can
then
be
written
as
(
1
)
and
the
optimal
parameter
is
described
as
(
2
)
</s>
</p>
<p>
<s>
Note
that
the
last
term
in
equation
(
1
)
is
independent
of
parameter
and
can
be
ignored
in
the
objective
function
for
optimization
.
</s>
</p>
<p>
<s>
Based
on
the
general
definition
,
the
conventional
neural
networks
can
be
regarded
as
a
special
case
with
a
specific
conditional
probability
distribution
,
.
</s>
<s>
For
example
,
consider
that
the
output
is
observed
with
additive
noise
to
the
deterministic
neural
networks
function
such
as
(
3
)
where
is
a
random
noise
vector
.
</s>
<s>
When
we
assume
that
the
random
vector
is
subject
to
the
standard
Gaussian
distribution
,
its
probability
density
is
defined
as
the
normal
distribution
function
,
and
its
loss
function
becomes
the
well-known
squared
error
function
,
which
can
be
written
as
(
4
)
</s>
</p>
<p>
<s>
Therefore
,
the
stochastic
neural
network
model
with
additive
Gaussian
noise
is
equivalent
to
the
typical
neural
network
model
trained
with
squared
error
function
,
which
is
widely
used
for
regression
task
.
</s>
</p>
<p>
<s>
On
the
other
hand
,
in
case
that
the
output
is
a
binary
vector
,
the
corresponding
probability
distribution
can
be
defined
by
using
a
logistic
model
,
such
as
(
5
)
where
and
are
the
<hi rend="italic">
i
</hi>
-th
component
of
<hi rend="italic">
L
</hi>
-dimensional
vector
and
,
respectively
.
</s>
<s>
Since
the
typical
problems
with
binary
target
output
vector
is
pattern
classification
,
the
logistic
model
is
appropriate
for
<hi rend="italic">
L
</hi>
-class
classification
tasks
.
</s>
<s>
The
corresponding
loss
function
of
the
logistic
model
is
obtained
by
taking
negative
log
likelihood
of
Equation
(
5
)
,
which
can
be
written
as
,
(
6
)
</s>
</p>
<p>
<s>
Noting
that
this
is
the
well-known
cross-entropy
error
,
we
can
say
that
the
logistic
regression
model
is
equivalent
to
the
typical
neural
networks
with
cross-entropy
error
,
which
is
widely
used
for
classification
task
.
</s>
</p>
<p>
<s>
Likewise
,
by
defining
a
proper
stochastic
model
,
we
can
derive
various
types
of
neural
network
models
,
which
can
explain
the
given
task
more
adequately
and
get
a
new
insight
to
solve
many
unresolved
problems
in
the
field
of
neural
network
learning
.
</s>
<s>
Natural
gradient
is
also
derived
from
a
new
metric
for
the
space
of
probability
density
function
of
stochastic
neural
networks
.
</s>
<s>
In
this
paper
,
we
present
explicit
algorithms
of
adaptive
natural
gradient
learning
method
for
two
representative
stochastic
neural
network
models
:
The
additive
Gaussian
noise
model
and
the
logistic
regression
model
.
</s>
</p>
<head>
<s>
2.2
.
</s>
<s>
Gradient
Descent
Learning
</s>
</head>
<p>
<s>
Once
a
specific
model
of
stochastic
neural
networks
and
its
corresponding
loss
function
are
determined
,
the
weight
parameters
are
optimized
by
gradient
descent
method
.
</s>
<s>
The
well-known
error-backpropagation
algorithm
is
the
standard
type
of
gradient
descent
learning
method
.
</s>
<s>
There
have
been
numerous
variations
of
the
standard
gradient
descent
method
,
including
second-order
methods
,
momentum
method
,
and
adaptive
learning
rate
methods
.
</s>
<s>
Since
the
natural
gradient
learning
method
is
also
based
on
the
gradient
descent
method
,
we
describe
the
basic
formula
of
gradient
descent
learning
and
its
online
version
that
is
called
stochastic
gradient
descent
method
.
</s>
</p>
<p>
<s>
When
a
set
of
training
data
is
given
,
a
neural
network
is
trained
in
order
to
find
an
input-output
mapping
that
is
specified
with
weight
parameter
vector
.
</s>
<s>
The
error
of
neural
network
for
the
whole
data
set
can
then
be
defined
by
using
a
loss
function
such
as
(
7
)
and
the
goal
of
learning
is
to
get
the
optimal
minimizing
.
</s>
<s>
To
achieve
the
goal
,
the
weight
parameter
is
updated
starting
from
the
current
position
,
according
to
the
opposite
direction
of
the
gradient
of
,
which
can
be
written
as
(
8
)
</s>
</p>
<p>
<s>
This
update
rule
is
called
batch
learning
mode
,
meaning
that
an
update
is
done
for
the
whole
batch
set
.
</s>
<s>
Theoretically
,
the
batch
mode
learning
gives
the
steepest
descent
direction
of
at
the
current
position
of
,
but
it
has
two
practical
drawbacks
.
</s>
<s>
First
,
it
is
too
stable
to
be
easily
trapped
in
undesirable
local
minima
.
</s>
<s>
In
addition
,
when
the
number
of
data
is
large
,
it
needs
large
amounts
of
computation
for
just
a
single
update
,
making
the
learning
process
slow
.
</s>
</p>
<p>
<s>
To
overcome
this
practical
inefficiency
,
online
stochastic
gradient
decent
learning
is
proposed
,
in
which
parameters
are
updated
for
each
data
sample
by
using
gradient
of
loss
function
defined
with
a
single
data
pair
,
such
as
(
9
)
</s>
</p>
</text>