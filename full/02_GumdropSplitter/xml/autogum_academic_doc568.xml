<?xml version="1.0" ?>
<text author="Xuewei Huang, Shiyuan Wang, Kui Xiong" dateCollected="2019-11-03" id="autogum_academic_doc568" shortTile="cauchy-conjugate" sourceURL="https://www.mdpi.com/2073-8994/11/10/1323/htm" speakerCount="0" speakerList="none" title="The Cauchy Conjugate Gradient Algorithm with Random Fourier Features" type="academic">
<head>
<s>
1
.
</s>
<s>
Introduction
</s>
</head>
<p>
<s>
Many
applications
in
the
real
world
,
such
as
system
identification
,
regression
,
and
online
kernel
learning
(
OKL
)
,
require
complex
nonlinear
models
.
</s>
<s>
The
kernel
method
using
a
Mercer
kernel
has
attracted
interests
in
tackling
these
complex
nonlinear
applications
,
which
transforms
nonlinear
applications
into
linear
ones
in
the
reproducing
kernel
Hilbert
space
(
RKHS
)
.
</s>
<s>
Developed
in
RKHS
,
a
kernel
adaptive
filter
(
KAF
)
is
the
most
celebrated
subfield
of
OKL
algorithms
.
</s>
<s>
Using
the
simplest
stochastic
gradient
descent
(
SGD
)
method
for
learning
,
KAFs
including
the
kernel
least
mean
square
(
KLMS
)
algorithm
,
kernel
affine
projection
algorithm
(
KAPA
)
,
and
kernel
recursive
least
squares
(
KRLS
)
algorithm
have
been
proposed
.
</s>
</p>
<p>
<s>
However
,
allocating
a
new
kernel
unit
as
a
radial
basis
function
(
RBF
)
center
with
the
coming
of
new
data
,
the
linearly
growing
structure
(
called
“
dictionary
”
hereafter
)
will
increase
the
computational
and
memory
requirements
in
KAFs
.
</s>
<s>
To
curb
the
growth
of
the
dictionary
,
two
categories
are
chosen
for
sparsification
.
</s>
<s>
The
first
category
accepts
only
informative
data
as
new
dictionary
centers
by
using
a
threshold
,
including
the
surprise
criterion
(
SC
)
,
the
coherence
criterion
(
CC
)
,
and
the
vector
quantization
(
VQ
)
.
</s>
<s>
However
,
these
methods
cannot
fully
address
the
growing
problem
and
still
introduce
additional
time
consumption
at
each
iteration
.
</s>
<s>
The
fixed
points
methods
as
the
second
category
,
including
the
fixed-budget
(
FB
)
,
the
sliding
window
(
SW
)
,
and
the
kernel
approximation
methods
(
e.
g.
,
the
Nystrm
method
and
random
Fourier
features
(
RFFs
)
method
)
,
are
used
to
overcome
the
sublinearly
growing
problem
.
</s>
<s>
However
,
the
FB
method
and
the
SW
method
cannot
guarantee
a
good
performance
in
specific
environments
with
a
small
amount
of
time
.
</s>
<s>
Compared
with
the
Nystrm
method
,
RFFs
are
drawn
from
a
distribution
that
is
randomly
independent
from
the
training
data
.
</s>
<s>
Due
to
a
data-independent
vector
representation
,
RFFs
can
provide
a
good
solution
to
non-stationary
circumstances
.
</s>
<s>
On
the
basis
of
RFFs
,
random
Fourier
mapping
(
RFM
)
is
proposed
by
mapping
input
data
into
a
finite-dimensional
random
Fourier
features
space
(
RFFS
)
using
a
randomized
feature
kernel
’s
Fourier
transform
in
a
fixed
network
structure
.
</s>
<s>
The
RFM
alleviates
the
computational
and
storage
burdens
of
KAFs
,
and
ensures
a
satisfactory
performance
under
non-stationary
conditions
.
</s>
<s>
The
examples
for
developing
KAFs
with
RFM
are
the
random
Fourier
features
kernel
least
mean
square
(
RFFKLMS
)
algorithm
,
random
Fourier
features
maximum
correntropy
(
RFFMC
)
algorithm
,
and
random
Fourier
features
conjugate
gradient
(
RFFCG
)
algorithm
.
</s>
</p>
<p>
<s>
For
the
loss
function
,
due
to
their
simplicity
,
smoothness
,
and
mathematical
tractability
,
the
second-order
statistical
measures
(
e.
g.
,
minimum
mean
square
error
(
MMSE
)
and
least
squares
)
are
widely
utilized
in
KAFs
.
</s>
<s>
However
,
KAFs
based
on
the
second-order
statistical
measures
are
sensitive
to
non-Gaussian
noises
including
the
sub-Gaussian
and
super-Gaussian
noises
,
which
means
that
their
performance
may
be
seriously
degraded
if
the
training
data
are
contaminated
by
outliers
.
</s>
<s>
To
handle
this
issue
,
robust
statistical
measures
have
therefore
gained
more
attention
,
among
which
the
lower-order
error
measure
and
the
higher-lower
error
measure
are
two
typical
examples
.
</s>
<s>
However
,
the
higher-order
error
measure
is
not
suitable
for
the
mixture
of
Gaussian
and
super-Gaussian
noises
(
Laplace
,
-stable
,
etc.
)
with
poor
stability
and
astringency
,
and
the
lower-order
measure
of
error
is
usually
more
desirable
in
these
noise
environments
with
slow
convergence
rate
.
</s>
<s>
Recently
,
the
information
theoretic
learning
(
ITL
)
similarity
measures
,
such
as
the
maximum
correntropy
criterion
(
MCC
)
and
minimum
error
entropy
criterion
(
MEE
)
,
have
been
introduced
to
implement
robust
KAFs
.
</s>
<s>
The
ITL
similarity
measures
have
been
shown
to
have
a
strong
robustness
against
non-Gaussian
noises
at
the
expense
of
increasing
computational
burden
in
training
processing
.
</s>
<s>
In
addition
,
minimizing
the
logarithmic
moments
of
the
error
,
the
logarithmic
error
measure
—
including
the
Cauchy
loss
(
CL
)
with
low
computational
complexity
—
is
an
appropriate
measure
of
optimality
.
</s>
<s>
Using
the
Cauchy
loss
to
penalize
the
noise
term
,
some
algorithms
based
on
the
minimum
Cauchy
loss
(
MCL
)
criterion
are
efficient
for
combating
non-Gaussian
noises
,
especially
for
heavy-tailed
-
stable
noises
.
</s>
</p>
<p>
<s>
From
the
aspect
of
the
optimization
method
,
the
stochastic
gradient
descent
(
SGD)-based
algorithms
cannot
find
the
minimum
using
the
negative
gradient
in
some
loss
functions
.
</s>
<s>
Toward
this
end
,
recursive-based
algorithms
address
these
issues
at
the
cost
of
increasing
computational
cost
.
</s>
<s>
In
comparison
with
the
SGD
method
and
recursive
method
,
the
conjugate
gradient
(
CG
)
method
and
Newton
’s
method
as
developments
of
SGD
have
become
alternative
optimization
methods
in
KAFs
.
</s>
<s>
The
inverse
of
matrix
of
Newton
’s
method
increases
the
computation
and
causes
the
divergence
of
algorithms
in
some
cases
.
</s>
<s>
However
,
the
CG
method
gives
a
trade-off
between
convergence
rate
and
computational
complexity
without
the
inverse
computation
,
and
has
been
successfully
applied
in
various
fields
,
including
compressed
sensing
,
neural
networks
,
and
large-scale
optimization
.
</s>
<s>
In
addition
,
the
kernel
conjugate
gradient
(
KCG
)
method
is
proposed
for
adaptive
filtering
.
</s>
<s>
KCG
with
low
computational
and
space
requirements
can
produce
a
better
solution
than
KLMS
,
and
has
comparable
accuracy
to
KRLS
.
</s>
</p>
</text>
