<?xml version="1.0" ?><text author="Karol Nowakowski, Michal Ptaszynski, Fumito Masui" dateCollected="2019-11-03" id="autogum_academic_doc296" shortTile="mingmatch-fast-ngram" sourceURL="https://www.mdpi.com/2078-2489/10/10/317/htm" speakerCount="0" speakerList="none" title="MiNgMatch — A Fast N-gram Model for Word Segmentation of the Ainu Language" type="academic">
<head>
<s>
6
.
</s>
<s>
Results
and
Discussion
</s>
</head>
<p>
<s>
The
results
of
the
evaluation
experiments
with
our
algorithm
are
presented
in
Table
5
.
</s>
<s>
The
variant
without
the
limit
of
n-grams
per
input
segment
produces
unbalanced
results
(
especially
on
SYOS
)
,
with
relatively
low
Precision
.
</s>
<s>
After
setting
the
limit
to
2
,
Precision
improves
at
the
cost
of
a
drop
in
Recall
.
</s>
<s>
The
F-score
is
better
for
SYOS
,
while
on
AKJ
there
is
a
very
slight
drop
.
</s>
</p>
<p>
<s>
Table
6
shows
the
results
of
experiments
with
the
Stupid
Backoff
model
.
</s>
<s>
When
no
backoff
factor
is
applied
,
results
for
both
test
sets
are
similar
to
those
from
the
MiNgMatch
Segmenter
without
the
limit
of
n-grams
per
input
segment
.
</s>
<s>
Setting
the
backoff
factor
to
an
appropriate
value
allows
for
significant
improvement
in
Precision
and
F-score
(
and
in
some
cases
also
small
improvements
in
Recall
)
.
</s>
<s>
For
the
F-score
,
it
is
better
to
set
a
low
backoff
factor
(
e.
g.
,
0.09
)
for
1-grams
only
,
than
to
set
it
to
a
fixed
value
for
all
backoff
steps
(
e.
g.
,
0.4
,
as
Brants
et
al.
did
)
.
</s>
<s>
A
backoff
factor
of
0.4
gives
significant
improvement
in
Precision
with
higher
order
n-gram
models
,
but
at
the
same
time
Recall
drops
drastically
and
overall
performance
deteriorates
.
</s>
<s>
For
models
with
an
n-gram
order
of
3
or
higher
,
the
backoff
factor
has
a
bigger
impact
on
the
results
than
further
increasing
the
order
of
n-grams
included
in
the
model
.
</s>
<s>
A
comparison
with
the
results
yielded
by
MiNgMatch
shows
that
setting
the
limit
of
n-grams
per
input
segment
is
more
effective
than
Stupid
Backoff
as
a
method
for
improving
precision
of
the
segmentation
process
—
it
leads
to
a
much
smaller
drop
in
Recall
.
</s>
</p>
<p>
<s>
The
results
of
the
experiment
with
models
employing
modified
Kneser-Ney
smoothing
are
shown
in
Table
7
.
</s>
<s>
They
achieve
higher
Precision
than
both
the
other
types
of
n-gram
models
.
</s>
<s>
Nevertheless
,
due
to
very
low
Recall
,
the
overall
results
are
low
.
</s>
</p>
<p>
<s>
The
results
obtained
by
the
Universal
Segmenter
are
presented
in
Table
8
.
</s>
<s>
The
default
model
(
regardless
of
what
kind
of
character
representations
are
used
—
conventional
character
embeddings
or
concatenated
n-gram
vectors
)
learns
from
the
training
data
that
the
first
and
the
last
character
of
a
word
(
corresponding
to
<tt>
B
</tt>
,
<tt>
E
</tt>
and
<tt>
S
</tt>
tags
)
are
always
adjacent
either
to
the
boundary
of
a
space-delimited
segment
or
to
a
punctuation
mark
.
</s>
<s>
As
a
result
,
the
model
separates
punctuation
from
alpha-numeric
strings
found
in
the
input
,
but
never
applies
further
segmentation
to
them
.
</s>
</p>
<p>
<s>
US-ISP
models
are
better
but
still
notably
worse
than
lexical
n-gram
models
(
especially
on
SYOS
)
.
</s>
<s>
Unlike
with
default
settings
,
the
model
trained
on
data
without
whitespaces
learns
to
predict
word
boundaries
within
strings
of
alpha-numeric
characters
.
</s>
<s>
However
,
when
presented
with
test
data
including
spaces
,
they
impede
the
segmentation
process
rather
than
supporting
it
.
</s>
<s>
As
shown
in
Table
9
,
if
we
only
take
into
account
the
word
boundaries
not
already
indicated
in
the
raw
test
set
,
the
model
makes
more
correct
predictions
in
data
where
the
whitespaces
have
all
been
removed
.
</s>
</p>
<p>
<s>
Models
with
multi-word
tokens
achieve
significantly
higher
results
.
</s>
<s>
Precision
of
the
US-MWTs
model
is
on
par
with
the
segmenter
applying
Kneser-Ney
smoothing
,
while
maintaining
relatively
high
Recall
.
</s>
<s>
It
yields
lower
Recall
than
the
model
with
randomly
generated
multi-word
tokens
,
but
the
F-score
is
higher
due
to
better
Precision
.
</s>
</p>
<p>
<s>
With
the
exception
of
the
US-ISP
model
on
SYOS
,
all
variants
of
the
neural
segmenter
achieved
the
best
performance
with
concatenated
9-gram
vectors
.
</s>
<s>
This
contrasts
with
the
results
reported
by
Shao
et
al.
for
Chinese
,
where
in
most
cases
there
was
no
further
improvement
beyond
3-grams
.
</s>
<s>
This
behavior
is
a
consequence
of
differences
between
writing
systems
:
words
in
Chinese
are
on
average
composed
of
less
characters
than
in
languages
using
alphabetic
scripts
.
</s>
<s>
Due
to
a
much
bigger
character
set
size
,
<hi rend="italic">
hanzi
</hi>
characters
are
also
more
informative
to
word
segmentation
,
hence
better
performance
with
models
using
shorter
context
.
</s>
</p>
<head>
<s>
6.1
.
</s>
<s>
General
Observations
</s>
</head>
<p>
<s>
Due
to
data
sparsity
,
n-gram
coverage
in
the
test
set
(
the
fraction
of
n-grams
in
the
test
data
that
can
be
found
in
the
training
set
)
is
low
(
see
Table
10
)
.
</s>
<s>
It
means
that
many
multi-word
tokens
from
the
test
set
are
known
to
n-gram
models
as
separate
unigrams
,
but
not
in
the
form
of
a
single
n-gram
.
</s>
<s>
The
Stupid
Backoff
model
with
a
backoff
factor
for
unigrams
set
to
a
moderate
value
(
such
as
0.09
)
is
able
to
segment
such
strings
correctly
.
</s>
<s>
However
,
it
also
erroneously
segments
some
OoV
single-word
tokens
whose
surface
forms
happen
to
be
interpretable
as
a
sequence
of
concatenated
in-vocabulary
unigrams
,
resulting
in
lower
Precision
.
</s>
<s>
On
the
other
hand
,
models
assigning
low
scores
to
unigrams
(
such
as
a
4-
or
5-gram
model
with
the
Stupid
Backoff
and
backoff
factor
set
as
suggested
by
Brants
et
al.
,
and
in
particular
the
model
applying
modified
Kneser-Ney
smoothing
)
are
better
at
handling
OoV
words
(
see
Table
11
)
,
but
as
a
result
of
probability
multiplication
,
in
many
cases
they
score
unseen
multi-word
segments
higher
than
a
sequence
of
unigrams
into
which
the
given
segment
should
be
divided
,
hence
yielding
lower
Recall
.
</s>
</p>
</text>