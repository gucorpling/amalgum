<?xml version="1.0" ?>
<text author="Hazel  Si Min Lim, Araz Taeihagh" dateCollected="2019-11-03" id="autogum_academic_doc130" shortTile="algorithmic-decisionmaking" sourceURL="https://www.mdpi.com/2071-1050/11/20/5791/htm" speakerCount="0" speakerList="none" title="Algorithmic Decision-Making in AVs: Understanding Ethical and Technical Concerns for Smart Cities" type="academic">
<head>
<s>
5
.
</s>
<s>
Ethical
Concerns
from
Algorithmic
Decision-Making
in
AVs
</s>
</head>
<p>
<s>
This
Section
explores
ethical
issues
associated
with
algorithmic
decision-making
in
AVs
,
their
implications
for
AV
safety
risks
and
discrimination
and
the
steps
taken
to
tackle
these
issues
.
</s>
<s>
Section
5.1
discusses
the
sources
of
bias
in
AVs
’
algorithms
that
can
yield
discrimination
by
disproportionately
allocating
more
safety
risks
to
some
groups
of
individuals
.
</s>
<s>
Next
,
Section
5.2
explores
approaches
to
incorporate
ethics
into
AV
algorithms
’
decision-making
and
highlight
their
implications
for
AV
safety
and
discrimination
.
</s>
<s>
Lastly
,
Section
5.3
examines
how
the
incentives
of
AV
stakeholders
shape
AV
algorithms
’
design
and
resulting
decisions
that
can
introduce
new
safety
risks
and
discrimination
.
</s>
</p>
<head>
<s>
5.1
.
</s>
<s>
Bias
</s>
</head>
<p>
<s>
A
system
is
considered
biased
when
it
contains
“
intended
”
or
“
unintended
”
characteristics
that
unfairly
discriminate
against
certain
individuals
or
groups
of
individuals
in
society
.
</s>
<s>
In
American
anti-discrimination
law
,
discrimination
exists
when
there
is
disparate
treatment
,
which
is
the
“
discriminatory
intent
or
the
formal
application
of
different
rules
to
people
of
different
groups
”
,
and/or
disparate
impact
,
which
is
the
result
that
“
differ
for
different
groups
”
.
</s>
<s>
Bias
can
be
introduced
into
AVs
during
the
human
designers
’
construction
of
the
datasets
,
models
,
and
the
parameters
of
the
algorithm
,
which
potentially
leads
to
unfair
or
discriminatory
allocations
of
safety
risks
.
</s>
<s>
Firstly
,
statistical
bias
exists
when
the
input
data
are
not
statistically
representative
of
the
overall
population
.
</s>
<s>
For
instance
,
training
an
AV
using
data
from
only
one
country
could
result
in
the
AV
learning
localised
patterns
and
not
accurately
modelling
driving
behaviours
that
apply
in
other
countries
or
contexts
.
</s>
<s>
Thus
,
the
under-
or
overrepresentation
of
certain
groups
in
the
data
can
lead
to
inaccurate
classifications
and
biased
outcomes
.
</s>
<s>
Secondly
,
the
algorithm
can
be
biased
relative
to
legal
and
moral
standards
if
it
utilises
sensitive
input
variables
.
</s>
<s>
Individual-specific
characteristics
,
such
as
a
person
’s
age
and
gender
that
are
used
as
decision-making
criteria
can
be
penalised
or
privileged
by
the
AVs
’
algorithms
to
meet
the
algorithm
’s
pre-defined
preferences
,
such
as
prioritising
the
safety
of
children
or
minimising
the
total
quantity
of
harm
,
causing
more
safety
risks
to
be
allocated
to
individuals
that
share
the
penalised
characteristics
.
</s>
<s>
These
forms
of
bias
can
be
introduced
unintentionally
or
intentionally
by
algorithm
designers
and
AV
manufacturers
to
maximise
profits
,
such
as
prioritising
the
safety
of
AV
passengers
to
maximise
profits
,
and
this
is
exacerbated
by
the
lack
of
legal
frameworks
to
hold
these
stakeholders
accountable
.
</s>
<s>
Section
5.2
explores
various
types
of
ethical
preferences
to
which
AVs
may
be
programmed
to
follow
and
their
implications
of
AV
safety
risks
in
greater
detail
,
and
Section
5.3
explores
how
perverse
incentives
influence
the
choice
of
preferences
that
are
programmed
into
AVs
’
algorithms
.
</s>
</p>
<p>
<s>
Lessening
bias
in
algorithms
is
therefore
crucial
to
mitigate
discriminatory
outcomes
from
AVs
.
</s>
<s>
In
autonomous
systems
in
general
,
scholars
have
recommended
ways
to
detect
and
offset
the
effects
of
bias
,
such
as
modifying
algorithmic
outputs
to
balance
the
effects
of
bias
between
protected
and
unprotected
groups
,
introducing
minimally
intrusive
modification
to
remove
bias
from
the
data
,
incorporating
individuals
from
potentially
discriminated
groups
,
testing
techniques
to
measure
discrimination
and
identify
groups
of
users
significantly
affected
by
bias
in
software
and
creating
algorithms
that
certify
the
absence
of
data
bias
.
</s>
<s>
Apart
from
bias
originating
from
the
data
and
selection
of
variables
and
criterion
,
Danks
and
London
recommend
clarifying
ethical
standards
such
as
fairness
to
evaluate
bias
.
</s>
<s>
Furthermore
,
scholars
recommend
increasing
transparency
to
identify
biases
,
such
as
designing
algorithms
whose
original
input
variables
can
be
traced
throughout
the
system
(
i.
e.
,
traceability
)
and
auditing
algorithms
to
enhance
their
interpretability
so
that
biases
can
be
detected
and
the
system
’s
outputs
can
be
verified
against
safety
requirements
.
</s>
</p>
<p>
<s>
However
,
there
are
challenges
in
identifying
bias
in
algorithms
and
their
discriminatory
effects
.
</s>
<s>
Firstly
,
many
algorithms
are
designed
to
be
highly
complex
for
greater
accuracy
,
but
this
renders
the
algorithm
opaque
and
difficult
to
interpret
even
by
the
designers
themselves
,
concealing
the
sources
of
bias
.
</s>
<s>
Secondly
,
as
ML
algorithms
make
decisions
mainly
based
on
the
training
data
that
changes
over
time
,
it
is
difficult
to
predict
potentially
discriminatory
effects
in
advance
.
</s>
<s>
Humans
are
also
excessively
trusting
and
insufficiently
critical
of
algorithmic
decisions
due
to
the
popular
perception
of
algorithms
as
objective
and
fair
,
a
problem
referred
to
as
“
automation
bias
”
and
the
seemingly
“
objective
”
correlations
that
the
algorithm
learns
from
the
data
makes
it
difficult
to
legally
establish
discriminatory
intent
in
algorithms
.
</s>
<s>
An
emerging
issue
is
the
aggregation
of
individually
biased
outcomes
when
AVs
with
similar
preferences
are
deployed
on
a
large-scale
,
as
doing
so
would
centralise
and
replicate
algorithmic
preferences
along
with
their
individually
biased
risk
allocation
decisions
.
</s>
<s>
This
could
lead
to
the
same
groups
of
people
being
consistently
allocated
more
safety
risks
and
perpetuate
systemic
discrimination
,
which
is
more
difficult
to
detect
as
it
results
from
the
accumulation
of
similar
driving
outcomes
.
</s>
</p>
</text>
