<?xml version="1.0" ?>
<text author="Fanliang Zeng, Zuxin Li, Zhe Zhou, Shuxin Du" dateCollected="2019-11-03" id="autogum_academic_doc302" shortTile="fault-classification" sourceURL="https://www.mdpi.com/2227-9717/7/11/783/htm" speakerCount="0" speakerList="none" title="Fault Classification Decision Fusion System Based on Combination Weights and an Improved Voting Method" type="academic">
<head>
<s>
3
.
</s>
<s>
The
Proposed
Method
</s>
</head>
<p>
<s>
The
ensemble
method
integrates
various
fault
classification
methods
to
improve
the
fault
classification
capability
.
</s>
<s>
A
basic
ensemble
method
includes
the
selection
of
base
classifiers
and
the
determination
of
fusion
strategies
.
</s>
<s>
In
this
study
,
six
basic
classifiers
were
selected
:
linear
discriminant
analysis
(
LDA
)
,
K-nearest
neighbor
(
KNN
)
,
Bayesian
classifier
(
BN
)
,
random
forest
(
RF
)
,
support
vector
machine
(
SVM
)
and
the
BP
neural
network
(
BP
)
.
</s>
<s>
Decision
fusion
is
achieved
by
the
CW
and
improved
voting
methods
.
</s>
</p>
<p>
<s>
The
specific
framework
of
our
proposed
fusion
system
is
shown
in
</s>
<figure>
<s>
Figure
1
</s>
</figure>
<s>
.
</s>
</p>
<head>
<s>
3.1
.
</s>
<s>
Selection
of
Base
Classifiers
</s>
</head>
<p>
<s>
Considering
the
requirement
of
diversity
for
the
ensemble
classification
system
,
we
selected
six
representative
classifiers
from
the
supervised
category
.
</s>
<s>
Among
them
,
LDA
is
a
linear
classifier
,
which
is
suitable
for
the
classification
of
linear
separable
problems
.
</s>
<s>
KNN
is
a
simple
classifier
,
which
determines
the
classification
result
by
comparing
the
distance
between
the
sample
to
be
classified
and
all
training
samples
.
</s>
<s>
BN
is
one
of
the
commonly
used
classifiers
.
</s>
<s>
It
has
an
advantage
in
terms
of
classification
speed
and
is
suitable
for
applications
with
small-scale
samples
and
missing
values
.
</s>
<s>
RF
is
an
integrated
classifier
based
on
the
decision
tree
and
has
the
advantage
of
a
low
computation
cost
.
</s>
<s>
It
can
deal
with
high-dimensional
samples
and
sample
imbalance
.
</s>
<s>
The
typical
advantage
of
SVM
is
that
it
can
use
small
samples
and
is
also
good
for
nonlinear
problems
.
</s>
<s>
The
BP
neural
network
is
a
multi-layer
feed-forward
neural
network
with
error
backpropagation
,
which
is
advantageous
for
dealing
with
nonlinear
problems
.
</s>
</p>
<head>
<s>
3.2
.
</s>
<s>
Classifier
Performance
Evaluation
</s>
</head>
<p>
<s>
To
measure
the
classification
performance
,
four
evaluation
indicators
were
used
:
(
21
)
(
22
)
(
23
)
where
<hi rend="italic">
t
</hi>
is
the
number
of
fault
classes
.
(
24
)
where
<hi rend="italic">
P
</hi>
is
the
precision
and
<hi rend="italic">
R
</hi>
is
the
recall
rate
.
</s>
</p>
<p>
<s>
The
information
used
to
calculate
these
performance
evaluation
indicators
is
given
by
the
confusion
matrix
.
</s>
<s>
The
confusion
matrix
is
also
a
way
to
measure
the
classifier
’s
performance
,
and
its
form
is
as
follows
:
(
25
)
where
represents
the
percentage
of
cases
where
fault
<hi rend="italic">
i
</hi>
classified
as
fault
<hi rend="italic">
j
</hi>
by
classifier
<hi rend="italic">
k
</hi>
.
<hi rend="italic">
c
</hi>
is
the
number
of
base
classifiers
,
and
<hi rend="italic">
t
</hi>
is
the
number
of
fault
classes
.
</s>
</p>
<head>
<s>
3.3
.
</s>
<s>
Formatting
of
Mathematical
Components
</s>
</head>
<p>
<s>
Voting
is
a
simple
and
practical
decision-making
fusion
strategy
,
but
its
fusion
results
are
often
unreasonable
because
it
ignores
the
performance
differences
of
various
methods
.
</s>
<s>
In
the
process
of
fusion
,
the
method
with
excellent
performance
should
have
greater/more
influence
on
the
voting
results
.
</s>
<s>
This
paper
proposes
a
validity
concept
based
on
the
confusion
matrix
to
improve
the
voting
method
.
</s>
</p>
<p>
<s>
The
concept
of
the
validity
value
is
defined
as
follows
:
(
26
)
where
represents
the
validity
of
classifier
<hi rend="italic">
k
</hi>
for
fault
<hi rend="italic">
j
</hi>
.
</s>
<s>
The
larger
the
value
of
,
the
higher
the
credibility
of
the
result
when
a
test
sample
is
classified
as
fault
<hi rend="italic">
j
</hi>
by
classifier
<hi rend="italic">
k
</hi>
.
</s>
<s>
Additionally
,
the
following
conditions
should
be
met
:
(
27
)
</s>
</p>
<p>
<s>
In
the
improved
voting
method
,
the
voting
result
is
determined
by
the
validity
value
of
the
base
classifier
for
different
faults
.
</s>
<s>
Different
from
the
conventional
voting
method
,
the
fusion
results
given
by
the
improved
voting
method
are
no
longer
crisp
(
i.
e.
,
either
0
or
1
)
,
but
they
take
values
in
the
interval
of
0
–
1
.
</s>
<s>
This
not
only
avoids
the
shortcomings
of
the
original
voting
method
,
which
does
not
consider
the
difference
in
performance
of
each
classifier
on
different
faults
,
but
also
enhances
the
impact
of
the
classifiers
with
good
performance
in
terms
of
voting
results
.
</s>
</p>
<p>
<s>
A
validity
matrix
can
be
obtained
by
using
the
improved
voting
method
:
(
28
)
</s>
</p>
<p>
<s>
Combined
with
the
combined
weights
,
the
decision
of
the
fusion
system
is
:
(
29
)
</s>
</p>
<p>
<s>
Finally
,
the
maximum
value
is
used
as
the
fusion
decision
:
(
30
)
</s>
</p>
<p>
<s>
In
summary
,
the
proposed
decision
fusion
system
uses
multiple
performance
evaluation
indicators
to
measure
the
performance
of
the
base
classifier
and
determine
the
combined
weights
based
on
AHP
and
EW-TOPSIS
for
the
base
classifiers
under
these
indicators
.
</s>
<s>
In
addition
,
an
improved
voting
method
based
on
validity
was
developed
to
improve
the
effectiveness
of
decision
fusion
.
</s>
<s>
In
the
next
section
,
the
proposed
method
is
verified
by
the
TE
process
.
</s>
</p>
</text>
