<?xml version="1.0" ?><text author="Arfan  Haider Wahla, Lan Chen, Yali Wang, Rong Chen" dateCollected="2019-11-03" id="autogum_academic_doc237" shortTile="automatic-wireless" sourceURL="https://www.mdpi.com/2078-2489/10/11/338/htm" speakerCount="0" speakerList="none" title="Automatic Wireless Signal Classification: A Neural-Induced Support Vector Machine-Based Approach" type="academic">
<head>
<s>
4
.
</s>
<s>
Simulation
Results
and
Discussion
</s>
</head>
<p>
<s>
In
this
section
,
the
performance
of
the
proposed
scheme
is
investigated
against
several
non-ideal
channel
conditions
such
as
frequency
offset
,
phase
offset
,
timing
offset
,
varying
symbol
number
as
well
as
different
Doppler
shifts
.
</s>
<s>
An
analytical
comparison
is
also
provided
with
previously
proposed
different
learning-based
modulation
classifiers
.
</s>
</p>
<head>
<s>
4.1
.
</s>
<s>
Dataset
</s>
</head>
<p>
<s>
In
this
paper
,
the
following
modulation
schemes
were
considered
.
</s>
<s>
The
transmitted
bit-stream
was
generated
randomly
to
ensure
each
has
equal
probability
.
</s>
<s>
The
received
signal
was
pre-processed
at
the
receiver
end
to
obtain
a
complex
based-band
signal
.
</s>
<s>
The
In-Phase-Quadrature
(
I-Q
)
samples
of
the
received
signal
were
sampled
simultaneously
to
a
frame
of
length
N.
They
are
combined
later
in
a
2
X
N
matrix
and
input
to
CNN
for
feature
extraction
.
</s>
<s>
In
this
study
,
the
value
of
<hi rend="italic">
N
</hi>
is
2048
but
some
experiments
are
performed
with
<hi rend="italic">
N
</hi>
=
1024
,
512
.
</s>
<s>
This
is
done
by
varying
the
number
of
symbols
in
the
frame
.
</s>
<s>
A
segment
has
the
same
results
as
that
of
a
full
observation
.
</s>
<s>
In
this
way
,
the
proposed
classifier
would
be
independent
of
N.
The
range
of
AWGN
noise
with
signal-to-noise
ratio
(
SNR
)
=
âˆ’8
dB
to
+8
dB
in
the
dataset
.
</s>
<s>
For
each
modulation
scheme
and
SNR
values
,
10,000
realizations
of
the
received
signal
are
generated
according
to
the
model
described
in
Section
2
.
</s>
<s>
The
rest
of
the
parameters
are
summarized
in
Table
1
in
detail
.
</s>
</p>
<head>
<s>
4.2
.
</s>
<s>
Training
and
Validation
Performance
</s>
</head>
<p>
<s>
In
the
following
experiment
,
the
proposed
classifier
NSVM
,
which
is
a
hybrid
combination
of
CNN
and
SVM
is
trained
and
validated
by
using
offline
deployment
.
</s>
<s>
In
order
to
analyze
the
performance
,
the
accuracy
and
loss
curves
of
training
and
cross-validation
are
plotted
in
<figure>
Figure
5
</figure>
.
</s>
<s>
Similar
experiments
are
performed
with
a
traditional
CNN
model
with
the
softmax
classifier
in
its
last
layer
,
and
the
results
are
plotted
in
<figure>
Figure
6
</figure>
.
</s>
<s>
The
CNN
settings
used
here
are
the
same
in
both
cases
,
as
shown
in
<figure>
Figure
4
</figure>
,
expect
one
is
trained
with
Gaussian
SVM
and
the
other
one
is
trained
with
softmax
as
the
decision
classifier
.
</s>
<s>
The
performance
of
NSVM
is
better
than
the
traditional
CNN
model
as
the
accuracy
and
loss
converges
to
their
minima
and
maxima
quickly
.
</s>
<s>
Along
with
that
,
these
results
show
that
it
smoothens
as
training
progresses
,
which
shows
the
effectiveness
of
the
proposed
scheme
.
</s>
<s>
This
also
reduces
the
cost
of
training
for
the
proposed
model
.
</s>
<s>
However
,
in
the
case
of
CNN
with
softmax
,
the
classifier
performance
is
hardly
stable
and
satisfactory
,
as
can
be
seen
in
<figure>
Figure
6
</figure>
.
</s>
</p>
</text>