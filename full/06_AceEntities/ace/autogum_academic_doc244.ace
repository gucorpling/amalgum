Although typical neural networks , including deep networks , are defined as deterministic input-output mapping functions of input , output , and parameter , the observed data for training the networks always have inevitable noises and thus the input-output relation can be described in the stochastic manner .
1,2 person
2,5 abstract|13,24 abstract|18,19 abstract|18,24 abstract|20,21 abstract|25,28 abstract|25,32 abstract|30,32 abstract|34,36 abstract|38,41 abstract|45,48 abstract

On the other hand , in case that the output is a binary vector , the corresponding probability distribution can be defined by using a logistic model , such as ( 5 ) where and are the i -th component of L -dimensional vector and , respectively .
1,2 person
9,11 abstract|16,20 abstract|25,28 abstract|37,45 abstract|37,48 abstract

Likewise , by defining a proper stochastic model , we can derive various types of neural network models , which can explain the given task more adequately and get a new insight to solve many unresolved problems in the field of neural network learning .
1,2 person
5,9 abstract|10,11 person|13,19 abstract|16,19 abstract|23,26 abstract|30,33 abstract|35,38 abstract|42,45 abstract

When we assume that the random vector is subject to the standard Gaussian distribution , its probability density is defined as the normal distribution function , and its loss function becomes the well-known squared error function , which can be written as ( 4 )
1,2 person
2,3 person|11,15 abstract|16,17 abstract|16,17 object|16,19 abstract|22,26 abstract|28,29 abstract|28,31 abstract|32,37 abstract|43,45 abstract|44,45 abstract

To overcome this practical inefficiency , online stochastic gradient decent learning is proposed , in which parameters are updated for each data sample by using gradient of loss function defined with a single data pair , such as ( 9 )
1,2 person
3,6 abstract|7,12 abstract|21,24 abstract|26,30 abstract|28,30 abstract|32,36 abstract

Noting that this is the well-known cross-entropy error , we can say that the logistic regression model is equivalent to the typical neural networks with cross-entropy error , which is widely used for classification task .
1,2 person
5,9 abstract|10,11 person|14,18 abstract|21,25 abstract|21,28 abstract|26,28 abstract|34,35 abstract|34,36 abstract

Since the natural gradient learning method is also based on the gradient descent method , we describe the basic formula of gradient descent learning and its online version that is called stochastic gradient descent method .
1,2 person
2,7 abstract|4,7 abstract|11,15 abstract|16,17 person|18,25 abstract|22,25 abstract|26,27 abstract|26,29 abstract|32,35 abstract|32,36 abstract

The error of neural network for the whole data set can then be defined by using a loss function such as ( 7 ) and the goal of learning is to get the optimal minimizing .
1,2 person
1,6 abstract|1,11 abstract|4,6 object|7,11 abstract|17,20 abstract|17,22 abstract|17,24 abstract|26,30 abstract|33,36 abstract

In other words , the observed output can be regarded as a random vector that is dependent on the deterministic function and some additional stochastic process that is described by the conditional probability .
1,2 person
5,8 abstract|19,22 abstract|23,27 abstract|31,34 abstract

To achieve the goal , the weight parameter is updated starting from the current position , according to the opposite direction of the gradient of , which can be written as ( 8 )
1,2 person
3,5 abstract|6,9 abstract|13,16 abstract|13,16 place|19,26 abstract|23,26 abstract|33,34 abstract

In this paper , we present explicit algorithms of adaptive natural gradient learning method for two representative stochastic neural network models : The additive Gaussian noise model and the logistic regression model .
1,2 person
2,4 abstract|2,4 object|5,6 person|7,14 abstract|7,15 abstract|10,14 abstract|16,22 abstract|23,28 abstract|29,33 abstract

Therefore , the stochastic neural network model with additive Gaussian noise is equivalent to the typical neural network model trained with squared error function , which is widely used for regression task .
1,2 person
3,12 abstract|9,12 abstract|15,20 abstract|22,25 abstract|31,33 abstract

For example , consider that the output is observed with additive noise to the deterministic neural networks function such as ( 3 ) where is a random noise vector .
1,2 person
1,3 abstract|6,8 abstract|11,13 abstract|14,18 abstract|14,21 abstract|14,23 abstract

The corresponding loss function of the logistic model is obtained by taking negative log likelihood of Equation ( 5 ) , which can be written as , ( 6 )
1,2 person
1,9 abstract|6,9 abstract|13,20 abstract|17,18 abstract

When a set of training data is given , a neural network is trained in order to find an input-output mapping that is specified with weight parameter vector .
1,2 person
2,7 abstract|10,13 abstract|19,22 abstract|26,29 object

In addition , when the number of data is large , it needs large amounts of computation for just a single update , making the learning process slow .
1,2 person
5,9 quantity|12,13 abstract|14,18 abstract|20,23 event|25,27 abstract|25,28 abstract

Then the goal of learning is to find an optimal value of parameter that minimizes the loss function defined as negative log likelihood of given input-output sample .
1,2 person
2,6 abstract|2,6 event|9,14 abstract|16,19 abstract|21,28 abstract|25,28 abstract

Since the natural gradient is derived from stochastic neural network models , let us start from the brief description of the two popular stochastic models .
1,2 person
2,5 abstract|8,11 abstract|8,12 abstract|14,15 person|17,26 abstract|21,26 abstract

Once a specific model of stochastic neural networks and its corresponding loss function are determined , the weight parameters are optimized by gradient descent method .
1,2 person
2,9 abstract|6,9 abstract|10,11 abstract|10,14 abstract|17,20 abstract|23,26 abstract

Theoretically , the batch mode learning gives the steepest descent direction of at the current position of , but it has two practical drawbacks .
1,2 person
1,2 abstract|3,7 abstract|8,18 abstract|20,21 abstract|22,25 abstract

There have been numerous variations of the standard gradient descent method , including second-order methods , momentum method , and adaptive learning rate methods .
1,2 person
4,12 abstract|7,12 abstract|14,16 abstract|17,18 abstract|17,19 abstract|21,24 abstract|21,25 abstract

Note that the last term in equation ( 1 ) is independent of parameter and can be ignored in the objective function for optimization .
1,2 person
3,10 abstract|7,10 abstract|14,15 abstract|20,23 abstract|20,25 abstract

Based on the general definition , the conventional neural networks can be regarded as a special case with a specific conditional probability distribution , .
1,2 person
3,6 abstract|7,11 abstract|19,24 abstract

Since the typical problems with binary target output vector is pattern classification , the logistic model is appropriate for L -class classification tasks .
1,2 person
2,10 abstract|6,10 object|11,12 abstract|11,13 abstract|14,17 abstract|20,23 abstract|20,24 abstract

Natural gradient is also derived from a new metric for the space of probability density function of stochastic neural networks .
1,2 person
1,3 abstract|7,21 abstract|14,21 abstract|18,21 abstract

The loss function can then be written as ( 1 ) and the optimal parameter is described as ( 2 )
1,2 person
1,4 abstract|13,16 abstract

This update rule is called batch learning mode , meaning that an update is done for the whole batch set .
1,2 person
1,4 abstract|6,9 abstract|12,14 abstract|17,21 abstract

First , it is too stable to be easily trapped in undesirable local minima .
1,2 person
3,4 abstract|12,15 abstract

The well-known error-backpropagation algorithm is the standard type of gradient descent learning method .
1,2 person
1,5 abstract|6,14 abstract|6,14 object|10,14 abstract

Gradient Descent Learning of Stochastic Neural Networks
1,2 person
1,8 abstract|5,8 abstract

Stochastic Neural Networks
1,2 person
1,4 abstract

Gradient Descent Learning
1,2 person
1,4 abstract

2.1 .
1,2 person
1,2 abstract

2.2 .
1,2 person
1,2 abstract
