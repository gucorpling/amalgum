Equation ( 1 ) can be expressed in a matrix operation form with a Laplacian matrix , and can be represented as Equation ( 2 ) . ( 2 ) where , the Laplacian matrix is defined as Equation ( 3 ) with a vertex degree matrix whose diagonal entries are obtained as and the remaining entries are 0 . ( 3 )
1,2 person
1,2 abstract|9,13 abstract|14,17 object|23,24 abstract|23,26 abstract|33,36 object|34,35 person|39,40 abstract|41,42 abstract|55,58 abstract

feature classes ( e. g. , land-use category and record ) with a greater degree of object sharing have close coordinates in their embedding space and feature classes with a lesser degree of object sharing have distant coordinates .
1,2 person
1,3 abstract|1,11 abstract|7,9 abstract|7,11 abstract|10,11 abstract|13,19 abstract|17,19 abstract|20,22 abstract|23,24 object|23,26 abstract|27,29 abstract|27,36 abstract|30,36 abstract|34,36 abstract|37,39 abstract

Equation ( 4 ) with the constraint can be solved by the Lagrange multiplier method as in Equations ( 5 ) – ( 7 ) . ( 5 ) ( 6 ) ( 7 )
1,2 person
1,8 abstract|6,8 abstract|12,15 abstract|12,16 abstract|13,14 person|18,19 abstract|18,26 abstract|28,29 abstract|34,35 abstract

Based on these mathematical properties , we determined the embedded coordinates as Equation ( 10 ) , because the increase in distance is proportional to that of the root-squared coordinate difference . ( 10 )
1,2 person
3,6 abstract|7,8 person|9,12 abstract|9,12 object|13,14 abstract|13,16 abstract|19,23 abstract|22,23 abstract|26,32 abstract|28,32 abstract|34,35 abstract

Sameh and Wisniewski also proved that the minimum value of in Equation ( 8 ) equals the sum of the corresponding eigenvalues , as shown by Equation ( 9 ) . ( 9 )
1,2 person
1,2 person|1,4 abstract|1,4 person|7,15 abstract|12,13 abstract|17,23 abstract|20,23 abstract|20,23 object|27,28 abstract|33,34 abstract

One-dimensional graph embedding finds a configuration of embedded vertices in one-dimensional space , such that the vertices ’ proximities from the edge weights are preserved as the embedded vertices ’ distances .
1,2 person
1,4 abstract|1,4 object|5,10 object|5,13 abstract|8,10 object|16,20 abstract|21,24 abstract|31,32 abstract

In the field of graph spectral theory , the eigenvector corresponding to the smallest eigenvalue larger than 0 is the proven solution , which is called a Fiedler vector .
1,2 person
2,8 abstract|5,8 abstract|13,19 abstract|20,23 abstract|27,30 abstract

Accordingly , we can assume the eigenvalue as the amount of either the penalty or the cost caused by the i th dimensional space in the embedding problem .
1,2 person
3,4 person|6,8 abstract|13,15 abstract|16,18 abstract|20,25 abstract|26,29 abstract

Sameh and Wisniewski proved that the solution to this trace minimization problem is obtained by the k-eigenvectors of that correspond to its smallest eigenvalues other than 0 .
1,2 person
1,2 person|1,4 abstract|1,4 person|3,4 person|6,13 abstract|9,13 abstract|10,13 abstract|22,23 abstract|22,28 abstract

This function could be minimized when vertices i and j with large are embedded at close coordinates , whereas vertices with small are embedded into distant coordinates .
1,2 person
1,3 abstract|7,13 object|12,13 quantity|20,23 object

Assuming each entry of a column vector as coordinates of the embedded vertices , this problem can be solved through minimization of the following objective function .
1,2 person
5,8 object|9,14 object|11,14 object|15,17 abstract|23,27 abstract

Now , the problem can be changed to find a vector that minimizes , and can be represented as Equation ( 4 ) . ( 4 )
1,2 person
3,5 abstract|10,12 object|20,21 abstract|26,27 abstract

Thus , the solution of Equation ( 8 ) is obtained by a matrix , where represents an eigenvector corresponding to eigenvalue under the condition .
1,2 person
3,9 abstract|6,7 abstract|6,9 abstract|13,15 object|24,26 abstract

Thus , the coordinates of vertices in one-dimensional embedding are obtained as components of the Fiedler vector as represented by Equation ( 7 ) .
1,2 person
3,10 abstract|6,10 object|15,18 object|21,22 abstract|23,24 abstract

Since the value of is vulnerable to the scaling of a vector , a constraint is imposed to remove any such arbitrary scaling effect .
1,2 person
2,5 abstract|2,13 abstract|8,13 abstract|8,13 event|11,13 object|14,16 abstract|20,25 abstract|21,25 abstract

These embedded coordinates are represented as an matrix , so that the i th row of , , contains the k-dimensional coordinates of vertex .
1,2 person
1,4 abstract|7,9 object|20,25 abstract|20,25 object|24,25 object

So , when , it is appropriate to apply more weight to than in measuring the proximity for a clustering analysis .
1,2 person
5,6 abstract|8,12 abstract|10,12 abstract|16,18 abstract|19,22 abstract

The diagonal matrix provides weights on the vertices , so that the higher is , the more important is that vertex .
1,2 person
1,4 object|7,9 object|20,22 place

Now , an objective function is defined as Equation ( 8 ) with the constraint , . ( 8 )
1,2 person
3,6 abstract|9,10 abstract|14,16 abstract|19,20 abstract

However , according to the rank of matrix , there could be more than one eigenvector .
1,2 person
5,9 abstract|13,17 abstract

Thus , the solution of one-dimensional embedding , , is obtained by solving the eigenproblem .
1,2 person
3,8 abstract|6,8 abstract|14,16 abstract

Thus , it is necessary to rescale them according to each dimension ’s relative importance .
1,2 person
3,4 event|8,9 object|11,16 abstract

However , the constraint normalizes the scales of the coordinates in each dimension .
1,2 person
3,5 abstract|6,11 abstract|9,11 abstract|12,14 abstract

Given a weighted graph , edge weights are represented as a weight matrix .
1,2 person
2,5 object|6,8 abstract|11,14 object

In this paper , we assume an undirected and connected graph .
1,2 person
2,4 abstract|2,4 object|5,6 person|7,12 place

In this study , this mathematical property is applied as follows :
1,2 person
2,4 abstract|5,8 abstract

The graph is represented by sets of vertices and edges .
1,2 person
1,3 object|1,3 place|6,11 abstract|6,11 object

Now , consider k-dimensional graph embedding .
1,2 person
4,7 abstract

2.2 . k-dimensional Embedding
1,2 person


Laplacian Graph Embedding
1,2 person
1,3 abstract

( 1 )
1,2 person
2,3 abstract

One-dimensional Embedding
1,2 person


2.1 .
1,2 person
1,2 abstract
