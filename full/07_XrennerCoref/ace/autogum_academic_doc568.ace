The fixed points methods as the second category , including the fixed-budget ( FB ) , the sliding window ( SW ) , and the kernel approximation methods ( e. g. , the Nystrm method and random Fourier features ( RFFs ) method ) , are used to overcome the sublinearly growing problem .
1,2 person
1,5 abstract|2,5 abstract|6,9 abstract|14,15 abstract|17,20 abstract|21,22 abstract|25,29 abstract|33,36 abstract|37,40 abstract|41,44 abstract|50,54 abstract

However , the higher-order error measure is not suitable for the mixture of Gaussian and super-Gaussian noises ( Laplace , -stable , etc. ) with poor stability and astringency , and the lower-order measure of error is usually more desirable in these noise environments with slow convergence rate .
1,2 person
3,7 abstract|11,18 abstract|14,15 person|14,18 abstract|19,20 person|26,28 abstract|32,37 abstract|36,37 abstract|46,49 abstract

Using the simplest stochastic gradient descent ( SGD ) method for learning , KAFs including the kernel least mean square ( KLMS ) algorithm , kernel affine projection algorithm ( KAPA ) , and kernel recursive least squares ( KRLS ) algorithm have been proposed .
1,2 person
2,7 abstract|2,13 abstract|4,7 abstract|8,9 abstract|14,15 abstract|16,21 abstract|16,30 abstract|26,30 abstract|26,30 object|35,39 abstract|40,41 abstract

However , allocating a new kernel unit as a radial basis function ( RBF ) center with the coming of new data , the linearly growing structure ( called “ dictionary ” hereafter ) will increase the computational and memory requirements in KAFs .
1,2 person
4,8 object|9,13 abstract|18,23 abstract|21,23 abstract|24,28 abstract|30,33 abstract|37,44 abstract|43,44 abstract

The examples for developing KAFs with RFM are the random Fourier features kernel least mean square ( RFFKLMS ) algorithm , random Fourier features maximum correntropy ( RFFMC ) algorithm , and random Fourier features conjugate gradient ( RFFCG ) algorithm .
1,2 person
5,6 abstract|7,8 abstract|9,17 abstract|10,17 abstract|18,21 abstract|22,27 abstract|28,31 abstract|33,38 abstract|39,42 abstract

On the basis of RFFs , random Fourier mapping ( RFM ) is proposed by mapping input data into a finite-dimensional random Fourier features space ( RFFS ) using a randomized feature kernel ’s Fourier transform in a fixed network structure .
1,2 person
2,6 abstract|5,6 abstract|7,10 abstract|11,12 abstract|17,19 abstract|20,26 abstract|30,37 abstract|30,42 abstract|32,37 abstract|38,42 abstract

For the loss function , due to their simplicity , smoothness , and mathematical tractability , the second-order statistical measures ( e. g. , minimum mean square error ( MMSE ) and least squares ) are widely utilized in KAFs .
1,2 person
2,5 abstract|8,9 abstract|8,10 abstract|8,12 abstract|8,16 abstract|11,12 abstract|14,16 abstract|17,21 abstract|25,29 abstract|33,35 abstract|40,41 abstract

However , KAFs based on the second-order statistical measures are sensitive to non-Gaussian noises including the sub-Gaussian and super-Gaussian noises , which means that their performance may be seriously degraded if the training data are contaminated by outliers .
1,2 person
3,4 abstract|6,10 abstract|13,15 abstract|16,21 abstract|25,26 object|25,27 abstract|32,35 abstract

However , the CG method gives a trade-off between convergence rate and computational complexity without the inverse computation , and has been successfully applied in various fields , including compressed sensing , neural networks , and large-scale optimization .
1,2 person
3,6 abstract|4,5 abstract|7,12 abstract|7,15 abstract|10,12 abstract|13,15 abstract|16,19 abstract|26,28 abstract|30,32 abstract|33,35 abstract|37,39 abstract

The first category accepts only informative data as new dictionary centers by using a threshold , including the surprise criterion ( SC ) , the coherence criterion ( CC ) , and the vector quantization ( VQ ) .
1,2 person
1,4 abstract|5,8 abstract|14,16 abstract|14,16 object|18,21 abstract|22,23 abstract|25,28 abstract|29,30 abstract|33,36 abstract

Recently , the information theoretic learning ( ITL ) similarity measures , such as the maximum correntropy criterion ( MCC ) and minimum error entropy criterion ( MEE ) , have been introduced to implement robust KAFs .
1,2 person
3,7 abstract|4,7 abstract|8,9 abstract|10,11 abstract|15,19 abstract|20,21 object|23,26 abstract|23,27 abstract|28,29 object|36,38 abstract

Using the Cauchy loss to penalize the noise term , some algorithms based on the minimum Cauchy loss ( MCL ) criterion are efficient for combating non-Gaussian noises , especially for heavy-tailed - stable noises .
1,2 person
2,5 abstract|3,5 abstract|7,10 abstract|8,9 abstract|11,13 abstract|15,19 abstract|15,23 abstract|20,21 object|20,23 abstract|27,29 abstract|32,36 abstract

In addition , minimizing the logarithmic moments of the error , the logarithmic error measure — including the Cauchy loss ( CL ) with low computational complexity — is an appropriate measure of optimality .
1,2 person
5,11 abstract|9,11 abstract|12,16 abstract|18,21 abstract|19,21 abstract|22,23 abstract|25,28 abstract|30,35 abstract

The kernel method using a Mercer kernel has attracted interests in tackling these complex nonlinear applications , which transforms nonlinear applications into linear ones in the reproducing kernel Hilbert space ( RKHS ) .
1,2 person
1,4 abstract|2,3 object|5,8 abstract|5,8 object|6,7 person|10,11 abstract|13,17 abstract|20,22 abstract|23,31 abstract|26,31 abstract|32,33 object

In comparison with the SGD method and recursive method , the conjugate gradient ( CG ) method and Newton ’s method as developments of SGD have become alternative optimization methods in KAFs .
1,2 person
4,7 abstract|5,6 abstract|5,7 abstract|8,10 abstract|11,18 abstract|11,22 abstract|19,21 person|19,22 abstract|23,26 abstract|25,26 abstract|28,33 abstract|32,33 abstract

To handle this issue , robust statistical measures have therefore gained more attention , among which the lower-order error measure and the higher-lower error measure are two typical examples .
1,2 person
3,5 abstract|6,9 abstract|12,14 abstract|17,21 abstract|22,26 abstract|27,30 abstract

From the aspect of the optimization method , the stochastic gradient descent ( SGD)-based algorithms cannot find the minimum using the negative gradient in some loss functions .
1,2 person
2,8 abstract|5,8 abstract|9,13 abstract|14,15 abstract|14,16 abstract|18,20 abstract|21,24 abstract|25,28 abstract

Many applications in the real world , such as system identification , regression , and online kernel learning ( OKL ) , require complex nonlinear models .
1,2 person
1,7 abstract|10,11 abstract|10,14 abstract|13,14 abstract|16,19 abstract|24,27 abstract

The ITL similarity measures have been shown to have a strong robustness against non-Gaussian noises at the expense of increasing computational burden in training processing .
1,2 person
1,5 abstract|2,5 abstract|10,13 abstract|14,16 abstract|20,26 abstract|24,26 abstract

However , the FB method and the SW method cannot guarantee a good performance in specific environments with a small amount of time .
1,2 person
3,6 abstract|4,5 abstract|7,10 abstract|12,15 abstract|16,18 abstract|19,24 abstract

KCG with low computational and space requirements can produce a better solution than KLMS , and has comparable accuracy to KRLS .
1,2 person
3,8 abstract|10,13 abstract|10,15 abstract|18,22 abstract

Compared with the Nystrm method , RFFs are drawn from a distribution that is randomly independent from the training data .
1,2 person
3,6 abstract|11,13 abstract|18,21 abstract

The inverse of matrix of Newton ’s method increases the computation and causes the divergence of algorithms in some cases .
1,2 person
1,9 abstract|4,9 abstract|6,8 person|6,9 abstract|10,12 abstract|14,18 abstract|14,21 abstract|17,18 abstract|19,21 abstract

Developed in RKHS , a kernel adaptive filter ( KAF ) is the most celebrated subfield of OKL algorithms .
1,2 person
5,9 abstract|18,20 abstract

The RFM alleviates the computational and storage burdens of KAFs , and ensures a satisfactory performance under non-stationary conditions .
1,2 person
1,3 abstract|4,11 abstract|10,11 abstract|14,17 abstract|18,20 abstract

However , these methods cannot fully address the growing problem and still introduce additional time consumption at each iteration .
1,2 person
3,5 abstract|8,11 abstract|14,17 abstract|18,20 abstract

Due to a data-independent vector representation , RFFs can provide a good solution to non-stationary circumstances .
1,2 person
3,7 abstract|8,9 object|11,17 abstract|15,17 abstract

Toward this end , recursive-based algorithms address these issues at the cost of increasing computational cost .
1,2 person
2,4 abstract|5,7 abstract|8,10 abstract|15,17 abstract

In addition , the kernel conjugate gradient ( KCG ) method is proposed for adaptive filtering .
1,2 person
4,12 abstract|9,12 abstract|15,17 abstract

To curb the growth of the dictionary , two categories are chosen for sparsification .
1,2 person
6,8 object|9,11 abstract|14,15 abstract

1 .
1,2 person


Introduction
1,2 person
1,2 abstract
