In autonomous systems in general , scholars have recommended ways to detect and offset the effects of bias , such as modifying algorithmic outputs to balance the effects of bias between protected and unprotected groups , introducing minimally intrusive modification to remove bias from the data , incorporating individuals from potentially discriminated groups , testing techniques to measure discrimination and identify groups of users significantly affected by bias in software and creating algorithms that certify the absence of data bias .
1,2 person
2,4 abstract|7,8 person|15,19 abstract|18,19 abstract|23,25 abstract|27,41 abstract|30,31 abstract|32,36 person|43,44 abstract|45,47 abstract|45,54 abstract|49,54 person|51,54 person|59,60 abstract|62,65 person|64,65 person|68,69 abstract|70,71 abstract|73,74 abstract|76,81 abstract|79,80 object|79,81 abstract

Individual-specific characteristics , such as a person ’s age and gender that are used as decision-making criteria can be penalised or privileged by the AVs ’ algorithms to meet the algorithm ’s pre-defined preferences , such as prioritising the safety of children or minimising the total quantity of harm , causing more safety risks to be allocated to individuals that share the penalised characteristics .
1,2 person
1,3 abstract|1,12 abstract|6,9 person|6,10 abstract|6,12 abstract|11,12 abstract|16,18 abstract|24,27 abstract|24,28 abstract|30,35 abstract|39,43 abstract|42,43 person|45,50 abstract|52,55 abstract|53,54 abstract|53,55 abstract|59,60 person|62,65 abstract

Humans are also excessively trusting and insufficiently critical of algorithmic decisions due to the popular perception of algorithms as objective and fair , a problem referred to as “ automation bias ” and the seemingly “ objective ” correlations that the algorithm learns from the data makes it difficult to legally establish discriminatory intent in algorithms .
1,2 person
1,2 person|10,12 abstract|18,19 abstract|24,26 abstract|29,32 abstract|30,31 abstract|30,32 abstract|34,40 abstract|41,43 abstract|45,47 abstract|48,49 event|53,55 abstract|56,57 abstract

Furthermore , scholars recommend increasing transparency to identify biases , such as designing algorithms whose original input variables can be traced throughout the system ( i. e. , traceability ) and auditing algorithms to enhance their interpretability so that biases can be detected and the system ’s outputs can be verified against safety requirements .
1,2 person
3,4 person|5,7 abstract|6,7 abstract|9,10 abstract|14,15 abstract|16,19 abstract|23,25 abstract|29,30 abstract|36,37 abstract|40,41 abstract|45,49 abstract|53,54 abstract|53,55 abstract

In American anti-discrimination law , discrimination exists when there is disparate treatment , which is the “ discriminatory intent or the formal application of different rules to people of different groups ” , and/or disparate impact , which is the result that “ differ for different groups ” .
1,2 person
2,5 abstract|6,7 abstract|11,13 abstract|16,20 abstract|18,20 abstract|21,37 abstract|25,27 abstract|25,37 abstract|28,32 person|28,37 abstract|35,37 abstract|40,42 abstract|43,49 abstract

These forms of bias can be introduced unintentionally or intentionally by algorithm designers and AV manufacturers to maximise profits , such as prioritising the safety of AV passengers to maximise profits , and this is exacerbated by the lack of legal frameworks to hold these stakeholders accountable .
1,2 person
1,5 abstract|12,13 abstract|12,14 person|15,17 organization|19,20 abstract|24,29 abstract|24,32 abstract|27,29 person|31,32 abstract|41,43 abstract|45,47 person

Section 5.2 explores various types of ethical preferences to which AVs may be programmed to follow and their implications of AV safety risks in greater detail , and Section 5.3 explores how perverse incentives influence the choice of preferences that are programmed into AVs ’ algorithms .
1,2 person
1,3 abstract|4,9 abstract|7,9 abstract|18,19 abstract|21,24 abstract|25,27 abstract|29,31 abstract|33,35 abstract|36,40 abstract|39,40 abstract|44,47 abstract

An emerging issue is the aggregation of individually biased outcomes when AVs with similar preferences are deployed on a large-scale , as doing so would centralise and replicate algorithmic preferences along with their individually biased risk allocation decisions .
1,2 person
1,4 abstract|5,11 abstract|8,11 abstract|12,16 abstract|14,16 abstract|29,31 abstract|33,38 abstract|33,39 abstract

Bias can be introduced into AVs during the human designers ’ construction of the datasets , models , and the parameters of the algorithm , which potentially leads to unfair or discriminatory allocations of safety risks .
1,2 person
1,2 abstract|6,7 abstract|9,12 person|14,16 object|23,25 abstract|30,37 abstract|35,37 abstract

This could lead to the same groups of people being consistently allocated more safety risks and perpetuate systemic discrimination , which is more difficult to detect as it results from the accumulation of similar driving outcomes .
1,2 person
1,2 abstract|5,10 person|9,10 person|13,16 abstract|14,16 abstract|18,20 abstract|28,29 abstract|31,37 abstract|34,37 abstract

Firstly , many algorithms are designed to be highly complex for greater accuracy , but this renders the algorithm opaque and difficult to interpret even by the designers themselves , concealing the sources of bias .
1,2 person
3,5 abstract|12,14 abstract|16,17 abstract|18,20 abstract|27,30 person|32,36 abstract|35,36 abstract

For instance , training an AV using data from only one country could result in the AV learning localised patterns and not accurately modelling driving behaviours that apply in other countries or contexts .
1,2 person
1,3 abstract|8,13 abstract|11,13 place|19,21 abstract|25,27 abstract|30,32 place|30,34 abstract|33,34 abstract

A system is considered biased when it contains “ intended ” or “ unintended ” characteristics that unfairly discriminate against certain individuals or groups of individuals in society .
1,2 person
1,3 abstract|7,8 abstract|9,17 abstract|13,17 abstract|21,23 person|24,29 person|28,29 abstract

This Section explores ethical issues associated with algorithmic decision-making in AVs , their implications for AV safety risks and discrimination and the steps taken to tackle these issues .
1,2 person
1,3 abstract|4,6 abstract|8,12 abstract|11,12 abstract|13,14 abstract|16,19 abstract|20,21 abstract|22,24 abstract|27,29 abstract

Secondly , as ML algorithms make decisions mainly based on the training data that changes over time , it is difficult to predict potentially discriminatory effects in advance .
1,2 person
4,6 abstract|7,8 abstract|11,14 abstract|22,27 abstract|24,27 abstract

Lastly , Section 5.3 examines how the incentives of AV stakeholders shape AV algorithms ’ design and resulting decisions that can introduce new safety risks and discrimination .
1,2 person
3,5 abstract|7,12 abstract|10,12 person|13,15 abstract|13,17 abstract|18,20 abstract|23,26 abstract|27,28 abstract

Apart from bias originating from the data and selection of variables and criterion , Danks and London recommend clarifying ethical standards such as fairness to evaluate bias .
1,2 person
3,4 abstract|6,8 abstract|9,14 abstract|11,14 abstract|15,16 person|15,18 person|17,18 person|19,25 abstract|24,25 abstract|27,28 abstract

Section 5.1 discusses the sources of bias in AVs ’ algorithms that can yield discrimination by disproportionately allocating more safety risks to some groups of individuals .
1,2 person
1,3 abstract|4,10 abstract|9,10 abstract|11,12 abstract|15,16 abstract|19,22 abstract|23,27 person|26,27 person

Next , Section 5.2 explores approaches to incorporate ethics into AV algorithms ’ decision-making and highlight their implications for AV safety and discrimination .
1,2 person
3,5 abstract|6,7 abstract|9,10 abstract|11,13 abstract|11,15 abstract|14,15 abstract|17,18 abstract|17,19 abstract|20,22 abstract|23,24 abstract

Thus , the under- or overrepresentation of certain groups in the data can lead to inaccurate classifications and biased outcomes .
1,2 person
3,10 abstract|8,10 person|11,13 abstract|16,18 abstract|16,21 abstract|19,21 abstract

Secondly , the algorithm can be biased relative to legal and moral standards if it utilises sensitive input variables .
1,2 person
3,5 abstract|10,14 abstract|15,16 abstract|17,20 abstract

Firstly , statistical bias exists when the input data are not statistically representative of the overall population .
1,2 person
3,5 abstract|7,10 abstract|15,18 abstract

However , there are challenges in identifying bias in algorithms and their discriminatory effects .
1,2 person
8,9 abstract|10,11 abstract|12,13 abstract|12,15 abstract

Lessening bias in algorithms is therefore crucial to mitigate discriminatory outcomes from AVs .
1,2 person
4,5 abstract|10,12 abstract|10,14 abstract|13,14 abstract

Ethical Concerns from Algorithmic Decision-Making in AVs
1,2 person
1,3 abstract|4,6 abstract|4,8 abstract|7,8 abstract

5.1 .
1,2 person
1,2 abstract

5 .
1,2 person


Bias
1,2 person
1,2 abstract
