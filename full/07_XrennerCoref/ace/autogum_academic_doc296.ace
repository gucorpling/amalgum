On the other hand , models assigning low scores to unigrams ( such as a 4- or 5-gram model with the Stupid Backoff and backoff factor set as suggested by Brants et al. , and in particular the model applying modified Kneser-Ney smoothing ) are better at handling OoV words ( see Table 11 ) , but as a result of probability multiplication , in many cases they score unseen multi-word segments higher than a sequence of unigrams into which the given segment should be divided , hence yielding lower Recall .
1,2 person
8,10 abstract|8,12 abstract|11,12 abstract|21,28 abstract|31,32 person|33,34 person|38,40 abstract|41,44 abstract|42,43 person|49,51 abstract|52,55 abstract|62,64 abstract|70,73 abstract|75,84 object|81,84 object|90,92 abstract

The default model ( regardless of what kind of character representations are used — conventional character embeddings or concatenated n-gram vectors ) learns from the training data that the first and the last character of a word ( corresponding to B , E and S tags ) are always adjacent either to the boundary of a space-delimited segment or to a punctuation mark .
1,2 person
1,4 abstract|10,12 abstract|15,22 abstract|19,22 object|25,28 abstract|29,38 abstract|36,38 abstract|41,42 object|41,47 abstract|45,46 abstract|53,59 object|56,59 object|61,64 object

For the F-score , it is better to set a low backoff factor ( e. g. , 0.09 ) for 1-grams only , than to set it to a fixed value for all backoff steps ( e. g. , 0.4 , as Brants et al. did ) .
1,2 person
2,4 abstract|3,4 abstract|5,6 event|10,14 abstract|18,19 quantity|21,22 quantity|27,28 abstract|29,32 abstract|29,32 quantity|33,36 abstract|40,41 quantity|43,44 person|45,46 person

A comparison with the results yielded by MiNgMatch shows that setting the limit of n-grams per input segment is more effective than Stupid Backoff as a method for improving precision of the segmentation process — it leads to a much smaller drop in Recall .
1,2 person
1,6 abstract|4,6 abstract|8,9 object|8,9 organization|12,19 abstract|12,19 quantity|15,19 abstract|15,19 quantity|17,19 abstract|23,35 abstract|26,35 abstract|30,35 abstract|32,35 abstract|36,37 abstract|39,45 abstract|44,45 abstract

As shown in Table 9 , if we only take into account the word boundaries not already indicated in the raw test set , the model makes more correct predictions in data where the whitespaces have all been removed .
1,2 person
4,6 object|8,9 person|13,16 abstract|20,24 abstract|25,27 abstract|32,33 abstract|34,36 abstract

Due to data sparsity , n-gram coverage in the test set ( the fraction of n-grams in the test data that can be found in the training set ) is low ( see Table 10 ) .
1,2 person
3,5 abstract|6,8 quantity|6,12 quantity|9,12 abstract|13,21 quantity|18,21 abstract|26,29 abstract

For models with an n-gram order of 3 or higher , the backoff factor has a bigger impact on the results than further increasing the order of n-grams included in the model .
1,2 person
4,11 abstract|4,11 quantity|5,11 quantity|12,15 abstract|16,19 abstract|20,22 abstract|28,29 quantity|31,33 abstract

However , it also erroneously segments some OoV single-word tokens whose surface forms happen to be interpretable as a sequence of concatenated in-vocabulary unigrams , resulting in lower Precision .
1,2 person
3,4 abstract|7,11 abstract|7,11 object|7,14 abstract|11,14 abstract|19,25 object|22,25 object|28,30 abstract

A backoff factor of 0.4 gives significant improvement in Precision with higher order n-gram models , but at the same time Recall drops drastically and overall performance deteriorates .
1,2 person
1,6 abstract|5,6 quantity|10,11 abstract|12,16 abstract|19,21 time|22,23 abstract|26,28 abstract

When no backoff factor is applied , results for both test sets are similar to those from the MiNgMatch Segmenter without the limit of n-grams per input segment .
1,2 person
3,5 abstract|10,13 abstract|16,21 abstract|18,21 object|19,20 object|19,21 object|22,29 abstract|25,29 quantity

It means that many multi-word tokens from the test set are known to n-gram models as separate unigrams , but not in the form of a single n-gram .
1,2 person
1,2 abstract|4,11 object|8,11 abstract|14,16 abstract|17,19 object|23,29 abstract|26,29 object

Setting the backoff factor to an appropriate value allows for significant improvement in Precision and F-score ( and in some cases also small improvements in Recall ) .
1,2 person
2,5 abstract|6,9 abstract|11,17 abstract|14,15 abstract|16,17 abstract|23,27 abstract|26,27 abstract

The Stupid Backoff model with a backoff factor for unigrams set to a moderate value ( such as 0.09 ) is able to segment such strings correctly .
1,2 person
1,5 abstract|6,16 abstract|13,16 abstract|13,16 quantity|19,20 quantity|25,27 abstract

Due to a much bigger character set size , hanzi characters are also more informative to word segmentation , hence better performance with models using shorter context .
1,2 person
3,9 abstract|3,9 quantity|10,12 abstract|17,19 abstract|21,28 abstract|26,28 abstract

This behavior is a consequence of differences between writing systems : words in Chinese are on average composed of less characters than in languages using alphabetic scripts .
1,2 person
1,3 abstract|9,11 abstract|12,15 abstract|14,15 abstract|20,22 abstract|24,28 abstract|26,28 abstract

With the exception of the US-ISP model on SYOS , all variants of the neural segmenter achieved the best performance with concatenated 9-gram vectors .
1,2 person
5,8 abstract|6,7 place|9,10 object|11,17 object|14,17 abstract|18,21 abstract|22,25 object

This contrasts with the results reported by Shao et al. for Chinese , where in most cases there was no further improvement beyond 3-grams .
1,2 person
1,2 abstract|4,6 abstract|8,9 person|10,11 person|16,18 abstract|21,23 abstract|24,25 substance

The variant without the limit of n-grams per input segment produces unbalanced results ( especially on SYOS ) , with relatively low Precision .
1,2 person
1,11 abstract|4,11 abstract|7,11 quantity|12,14 abstract|17,18 abstract|21,24 abstract

As a result , the model separates punctuation from alpha-numeric strings found in the input , but never applies further segmentation to them .
1,2 person
5,7 abstract|8,9 abstract|10,12 abstract|14,16 abstract|20,22 abstract|23,24 abstract

It yields lower Recall than the model with randomly generated multi-word tokens , but the F-score is higher due to better Precision .
1,2 person
1,2 abstract|3,5 abstract|6,13 abstract|9,13 object|15,17 abstract|21,23 abstract

Unlike with default settings , the model trained on data without whitespaces learns to predict word boundaries within strings of alpha-numeric characters .
1,2 person
3,5 abstract|6,8 abstract|16,23 abstract|19,23 abstract|19,23 object|21,23 abstract

Precision of the US-MWTs model is on par with the segmenter applying Kneser-Ney smoothing , while maintaining relatively high Recall .
1,2 person
1,6 abstract|3,6 abstract|10,12 object|13,15 abstract|18,21 abstract|20,21 abstract

However , when presented with test data including spaces , they impede the segmentation process rather than supporting it .
1,2 person
6,8 abstract|9,10 abstract|11,12 abstract|13,16 abstract|19,20 abstract

US-ISP models are better but still notably worse than lexical n-gram models ( especially on SYOS ) .
1,2 person
1,2 place|1,3 abstract|10,13 abstract

After setting the limit to 2 , Precision improves at the cost of a drop in Recall .
1,2 person
3,5 abstract|6,7 quantity|8,9 abstract|14,18 abstract|17,18 abstract

The results of the experiment with models employing modified Kneser-Ney smoothing are shown in Table 7 .
1,2 person
1,12 abstract|9,12 abstract|10,11 person|15,17 abstract

The F-score is better for SYOS , while on AKJ there is a very slight drop .
1,2 person
1,3 abstract|10,11 person|13,17 abstract

The results of the evaluation experiments with our algorithm are presented in Table 5 .
1,2 person
1,10 abstract|8,9 person|8,10 abstract|13,15 abstract

Nevertheless , due to very low Recall , the overall results are low .
1,2 person
5,8 abstract|7,8 event|9,12 abstract

They achieve higher Precision than both the other types of n-gram models .
1,2 person
1,2 object|3,5 abstract|6,13 abstract

The results obtained by the Universal Segmenter are presented in Table 8 .
1,2 person
1,3 abstract|5,8 abstract|5,8 object|11,13 abstract

Table 6 shows the results of experiments with the Stupid Backoff model .
1,2 person
1,3 object|4,13 abstract|4,13 object|9,13 abstract

Models with multi-word tokens achieve significantly higher results .
1,2 person
1,5 abstract|1,5 object|3,5 object|6,9 abstract

Results and Discussion
1,2 person
1,2 abstract|3,4 abstract

General Observations
1,2 person
1,3 abstract

6.1 .
1,2 person
1,2 abstract
