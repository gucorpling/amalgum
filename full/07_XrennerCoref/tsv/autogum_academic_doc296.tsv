#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=6 .
1-1	0-1	6	quantity	new	_	_
1-2	2-3	.	_	_	_	_

#Text=Results and Discussion
2-1	4-11	Results	abstract|abstract[3]	new|new[3]	coref|coref|coref|coref	3-1[5_0]|14-1[100_3]|3-1[5_0]|14-1[100_3]
2-2	12-15	and	abstract[3]	new[3]	_	_
2-3	16-26	Discussion	abstract[3]|abstract	new[3]|new	_	_

#Text=The results of the evaluation experiments with our algorithm are presented in Table 5 .
3-1	27-30	The	abstract[5]	giv[5]	coref	4-12[16_5]
3-2	31-38	results	abstract[5]	giv[5]	_	_
3-3	39-41	of	abstract[5]	giv[5]	_	_
3-4	42-45	the	abstract[5]|event[7]	giv[5]|new[7]	coref	7-7[31_7]
3-5	46-56	evaluation	abstract[5]|abstract|event[7]	giv[5]|new|new[7]	_	_
3-6	57-68	experiments	abstract[5]|event[7]	giv[5]|new[7]	_	_
3-7	69-73	with	abstract[5]|event[7]	giv[5]|new[7]	_	_
3-8	74-77	our	abstract[5]|event[7]|person|abstract[9]	giv[5]|new[7]|acc|new[9]	ana|ana	23-8|23-8
3-9	78-87	algorithm	abstract[5]|event[7]|abstract[9]	giv[5]|new[7]|new[9]	_	_
3-10	88-91	are	_	_	_	_
3-11	92-101	presented	_	_	_	_
3-12	102-104	in	_	_	_	_
3-13	105-110	Table	abstract[10]	new[10]	coref	14-15[0_10]
3-14	111-112	5	abstract[10]	new[10]	_	_
3-15	113-114	.	_	_	_	_

#Text=The variant without the limit of n-grams per input segment produces unbalanced results ( especially on SYOS ) , with relatively low Precision .
4-1	115-118	The	abstract[11]	new[11]	_	_
4-2	119-126	variant	abstract[11]	new[11]	_	_
4-3	127-134	without	abstract[11]	new[11]	_	_
4-4	135-138	the	abstract[11]|abstract[12]	new[11]|new[12]	coref	5-3[19_12]
4-5	139-144	limit	abstract[11]|abstract[12]	new[11]|new[12]	_	_
4-6	145-147	of	abstract[11]|abstract[12]	new[11]|new[12]	_	_
4-7	148-155	n-grams	abstract[11]|abstract[12]|quantity[13]	new[11]|new[12]|new[13]	coref	8-25[44_13]
4-8	156-159	per	abstract[11]|abstract[12]|quantity[13]	new[11]|new[12]|new[13]	_	_
4-9	160-165	input	abstract[11]|abstract[12]|quantity[13]|abstract|abstract[15]	new[11]|new[12]|new[13]|new|new[15]	coref|coref|coref|coref	8-27|8-27[46_15]|8-27|8-27[46_15]
4-10	166-173	segment	abstract[11]|abstract[12]|quantity[13]|abstract[15]	new[11]|new[12]|new[13]|new[15]	_	_
4-11	174-182	produces	_	_	_	_
4-12	183-193	unbalanced	abstract[16]	giv[16]	appos	4-15[17_16]
4-13	194-201	results	abstract[16]	giv[16]	_	_
4-14	202-203	(	_	_	_	_
4-15	204-214	especially	abstract[17]	giv[17]	coref	20-14[143_17]
4-16	215-217	on	abstract[17]	giv[17]	_	_
4-17	218-222	SYOS	abstract[17]	giv[17]	_	_
4-18	223-224	)	_	_	_	_
4-19	225-226	,	_	_	_	_
4-20	227-231	with	_	_	_	_
4-21	232-242	relatively	abstract[18]	new[18]	coref	5-8[0_18]
4-22	243-246	low	abstract[18]	new[18]	_	_
4-23	247-256	Precision	abstract[18]	new[18]	_	_
4-24	257-258	.	_	_	_	_

#Text=After setting the limit to 2 , Precision improves at the cost of a drop in Recall .
5-1	259-264	After	_	_	_	_
5-2	265-272	setting	_	_	_	_
5-3	273-276	the	abstract[19]	giv[19]	_	_
5-4	277-282	limit	abstract[19]	giv[19]	_	_
5-5	283-285	to	_	_	_	_
5-6	286-287	2	quantity	new	_	_
5-7	288-289	,	_	_	_	_
5-8	290-299	Precision	abstract	giv	coref	9-14
5-9	300-308	improves	_	_	_	_
5-10	309-311	at	_	_	_	_
5-11	312-315	the	abstract[22]	new[22]	_	_
5-12	316-320	cost	abstract[22]	new[22]	_	_
5-13	321-323	of	abstract[22]	new[22]	_	_
5-14	324-325	a	abstract[22]|abstract[23]	new[22]|new[23]	coref	6-13[28_23]
5-15	326-330	drop	abstract[22]|abstract[23]	new[22]|new[23]	_	_
5-16	331-333	in	abstract[22]|abstract[23]	new[22]|new[23]	_	_
5-17	334-340	Recall	abstract[22]|abstract[23]|abstract	new[22]|new[23]|new	coref	9-26
5-18	341-342	.	_	_	_	_

#Text=The F-score is better for SYOS , while on AKJ there is a very slight drop .
6-1	343-346	The	abstract[25]	new[25]	coref	9-16[0_25]
6-2	347-354	F-score	abstract[25]	new[25]	_	_
6-3	355-357	is	_	_	_	_
6-4	358-364	better	_	_	_	_
6-5	365-368	for	_	_	_	_
6-6	369-373	SYOS	abstract	new	coref	27-9
6-7	374-375	,	_	_	_	_
6-8	376-381	while	_	_	_	_
6-9	382-384	on	_	_	_	_
6-10	385-388	AKJ	person	new	_	_
6-11	389-394	there	_	_	_	_
6-12	395-397	is	_	_	_	_
6-13	398-399	a	abstract[28]	giv[28]	coref	13-39[98_28]
6-14	400-404	very	abstract[28]	giv[28]	_	_
6-15	405-411	slight	abstract[28]	giv[28]	_	_
6-16	412-416	drop	abstract[28]	giv[28]	_	_
6-17	417-418	.	_	_	_	_

#Text=Table 6 shows the results of experiments with the Stupid Backoff model .
7-1	419-424	Table	object[29]	new[29]	_	_
7-2	425-426	6	object[29]	new[29]	_	_
7-3	427-432	shows	_	_	_	_
7-4	433-436	the	abstract[30]	new[30]	coref	8-8[37_30]
7-5	437-444	results	abstract[30]	new[30]	_	_
7-6	445-447	of	abstract[30]	new[30]	_	_
7-7	448-459	experiments	abstract[30]|event[31]	new[30]|giv[31]	_	_
7-8	460-464	with	abstract[30]|event[31]	new[30]|giv[31]	_	_
7-9	465-468	the	abstract[30]|event[31]|abstract[34]	new[30]|giv[31]|new[34]	coref	12-31[85_34]
7-10	469-475	Stupid	abstract[30]|event[31]|person|abstract[34]	new[30]|giv[31]|new|new[34]	_	_
7-11	476-483	Backoff	abstract[30]|event[31]|event|abstract[34]	new[30]|giv[31]|new|new[34]	coref	8-3
7-12	484-489	model	abstract[30]|event[31]|abstract[34]	new[30]|giv[31]|new[34]	_	_
7-13	490-491	.	_	_	_	_

#Text=When no backoff factor is applied , results for both test sets are similar to those from the MiNgMatch Segmenter without the limit of n-grams per input segment .
8-1	492-496	When	_	_	_	_
8-2	497-499	no	abstract[36]	new[36]	ana	8-16[40_36]
8-3	500-507	backoff	abstract|abstract[36]	giv|new[36]	coref	9-3
8-4	508-514	factor	abstract[36]	new[36]	_	_
8-5	515-517	is	_	_	_	_
8-6	518-525	applied	_	_	_	_
8-7	526-527	,	_	_	_	_
8-8	528-535	results	abstract[37]	giv[37]	coref	12-20[82_37]
8-9	536-539	for	abstract[37]	giv[37]	_	_
8-10	540-544	both	abstract[37]|abstract[39]	giv[37]|new[39]	_	_
8-11	545-549	test	abstract[37]|abstract|abstract[39]	giv[37]|new|new[39]	coref	22-6
8-12	550-554	sets	abstract[37]|abstract[39]	giv[37]|new[39]	_	_
8-13	555-558	are	_	_	_	_
8-14	559-566	similar	_	_	_	_
8-15	567-569	to	_	_	_	_
8-16	570-575	those	abstract[40]	giv[40]	coref	9-2[48_40]
8-17	576-580	from	abstract[40]	giv[40]	_	_
8-18	581-584	the	abstract[40]|object[42]	giv[40]|new[42]	coref	17-5[114_42]
8-19	585-594	MiNgMatch	abstract[40]|object|object[42]	giv[40]|new|new[42]	coref	13-8
8-20	595-604	Segmenter	abstract[40]|object[42]	giv[40]|new[42]	_	_
8-21	605-612	without	_	_	_	_
8-22	613-616	the	abstract[43]	new[43]	coref	13-12[89_43]
8-23	617-622	limit	abstract[43]	new[43]	_	_
8-24	623-625	of	abstract[43]	new[43]	_	_
8-25	626-633	n-grams	abstract[43]|quantity[44]	new[43]|giv[44]	coref	12-28[0_44]
8-26	634-637	per	abstract[43]|quantity[44]	new[43]|giv[44]	_	_
8-27	638-643	input	abstract[43]|quantity[44]|abstract|abstract[46]	new[43]|giv[44]|giv|giv[46]	coref|coref|coref|coref	13-17|13-17[92_46]|13-17|13-17[92_46]
8-28	644-651	segment	abstract[43]|quantity[44]|abstract[46]	new[43]|giv[44]|giv[46]	_	_
8-29	652-653	.	_	_	_	_

#Text=Setting the backoff factor to an appropriate value allows for significant improvement in Precision and F-score ( and in some cases also small improvements in Recall ) .
9-1	654-661	Setting	_	_	_	_
9-2	662-665	the	abstract[48]	giv[48]	coref	10-10[59_48]
9-3	666-673	backoff	abstract|abstract[48]	giv|giv[48]	coref	10-12
9-4	674-680	factor	abstract[48]	giv[48]	_	_
9-5	681-683	to	abstract[48]	giv[48]	_	_
9-6	684-686	an	abstract[48]|abstract[49]	giv[48]|new[49]	coref	10-29[62_49]
9-7	687-698	appropriate	abstract[48]|abstract[49]	giv[48]|new[49]	_	_
9-8	699-704	value	abstract[48]|abstract[49]	giv[48]|new[49]	_	_
9-9	705-711	allows	_	_	_	_
9-10	712-715	for	_	_	_	_
9-11	716-727	significant	abstract[50]	new[50]	coref	11-7[70_50]
9-12	728-739	improvement	abstract[50]	new[50]	_	_
9-13	740-742	in	abstract[50]	new[50]	_	_
9-14	743-752	Precision	abstract[50]|abstract	new[50]|giv	coref	11-10[71_0]
9-15	753-756	and	abstract[50]	new[50]	_	_
9-16	757-764	F-score	abstract[50]|abstract	new[50]|giv	coref	10-2[56_0]
9-17	765-766	(	_	_	_	_
9-18	767-770	and	_	_	_	_
9-19	771-773	in	_	_	_	_
9-20	774-778	some	abstract[53]|abstract[54]	new[53]|new[54]	coref|coref	28-16[199_53]|28-16[199_53]
9-21	779-784	cases	abstract[53]|abstract[54]	new[53]|new[54]	_	_
9-22	785-789	also	abstract[54]	new[54]	_	_
9-23	790-795	small	abstract[54]	new[54]	_	_
9-24	796-808	improvements	abstract[54]	new[54]	_	_
9-25	809-811	in	abstract[54]	new[54]	_	_
9-26	812-818	Recall	abstract[54]|abstract	new[54]|giv	ana	10-5
9-27	819-820	)	_	_	_	_
9-28	821-822	.	_	_	_	_

#Text=For the F-score , it is better to set a low backoff factor ( e. g. , 0.09 ) for 1-grams only , than to set it to a fixed value for all backoff steps ( e. g. , 0.4 , as Brants et al. did ) .
10-1	823-826	For	_	_	_	_
10-2	827-830	the	abstract[56]	giv[56]	coref	26-15[185_56]
10-3	831-838	F-score	abstract[56]	giv[56]	_	_
10-4	839-840	,	_	_	_	_
10-5	841-843	it	abstract	giv	ana	10-27
10-6	844-846	is	_	_	_	_
10-7	847-853	better	_	_	_	_
10-8	854-856	to	_	_	_	_
10-9	857-860	set	_	_	_	_
10-10	861-862	a	abstract[59]	giv[59]	coref	11-1[68_59]
10-11	863-866	low	abstract[59]	giv[59]	_	_
10-12	867-874	backoff	abstract|abstract[59]	giv|giv[59]	coref	10-34
10-13	875-881	factor	abstract[59]	giv[59]	_	_
10-14	882-883	(	_	_	_	_
10-15	884-886	e.	_	_	_	_
10-16	887-889	g.	_	_	_	_
10-17	890-891	,	_	_	_	_
10-18	892-896	0.09	_	_	_	_
10-19	897-898	)	_	_	_	_
10-20	899-902	for	_	_	_	_
10-21	903-910	1-grams	quantity[60]	new[60]	_	_
10-22	911-915	only	quantity[60]	new[60]	_	_
10-23	916-917	,	_	_	_	_
10-24	918-922	than	_	_	_	_
10-25	923-925	to	_	_	_	_
10-26	926-929	set	_	_	_	_
10-27	930-932	it	abstract	giv	coref	13-44
10-28	933-935	to	_	_	_	_
10-29	936-937	a	abstract[62]	giv[62]	_	_
10-30	938-943	fixed	abstract[62]	giv[62]	_	_
10-31	944-949	value	abstract[62]	giv[62]	_	_
10-32	950-953	for	abstract[62]	giv[62]	_	_
10-33	954-957	all	abstract[62]|person[64]	giv[62]|new[64]	appos	10-37[65_64]
10-34	958-965	backoff	abstract[62]|event|person[64]	giv[62]|giv|new[64]	coref	11-2
10-35	966-971	steps	abstract[62]|person[64]	giv[62]|new[64]	_	_
10-36	972-973	(	_	_	_	_
10-37	974-976	e.	abstract[65]	giv[65]	coref	37-31[0_65]
10-38	977-979	g.	abstract[65]	giv[65]	_	_
10-39	980-981	,	abstract[65]	giv[65]	_	_
10-40	982-985	0.4	abstract[65]|quantity	giv[65]|new	coref	11-5
10-41	986-987	,	abstract[65]	giv[65]	_	_
10-42	988-990	as	abstract[65]	giv[65]	_	_
10-43	991-997	Brants	abstract[65]	giv[65]	_	_
10-44	998-1000	et	abstract[65]	giv[65]	_	_
10-45	1001-1004	al.	abstract[65]	giv[65]	_	_
10-46	1005-1008	did	_	_	_	_
10-47	1009-1010	)	_	_	_	_
10-48	1011-1012	.	_	_	_	_

#Text=A backoff factor of 0.4 gives significant improvement in Precision with higher order n-gram models , but at the same time Recall drops drastically and overall performance deteriorates .
11-1	1013-1014	A	abstract[68]	giv[68]	coref	12-12[80_68]
11-2	1015-1022	backoff	abstract|abstract[68]	giv|giv[68]	coref	12-13
11-3	1023-1029	factor	abstract[68]	giv[68]	_	_
11-4	1030-1032	of	abstract[68]	giv[68]	_	_
11-5	1033-1036	0.4	abstract[68]|quantity	giv[68]|giv	_	_
11-6	1037-1042	gives	_	_	_	_
11-7	1043-1054	significant	abstract[70]	giv[70]	coref	28-20[200_70]
11-8	1055-1066	improvement	abstract[70]	giv[70]	_	_
11-9	1067-1069	in	abstract[70]	giv[70]	_	_
11-10	1070-1079	Precision	abstract[70]|abstract[71]	giv[70]|giv[71]	coref	13-30[94_71]
11-11	1080-1084	with	abstract[70]|abstract[71]	giv[70]|giv[71]	_	_
11-12	1085-1091	higher	abstract[70]|abstract[71]|abstract[72]	giv[70]|giv[71]|new[72]	coref	12-4[77_72]
11-13	1092-1097	order	abstract[70]|abstract[71]|abstract[72]	giv[70]|giv[71]|new[72]	_	_
11-14	1098-1104	n-gram	abstract[70]|abstract[71]|abstract[73]	giv[70]|giv[71]|new[73]	coref	12-2[76_73]
11-15	1105-1111	models	abstract[70]|abstract[71]|abstract[73]	giv[70]|giv[71]|new[73]	_	_
11-16	1112-1113	,	_	_	_	_
11-17	1114-1117	but	_	_	_	_
11-18	1118-1120	at	_	_	_	_
11-19	1121-1124	the	time[74]	new[74]	_	_
11-20	1125-1129	same	time[74]	new[74]	_	_
11-21	1130-1134	time	time[74]	new[74]	_	_
11-22	1135-1141	Recall	_	_	_	_
11-23	1142-1147	drops	_	_	_	_
11-24	1148-1159	drastically	_	_	_	_
11-25	1160-1163	and	_	_	_	_
11-26	1164-1171	overall	abstract[75]	new[75]	coref	27-18[193_75]
11-27	1172-1183	performance	abstract[75]	new[75]	_	_
11-28	1184-1196	deteriorates	_	_	_	_
11-29	1197-1198	.	_	_	_	_

#Text=For models with an n-gram order of 3 or higher , the backoff factor has a bigger impact on the results than further increasing the order of n-grams included in the model .
12-1	1199-1202	For	_	_	_	_
12-2	1203-1209	models	abstract[76]	giv[76]	coref	14-7[0_76]
12-3	1210-1214	with	abstract[76]	giv[76]	_	_
12-4	1215-1217	an	abstract[76]|abstract[77]	giv[76]|giv[77]	_	_
12-5	1218-1224	n-gram	abstract[76]|abstract[77]	giv[76]|giv[77]	_	_
12-6	1225-1230	order	abstract[76]|abstract[77]	giv[76]|giv[77]	_	_
12-7	1231-1233	of	abstract[76]|abstract[77]	giv[76]|giv[77]	_	_
12-8	1234-1235	3	abstract[76]|abstract[77]|quantity	giv[76]|giv[77]|new	_	_
12-9	1236-1238	or	abstract[76]|abstract[77]	giv[76]|giv[77]	_	_
12-10	1239-1245	higher	abstract[76]|abstract[77]	giv[76]|giv[77]	_	_
12-11	1246-1247	,	_	_	_	_
12-12	1248-1251	the	abstract[80]	giv[80]	_	_
12-13	1252-1259	backoff	abstract|abstract[80]	giv|giv[80]	coref	13-23[93_0]
12-14	1260-1266	factor	abstract[80]	giv[80]	_	_
12-15	1267-1270	has	_	_	_	_
12-16	1271-1272	a	abstract[81]	new[81]	_	_
12-17	1273-1279	bigger	abstract[81]	new[81]	_	_
12-18	1280-1286	impact	abstract[81]	new[81]	_	_
12-19	1287-1289	on	abstract[81]	new[81]	_	_
12-20	1290-1293	the	abstract[81]|abstract[82]	new[81]|giv[82]	coref	13-4[87_82]
12-21	1294-1301	results	abstract[81]|abstract[82]	new[81]|giv[82]	_	_
12-22	1302-1306	than	_	_	_	_
12-23	1307-1314	further	_	_	_	_
12-24	1315-1325	increasing	_	_	_	_
12-25	1326-1329	the	abstract[83]	new[83]	_	_
12-26	1330-1335	order	abstract[83]	new[83]	_	_
12-27	1336-1338	of	abstract[83]	new[83]	_	_
12-28	1339-1346	n-grams	abstract[83]|quantity	new[83]|giv	coref	13-15[90_0]
12-29	1347-1355	included	_	_	_	_
12-30	1356-1358	in	_	_	_	_
12-31	1359-1362	the	abstract[85]	giv[85]	coref	18-1[116_85]
12-32	1363-1368	model	abstract[85]	giv[85]	_	_
12-33	1369-1370	.	_	_	_	_

#Text=A comparison with the results yielded by MiNgMatch shows that setting the limit of n-grams per input segment is more effective than Stupid Backoff as a method for improving precision of the segmentation process — it leads to a much smaller drop in Recall .
13-1	1371-1372	A	abstract[86]	new[86]	_	_
13-2	1373-1383	comparison	abstract[86]	new[86]	_	_
13-3	1384-1388	with	abstract[86]	new[86]	_	_
13-4	1389-1392	the	abstract[86]|abstract[87]	new[86]|giv[87]	_	_
13-5	1393-1400	results	abstract[86]|abstract[87]	new[86]|giv[87]	_	_
13-6	1401-1408	yielded	_	_	_	_
13-7	1409-1411	by	_	_	_	_
13-8	1412-1421	MiNgMatch	organization	giv	_	_
13-9	1422-1427	shows	_	_	_	_
13-10	1428-1432	that	_	_	_	_
13-11	1433-1440	setting	_	_	_	_
13-12	1441-1444	the	quantity[89]	giv[89]	_	_
13-13	1445-1450	limit	quantity[89]	giv[89]	_	_
13-14	1451-1453	of	quantity[89]	giv[89]	_	_
13-15	1454-1461	n-grams	quantity[89]|quantity[90]	giv[89]|giv[90]	coref	33-16[230_90]
13-16	1462-1465	per	quantity[89]|quantity[90]	giv[89]|giv[90]	_	_
13-17	1466-1471	input	quantity[89]|quantity[90]|abstract|abstract[92]	giv[89]|giv[90]|giv|giv[92]	coref|coref|coref|coref	18-56[132_92]|19-14[138_0]|18-56[132_92]|19-14[138_0]
13-18	1472-1479	segment	quantity[89]|quantity[90]|abstract[92]	giv[89]|giv[90]|giv[92]	_	_
13-19	1480-1482	is	_	_	_	_
13-20	1483-1487	more	_	_	_	_
13-21	1488-1497	effective	_	_	_	_
13-22	1498-1502	than	_	_	_	_
13-23	1503-1509	Stupid	event[93]	giv[93]	coref	35-3[0_93]
13-24	1510-1517	Backoff	event[93]	giv[93]	_	_
13-25	1518-1520	as	_	_	_	_
13-26	1521-1522	a	_	_	_	_
13-27	1523-1529	method	_	_	_	_
13-28	1530-1533	for	_	_	_	_
13-29	1534-1543	improving	_	_	_	_
13-30	1544-1553	precision	abstract[94]	giv[94]	coref	36-28[255_94]
13-31	1554-1556	of	abstract[94]	giv[94]	_	_
13-32	1557-1560	the	abstract[94]|abstract[96]	giv[94]|new[96]	ana	13-36[0_96]
13-33	1561-1573	segmentation	abstract[94]|abstract|abstract[96]	giv[94]|new|new[96]	coref	19-20[139_0]
13-34	1574-1581	process	abstract[94]|abstract[96]	giv[94]|new[96]	_	_
13-35	1582-1583	—	_	_	_	_
13-36	1584-1586	it	abstract	giv	coref	22-13[157_0]
13-37	1587-1592	leads	_	_	_	_
13-38	1593-1595	to	_	_	_	_
13-39	1596-1597	a	abstract[98]	giv[98]	_	_
13-40	1598-1602	much	abstract[98]	giv[98]	_	_
13-41	1603-1610	smaller	abstract[98]	giv[98]	_	_
13-42	1611-1615	drop	abstract[98]	giv[98]	_	_
13-43	1616-1618	in	abstract[98]	giv[98]	_	_
13-44	1619-1625	Recall	abstract[98]|abstract	giv[98]|giv	coref	16-5[111_0]
13-45	1626-1627	.	_	_	_	_

#Text=The results of the experiment with models employing modified Kneser-Ney smoothing are shown in Table 7 .
14-1	1628-1631	The	abstract[100]	giv[100]	coref	16-9[112_100]
14-2	1632-1639	results	abstract[100]	giv[100]	_	_
14-3	1640-1642	of	abstract[100]	giv[100]	_	_
14-4	1643-1646	the	abstract[100]|event[101]	giv[100]|new[101]	_	_
14-5	1647-1657	experiment	abstract[100]|event[101]	giv[100]|new[101]	_	_
14-6	1658-1662	with	abstract[100]|event[101]	giv[100]|new[101]	_	_
14-7	1663-1669	models	abstract[100]|event[101]|abstract	giv[100]|new[101]|giv	ana	15-1
14-8	1670-1679	employing	_	_	_	_
14-9	1680-1688	modified	abstract[104]	new[104]	coref	25-13[180_104]
14-10	1689-1699	Kneser-Ney	person|abstract[104]	new|new[104]	coref	25-13
14-11	1700-1709	smoothing	abstract[104]	new[104]	_	_
14-12	1710-1713	are	_	_	_	_
14-13	1714-1719	shown	_	_	_	_
14-14	1720-1722	in	_	_	_	_
14-15	1723-1728	Table	abstract|abstract[106]	giv|new[106]	coref|coref	23-4|23-4
14-16	1729-1730	7	abstract[106]	new[106]	_	_
14-17	1731-1732	.	_	_	_	_

#Text=They achieve higher Precision than both the other types of n-gram models .
15-1	1733-1737	They	abstract	giv	coref	15-11[110_0]
15-2	1738-1745	achieve	_	_	_	_
15-3	1746-1752	higher	abstract[108]	new[108]	coref	25-1[174_108]
15-4	1753-1762	Precision	abstract[108]	new[108]	_	_
15-5	1763-1767	than	abstract[108]	new[108]	_	_
15-6	1768-1772	both	abstract[108]|abstract[109]	new[108]|new[109]	_	_
15-7	1773-1776	the	abstract[108]|abstract[109]	new[108]|new[109]	_	_
15-8	1777-1782	other	abstract[108]|abstract[109]	new[108]|new[109]	_	_
15-9	1783-1788	types	abstract[108]|abstract[109]	new[108]|new[109]	_	_
15-10	1789-1791	of	abstract[108]|abstract[109]	new[108]|new[109]	_	_
15-11	1792-1798	n-gram	abstract[108]|abstract[109]|abstract[110]	new[108]|new[109]|giv[110]	coref	20-1[141_110]
15-12	1799-1805	models	abstract[108]|abstract[109]|abstract[110]	new[108]|new[109]|giv[110]	_	_
15-13	1806-1807	.	_	_	_	_

#Text=Nevertheless , due to very low Recall , the overall results are low .
16-1	1808-1820	Nevertheless	_	_	_	_
16-2	1821-1822	,	_	_	_	_
16-3	1823-1826	due	_	_	_	_
16-4	1827-1829	to	_	_	_	_
16-5	1830-1834	very	abstract[111]	giv[111]	_	_
16-6	1835-1838	low	abstract[111]	giv[111]	_	_
16-7	1839-1845	Recall	abstract[111]	giv[111]	_	_
16-8	1846-1847	,	_	_	_	_
16-9	1848-1851	the	abstract[112]	giv[112]	coref	17-1[113_112]
16-10	1852-1859	overall	abstract[112]	giv[112]	_	_
16-11	1860-1867	results	abstract[112]	giv[112]	_	_
16-12	1868-1871	are	_	_	_	_
16-13	1872-1875	low	_	_	_	_
16-14	1876-1877	.	_	_	_	_

#Text=The results obtained by the Universal Segmenter are presented in Table 8 .
17-1	1878-1881	The	abstract[113]	giv[113]	coref	24-6[173_113]
17-2	1882-1889	results	abstract[113]	giv[113]	_	_
17-3	1890-1898	obtained	_	_	_	_
17-4	1899-1901	by	_	_	_	_
17-5	1902-1905	the	object[114]	giv[114]	coref	25-10[178_114]
17-6	1906-1915	Universal	object[114]	giv[114]	_	_
17-7	1916-1925	Segmenter	object[114]	giv[114]	_	_
17-8	1926-1929	are	_	_	_	_
17-9	1930-1939	presented	_	_	_	_
17-10	1940-1942	in	_	_	_	_
17-11	1943-1948	Table	abstract[115]	new[115]	_	_
17-12	1949-1950	8	abstract[115]	new[115]	_	_
17-13	1951-1952	.	_	_	_	_

#Text=The default model ( regardless of what kind of character representations are used — conventional character embeddings or concatenated n-gram vectors ) learns from the training data that the first and the last character of a word ( corresponding to B , E and S tags ) are always adjacent either to the boundary of a space-delimited segment or to a punctuation mark .
18-1	1953-1956	The	abstract[116]	giv[116]	coref	19-5[135_116]
18-2	1957-1964	default	abstract[116]	giv[116]	_	_
18-3	1965-1970	model	abstract[116]	giv[116]	_	_
18-4	1971-1972	(	_	_	_	_
18-5	1973-1983	regardless	animal[117]	new[117]	_	_
18-6	1984-1986	of	animal[117]	new[117]	_	_
18-7	1987-1991	what	animal[117]	new[117]	_	_
18-8	1992-1996	kind	animal[117]|abstract[119]	new[117]|new[119]	_	_
18-9	1997-1999	of	animal[117]|abstract[119]	new[117]|new[119]	_	_
18-10	2000-2009	character	animal[117]|abstract|abstract[119]	new[117]|new|new[119]	coref	18-16
18-11	2010-2025	representations	animal[117]|abstract[119]	new[117]|new[119]	_	_
18-12	2026-2029	are	_	_	_	_
18-13	2030-2034	used	_	_	_	_
18-14	2035-2036	—	_	_	_	_
18-15	2037-2049	conventional	abstract[121]	new[121]	_	_
18-16	2050-2059	character	abstract|abstract[121]	giv|new[121]	coref	18-32[125_0]
18-17	2060-2070	embeddings	abstract[121]	new[121]	_	_
18-18	2071-2073	or	_	_	_	_
18-19	2074-2086	concatenated	object[122]	new[122]	coref	27-22[194_122]
18-20	2087-2093	n-gram	object[122]	new[122]	_	_
18-21	2094-2101	vectors	object[122]	new[122]	_	_
18-22	2102-2103	)	_	_	_	_
18-23	2104-2110	learns	_	_	_	_
18-24	2111-2115	from	_	_	_	_
18-25	2116-2119	the	abstract[124]	new[124]	coref	21-10[0_124]
18-26	2120-2128	training	abstract|abstract[124]	new|new[124]	coref	33-27
18-27	2129-2133	data	abstract[124]	new[124]	_	_
18-28	2134-2138	that	_	_	_	_
18-29	2139-2142	the	_	_	_	_
18-30	2143-2148	first	_	_	_	_
18-31	2149-2152	and	_	_	_	_
18-32	2153-2156	the	abstract[125]	giv[125]	coref	30-6[0_125]
18-33	2157-2161	last	abstract[125]	giv[125]	_	_
18-34	2162-2171	character	abstract[125]	giv[125]	_	_
18-35	2172-2174	of	abstract[125]	giv[125]	_	_
18-36	2175-2176	a	abstract[125]|abstract[126]	giv[125]|new[126]	coref	21-16[0_126]
18-37	2177-2181	word	abstract[125]|abstract[126]	giv[125]|new[126]	_	_
18-38	2182-2183	(	_	_	_	_
18-39	2184-2197	corresponding	_	_	_	_
18-40	2198-2200	to	_	_	_	_
18-41	2201-2202	B	object	new	_	_
18-42	2203-2204	,	_	_	_	_
18-43	2205-2206	E	person	new	_	_
18-44	2207-2210	and	_	_	_	_
18-45	2211-2212	S	abstract|abstract[130]	new|new[130]	_	_
18-46	2213-2217	tags	abstract[130]	new[130]	_	_
18-47	2218-2219	)	_	_	_	_
18-48	2220-2223	are	_	_	_	_
18-49	2224-2230	always	_	_	_	_
18-50	2231-2239	adjacent	_	_	_	_
18-51	2240-2246	either	place[131]	new[131]	_	_
18-52	2247-2249	to	place[131]	new[131]	_	_
18-53	2250-2253	the	place[131]	new[131]	_	_
18-54	2254-2262	boundary	place[131]	new[131]	_	_
18-55	2263-2265	of	place[131]	new[131]	_	_
18-56	2266-2267	a	place[131]|abstract[132]	new[131]|giv[132]	coref	37-81[279_132]
18-57	2268-2283	space-delimited	place[131]|abstract[132]	new[131]|giv[132]	_	_
18-58	2284-2291	segment	place[131]|abstract[132]	new[131]|giv[132]	_	_
18-59	2292-2294	or	_	_	_	_
18-60	2295-2297	to	_	_	_	_
18-61	2298-2299	a	object[134]	new[134]	_	_
18-62	2300-2311	punctuation	abstract|object[134]	new|new[134]	coref	19-8
18-63	2312-2316	mark	object[134]	new[134]	_	_
18-64	2317-2318	.	_	_	_	_

#Text=As a result , the model separates punctuation from alpha-numeric strings found in the input , but never applies further segmentation to them .
19-1	2319-2321	As	_	_	_	_
19-2	2322-2323	a	_	_	_	_
19-3	2324-2330	result	_	_	_	_
19-4	2331-2332	,	_	_	_	_
19-5	2333-2336	the	abstract[135]	giv[135]	coref	21-6[145_135]
19-6	2337-2342	model	abstract[135]	giv[135]	_	_
19-7	2343-2352	separates	_	_	_	_
19-8	2353-2364	punctuation	abstract	giv	_	_
19-9	2365-2369	from	_	_	_	_
19-10	2370-2383	alpha-numeric	abstract[137]	new[137]	ana	19-23[0_137]
19-11	2384-2391	strings	abstract[137]	new[137]	_	_
19-12	2392-2397	found	_	_	_	_
19-13	2398-2400	in	_	_	_	_
19-14	2401-2404	the	abstract[138]	giv[138]	_	_
19-15	2405-2410	input	abstract[138]	giv[138]	_	_
19-16	2411-2412	,	_	_	_	_
19-17	2413-2416	but	_	_	_	_
19-18	2417-2422	never	_	_	_	_
19-19	2423-2430	applies	_	_	_	_
19-20	2431-2438	further	abstract[139]	giv[139]	coref	22-14[0_139]
19-21	2439-2451	segmentation	abstract[139]	giv[139]	_	_
19-22	2452-2454	to	abstract[139]	giv[139]	_	_
19-23	2455-2459	them	abstract[139]|abstract	giv[139]|giv	coref	21-19[150_0]
19-24	2460-2461	.	_	_	_	_

#Text=US-ISP models are better but still notably worse than lexical n-gram models ( especially on SYOS ) .
20-1	2462-2468	US-ISP	abstract[141]	giv[141]	coref	20-10[142_141]
20-2	2469-2475	models	abstract[141]	giv[141]	_	_
20-3	2476-2479	are	_	_	_	_
20-4	2480-2486	better	_	_	_	_
20-5	2487-2490	but	_	_	_	_
20-6	2491-2496	still	_	_	_	_
20-7	2497-2504	notably	_	_	_	_
20-8	2505-2510	worse	_	_	_	_
20-9	2511-2515	than	_	_	_	_
20-10	2516-2523	lexical	abstract[142]	giv[142]	coref	30-24[0_142]
20-11	2524-2530	n-gram	abstract[142]	giv[142]	_	_
20-12	2531-2537	models	abstract[142]	giv[142]	_	_
20-13	2538-2539	(	abstract[142]	giv[142]	_	_
20-14	2540-2550	especially	abstract[142]|abstract[143]	giv[142]|giv[143]	_	_
20-15	2551-2553	on	abstract[142]|abstract[143]	giv[142]|giv[143]	_	_
20-16	2554-2558	SYOS	abstract[142]|abstract[143]	giv[142]|giv[143]	_	_
20-17	2559-2560	)	abstract[142]	giv[142]	_	_
20-18	2561-2562	.	_	_	_	_

#Text=Unlike with default settings , the model trained on data without whitespaces learns to predict word boundaries within strings of alpha-numeric characters .
21-1	2563-2569	Unlike	abstract[144]	new[144]	_	_
21-2	2570-2574	with	abstract[144]	new[144]	_	_
21-3	2575-2582	default	abstract[144]	new[144]	_	_
21-4	2583-2591	settings	abstract[144]	new[144]	_	_
21-5	2592-2593	,	_	_	_	_
21-6	2594-2597	the	abstract[145]	giv[145]	coref	23-25[167_145]
21-7	2598-2603	model	abstract[145]	giv[145]	_	_
21-8	2604-2611	trained	_	_	_	_
21-9	2612-2614	on	_	_	_	_
21-10	2615-2619	data	abstract	giv	coref	22-6[153_0]
21-11	2620-2627	without	_	_	_	_
21-12	2628-2639	whitespaces	abstract	new	coref	23-34[170_0]
21-13	2640-2646	learns	_	_	_	_
21-14	2647-2649	to	_	_	_	_
21-15	2650-2657	predict	_	_	_	_
21-16	2658-2662	word	abstract|place[149]	giv|new[149]	coref|coref|coref|coref	23-14|23-13[164_149]|23-14|23-13[164_149]
21-17	2663-2673	boundaries	place[149]	new[149]	_	_
21-18	2674-2680	within	_	_	_	_
21-19	2681-2688	strings	abstract[150]	giv[150]	coref	35-25[250_150]
21-20	2689-2691	of	abstract[150]	giv[150]	_	_
21-21	2692-2705	alpha-numeric	abstract[150]|abstract[151]	giv[150]|new[151]	coref	29-20[208_151]
21-22	2706-2716	characters	abstract[150]|abstract[151]	giv[150]|new[151]	_	_
21-23	2717-2718	.	_	_	_	_

#Text=However , when presented with test data including spaces , they impede the segmentation process rather than supporting it .
22-1	2719-2726	However	_	_	_	_
22-2	2727-2728	,	_	_	_	_
22-3	2729-2733	when	_	_	_	_
22-4	2734-2743	presented	_	_	_	_
22-5	2744-2748	with	_	_	_	_
22-6	2749-2753	test	abstract|abstract[153]	giv|giv[153]	coref|coref|coref|coref	23-22|23-32[0_153]|23-22|23-32[0_153]
22-7	2754-2758	data	abstract[153]	giv[153]	_	_
22-8	2759-2768	including	abstract[153]	giv[153]	_	_
22-9	2769-2775	spaces	abstract[153]|abstract	giv[153]|new	ana	22-11
22-10	2776-2777	,	_	_	_	_
22-11	2778-2782	they	abstract	giv	_	_
22-12	2783-2789	impede	_	_	_	_
22-13	2790-2793	the	abstract[157]	giv[157]	ana	22-19[0_157]
22-14	2794-2806	segmentation	abstract|abstract[157]	giv|giv[157]	coref	30-17[217_0]
22-15	2807-2814	process	abstract[157]	giv[157]	_	_
22-16	2815-2821	rather	_	_	_	_
22-17	2822-2826	than	_	_	_	_
22-18	2827-2837	supporting	_	_	_	_
22-19	2838-2840	it	abstract	giv	_	_
22-20	2841-2842	.	_	_	_	_

#Text=As shown in Table 9 , if we only take into account the word boundaries not already indicated in the raw test set , the model makes more correct predictions in data where the whitespaces have all been removed .
23-1	2843-2845	As	_	_	_	_
23-2	2846-2851	shown	_	_	_	_
23-3	2852-2854	in	_	_	_	_
23-4	2855-2860	Table	abstract|object[160]	giv|new[160]	_	_
23-5	2861-2862	9	object[160]	new[160]	_	_
23-6	2863-2864	,	_	_	_	_
23-7	2865-2867	if	_	_	_	_
23-8	2868-2870	we	person	giv	_	_
23-9	2871-2875	only	_	_	_	_
23-10	2876-2880	take	_	_	_	_
23-11	2881-2885	into	_	_	_	_
23-12	2886-2893	account	abstract	new	_	_
23-13	2894-2897	the	place[164]	giv[164]	_	_
23-14	2898-2902	word	abstract|place[164]	giv|giv[164]	coref	30-17
23-15	2903-2913	boundaries	place[164]	giv[164]	_	_
23-16	2914-2917	not	_	_	_	_
23-17	2918-2925	already	_	_	_	_
23-18	2926-2935	indicated	_	_	_	_
23-19	2936-2938	in	_	_	_	_
23-20	2939-2942	the	abstract[166]	new[166]	coref	30-7[0_166]
23-21	2943-2946	raw	abstract[166]	new[166]	_	_
23-22	2947-2951	test	abstract|abstract[166]	giv|new[166]	coref	33-10
23-23	2952-2955	set	abstract[166]	new[166]	_	_
23-24	2956-2957	,	_	_	_	_
23-25	2958-2961	the	abstract[167]	giv[167]	coref	25-3[176_167]
23-26	2962-2967	model	abstract[167]	giv[167]	_	_
23-27	2968-2973	makes	_	_	_	_
23-28	2974-2978	more	event[168]	new[168]	_	_
23-29	2979-2986	correct	event[168]	new[168]	_	_
23-30	2987-2998	predictions	event[168]	new[168]	_	_
23-31	2999-3001	in	event[168]	new[168]	_	_
23-32	3002-3006	data	event[168]|abstract	new[168]|giv	coref	33-3
23-33	3007-3012	where	_	_	_	_
23-34	3013-3016	the	abstract[170]	giv[170]	_	_
23-35	3017-3028	whitespaces	abstract[170]	giv[170]	_	_
23-36	3029-3033	have	_	_	_	_
23-37	3034-3037	all	_	_	_	_
23-38	3038-3042	been	_	_	_	_
23-39	3043-3050	removed	_	_	_	_
23-40	3051-3052	.	_	_	_	_

#Text=Models with multi-word tokens achieve significantly higher results .
24-1	3053-3059	Models	object[171]	new[171]	_	_
24-2	3060-3064	with	object[171]	new[171]	_	_
24-3	3065-3075	multi-word	object[171]|object[172]	new[171]|new[172]	coref	26-9[184_172]
24-4	3076-3082	tokens	object[171]|object[172]	new[171]|new[172]	_	_
24-5	3083-3090	achieve	_	_	_	_
24-6	3091-3104	significantly	abstract[173]	giv[173]	coref	28-4[196_173]
24-7	3105-3111	higher	abstract[173]	giv[173]	_	_
24-8	3112-3119	results	abstract[173]	giv[173]	_	_
24-9	3120-3121	.	_	_	_	_

#Text=Precision of the US-MWTs model is on par with the segmenter applying Kneser-Ney smoothing , while maintaining relatively high Recall .
25-1	3122-3131	Precision	abstract[174]	giv[174]	coref	26-21[186_174]
25-2	3132-3134	of	abstract[174]	giv[174]	_	_
25-3	3135-3138	the	abstract[174]|abstract[176]	giv[174]|giv[176]	coref	26-6[183_176]
25-4	3139-3146	US-MWTs	abstract[174]|object|abstract[176]	giv[174]|new|giv[176]	_	_
25-5	3147-3152	model	abstract[174]|abstract[176]	giv[174]|giv[176]	_	_
25-6	3153-3155	is	_	_	_	_
25-7	3156-3158	on	_	_	_	_
25-8	3159-3162	par	person[177]	new[177]	_	_
25-9	3163-3167	with	person[177]	new[177]	_	_
25-10	3168-3171	the	person[177]|object[178]	new[177]|giv[178]	coref	27-14[192_178]
25-11	3172-3181	segmenter	person[177]|object[178]	new[177]|giv[178]	_	_
25-12	3182-3190	applying	_	_	_	_
25-13	3191-3201	Kneser-Ney	person|abstract[180]	giv|giv[180]	coref|coref|coref|coref	37-42|37-41[268_180]|37-42|37-41[268_180]
25-14	3202-3211	smoothing	abstract[180]	giv[180]	_	_
25-15	3212-3213	,	_	_	_	_
25-16	3214-3219	while	_	_	_	_
25-17	3220-3231	maintaining	_	_	_	_
25-18	3232-3242	relatively	abstract[181]	new[181]	ana	26-1[0_181]
25-19	3243-3247	high	abstract[181]	new[181]	_	_
25-20	3248-3254	Recall	abstract[181]	new[181]	_	_
25-21	3255-3256	.	_	_	_	_

#Text=It yields lower Recall than the model with randomly generated multi-word tokens , but the F-score is higher due to better Precision .
26-1	3257-3259	It	abstract	giv	_	_
26-2	3260-3266	yields	_	_	_	_
26-3	3267-3272	lower	_	_	_	_
26-4	3273-3279	Recall	_	_	_	_
26-5	3280-3284	than	_	_	_	_
26-6	3285-3288	the	abstract[183]	giv[183]	coref	27-5[189_183]
26-7	3289-3294	model	abstract[183]	giv[183]	_	_
26-8	3295-3299	with	abstract[183]	giv[183]	_	_
26-9	3300-3308	randomly	abstract[183]|object[184]	giv[183]|giv[184]	coref	34-4[237_184]
26-10	3309-3318	generated	abstract[183]|object[184]	giv[183]|giv[184]	_	_
26-11	3319-3329	multi-word	abstract[183]|object[184]	giv[183]|giv[184]	_	_
26-12	3330-3336	tokens	abstract[183]|object[184]	giv[183]|giv[184]	_	_
26-13	3337-3338	,	_	_	_	_
26-14	3339-3342	but	_	_	_	_
26-15	3343-3346	the	abstract[185]	giv[185]	_	_
26-16	3347-3354	F-score	abstract[185]	giv[185]	_	_
26-17	3355-3357	is	_	_	_	_
26-18	3358-3364	higher	_	_	_	_
26-19	3365-3368	due	_	_	_	_
26-20	3369-3371	to	_	_	_	_
26-21	3372-3378	better	abstract[186]	giv[186]	_	_
26-22	3379-3388	Precision	abstract[186]	giv[186]	_	_
26-23	3389-3390	.	_	_	_	_

#Text=With the exception of the US-ISP model on SYOS , all variants of the neural segmenter achieved the best performance with concatenated 9-gram vectors .
27-1	3391-3395	With	_	_	_	_
27-2	3396-3399	the	abstract[187]	new[187]	_	_
27-3	3400-3409	exception	abstract[187]	new[187]	_	_
27-4	3410-3412	of	abstract[187]	new[187]	_	_
27-5	3413-3416	the	abstract[187]|abstract[189]	new[187]|giv[189]	coref	35-1[244_189]
27-6	3417-3423	US-ISP	abstract[187]|place|abstract[189]	new[187]|new|giv[189]	_	_
27-7	3424-3429	model	abstract[187]|abstract[189]	new[187]|giv[189]	_	_
27-8	3430-3432	on	abstract[187]|abstract[189]	new[187]|giv[189]	_	_
27-9	3433-3437	SYOS	abstract[187]|abstract[189]|object	new[187]|giv[189]|giv	_	_
27-10	3438-3439	,	_	_	_	_
27-11	3440-3443	all	object[191]	new[191]	_	_
27-12	3444-3452	variants	object[191]	new[191]	_	_
27-13	3453-3455	of	object[191]	new[191]	_	_
27-14	3456-3459	the	object[191]|object[192]	new[191]|giv[192]	_	_
27-15	3460-3466	neural	object[191]|object[192]	new[191]|giv[192]	_	_
27-16	3467-3476	segmenter	object[191]|object[192]	new[191]|giv[192]	_	_
27-17	3477-3485	achieved	_	_	_	_
27-18	3486-3489	the	abstract[193]	giv[193]	ana	28-1[0_193]
27-19	3490-3494	best	abstract[193]	giv[193]	_	_
27-20	3495-3506	performance	abstract[193]	giv[193]	_	_
27-21	3507-3511	with	abstract[193]	giv[193]	_	_
27-22	3512-3524	concatenated	abstract[193]|object[194]	giv[193]|giv[194]	_	_
27-23	3525-3531	9-gram	abstract[193]|object[194]	giv[193]|giv[194]	_	_
27-24	3532-3539	vectors	abstract[193]|object[194]	giv[193]|giv[194]	_	_
27-25	3540-3541	.	_	_	_	_

#Text=This contrasts with the results reported by Shao et al. for Chinese , where in most cases there was no further improvement beyond 3-grams .
28-1	3542-3546	This	abstract	giv	_	_
28-2	3547-3556	contrasts	_	_	_	_
28-3	3557-3561	with	_	_	_	_
28-4	3562-3565	the	abstract[196]	giv[196]	_	_
28-5	3566-3573	results	abstract[196]	giv[196]	_	_
28-6	3574-3582	reported	_	_	_	_
28-7	3583-3585	by	_	_	_	_
28-8	3586-3590	Shao	person	new	_	_
28-9	3591-3593	et	_	_	_	_
28-10	3594-3597	al.	_	_	_	_
28-11	3598-3601	for	_	_	_	_
28-12	3602-3609	Chinese	abstract	new	coref	29-14
28-13	3610-3611	,	_	_	_	_
28-14	3612-3617	where	_	_	_	_
28-15	3618-3620	in	_	_	_	_
28-16	3621-3625	most	abstract[199]	giv[199]	coref	37-66[274_199]
28-17	3626-3631	cases	abstract[199]	giv[199]	_	_
28-18	3632-3637	there	_	_	_	_
28-19	3638-3641	was	_	_	_	_
28-20	3642-3644	no	abstract[200]	giv[200]	_	_
28-21	3645-3652	further	abstract[200]	giv[200]	_	_
28-22	3653-3664	improvement	abstract[200]	giv[200]	_	_
28-23	3665-3671	beyond	abstract[200]	giv[200]	_	_
28-24	3672-3679	3-grams	abstract[200]|substance	giv[200]|new	_	_
28-25	3680-3681	.	_	_	_	_

#Text=This behavior is a consequence of differences between writing systems : words in Chinese are on average composed of less characters than in languages using alphabetic scripts .
29-1	3682-3686	This	abstract[202]	new[202]	coref	29-4[203_202]
29-2	3687-3695	behavior	abstract[202]	new[202]	_	_
29-3	3696-3698	is	_	_	_	_
29-4	3699-3700	a	abstract[203]	giv[203]	_	_
29-5	3701-3712	consequence	abstract[203]	giv[203]	_	_
29-6	3713-3715	of	abstract[203]	giv[203]	_	_
29-7	3716-3727	differences	abstract[203]|abstract	giv[203]|new	_	_
29-8	3728-3735	between	_	_	_	_
29-9	3736-3743	writing	_	_	_	_
29-10	3744-3751	systems	abstract	new	_	_
29-11	3752-3753	:	_	_	_	_
29-12	3754-3759	words	abstract[206]	new[206]	coref	37-49[270_206]
29-13	3760-3762	in	abstract[206]	new[206]	_	_
29-14	3763-3770	Chinese	abstract[206]|abstract	new[206]|giv	_	_
29-15	3771-3774	are	_	_	_	_
29-16	3775-3777	on	_	_	_	_
29-17	3778-3785	average	_	_	_	_
29-18	3786-3794	composed	_	_	_	_
29-19	3795-3797	of	_	_	_	_
29-20	3798-3802	less	abstract[208]	giv[208]	coref	30-10[215_208]
29-21	3803-3813	characters	abstract[208]	giv[208]	_	_
29-22	3814-3818	than	abstract[208]	giv[208]	_	_
29-23	3819-3821	in	abstract[208]	giv[208]	_	_
29-24	3822-3831	languages	abstract[208]|abstract	giv[208]|new	_	_
29-25	3832-3837	using	_	_	_	_
29-26	3838-3848	alphabetic	abstract[210]	new[210]	_	_
29-27	3849-3856	scripts	abstract[210]	new[210]	_	_
29-28	3857-3858	.	_	_	_	_

#Text=Due to a much bigger character set size , hanzi characters are also more informative to word segmentation , hence better performance with models using shorter context .
30-1	3859-3862	Due	_	_	_	_
30-2	3863-3865	to	_	_	_	_
30-3	3866-3867	a	quantity[213]	new[213]	_	_
30-4	3868-3872	much	quantity[213]	new[213]	_	_
30-5	3873-3879	bigger	quantity[213]	new[213]	_	_
30-6	3880-3889	character	abstract|quantity[213]	giv|new[213]	_	_
30-7	3890-3893	set	abstract|quantity[213]	giv|new[213]	coref	33-9[228_0]
30-8	3894-3898	size	quantity[213]	new[213]	_	_
30-9	3899-3900	,	_	_	_	_
30-10	3901-3906	hanzi	person|abstract[215]	new|giv[215]	_	_
30-11	3907-3917	characters	abstract[215]	giv[215]	_	_
30-12	3918-3921	are	_	_	_	_
30-13	3922-3926	also	_	_	_	_
30-14	3927-3931	more	_	_	_	_
30-15	3932-3943	informative	_	_	_	_
30-16	3944-3946	to	_	_	_	_
30-17	3947-3951	word	abstract|abstract[217]	giv|giv[217]	appos|appos	30-20[218_217]|30-20[218_217]
30-18	3952-3964	segmentation	abstract[217]	giv[217]	_	_
30-19	3965-3966	,	_	_	_	_
30-20	3967-3972	hence	abstract[218]	giv[218]	_	_
30-21	3973-3979	better	abstract[218]	giv[218]	_	_
30-22	3980-3991	performance	abstract[218]	giv[218]	_	_
30-23	3992-3996	with	abstract[218]	giv[218]	_	_
30-24	3997-4003	models	abstract[218]|abstract	giv[218]|giv	coref	34-14[240_0]
30-25	4004-4009	using	_	_	_	_
30-26	4010-4017	shorter	abstract[220]	new[220]	_	_
30-27	4018-4025	context	abstract[220]	new[220]	_	_
30-28	4026-4027	.	_	_	_	_

#Text=6.1 .
31-1	4028-4031	6.1	abstract	new	_	_
31-2	4032-4033	.	_	_	_	_

#Text=General Observations
32-1	4034-4041	General	person|abstract[223]	new|new[223]	_	_
32-2	4042-4054	Observations	abstract[223]	new[223]	_	_

#Text=Due to data sparsity , n-gram coverage in the test set ( the fraction of n-grams in the test data that can be found in the training set ) is low ( see Table 10 ) .
33-1	4055-4058	Due	_	_	_	_
33-2	4059-4061	to	_	_	_	_
33-3	4062-4066	data	abstract|abstract[225]	giv|new[225]	coref|coref	33-18[232_0]|33-18[232_0]
33-4	4067-4075	sparsity	abstract[225]	new[225]	_	_
33-5	4076-4077	,	_	_	_	_
33-6	4078-4084	n-gram	quantity[226]	new[226]	_	_
33-7	4085-4093	coverage	quantity[226]	new[226]	_	_
33-8	4094-4096	in	quantity[226]	new[226]	_	_
33-9	4097-4100	the	quantity[226]|abstract[228]	new[226]|giv[228]	coref	33-26[234_228]
33-10	4101-4105	test	quantity[226]|abstract|abstract[228]	new[226]|giv|giv[228]	coref	33-19
33-11	4106-4109	set	quantity[226]|abstract[228]	new[226]|giv[228]	_	_
33-12	4110-4111	(	_	_	_	_
33-13	4112-4115	the	quantity[229]	new[229]	_	_
33-14	4116-4124	fraction	quantity[229]	new[229]	_	_
33-15	4125-4127	of	quantity[229]	new[229]	_	_
33-16	4128-4135	n-grams	quantity[229]|quantity[230]	new[229]|giv[230]	_	_
33-17	4136-4138	in	quantity[229]|quantity[230]	new[229]|giv[230]	_	_
33-18	4139-4142	the	quantity[229]|quantity[230]|abstract[232]	new[229]|giv[230]|giv[232]	_	_
33-19	4143-4147	test	quantity[229]|quantity[230]|abstract|abstract[232]	new[229]|giv[230]|giv|giv[232]	coref	34-9
33-20	4148-4152	data	quantity[229]|quantity[230]|abstract[232]	new[229]|giv[230]|giv[232]	_	_
33-21	4153-4157	that	_	_	_	_
33-22	4158-4161	can	_	_	_	_
33-23	4162-4164	be	_	_	_	_
33-24	4165-4170	found	_	_	_	_
33-25	4171-4173	in	_	_	_	_
33-26	4174-4177	the	abstract[234]	giv[234]	coref	34-8[239_234]
33-27	4178-4186	training	abstract|abstract[234]	giv|giv[234]	_	_
33-28	4187-4190	set	abstract[234]	giv[234]	_	_
33-29	4191-4192	)	_	_	_	_
33-30	4193-4195	is	_	_	_	_
33-31	4196-4199	low	_	_	_	_
33-32	4200-4201	(	_	_	_	_
33-33	4202-4205	see	_	_	_	_
33-34	4206-4211	Table	abstract[235]	new[235]	ana	34-1[0_235]
33-35	4212-4214	10	abstract[235]	new[235]	_	_
33-36	4215-4216	)	_	_	_	_
33-37	4217-4218	.	_	_	_	_

#Text=It means that many multi-word tokens from the test set are known to n-gram models as separate unigrams , but not in the form of a single n-gram .
34-1	4219-4221	It	abstract	giv	_	_
34-2	4222-4227	means	_	_	_	_
34-3	4228-4232	that	_	_	_	_
34-4	4233-4237	many	object[237]	giv[237]	coref	36-7[252_237]
34-5	4238-4248	multi-word	object[237]	giv[237]	_	_
34-6	4249-4255	tokens	object[237]	giv[237]	_	_
34-7	4256-4260	from	object[237]	giv[237]	_	_
34-8	4261-4264	the	object[237]|abstract[239]	giv[237]|giv[239]	_	_
34-9	4265-4269	test	object[237]|abstract|abstract[239]	giv[237]|giv|giv[239]	_	_
34-10	4270-4273	set	object[237]|abstract[239]	giv[237]|giv[239]	_	_
34-11	4274-4277	are	_	_	_	_
34-12	4278-4283	known	_	_	_	_
34-13	4284-4286	to	_	_	_	_
34-14	4287-4293	n-gram	abstract[240]	giv[240]	coref	37-6[0_240]
34-15	4294-4300	models	abstract[240]	giv[240]	_	_
34-16	4301-4303	as	abstract[240]	giv[240]	_	_
34-17	4304-4312	separate	abstract[240]	giv[240]	_	_
34-18	4313-4321	unigrams	abstract[240]	giv[240]	_	_
34-19	4322-4323	,	_	_	_	_
34-20	4324-4327	but	_	_	_	_
34-21	4328-4331	not	_	_	_	_
34-22	4332-4334	in	_	_	_	_
34-23	4335-4338	the	abstract[241]	new[241]	_	_
34-24	4339-4343	form	abstract[241]	new[241]	_	_
34-25	4344-4346	of	abstract[241]	new[241]	_	_
34-26	4347-4348	a	abstract[241]|object[242]	new[241]|new[242]	_	_
34-27	4349-4355	single	abstract[241]|object[242]	new[241]|new[242]	_	_
34-28	4356-4362	n-gram	abstract[241]|object[242]	new[241]|new[242]	_	_
34-29	4363-4364	.	_	_	_	_

#Text=The Stupid Backoff model with a backoff factor for unigrams set to a moderate value ( such as 0.09 ) is able to segment such strings correctly .
35-1	4365-4368	The	abstract[244]	giv[244]	coref	37-18[260_244]
35-2	4369-4375	Stupid	abstract[244]	giv[244]	_	_
35-3	4376-4383	Backoff	event|abstract[244]	giv|giv[244]	coref	35-7
35-4	4384-4389	model	abstract[244]	giv[244]	_	_
35-5	4390-4394	with	abstract[244]	giv[244]	_	_
35-6	4395-4396	a	abstract[244]|abstract[246]	giv[244]|new[246]	coref	37-26[0_246]
35-7	4397-4404	backoff	abstract[244]|abstract|abstract[246]	giv[244]|giv|new[246]	coref	37-21[261_0]
35-8	4405-4411	factor	abstract[244]|abstract[246]	giv[244]|new[246]	_	_
35-9	4412-4415	for	abstract[244]|abstract[246]	giv[244]|new[246]	_	_
35-10	4416-4424	unigrams	abstract[244]|abstract[246]|abstract	giv[244]|new[246]|new	coref	36-22[254_0]
35-11	4425-4428	set	_	_	_	_
35-12	4429-4431	to	_	_	_	_
35-13	4432-4433	a	abstract[248]	new[248]	ana	36-3[0_248]
35-14	4434-4442	moderate	abstract[248]	new[248]	_	_
35-15	4443-4448	value	abstract[248]	new[248]	_	_
35-16	4449-4450	(	abstract[248]	new[248]	_	_
35-17	4451-4455	such	abstract[248]	new[248]	_	_
35-18	4456-4458	as	abstract[248]	new[248]	_	_
35-19	4459-4463	0.09	abstract[248]|quantity	new[248]|new	_	_
35-20	4464-4465	)	abstract[248]	new[248]	_	_
35-21	4466-4468	is	_	_	_	_
35-22	4469-4473	able	_	_	_	_
35-23	4474-4476	to	_	_	_	_
35-24	4477-4484	segment	_	_	_	_
35-25	4485-4489	such	abstract[250]	giv[250]	_	_
35-26	4490-4497	strings	abstract[250]	giv[250]	_	_
35-27	4498-4507	correctly	_	_	_	_
35-28	4508-4509	.	_	_	_	_

#Text=However , it also erroneously segments some OoV single-word tokens whose surface forms happen to be interpretable as a sequence of concatenated in-vocabulary unigrams , resulting in lower Precision .
36-1	4510-4517	However	_	_	_	_
36-2	4518-4519	,	_	_	_	_
36-3	4520-4522	it	abstract	giv	_	_
36-4	4523-4527	also	_	_	_	_
36-5	4528-4539	erroneously	_	_	_	_
36-6	4540-4548	segments	_	_	_	_
36-7	4549-4553	some	object[252]	giv[252]	_	_
36-8	4554-4557	OoV	object[252]	giv[252]	_	_
36-9	4558-4569	single-word	object[252]	giv[252]	_	_
36-10	4570-4576	tokens	object[252]	giv[252]	_	_
36-11	4577-4582	whose	place[253]	new[253]	_	_
36-12	4583-4590	surface	place[253]	new[253]	_	_
36-13	4591-4596	forms	_	_	_	_
36-14	4597-4603	happen	_	_	_	_
36-15	4604-4606	to	_	_	_	_
36-16	4607-4609	be	_	_	_	_
36-17	4610-4623	interpretable	_	_	_	_
36-18	4624-4626	as	_	_	_	_
36-19	4627-4628	a	_	_	_	_
36-20	4629-4637	sequence	_	_	_	_
36-21	4638-4640	of	_	_	_	_
36-22	4641-4653	concatenated	abstract[254]	giv[254]	coref	37-11[258_254]
36-23	4654-4667	in-vocabulary	abstract[254]	giv[254]	_	_
36-24	4668-4676	unigrams	abstract[254]	giv[254]	_	_
36-25	4677-4678	,	_	_	_	_
36-26	4679-4688	resulting	_	_	_	_
36-27	4689-4691	in	_	_	_	_
36-28	4692-4697	lower	abstract[255]	giv[255]	_	_
36-29	4698-4707	Precision	abstract[255]	giv[255]	_	_
36-30	4708-4709	.	_	_	_	_

#Text=On the other hand , models assigning low scores to unigrams ( such as a 4- or 5-gram model with the Stupid Backoff and backoff factor set as suggested by Brants et al. , and in particular the model applying modified Kneser-Ney smoothing ) are better at handling OoV words ( see Table 11 ) , but as a result of probability multiplication , in many cases they score unseen multi-word segments higher than a sequence of unigrams into which the given segment should be divided , hence yielding lower Recall .
37-1	4710-4712	On	_	_	_	_
37-2	4713-4716	the	_	_	_	_
37-3	4717-4722	other	_	_	_	_
37-4	4723-4727	hand	_	_	_	_
37-5	4728-4729	,	_	_	_	_
37-6	4730-4736	models	abstract	giv	_	_
37-7	4737-4746	assigning	_	_	_	_
37-8	4747-4750	low	abstract[257]	new[257]	_	_
37-9	4751-4757	scores	abstract[257]	new[257]	_	_
37-10	4758-4760	to	_	_	_	_
37-11	4761-4769	unigrams	abstract[258]	giv[258]	coref	37-78[0_258]
37-12	4770-4771	(	abstract[258]	giv[258]	_	_
37-13	4772-4776	such	abstract[258]	giv[258]	_	_
37-14	4777-4779	as	abstract[258]	giv[258]	_	_
37-15	4780-4781	a	abstract[258]|abstract[259]	giv[258]|new[259]	_	_
37-16	4782-4784	4-	abstract[258]|abstract[259]	giv[258]|new[259]	_	_
37-17	4785-4787	or	abstract[258]	giv[258]	_	_
37-18	4788-4794	5-gram	abstract[258]|abstract[260]	giv[258]|giv[260]	coref	37-38[266_260]
37-19	4795-4800	model	abstract[258]|abstract[260]	giv[258]|giv[260]	_	_
37-20	4801-4805	with	abstract[258]|abstract[260]	giv[258]|giv[260]	_	_
37-21	4806-4809	the	abstract[258]|abstract[260]|event[261]|abstract[262]	giv[258]|giv[260]|giv[261]|new[262]	coref|ana|coref|ana	37-25[0_261]|37-68[0_262]|37-25[0_261]|37-68[0_262]
37-22	4810-4816	Stupid	abstract[258]|abstract[260]|event[261]|abstract[262]	giv[258]|giv[260]|giv[261]|new[262]	_	_
37-23	4817-4824	Backoff	abstract[258]|abstract[260]|event[261]|abstract[262]	giv[258]|giv[260]|giv[261]|new[262]	_	_
37-24	4825-4828	and	abstract[258]|abstract[260]|abstract[262]	giv[258]|giv[260]|new[262]	_	_
37-25	4829-4836	backoff	abstract[258]|abstract[260]|abstract[262]|event	giv[258]|giv[260]|new[262]|giv	_	_
37-26	4837-4843	factor	abstract[258]|abstract[260]|abstract[262]|abstract	giv[258]|giv[260]|new[262]|giv	_	_
37-27	4844-4847	set	_	_	_	_
37-28	4848-4850	as	_	_	_	_
37-29	4851-4860	suggested	_	_	_	_
37-30	4861-4863	by	_	_	_	_
37-31	4864-4870	Brants	person	giv	_	_
37-32	4871-4873	et	_	_	_	_
37-33	4874-4877	al.	_	_	_	_
37-34	4878-4879	,	_	_	_	_
37-35	4880-4883	and	_	_	_	_
37-36	4884-4886	in	_	_	_	_
37-37	4887-4897	particular	_	_	_	_
37-38	4898-4901	the	abstract[266]	giv[266]	_	_
37-39	4902-4907	model	abstract[266]	giv[266]	_	_
37-40	4908-4916	applying	_	_	_	_
37-41	4917-4925	modified	abstract[268]	giv[268]	_	_
37-42	4926-4936	Kneser-Ney	person|abstract[268]	giv|giv[268]	_	_
37-43	4937-4946	smoothing	abstract[268]	giv[268]	_	_
37-44	4947-4948	)	_	_	_	_
37-45	4949-4952	are	_	_	_	_
37-46	4953-4959	better	_	_	_	_
37-47	4960-4962	at	_	_	_	_
37-48	4963-4971	handling	_	_	_	_
37-49	4972-4975	OoV	abstract|abstract[270]	new|giv[270]	_	_
37-50	4976-4981	words	abstract[270]	giv[270]	_	_
37-51	4982-4983	(	_	_	_	_
37-52	4984-4987	see	_	_	_	_
37-53	4988-4993	Table	abstract[271]	new[271]	_	_
37-54	4994-4996	11	abstract[271]	new[271]	_	_
37-55	4997-4998	)	_	_	_	_
37-56	4999-5000	,	_	_	_	_
37-57	5001-5004	but	_	_	_	_
37-58	5005-5007	as	_	_	_	_
37-59	5008-5009	a	_	_	_	_
37-60	5010-5016	result	_	_	_	_
37-61	5017-5019	of	_	_	_	_
37-62	5020-5031	probability	abstract|abstract[273]	new|new[273]	_	_
37-63	5032-5046	multiplication	abstract[273]	new[273]	_	_
37-64	5047-5048	,	_	_	_	_
37-65	5049-5051	in	_	_	_	_
37-66	5052-5056	many	abstract[274]	giv[274]	_	_
37-67	5057-5062	cases	abstract[274]	giv[274]	_	_
37-68	5063-5067	they	abstract	giv	_	_
37-69	5068-5073	score	_	_	_	_
37-70	5074-5080	unseen	abstract[276]	new[276]	_	_
37-71	5081-5091	multi-word	abstract[276]	new[276]	_	_
37-72	5092-5100	segments	abstract[276]	new[276]	_	_
37-73	5101-5107	higher	abstract[276]	new[276]	_	_
37-74	5108-5112	than	_	_	_	_
37-75	5113-5114	a	abstract[277]	new[277]	_	_
37-76	5115-5123	sequence	abstract[277]	new[277]	_	_
37-77	5124-5126	of	abstract[277]	new[277]	_	_
37-78	5127-5135	unigrams	abstract[277]|abstract	new[277]|giv	_	_
37-79	5136-5140	into	_	_	_	_
37-80	5141-5146	which	_	_	_	_
37-81	5147-5150	the	abstract[279]	giv[279]	_	_
37-82	5151-5156	given	abstract[279]	giv[279]	_	_
37-83	5157-5164	segment	abstract[279]	giv[279]	_	_
37-84	5165-5171	should	_	_	_	_
37-85	5172-5174	be	_	_	_	_
37-86	5175-5182	divided	_	_	_	_
37-87	5183-5184	,	_	_	_	_
37-88	5185-5190	hence	_	_	_	_
37-89	5191-5199	yielding	_	_	_	_
37-90	5200-5205	lower	_	_	_	_
37-91	5206-5212	Recall	_	_	_	_
37-92	5213-5214	.	_	_	_	_
