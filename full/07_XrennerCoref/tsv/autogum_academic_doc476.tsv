#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=3 .
1-1	0-1	3	quantity	new	_	_
1-2	2-3	.	_	_	_	_

#Text=Advantages of Deep Learning Methods
2-1	4-14	Advantages	abstract[2]	new[2]	_	_
2-2	15-17	of	abstract[2]	new[2]	_	_
2-3	18-22	Deep	abstract[2]|abstract[4]	new[2]|new[4]	coref	3-11[8_4]
2-4	23-31	Learning	abstract[2]|person|abstract[4]	new[2]|new|new[4]	_	_
2-5	32-39	Methods	abstract[2]|abstract[4]	new[2]|new[4]	_	_

#Text=To overcome the aforementioned challenges , various image processing and machine learning methods have been proposed and have so far achieved great progress .
3-1	40-42	To	_	_	_	_
3-2	43-51	overcome	_	_	_	_
3-3	52-55	the	abstract[5]	new[5]	_	_
3-4	56-70	aforementioned	abstract[5]	new[5]	_	_
3-5	71-81	challenges	abstract[5]	new[5]	_	_
3-6	82-83	,	_	_	_	_
3-7	84-91	various	abstract[7]	new[7]	_	_
3-8	92-97	image	object|abstract[7]	new|new[7]	coref	5-18
3-9	98-108	processing	abstract[7]	new[7]	_	_
3-10	109-112	and	_	_	_	_
3-11	113-120	machine	abstract[8]	giv[8]	coref	4-11[14_8]
3-12	121-129	learning	abstract[8]	giv[8]	_	_
3-13	130-137	methods	abstract[8]	giv[8]	_	_
3-14	138-142	have	_	_	_	_
3-15	143-147	been	_	_	_	_
3-16	148-156	proposed	_	_	_	_
3-17	157-160	and	_	_	_	_
3-18	161-165	have	_	_	_	_
3-19	166-168	so	_	_	_	_
3-20	169-172	far	_	_	_	_
3-21	173-181	achieved	_	_	_	_
3-22	182-187	great	abstract[9]	new[9]	_	_
3-23	188-196	progress	abstract[9]	new[9]	_	_
3-24	197-198	.	_	_	_	_

#Text=However , it is important to note the advantages of deep learning methods over non-deep-learning methods ( also called shallow-learning methods ) .
4-1	199-206	However	_	_	_	_
4-2	207-208	,	_	_	_	_
4-3	209-211	it	abstract	new	cata	4-3[0_11]
4-4	212-214	is	_	_	_	_
4-5	215-224	important	_	_	_	_
4-6	225-227	to	abstract[11]	new[11]	_	_
4-7	228-232	note	abstract[11]	new[11]	_	_
4-8	233-236	the	abstract[11]|abstract[12]	new[11]|new[12]	_	_
4-9	237-247	advantages	abstract[11]|abstract[12]	new[11]|new[12]	_	_
4-10	248-250	of	abstract[11]|abstract[12]	new[11]|new[12]	_	_
4-11	251-255	deep	abstract[11]|abstract[12]|abstract[14]	new[11]|new[12]|giv[14]	coref	4-20[15_14]
4-12	256-264	learning	abstract[11]|abstract[12]|abstract|abstract[14]	new[11]|new[12]|new|giv[14]	coref	5-15
4-13	265-272	methods	abstract[11]|abstract[12]|abstract[14]	new[11]|new[12]|giv[14]	_	_
4-14	273-277	over	abstract[11]|abstract[12]|abstract[14]	new[11]|new[12]|giv[14]	_	_
4-15	278-295	non-deep-learning	abstract[11]|abstract[12]|abstract[14]	new[11]|new[12]|giv[14]	_	_
4-16	296-303	methods	abstract[11]|abstract[12]|abstract[14]	new[11]|new[12]|giv[14]	_	_
4-17	304-305	(	_	_	_	_
4-18	306-310	also	_	_	_	_
4-19	311-317	called	_	_	_	_
4-20	318-334	shallow-learning	abstract[15]	giv[15]	coref	22-4[146_15]
4-21	335-342	methods	abstract[15]	giv[15]	_	_
4-22	343-344	)	_	_	_	_
4-23	345-346	.	_	_	_	_

#Text=Currently , convolutional neural networks ( CNNs ) are the most frequently used deep learning model for image data classification , including tumor detection in pathology images of breast cancer , renal cell carcinoma , prostate cancer , and head and neck cancer .
5-1	347-356	Currently	_	_	_	_
5-2	357-358	,	_	_	_	_
5-3	359-372	convolutional	place[16]	new[16]	coref	5-10[19_16]
5-4	373-379	neural	place[16]	new[16]	_	_
5-5	380-388	networks	place[16]	new[16]	_	_
5-6	389-390	(	_	_	_	_
5-7	391-395	CNNs	organization	new	coref	6-10[37_0]
5-8	396-397	)	_	_	_	_
5-9	398-401	are	_	_	_	_
5-10	402-405	the	place[19]	giv[19]	coref	6-16[40_19]
5-11	406-410	most	place[19]	giv[19]	_	_
5-12	411-421	frequently	place[19]	giv[19]	_	_
5-13	422-426	used	place[19]	giv[19]	_	_
5-14	427-431	deep	place[19]	giv[19]	_	_
5-15	432-440	learning	abstract|place[19]	giv|giv[19]	coref	8-8
5-16	441-446	model	place[19]	giv[19]	_	_
5-17	447-450	for	place[19]	giv[19]	_	_
5-18	451-456	image	place[19]|object|abstract[22]	giv[19]|giv|new[22]	coref|coref|coref|coref	6-12|7-27[51_22]|6-12|7-27[51_22]
5-19	457-461	data	place[19]|abstract|abstract[22]	giv[19]|new|new[22]	coref	20-25[132_0]
5-20	462-476	classification	place[19]|abstract[22]	giv[19]|new[22]	_	_
5-21	477-478	,	place[19]	giv[19]	_	_
5-22	479-488	including	place[19]	giv[19]	_	_
5-23	489-494	tumor	place[19]|abstract|abstract[24]	giv[19]|new|new[24]	coref|coref	8-30[66_24]|8-30[66_24]
5-24	495-504	detection	place[19]|abstract[24]	giv[19]|new[24]	_	_
5-25	505-507	in	place[19]|abstract[24]	giv[19]|new[24]	_	_
5-26	508-517	pathology	place[19]|abstract[24]|abstract|object[26]	giv[19]|new[24]|new|new[26]	coref|coref|coref|coref	8-24|8-24[62_26]|8-24|8-24[62_26]
5-27	518-524	images	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-28	525-527	of	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-29	528-534	breast	place[19]|abstract[24]|object[26]|abstract[27]	giv[19]|new[24]|new[26]|new[27]	_	_
5-30	535-541	cancer	place[19]|abstract[24]|object[26]|abstract[27]	giv[19]|new[24]|new[26]|new[27]	_	_
5-31	542-543	,	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-32	544-549	renal	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-33	550-554	cell	place[19]|abstract[24]|object[26]|place|object[29]	giv[19]|new[24]|new[26]|new|new[29]	coref|coref	8-30|8-30
5-34	555-564	carcinoma	place[19]|abstract[24]|object[26]|object[29]	giv[19]|new[24]|new[26]|new[29]	_	_
5-35	565-566	,	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-36	567-575	prostate	place[19]|abstract[24]|object[26]|place|abstract[31]	giv[19]|new[24]|new[26]|new|new[31]	_	_
5-37	576-582	cancer	place[19]|abstract[24]|object[26]|abstract[31]	giv[19]|new[24]|new[26]|new[31]	_	_
5-38	583-584	,	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-39	585-588	and	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-40	589-593	head	place[19]|abstract[24]|object[26]|person	giv[19]|new[24]|new[26]|new	_	_
5-41	594-597	and	place[19]|abstract[24]|object[26]	giv[19]|new[24]|new[26]	_	_
5-42	598-602	neck	place[19]|abstract[24]|object[26]|object|abstract[34]	giv[19]|new[24]|new[26]|new|new[34]	_	_
5-43	603-609	cancer	place[19]|abstract[24]|object[26]|abstract[34]	giv[19]|new[24]|new[26]|new[34]	_	_
5-44	610-611	.	_	_	_	_

#Text=Several forms of neural network have been derived from CNNs for image segmentation , including fully convolutional networks ( FCNs ) and mask-regional convolutional neural networks ( mask-RCNNs ) .
6-1	612-619	Several	abstract[35]	new[35]	_	_
6-2	620-625	forms	abstract[35]	new[35]	_	_
6-3	626-628	of	abstract[35]	new[35]	_	_
6-4	629-635	neural	abstract[35]|abstract[36]	new[35]|new[36]	_	_
6-5	636-643	network	abstract[35]|abstract[36]	new[35]|new[36]	_	_
6-6	644-648	have	_	_	_	_
6-7	649-653	been	_	_	_	_
6-8	654-661	derived	_	_	_	_
6-9	662-666	from	_	_	_	_
6-10	667-671	CNNs	organization[37]	giv[37]	coref	13-10[0_37]
6-11	672-675	for	organization[37]	giv[37]	_	_
6-12	676-681	image	organization[37]|object|abstract[39]	giv[37]|giv|new[39]	coref|coref|coref|coref	7-28|7-31[53_39]|7-28|7-31[53_39]
6-13	682-694	segmentation	organization[37]|abstract[39]	giv[37]|new[39]	_	_
6-14	695-696	,	_	_	_	_
6-15	697-706	including	_	_	_	_
6-16	707-712	fully	place[40]	giv[40]	coref	6-23[42_40]
6-17	713-726	convolutional	place[40]	giv[40]	_	_
6-18	727-735	networks	place[40]	giv[40]	_	_
6-19	736-737	(	_	_	_	_
6-20	738-742	FCNs	object	new	_	_
6-21	743-744	)	_	_	_	_
6-22	745-748	and	_	_	_	_
6-23	749-762	mask-regional	place[42]	giv[42]	coref	7-1[44_42]
6-24	763-776	convolutional	place[42]	giv[42]	_	_
6-25	777-783	neural	place[42]	giv[42]	_	_
6-26	784-792	networks	place[42]	giv[42]	_	_
6-27	793-794	(	_	_	_	_
6-28	795-805	mask-RCNNs	abstract	new	_	_
6-29	806-807	)	_	_	_	_
6-30	808-809	.	_	_	_	_

#Text=Recurrent neural networks ( RNNs ) , which are well known for modeling dynamic sequence behavior such as speech recognition , have also been explored in multi-label image classification and image segmentation .
7-1	810-819	Recurrent	place[44]	giv[44]	coref	9-4[71_44]
7-2	820-826	neural	place[44]	giv[44]	_	_
7-3	827-835	networks	place[44]	giv[44]	_	_
7-4	836-837	(	_	_	_	_
7-5	838-842	RNNs	abstract	new	_	_
7-6	843-844	)	_	_	_	_
7-7	845-846	,	_	_	_	_
7-8	847-852	which	_	_	_	_
7-9	853-856	are	_	_	_	_
7-10	857-861	well	_	_	_	_
7-11	862-867	known	_	_	_	_
7-12	868-871	for	_	_	_	_
7-13	872-880	modeling	_	_	_	_
7-14	881-888	dynamic	abstract[47]	new[47]	_	_
7-15	889-897	sequence	abstract|abstract[47]	new|new[47]	_	_
7-16	898-906	behavior	abstract[47]	new[47]	_	_
7-17	907-911	such	abstract[47]	new[47]	_	_
7-18	912-914	as	abstract[47]	new[47]	_	_
7-19	915-921	speech	abstract[47]|abstract|abstract[49]	new[47]|new|new[49]	_	_
7-20	922-933	recognition	abstract[47]|abstract[49]	new[47]|new[49]	_	_
7-21	934-935	,	_	_	_	_
7-22	936-940	have	_	_	_	_
7-23	941-945	also	_	_	_	_
7-24	946-950	been	_	_	_	_
7-25	951-959	explored	_	_	_	_
7-26	960-962	in	_	_	_	_
7-27	963-974	multi-label	abstract[51]	giv[51]	_	_
7-28	975-980	image	object|abstract[51]	giv|giv[51]	coref	7-31
7-29	981-995	classification	abstract[51]	giv[51]	_	_
7-30	996-999	and	_	_	_	_
7-31	1000-1005	image	object|abstract[53]	giv|giv[53]	coref|coref	8-34|8-34
7-32	1006-1018	segmentation	abstract[53]	giv[53]	_	_
7-33	1019-1020	.	_	_	_	_

#Text=In additional to the aforementioned supervised deep learning models , autoencoder , an unsupervised deep learning model , has shown ability in analyzing pathology images through pre-training models , cell detection , and image feature extraction .
8-1	1021-1023	In	_	_	_	_
8-2	1024-1034	additional	_	_	_	_
8-3	1035-1037	to	_	_	_	_
8-4	1038-1041	the	abstract[55]|abstract[56]	new[55]|new[56]	coref|coref|coref|coref	8-27[63_55]|8-27[64_56]|8-27[63_55]|8-27[64_56]
8-5	1042-1056	aforementioned	abstract[55]|abstract[56]	new[55]|new[56]	_	_
8-6	1057-1067	supervised	abstract[55]|abstract[56]	new[55]|new[56]	_	_
8-7	1068-1072	deep	abstract[55]|abstract[56]	new[55]|new[56]	_	_
8-8	1073-1081	learning	abstract|abstract[55]|abstract[56]	giv|new[55]|new[56]	coref	8-16
8-9	1082-1088	models	abstract[55]|abstract[56]	new[55]|new[56]	_	_
8-10	1089-1090	,	abstract[56]	new[56]	_	_
8-11	1091-1102	autoencoder	abstract[56]|object	new[56]|new	_	_
8-12	1103-1104	,	abstract[56]	new[56]	_	_
8-13	1105-1107	an	abstract[56]|abstract[59]	new[56]|new[59]	coref	17-2[99_59]
8-14	1108-1120	unsupervised	abstract[56]|abstract[59]	new[56]|new[59]	_	_
8-15	1121-1125	deep	abstract[56]|abstract[59]	new[56]|new[59]	_	_
8-16	1126-1134	learning	abstract[56]|abstract|abstract[59]	new[56]|giv|new[59]	coref	14-17[85_0]
8-17	1135-1140	model	abstract[56]|abstract[59]	new[56]|new[59]	_	_
8-18	1141-1142	,	_	_	_	_
8-19	1143-1146	has	_	_	_	_
8-20	1147-1152	shown	_	_	_	_
8-21	1153-1160	ability	abstract	new	_	_
8-22	1161-1163	in	_	_	_	_
8-23	1164-1173	analyzing	_	_	_	_
8-24	1174-1183	pathology	abstract|object[62]	giv|giv[62]	coref|coref|coref|coref	26-6|26-18[176_62]|26-6|26-18[176_62]
8-25	1184-1190	images	object[62]	giv[62]	_	_
8-26	1191-1198	through	object[62]	giv[62]	_	_
8-27	1199-1211	pre-training	object[62]|abstract[63]|abstract[64]	giv[62]|giv[63]|giv[64]	coref|coref	22-25[0_63]|22-25[0_63]
8-28	1212-1218	models	object[62]|abstract[63]|abstract[64]	giv[62]|giv[63]|giv[64]	_	_
8-29	1219-1220	,	object[62]|abstract[64]	giv[62]|giv[64]	_	_
8-30	1221-1225	cell	object[62]|abstract[64]|object|abstract[66]	giv[62]|giv[64]|giv|giv[66]	_	_
8-31	1226-1235	detection	object[62]|abstract[64]|abstract[66]	giv[62]|giv[64]|giv[66]	_	_
8-32	1236-1237	,	object[62]|abstract[64]	giv[62]|giv[64]	_	_
8-33	1238-1241	and	object[62]|abstract[64]	giv[62]|giv[64]	_	_
8-34	1242-1247	image	object[62]|abstract[64]|object|abstract[69]	giv[62]|giv[64]|giv|new[69]	coref|coref|coref|coref	9-10|20-19[130_69]|9-10|20-19[130_69]
8-35	1248-1255	feature	object[62]|abstract[64]|abstract|abstract[69]	giv[62]|giv[64]|new|new[69]	coref	17-10[102_0]
8-36	1256-1266	extraction	object[62]|abstract[64]|abstract[69]	giv[62]|giv[64]|new[69]	_	_
8-37	1267-1268	.	_	_	_	_

#Text=The taxonomy of the common neural networks used in image analysis is summarized in
9-1	1269-1272	The	abstract[70]	new[70]	_	_
9-2	1273-1281	taxonomy	abstract[70]	new[70]	_	_
9-3	1282-1284	of	abstract[70]	new[70]	_	_
9-4	1285-1288	the	abstract[70]|place[71]	new[70]|giv[71]	coref	13-6[80_71]
9-5	1289-1295	common	abstract[70]|place[71]	new[70]|giv[71]	_	_
9-6	1296-1302	neural	abstract[70]|place[71]	new[70]|giv[71]	_	_
9-7	1303-1311	networks	abstract[70]|place[71]	new[70]|giv[71]	_	_
9-8	1312-1316	used	_	_	_	_
9-9	1317-1319	in	_	_	_	_
9-10	1320-1325	image	object|abstract[73]	giv|new[73]	coref|coref|coref|coref	19-25[122_0]|19-27[0_73]|19-25[122_0]|19-27[0_73]
9-11	1326-1334	analysis	abstract[73]	new[73]	_	_
9-12	1335-1337	is	_	_	_	_
9-13	1338-1348	summarized	_	_	_	_
9-14	1349-1351	in	_	_	_	_

#Text=Figure 2
10-1	1352-1358	Figure	abstract[74]	new[74]	_	_
10-2	1359-1360	2	abstract[74]	new[74]	_	_

#Text=.
11-1	1361-1362	.	_	_	_	_

#Text=3.1 .
12-1	1363-1366	3.1	abstract	new	_	_
12-2	1367-1368	.	_	_	_	_

#Text=Inherent Characteristics and Advantages of Convolutional Neural Networks ( CNNs )
13-1	1369-1377	Inherent	abstract[76]|abstract[77]	new[76]|new[77]	coref|coref	20-6[127_77]|20-6[127_77]
13-2	1378-1393	Characteristics	abstract[76]|abstract[77]	new[76]|new[77]	_	_
13-3	1394-1397	and	abstract[77]	new[77]	_	_
13-4	1398-1408	Advantages	abstract[77]|abstract[78]	new[77]|new[78]	coref	23-10[155_78]
13-5	1409-1411	of	abstract[77]|abstract[78]	new[77]|new[78]	_	_
13-6	1412-1425	Convolutional	abstract[77]|abstract[78]|abstract[80]	new[77]|new[78]|giv[80]	coref	14-10[84_80]
13-7	1426-1432	Neural	abstract[77]|abstract[78]|abstract|abstract[80]	new[77]|new[78]|new|giv[80]	_	_
13-8	1433-1441	Networks	abstract[77]|abstract[78]|abstract[80]	new[77]|new[78]|giv[80]	_	_
13-9	1442-1443	(	_	_	_	_
13-10	1444-1448	CNNs	object	giv	coref	32-4
13-11	1449-1450	)	_	_	_	_

#Text=Inspired by the working mechanisms of the brain , deep neural networks , also called “ deep learning ” , have one or more “ hidden ” layers between the input and output layers .
14-1	1451-1459	Inspired	_	_	_	_
14-2	1460-1462	by	_	_	_	_
14-3	1463-1466	the	abstract[82]	new[82]	_	_
14-4	1467-1474	working	abstract[82]	new[82]	_	_
14-5	1475-1485	mechanisms	abstract[82]	new[82]	_	_
14-6	1486-1488	of	abstract[82]	new[82]	_	_
14-7	1489-1492	the	abstract[82]|object[83]	new[82]|new[83]	_	_
14-8	1493-1498	brain	abstract[82]|object[83]	new[82]|new[83]	_	_
14-9	1499-1500	,	abstract[82]	new[82]	_	_
14-10	1501-1505	deep	abstract[82]|place[84]	new[82]|giv[84]	coref	21-4[139_84]
14-11	1506-1512	neural	abstract[82]|place[84]	new[82]|giv[84]	_	_
14-12	1513-1521	networks	abstract[82]|place[84]	new[82]|giv[84]	_	_
14-13	1522-1523	,	_	_	_	_
14-14	1524-1528	also	_	_	_	_
14-15	1529-1535	called	_	_	_	_
14-16	1536-1537	“	_	_	_	_
14-17	1538-1542	deep	abstract[85]	giv[85]	coref	20-3[0_85]
14-18	1543-1551	learning	abstract[85]	giv[85]	_	_
14-19	1552-1553	”	_	_	_	_
14-20	1554-1555	,	_	_	_	_
14-21	1556-1560	have	_	_	_	_
14-22	1561-1564	one	abstract[86]	new[86]	_	_
14-23	1565-1567	or	abstract[86]	new[86]	_	_
14-24	1568-1572	more	abstract[86]	new[86]	_	_
14-25	1573-1574	“	abstract[86]	new[86]	_	_
14-26	1575-1581	hidden	abstract[86]	new[86]	_	_
14-27	1582-1583	”	abstract[86]	new[86]	_	_
14-28	1584-1590	layers	abstract[86]	new[86]	_	_
14-29	1591-1598	between	abstract[86]	new[86]	_	_
14-30	1599-1602	the	abstract[86]|abstract[87]	new[86]|new[87]	coref	17-26[0_87]
14-31	1603-1608	input	abstract[86]|abstract[87]	new[86]|new[87]	_	_
14-32	1609-1612	and	abstract[86]	new[86]	_	_
14-33	1613-1619	output	abstract[86]|abstract|abstract[89]	new[86]|new|new[89]	coref|coref|coref|coref	16-14[97_0]|20-34[134_89]|16-14[97_0]|20-34[134_89]
14-34	1620-1626	layers	abstract[86]|abstract[89]	new[86]|new[89]	_	_
14-35	1627-1628	.	_	_	_	_

#Text=In each layer , there are many neurons , also called kernels .
15-1	1629-1631	In	_	_	_	_
15-2	1632-1636	each	abstract[90]	new[90]	coref	18-15[111_90]
15-3	1637-1642	layer	abstract[90]	new[90]	_	_
15-4	1643-1644	,	_	_	_	_
15-5	1645-1650	there	_	_	_	_
15-6	1651-1654	are	_	_	_	_
15-7	1655-1659	many	object[91]	new[91]	_	_
15-8	1660-1667	neurons	object[91]	new[91]	_	_
15-9	1668-1669	,	_	_	_	_
15-10	1670-1674	also	_	_	_	_
15-11	1675-1681	called	_	_	_	_
15-12	1682-1689	kernels	object	new	coref	20-37[135_0]
15-13	1690-1691	.	_	_	_	_

#Text=Each kernel ( usually a function in mathematics ) takes inputs and computes an output .
16-1	1692-1696	Each	object[93]	new[93]	coref	17-6[101_93]
16-2	1697-1703	kernel	object[93]	new[93]	_	_
16-3	1704-1705	(	_	_	_	_
16-4	1706-1713	usually	abstract[94]	new[94]	_	_
16-5	1714-1715	a	abstract[94]	new[94]	_	_
16-6	1716-1724	function	abstract[94]	new[94]	_	_
16-7	1725-1727	in	abstract[94]	new[94]	_	_
16-8	1728-1739	mathematics	abstract[94]|abstract	new[94]|new	_	_
16-9	1740-1741	)	_	_	_	_
16-10	1742-1747	takes	_	_	_	_
16-11	1748-1754	inputs	abstract	new	_	_
16-12	1755-1758	and	_	_	_	_
16-13	1759-1767	computes	abstract	new	none|none	16-13[0_206]|17-9
16-14	1768-1770	an	abstract[97]	giv[97]	_	_
16-15	1771-1777	output	abstract[97]	giv[97]	_	_
16-16	1778-1779	.	_	_	_	_

#Text=In a CNN model , a convolution kernel computes a feature at a specific location , called a “ receptive field ” , in the input space .
17-1	1780-1782	In	_	_	_	_
17-2	1783-1784	a	abstract[99]	giv[99]	coref	20-1[126_99]
17-3	1785-1788	CNN	object|abstract[99]	new|giv[99]	coref	19-20
17-4	1789-1794	model	abstract[99]	giv[99]	_	_
17-5	1795-1796	,	_	_	_	_
17-6	1797-1798	a	object[101]	giv[101]	_	_
17-7	1799-1810	convolution	abstract|object[101]	new|giv[101]	coref	18-27
17-8	1811-1817	kernel	object[101]	giv[101]	_	_
17-9	1818-1826	computes	abstract	new	coref	28-3[194_0]
17-10	1827-1828	a	abstract[102]	giv[102]	coref	18-22[0_102]
17-11	1829-1836	feature	abstract[102]	giv[102]	_	_
17-12	1837-1839	at	_	_	_	_
17-13	1840-1841	a	place[103]	new[103]	_	_
17-14	1842-1850	specific	place[103]	new[103]	_	_
17-15	1851-1859	location	place[103]	new[103]	_	_
17-16	1860-1861	,	_	_	_	_
17-17	1862-1868	called	_	_	_	_
17-18	1869-1870	a	abstract[104]	new[104]	_	_
17-19	1871-1872	“	abstract[104]	new[104]	_	_
17-20	1873-1882	receptive	abstract[104]	new[104]	_	_
17-21	1883-1888	field	abstract[104]	new[104]	_	_
17-22	1889-1890	”	abstract[104]	new[104]	_	_
17-23	1891-1892	,	_	_	_	_
17-24	1893-1895	in	_	_	_	_
17-25	1896-1899	the	place[106]	new[106]	_	_
17-26	1900-1905	input	abstract|place[106]	giv|new[106]	coref	18-16
17-27	1906-1911	space	place[106]	new[106]	_	_
17-28	1912-1913	.	_	_	_	_

#Text=The term “ convolutional ” denotes the operation of sliding the receptive fields through the input layer to generate the “ feature map ” from the convolution layer as the outputs .
18-1	1914-1917	The	abstract[107]	new[107]	_	_
18-2	1918-1922	term	abstract[107]	new[107]	_	_
18-3	1923-1924	“	abstract[107]	new[107]	_	_
18-4	1925-1938	convolutional	abstract[107]	new[107]	_	_
18-5	1939-1940	”	abstract[107]	new[107]	_	_
18-6	1941-1948	denotes	_	_	_	_
18-7	1949-1952	the	event[108]	new[108]	coref	19-4[116_108]
18-8	1953-1962	operation	event[108]	new[108]	_	_
18-9	1963-1965	of	_	_	_	_
18-10	1966-1973	sliding	_	_	_	_
18-11	1974-1977	the	abstract[109]	new[109]	_	_
18-12	1978-1987	receptive	abstract[109]	new[109]	_	_
18-13	1988-1994	fields	abstract[109]	new[109]	_	_
18-14	1995-2002	through	_	_	_	_
18-15	2003-2006	the	abstract[111]	giv[111]	coref	18-26[115_111]
18-16	2007-2012	input	abstract|abstract[111]	giv|giv[111]	coref	25-40
18-17	2013-2018	layer	abstract[111]	giv[111]	_	_
18-18	2019-2021	to	_	_	_	_
18-19	2022-2030	generate	_	_	_	_
18-20	2031-2034	the	object[113]	new[113]	_	_
18-21	2035-2036	“	object[113]	new[113]	_	_
18-22	2037-2044	feature	abstract|object[113]	giv|new[113]	coref	25-2
18-23	2045-2048	map	object[113]	new[113]	_	_
18-24	2049-2050	”	object[113]	new[113]	_	_
18-25	2051-2055	from	_	_	_	_
18-26	2056-2059	the	abstract[115]	giv[115]	_	_
18-27	2060-2071	convolution	substance|abstract[115]	giv|giv[115]	coref	33-50
18-28	2072-2077	layer	abstract[115]	giv[115]	_	_
18-29	2078-2080	as	abstract[115]	giv[115]	_	_
18-30	2081-2084	the	abstract[115]	giv[115]	_	_
18-31	2085-2092	outputs	abstract[115]	giv[115]	_	_
18-32	2093-2094	.	_	_	_	_

#Text=In essence , this operation was inspired by the functional mechanism of the visual cortex , and it makes CNN a great solution for many image analysis tasks .
19-1	2095-2097	In	_	_	_	_
19-2	2098-2105	essence	_	_	_	_
19-3	2106-2107	,	_	_	_	_
19-4	2108-2112	this	event[116]	giv[116]	coref	33-50[244_116]
19-5	2113-2122	operation	event[116]	giv[116]	_	_
19-6	2123-2126	was	_	_	_	_
19-7	2127-2135	inspired	_	_	_	_
19-8	2136-2138	by	_	_	_	_
19-9	2139-2142	the	abstract[117]	new[117]	_	_
19-10	2143-2153	functional	abstract[117]	new[117]	_	_
19-11	2154-2163	mechanism	abstract[117]	new[117]	_	_
19-12	2164-2166	of	abstract[117]	new[117]	_	_
19-13	2167-2170	the	abstract[117]|object[118]	new[117]|new[118]	ana	19-18[0_118]
19-14	2171-2177	visual	abstract[117]|object[118]	new[117]|new[118]	_	_
19-15	2178-2184	cortex	abstract[117]|object[118]	new[117]|new[118]	_	_
19-16	2185-2186	,	_	_	_	_
19-17	2187-2190	and	_	_	_	_
19-18	2191-2193	it	object	giv	_	_
19-19	2194-2199	makes	_	_	_	_
19-20	2200-2203	CNN	object	giv	coref	33-35
19-21	2204-2205	a	abstract[121]	new[121]	_	_
19-22	2206-2211	great	abstract[121]	new[121]	_	_
19-23	2212-2220	solution	abstract[121]	new[121]	_	_
19-24	2221-2224	for	abstract[121]	new[121]	_	_
19-25	2225-2229	many	abstract[121]|object[122]|abstract[124]	new[121]|giv[122]|new[124]	coref|coref	26-7[0_122]|26-7[0_122]
19-26	2230-2235	image	abstract[121]|object[122]|abstract[124]	new[121]|giv[122]|new[124]	_	_
19-27	2236-2244	analysis	abstract[121]|abstract|abstract[124]	new[121]|giv|new[124]	coref	26-6[174_0]
19-28	2245-2250	tasks	abstract[121]|abstract[124]	new[121]|new[124]	_	_
19-29	2251-2252	.	_	_	_	_

#Text=A deep learning model has two important characteristics : ( 1 ) it allows for the construction and extraction of flexible representational features from input data , and ( 2 ) it contains multiple layers and many kernels that enable it to approximate basically any complex functions using the extracted features .
20-1	2253-2254	A	abstract[126]	giv[126]	ana	20-13[0_126]
20-2	2255-2259	deep	abstract[126]	giv[126]	_	_
20-3	2260-2268	learning	abstract|abstract[126]	giv|giv[126]	coref	22-6
20-4	2269-2274	model	abstract[126]	giv[126]	_	_
20-5	2275-2278	has	_	_	_	_
20-6	2279-2282	two	abstract[127]	giv[127]	_	_
20-7	2283-2292	important	abstract[127]	giv[127]	_	_
20-8	2293-2308	characteristics	abstract[127]	giv[127]	_	_
20-9	2309-2310	:	abstract[127]	giv[127]	_	_
20-10	2311-2312	(	abstract[127]	giv[127]	_	_
20-11	2313-2314	1	abstract[127]	giv[127]	_	_
20-12	2315-2316	)	abstract[127]	giv[127]	_	_
20-13	2317-2319	it	abstract	giv	ana	20-32
20-14	2320-2326	allows	_	_	_	_
20-15	2327-2330	for	_	_	_	_
20-16	2331-2334	the	event[129]	new[129]	_	_
20-17	2335-2347	construction	event[129]	new[129]	_	_
20-18	2348-2351	and	_	_	_	_
20-19	2352-2362	extraction	abstract[130]	giv[130]	coref	25-1[161_130]
20-20	2363-2365	of	abstract[130]	giv[130]	_	_
20-21	2366-2374	flexible	abstract[130]|abstract[131]	giv[130]|new[131]	coref	20-49[138_131]
20-22	2375-2391	representational	abstract[130]|abstract[131]	giv[130]|new[131]	_	_
20-23	2392-2400	features	abstract[130]|abstract[131]	giv[130]|new[131]	_	_
20-24	2401-2405	from	abstract[130]|abstract[131]	giv[130]|new[131]	_	_
20-25	2406-2411	input	abstract[130]|abstract[131]|abstract[132]	giv[130]|new[131]|giv[132]	coref	25-40[169_132]
20-26	2412-2416	data	abstract[130]|abstract[131]|abstract[132]	giv[130]|new[131]|giv[132]	_	_
20-27	2417-2418	,	_	_	_	_
20-28	2419-2422	and	_	_	_	_
20-29	2423-2424	(	_	_	_	_
20-30	2425-2426	2	_	_	_	_
20-31	2427-2428	)	_	_	_	_
20-32	2429-2431	it	abstract	giv	ana	20-41
20-33	2432-2440	contains	_	_	_	_
20-34	2441-2449	multiple	abstract[134]	giv[134]	_	_
20-35	2450-2456	layers	abstract[134]	giv[134]	_	_
20-36	2457-2460	and	abstract[134]	giv[134]	_	_
20-37	2461-2465	many	abstract[134]|object[135]	giv[134]|giv[135]	_	_
20-38	2466-2473	kernels	abstract[134]|object[135]	giv[134]|giv[135]	_	_
20-39	2474-2478	that	_	_	_	_
20-40	2479-2485	enable	_	_	_	_
20-41	2486-2488	it	abstract	giv	coref	30-24[212_0]
20-42	2489-2491	to	_	_	_	_
20-43	2492-2503	approximate	abstract[137]	new[137]	_	_
20-44	2504-2513	basically	abstract[137]	new[137]	_	_
20-45	2514-2517	any	abstract[137]	new[137]	_	_
20-46	2518-2525	complex	abstract[137]	new[137]	_	_
20-47	2526-2535	functions	abstract[137]	new[137]	_	_
20-48	2536-2541	using	_	_	_	_
20-49	2542-2545	the	abstract[138]	giv[138]	coref	21-12[0_138]
20-50	2546-2555	extracted	abstract[138]	giv[138]	_	_
20-51	2556-2564	features	abstract[138]	giv[138]	_	_
20-52	2565-2566	.	_	_	_	_

#Text=In all , deep neural networks are capable of automatically extracting features and solving highly complex prediction problems .
21-1	2567-2569	In	_	_	_	_
21-2	2570-2573	all	_	_	_	_
21-3	2574-2575	,	_	_	_	_
21-4	2576-2580	deep	place[139]	giv[139]	_	_
21-5	2581-2587	neural	place[139]	giv[139]	_	_
21-6	2588-2596	networks	place[139]	giv[139]	_	_
21-7	2597-2600	are	_	_	_	_
21-8	2601-2608	capable	_	_	_	_
21-9	2609-2611	of	_	_	_	_
21-10	2612-2625	automatically	_	_	_	_
21-11	2626-2636	extracting	_	_	_	_
21-12	2637-2645	features	abstract	giv	coref	22-17[148_0]
21-13	2646-2649	and	_	_	_	_
21-14	2650-2657	solving	_	_	_	_
21-15	2658-2664	highly	abstract[142]	new[142]	coref	31-15[223_142]
21-16	2665-2672	complex	abstract[142]	new[142]	_	_
21-17	2673-2683	prediction	event|abstract[142]	new|new[142]	coref	25-22
21-18	2684-2692	problems	abstract[142]	new[142]	_	_
21-19	2693-2694	.	_	_	_	_

#Text=In contrast , traditional machine learning methods have two major steps : ( 1 ) defining the features , and ( 2 ) constructing models using these handcrafted features .
22-1	2695-2697	In	_	_	_	_
22-2	2698-2706	contrast	abstract	new	_	_
22-3	2707-2708	,	_	_	_	_
22-4	2709-2720	traditional	abstract[146]	giv[146]	coref	23-3[152_146]
22-5	2721-2728	machine	object|abstract[146]	new|giv[146]	_	_
22-6	2729-2737	learning	abstract|abstract[146]	giv|giv[146]	coref	23-7
22-7	2738-2745	methods	abstract[146]	giv[146]	_	_
22-8	2746-2750	have	_	_	_	_
22-9	2751-2754	two	abstract[147]	new[147]	coref	30-32[215_147]
22-10	2755-2760	major	abstract[147]	new[147]	_	_
22-11	2761-2766	steps	abstract[147]	new[147]	_	_
22-12	2767-2768	:	abstract[147]	new[147]	_	_
22-13	2769-2770	(	abstract[147]	new[147]	_	_
22-14	2771-2772	1	abstract[147]	new[147]	_	_
22-15	2773-2774	)	abstract[147]	new[147]	_	_
22-16	2775-2783	defining	_	_	_	_
22-17	2784-2787	the	abstract[148]	giv[148]	coref	22-27[151_148]
22-18	2788-2796	features	abstract[148]	giv[148]	_	_
22-19	2797-2798	,	_	_	_	_
22-20	2799-2802	and	_	_	_	_
22-21	2803-2804	(	_	_	_	_
22-22	2805-2806	2	quantity	new	_	_
22-23	2807-2808	)	_	_	_	_
22-24	2809-2821	constructing	_	_	_	_
22-25	2822-2828	models	abstract	giv	coref	23-6[154_0]
22-26	2829-2834	using	_	_	_	_
22-27	2835-2840	these	abstract[151]	giv[151]	coref	24-15[0_151]
22-28	2841-2852	handcrafted	abstract[151]	giv[151]	_	_
22-29	2853-2861	features	abstract[151]	giv[151]	_	_
22-30	2862-2863	.	_	_	_	_

#Text=Compared with traditional methods , deep learning models have the following advantages :
23-1	2864-2872	Compared	_	_	_	_
23-2	2873-2877	with	_	_	_	_
23-3	2878-2889	traditional	abstract[152]	giv[152]	coref	30-39[218_152]
23-4	2890-2897	methods	abstract[152]	giv[152]	_	_
23-5	2898-2899	,	_	_	_	_
23-6	2900-2904	deep	abstract[154]	giv[154]	coref	24-3[157_154]
23-7	2905-2913	learning	abstract|abstract[154]	giv|giv[154]	coref	24-4
23-8	2914-2920	models	abstract[154]	giv[154]	_	_
23-9	2921-2925	have	_	_	_	_
23-10	2926-2929	the	abstract[155]	giv[155]	_	_
23-11	2930-2939	following	abstract[155]	giv[155]	_	_
23-12	2940-2950	advantages	abstract[155]	giv[155]	_	_
23-13	2951-2952	:	_	_	_	_

#Text=First , deep learning models greatly simplify or remove the task of manually defining features .
24-1	2953-2958	First	_	_	_	_
24-2	2959-2960	,	_	_	_	_
24-3	2961-2965	deep	abstract[157]	giv[157]	coref	33-8[231_157]
24-4	2966-2974	learning	abstract|abstract[157]	giv|giv[157]	coref	27-21[193_0]
24-5	2975-2981	models	abstract[157]	giv[157]	_	_
24-6	2982-2989	greatly	_	_	_	_
24-7	2990-2998	simplify	_	_	_	_
24-8	2999-3001	or	_	_	_	_
24-9	3002-3008	remove	_	_	_	_
24-10	3009-3012	the	abstract[158]	new[158]	_	_
24-11	3013-3017	task	abstract[158]	new[158]	_	_
24-12	3018-3020	of	_	_	_	_
24-13	3021-3029	manually	_	_	_	_
24-14	3030-3038	defining	_	_	_	_
24-15	3039-3047	features	abstract	giv	coref	31-9[220_0]
24-16	3048-3049	.	_	_	_	_

#Text=Manual feature extraction is very challenging and time consuming , especially in the following two scenarios : ( 1 ) the prediction problem is complex , and/or ( 2 ) there is limited prior knowledge about the relationship between input data and the outcomes to be predicted .
25-1	3050-3056	Manual	abstract[161]	giv[161]	coref	30-34[0_161]
25-2	3057-3064	feature	abstract|abstract[161]	giv|giv[161]	coref	30-33
25-3	3065-3075	extraction	abstract[161]	giv[161]	_	_
25-4	3076-3078	is	_	_	_	_
25-5	3079-3083	very	_	_	_	_
25-6	3084-3095	challenging	_	_	_	_
25-7	3096-3099	and	_	_	_	_
25-8	3100-3104	time	time	new	_	_
25-9	3105-3114	consuming	_	_	_	_
25-10	3115-3116	,	_	_	_	_
25-11	3117-3127	especially	event[163]	new[163]	coref	26-1[171_163]
25-12	3128-3130	in	event[163]	new[163]	_	_
25-13	3131-3134	the	event[163]	new[163]	_	_
25-14	3135-3144	following	event[163]	new[163]	_	_
25-15	3145-3148	two	event[163]	new[163]	_	_
25-16	3149-3158	scenarios	event[163]	new[163]	_	_
25-17	3159-3160	:	event[163]	new[163]	_	_
25-18	3161-3162	(	event[163]	new[163]	_	_
25-19	3163-3164	1	event[163]	new[163]	_	_
25-20	3165-3166	)	event[163]	new[163]	_	_
25-21	3167-3170	the	abstract[165]	new[165]	_	_
25-22	3171-3181	prediction	event|abstract[165]	giv|new[165]	coref	26-12
25-23	3182-3189	problem	abstract[165]	new[165]	_	_
25-24	3190-3192	is	_	_	_	_
25-25	3193-3200	complex	_	_	_	_
25-26	3201-3202	,	_	_	_	_
25-27	3203-3209	and/or	_	_	_	_
25-28	3210-3211	(	_	_	_	_
25-29	3212-3213	2	_	_	_	_
25-30	3214-3215	)	_	_	_	_
25-31	3216-3221	there	_	_	_	_
25-32	3222-3224	is	_	_	_	_
25-33	3225-3232	limited	_	_	_	_
25-34	3233-3238	prior	_	_	_	_
25-35	3239-3248	knowledge	abstract[166]	new[166]	coref	26-41[183_166]
25-36	3249-3254	about	abstract[166]	new[166]	_	_
25-37	3255-3258	the	abstract[166]|abstract[167]	new[166]|new[167]	_	_
25-38	3259-3271	relationship	abstract[166]|abstract[167]	new[166]|new[167]	_	_
25-39	3272-3279	between	abstract[166]|abstract[167]	new[166]|new[167]	_	_
25-40	3280-3285	input	abstract[166]|abstract[167]|abstract|abstract[169]	new[166]|new[167]|giv|giv[169]	coref|coref	33-19[233_169]|33-19[233_169]
25-41	3286-3290	data	abstract[166]|abstract[167]|abstract[169]	new[166]|new[167]|giv[169]	_	_
25-42	3291-3294	and	abstract[166]|abstract[167]|abstract[169]	new[166]|new[167]|giv[169]	_	_
25-43	3295-3298	the	abstract[166]|abstract[167]|abstract[169]|event[170]	new[166]|new[167]|giv[169]|new[170]	coref	26-22[178_170]
25-44	3299-3307	outcomes	abstract[166]|abstract[167]|abstract[169]|event[170]	new[166]|new[167]|giv[169]|new[170]	_	_
25-45	3308-3310	to	_	_	_	_
25-46	3311-3313	be	_	_	_	_
25-47	3314-3323	predicted	_	_	_	_
25-48	3324-3325	.	_	_	_	_

#Text=Both scenarios are true of pathology image analysis , as the prediction problems ( such as using pathology images to predict patient outcomes or recognizing various tissue structures and cells from H&E-stained images ) are very complex , and despite the accumulated knowledge from pathologists , little is known about which quantitative image features predict the outcomes .
26-1	3326-3330	Both	event[171]	giv[171]	_	_
26-2	3331-3340	scenarios	event[171]	giv[171]	_	_
26-3	3341-3344	are	_	_	_	_
26-4	3345-3349	true	_	_	_	_
26-5	3350-3352	of	_	_	_	_
26-6	3353-3362	pathology	abstract|abstract[174]	giv|giv[174]	coref|coref|coref|coref	27-8|27-8[191_174]|27-8|27-8[191_174]
26-7	3363-3368	image	object|abstract[174]	giv|giv[174]	coref	26-52[185_0]
26-8	3369-3377	analysis	abstract[174]	giv[174]	_	_
26-9	3378-3379	,	abstract[174]	giv[174]	_	_
26-10	3380-3382	as	abstract[174]	giv[174]	_	_
26-11	3383-3386	the	abstract[174]	giv[174]	_	_
26-12	3387-3397	prediction	abstract[174]|event	giv[174]|giv	coref	31-18
26-13	3398-3406	problems	abstract[174]	giv[174]	_	_
26-14	3407-3408	(	_	_	_	_
26-15	3409-3413	such	_	_	_	_
26-16	3414-3416	as	_	_	_	_
26-17	3417-3422	using	_	_	_	_
26-18	3423-3432	pathology	object[176]	giv[176]	coref	26-32[182_176]
26-19	3433-3439	images	object[176]	giv[176]	_	_
26-20	3440-3442	to	_	_	_	_
26-21	3443-3450	predict	_	_	_	_
26-22	3451-3458	patient	person|event[178]	new|giv[178]	coref|coref	26-56[186_178]|26-56[186_178]
26-23	3459-3467	outcomes	event[178]	giv[178]	_	_
26-24	3468-3470	or	_	_	_	_
26-25	3471-3482	recognizing	_	_	_	_
26-26	3483-3490	various	abstract[180]	new[180]	_	_
26-27	3491-3497	tissue	object|abstract[180]	new|new[180]	_	_
26-28	3498-3508	structures	abstract[180]	new[180]	_	_
26-29	3509-3512	and	_	_	_	_
26-30	3513-3518	cells	object[181]	new[181]	_	_
26-31	3519-3523	from	object[181]	new[181]	_	_
26-32	3524-3535	H&E-stained	object[181]|object[182]	new[181]|giv[182]	_	_
26-33	3536-3542	images	object[181]|object[182]	new[181]|giv[182]	_	_
26-34	3543-3544	)	_	_	_	_
26-35	3545-3548	are	_	_	_	_
26-36	3549-3553	very	_	_	_	_
26-37	3554-3561	complex	_	_	_	_
26-38	3562-3563	,	_	_	_	_
26-39	3564-3567	and	_	_	_	_
26-40	3568-3575	despite	_	_	_	_
26-41	3576-3579	the	abstract[183]	giv[183]	_	_
26-42	3580-3591	accumulated	abstract[183]	giv[183]	_	_
26-43	3592-3601	knowledge	abstract[183]	giv[183]	_	_
26-44	3602-3606	from	abstract[183]	giv[183]	_	_
26-45	3607-3619	pathologists	abstract[183]|person	giv[183]|new	_	_
26-46	3620-3621	,	_	_	_	_
26-47	3622-3628	little	_	_	_	_
26-48	3629-3631	is	_	_	_	_
26-49	3632-3637	known	_	_	_	_
26-50	3638-3643	about	_	_	_	_
26-51	3644-3649	which	_	_	_	_
26-52	3650-3662	quantitative	object[185]	giv[185]	coref	27-9[0_185]
26-53	3663-3668	image	object[185]	giv[185]	_	_
26-54	3669-3677	features	_	_	_	_
26-55	3678-3685	predict	_	_	_	_
26-56	3686-3689	the	event[186]	giv[186]	_	_
26-57	3690-3698	outcomes	event[186]	giv[186]	_	_
26-58	3699-3700	.	_	_	_	_

#Text=As a result , the advance of pathology image analysis had been slow and limited until the recent development of deep learning .
27-1	3701-3703	As	_	_	_	_
27-2	3704-3705	a	abstract[187]	new[187]	_	_
27-3	3706-3712	result	abstract[187]	new[187]	_	_
27-4	3713-3714	,	_	_	_	_
27-5	3715-3718	the	abstract[188]	new[188]	_	_
27-6	3719-3726	advance	abstract[188]	new[188]	_	_
27-7	3727-3729	of	abstract[188]	new[188]	_	_
27-8	3730-3739	pathology	abstract[188]|abstract|abstract[191]	new[188]|giv|giv[191]	coref|coref|coref|coref	33-84|33-84[256_191]|33-84|33-84[256_191]
27-9	3740-3745	image	abstract[188]|object|abstract[191]	new[188]|giv|giv[191]	coref	30-11[209_0]
27-10	3746-3754	analysis	abstract[188]|abstract[191]	new[188]|giv[191]	_	_
27-11	3755-3758	had	_	_	_	_
27-12	3759-3763	been	_	_	_	_
27-13	3764-3768	slow	_	_	_	_
27-14	3769-3772	and	_	_	_	_
27-15	3773-3780	limited	_	_	_	_
27-16	3781-3786	until	_	_	_	_
27-17	3787-3790	the	abstract[192]	new[192]	_	_
27-18	3791-3797	recent	abstract[192]	new[192]	_	_
27-19	3798-3809	development	abstract[192]	new[192]	_	_
27-20	3810-3812	of	abstract[192]	new[192]	_	_
27-21	3813-3817	deep	abstract[192]|abstract[193]	new[192]|giv[193]	coref	28-7[0_193]
27-22	3818-3826	learning	abstract[192]|abstract[193]	new[192]|giv[193]	_	_
27-23	3827-3828	.	_	_	_	_

#Text=Second , the computation of deep learning algorithms can be highly parallel .
28-1	3829-3835	Second	_	_	_	_
28-2	3836-3837	,	_	_	_	_
28-3	3838-3841	the	abstract[194]	new[194]	coref	30-2[206_194]
28-4	3842-3853	computation	abstract[194]	new[194]	_	_
28-5	3854-3856	of	abstract[194]	new[194]	_	_
28-6	3857-3861	deep	abstract[194]|abstract[196]	new[194]|new[196]	_	_
28-7	3862-3870	learning	abstract[194]|abstract|abstract[196]	new[194]|giv|new[196]	coref	29-5[197_0]
28-8	3871-3881	algorithms	abstract[194]|abstract[196]	new[194]|new[196]	_	_
28-9	3882-3885	can	_	_	_	_
28-10	3886-3888	be	_	_	_	_
28-11	3889-3895	highly	_	_	_	_
28-12	3896-3904	parallel	_	_	_	_
28-13	3905-3906	.	_	_	_	_

#Text=As a result , deep learning can largely leverage the parallel computing power from the recent developments in GPU ( graphics processing unit ) hardware .
29-1	3907-3909	As	_	_	_	_
29-2	3910-3911	a	_	_	_	_
29-3	3912-3918	result	_	_	_	_
29-4	3919-3920	,	_	_	_	_
29-5	3921-3925	deep	abstract[197]	giv[197]	coref	30-26[0_197]
29-6	3926-3934	learning	abstract[197]	giv[197]	_	_
29-7	3935-3938	can	_	_	_	_
29-8	3939-3946	largely	_	_	_	_
29-9	3947-3955	leverage	_	_	_	_
29-10	3956-3959	the	abstract[199]	new[199]	coref	32-7[226_199]
29-11	3960-3968	parallel	abstract[199]	new[199]	_	_
29-12	3969-3978	computing	abstract|abstract[199]	new|new[199]	_	_
29-13	3979-3984	power	abstract[199]	new[199]	_	_
29-14	3985-3989	from	_	_	_	_
29-15	3990-3993	the	abstract[200]	new[200]	_	_
29-16	3994-4000	recent	abstract[200]	new[200]	_	_
29-17	4001-4013	developments	abstract[200]	new[200]	_	_
29-18	4014-4016	in	abstract[200]	new[200]	_	_
29-19	4017-4020	GPU	abstract[200]|object	new[200]|new	_	_
29-20	4021-4022	(	abstract[200]|abstract[205]	new[200]|new[205]	_	_
29-21	4023-4031	graphics	abstract[200]|abstract|object[204]|abstract[205]	new[200]|new|new[204]|new[205]	_	_
29-22	4032-4042	processing	abstract[200]|abstract|object[204]|abstract[205]	new[200]|new|new[204]|new[205]	_	_
29-23	4043-4047	unit	abstract[200]|object[204]|abstract[205]	new[200]|new[204]|new[205]	_	_
29-24	4048-4049	)	abstract[200]|abstract[205]	new[200]|new[205]	_	_
29-25	4050-4058	hardware	abstract[200]|abstract[205]	new[200]|new[205]	_	_
29-26	4059-4060	.	_	_	_	_

#Text=With GPU-aided computation , processing ( classifying or segmenting ) a 1000 × 1000 pixels image usually takes less than one second for a deep learning model , much faster than traditional feature extraction steps and non-deep-learning-based image segmentation methods .
30-1	4061-4065	With	_	_	_	_
30-2	4066-4075	GPU-aided	abstract[206]	giv[206]	_	_
30-3	4076-4087	computation	abstract[206]	giv[206]	_	_
30-4	4088-4089	,	_	_	_	_
30-5	4090-4100	processing	_	_	_	_
30-6	4101-4102	(	_	_	_	_
30-7	4103-4114	classifying	_	_	_	_
30-8	4115-4117	or	_	_	_	_
30-9	4118-4128	segmenting	_	_	_	_
30-10	4129-4130	)	_	_	_	_
30-11	4131-4132	a	object[209]	giv[209]	coref	30-37[216_209]
30-12	4133-4137	1000	abstract[208]|object[209]	new[208]|giv[209]	_	_
30-13	4138-4139	×	abstract[208]|object[209]	new[208]|giv[209]	_	_
30-14	4140-4144	1000	quantity|abstract[208]|object[209]	new|new[208]|giv[209]	_	_
30-15	4145-4151	pixels	abstract[208]|object[209]	new[208]|giv[209]	_	_
30-16	4152-4157	image	object[209]	giv[209]	_	_
30-17	4158-4165	usually	_	_	_	_
30-18	4166-4171	takes	_	_	_	_
30-19	4172-4176	less	_	_	_	_
30-20	4177-4181	than	_	_	_	_
30-21	4182-4185	one	time[210]	new[210]	_	_
30-22	4186-4192	second	time[210]	new[210]	_	_
30-23	4193-4196	for	_	_	_	_
30-24	4197-4198	a	abstract[212]	giv[212]	coref	33-29[236_212]
30-25	4199-4203	deep	abstract[212]	giv[212]	_	_
30-26	4204-4212	learning	abstract|abstract[212]	giv|giv[212]	coref	31-4[219_0]
30-27	4213-4218	model	abstract[212]	giv[212]	_	_
30-28	4219-4220	,	_	_	_	_
30-29	4221-4225	much	_	_	_	_
30-30	4226-4232	faster	_	_	_	_
30-31	4233-4237	than	_	_	_	_
30-32	4238-4249	traditional	abstract[215]	giv[215]	_	_
30-33	4250-4257	feature	abstract|abstract[215]	giv|giv[215]	_	_
30-34	4258-4268	extraction	abstract|abstract[215]	giv|giv[215]	_	_
30-35	4269-4274	steps	abstract[215]	giv[215]	_	_
30-36	4275-4278	and	_	_	_	_
30-37	4279-4302	non-deep-learning-based	object[216]	giv[216]	coref	33-19[0_216]
30-38	4303-4308	image	object[216]	giv[216]	_	_
30-39	4309-4321	segmentation	abstract|abstract[218]	new|giv[218]	coref|coref	33-4[230_218]|33-4[230_218]
30-40	4322-4329	methods	abstract[218]	giv[218]	_	_
30-41	4330-4331	.	_	_	_	_

#Text=Furthermore , since deep learning does not require handcrafted features , it can handle much more complex prediction problems and is able to recognize multiple objects simultaneously .
31-1	4332-4343	Furthermore	_	_	_	_
31-2	4344-4345	,	_	_	_	_
31-3	4346-4351	since	_	_	_	_
31-4	4352-4356	deep	abstract[219]	giv[219]	ana	31-12[0_219]
31-5	4357-4365	learning	abstract[219]	giv[219]	_	_
31-6	4366-4370	does	_	_	_	_
31-7	4371-4374	not	_	_	_	_
31-8	4375-4382	require	_	_	_	_
31-9	4383-4394	handcrafted	abstract[220]	giv[220]	_	_
31-10	4395-4403	features	abstract[220]	giv[220]	_	_
31-11	4404-4405	,	_	_	_	_
31-12	4406-4408	it	abstract	giv	coref	33-5
31-13	4409-4412	can	_	_	_	_
31-14	4413-4419	handle	_	_	_	_
31-15	4420-4424	much	abstract[223]	giv[223]	_	_
31-16	4425-4429	more	abstract[223]	giv[223]	_	_
31-17	4430-4437	complex	abstract[223]	giv[223]	_	_
31-18	4438-4448	prediction	event|abstract[223]	giv|giv[223]	coref	33-29
31-19	4449-4457	problems	abstract[223]	giv[223]	_	_
31-20	4458-4461	and	_	_	_	_
31-21	4462-4464	is	_	_	_	_
31-22	4465-4469	able	_	_	_	_
31-23	4470-4472	to	_	_	_	_
31-24	4473-4482	recognize	_	_	_	_
31-25	4483-4491	multiple	abstract[224]	new[224]	_	_
31-26	4492-4499	objects	abstract[224]	new[224]	_	_
31-27	4500-4514	simultaneously	_	_	_	_
31-28	4515-4516	.	_	_	_	_

#Text=For example , CNNs have shown great power in distinguishing as many as 1000 object categories .
32-1	4517-4520	For	_	_	_	_
32-2	4521-4528	example	_	_	_	_
32-3	4529-4530	,	_	_	_	_
32-4	4531-4535	CNNs	object	giv	_	_
32-5	4536-4540	have	_	_	_	_
32-6	4541-4546	shown	_	_	_	_
32-7	4547-4552	great	abstract[226]	giv[226]	_	_
32-8	4553-4558	power	abstract[226]	giv[226]	_	_
32-9	4559-4561	in	_	_	_	_
32-10	4562-4576	distinguishing	_	_	_	_
32-11	4577-4579	as	_	_	_	_
32-12	4580-4584	many	_	_	_	_
32-13	4585-4587	as	_	_	_	_
32-14	4588-4592	1000	_	_	_	_
32-15	4593-4599	object	abstract	new	coref	33-40
32-16	4600-4610	categories	_	_	_	_
32-17	4611-4612	.	_	_	_	_

#Text=Other advantages of deep learning methods include the following : ( 1 ) deep learning models fully utilize image data , as every pixel can be utilized in prediction model ; ( 2 ) CNN models are insensitive to object position on the image , an inherent property of convolution operation ; and ( 3 ) as discussed in the next section , by using extensive data augmentations in the model training process , CNN models are robust to different staining conditions in pathology image analysis .
33-1	4613-4618	Other	abstract[228]	new[228]	_	_
33-2	4619-4629	advantages	abstract[228]	new[228]	_	_
33-3	4630-4632	of	abstract[228]	new[228]	_	_
33-4	4633-4637	deep	abstract[228]|abstract[230]	new[228]|giv[230]	_	_
33-5	4638-4646	learning	abstract[228]|abstract|abstract[230]	new[228]|giv|giv[230]	_	_
33-6	4647-4654	methods	abstract[228]|abstract[230]	new[228]|giv[230]	_	_
33-7	4655-4662	include	_	_	_	_
33-8	4663-4666	the	abstract[231]	giv[231]	coref	33-35[238_231]
33-9	4667-4676	following	abstract[231]	giv[231]	_	_
33-10	4677-4678	:	abstract[231]	giv[231]	_	_
33-11	4679-4680	(	abstract[231]	giv[231]	_	_
33-12	4681-4682	1	abstract[231]	giv[231]	_	_
33-13	4683-4684	)	abstract[231]	giv[231]	_	_
33-14	4685-4689	deep	abstract[231]	giv[231]	_	_
33-15	4690-4698	learning	abstract[231]	giv[231]	_	_
33-16	4699-4705	models	abstract[231]	giv[231]	_	_
33-17	4706-4711	fully	_	_	_	_
33-18	4712-4719	utilize	_	_	_	_
33-19	4720-4725	image	object|abstract[233]	giv|giv[233]	coref|coref|coref|coref	33-43[241_0]|33-67[0_233]|33-43[241_0]|33-67[0_233]
33-20	4726-4730	data	abstract[233]	giv[233]	_	_
33-21	4731-4732	,	_	_	_	_
33-22	4733-4735	as	_	_	_	_
33-23	4736-4741	every	object[234]	new[234]	_	_
33-24	4742-4747	pixel	object[234]	new[234]	_	_
33-25	4748-4751	can	_	_	_	_
33-26	4752-4754	be	_	_	_	_
33-27	4755-4763	utilized	_	_	_	_
33-28	4764-4766	in	_	_	_	_
33-29	4767-4777	prediction	event|abstract[236]	giv|giv[236]	coref|coref	33-71[0_236]|33-71[0_236]
33-30	4778-4783	model	abstract[236]	giv[236]	_	_
33-31	4784-4785	;	_	_	_	_
33-32	4786-4787	(	_	_	_	_
33-33	4788-4789	2	_	_	_	_
33-34	4790-4791	)	_	_	_	_
33-35	4792-4795	CNN	organization|abstract[238]	giv|giv[238]	coref|coref|coref|coref	33-75|33-75[252_238]|33-75|33-75[252_238]
33-36	4796-4802	models	abstract[238]	giv[238]	_	_
33-37	4803-4806	are	_	_	_	_
33-38	4807-4818	insensitive	_	_	_	_
33-39	4819-4821	to	_	_	_	_
33-40	4822-4828	object	object|object[240]	giv|new[240]	_	_
33-41	4829-4837	position	object[240]	new[240]	_	_
33-42	4838-4840	on	object[240]	new[240]	_	_
33-43	4841-4844	the	object[240]|object[241]	new[240]|giv[241]	coref	33-85[0_241]
33-44	4845-4850	image	object[240]|object[241]	new[240]|giv[241]	_	_
33-45	4851-4852	,	_	_	_	_
33-46	4853-4855	an	abstract[242]	new[242]	_	_
33-47	4856-4864	inherent	abstract[242]	new[242]	_	_
33-48	4865-4873	property	abstract[242]	new[242]	_	_
33-49	4874-4876	of	abstract[242]	new[242]	_	_
33-50	4877-4888	convolution	abstract[242]|abstract|event[244]	new[242]|giv|giv[244]	_	_
33-51	4889-4898	operation	abstract[242]|event[244]	new[242]|giv[244]	_	_
33-52	4899-4900	;	abstract[242]	new[242]	_	_
33-53	4901-4904	and	abstract[242]	new[242]	_	_
33-54	4905-4906	(	abstract[242]	new[242]	_	_
33-55	4907-4908	3	abstract[242]	new[242]	_	_
33-56	4909-4910	)	abstract[242]	new[242]	_	_
33-57	4911-4913	as	_	_	_	_
33-58	4914-4923	discussed	_	_	_	_
33-59	4924-4926	in	_	_	_	_
33-60	4927-4930	the	abstract[245]	new[245]	_	_
33-61	4931-4935	next	abstract[245]	new[245]	_	_
33-62	4936-4943	section	abstract[245]	new[245]	_	_
33-63	4944-4945	,	_	_	_	_
33-64	4946-4948	by	_	_	_	_
33-65	4949-4954	using	_	_	_	_
33-66	4955-4964	extensive	abstract[247]	new[247]	_	_
33-67	4965-4969	data	abstract|abstract[247]	giv|new[247]	_	_
33-68	4970-4983	augmentations	abstract[247]	new[247]	_	_
33-69	4984-4986	in	abstract[247]	new[247]	_	_
33-70	4987-4990	the	abstract[247]|abstract[250]	new[247]|new[250]	_	_
33-71	4991-4996	model	abstract[247]|abstract|abstract[250]	new[247]|giv|new[250]	_	_
33-72	4997-5005	training	abstract[247]|abstract|abstract[250]	new[247]|new|new[250]	_	_
33-73	5006-5013	process	abstract[247]|abstract[250]	new[247]|new[250]	_	_
33-74	5014-5015	,	_	_	_	_
33-75	5016-5019	CNN	organization|abstract[252]	giv|giv[252]	_	_
33-76	5020-5026	models	abstract[252]	giv[252]	_	_
33-77	5027-5030	are	_	_	_	_
33-78	5031-5037	robust	_	_	_	_
33-79	5038-5040	to	_	_	_	_
33-80	5041-5050	different	abstract[253]	new[253]	_	_
33-81	5051-5059	staining	abstract[253]	new[253]	_	_
33-82	5060-5070	conditions	abstract[253]	new[253]	_	_
33-83	5071-5073	in	abstract[253]	new[253]	_	_
33-84	5074-5083	pathology	abstract[253]|abstract|abstract[256]	new[253]|giv|giv[256]	_	_
33-85	5084-5089	image	abstract[253]|object|abstract[256]	new[253]|giv|giv[256]	_	_
33-86	5090-5098	analysis	abstract[253]|abstract[256]	new[253]|giv[256]	_	_
33-87	5099-5100	.	_	_	_	_
