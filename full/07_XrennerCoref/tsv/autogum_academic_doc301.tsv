#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=1 .
1-1	0-1	1	quantity	new	_	_
1-2	2-3	.	_	_	_	_

#Text=Introduction
2-1	4-16	Introduction	abstract	new	_	_

#Text=Nowadays , position information has become key information in people ’s daily lives .
3-1	17-25	Nowadays	_	_	_	_
3-2	26-27	,	_	_	_	_
3-3	28-36	position	abstract|abstract[4]	new|new[4]	coref|coref|coref|coref	5-4[12_0]|19-10[93_4]|5-4[12_0]|19-10[93_4]
3-4	37-48	information	abstract[4]	new[4]	_	_
3-5	49-52	has	_	_	_	_
3-6	53-59	become	_	_	_	_
3-7	60-63	key	_	_	_	_
3-8	64-75	information	_	_	_	_
3-9	76-78	in	_	_	_	_
3-10	79-85	people	person[5]|abstract[6]	new[5]|new[6]	ana|coref|ana|coref	4-1[0_6]|7-7[0_5]|4-1[0_6]|7-7[0_5]
3-11	86-88	’s	person[5]|abstract[6]	new[5]|new[6]	_	_
3-12	89-94	daily	abstract[6]	new[6]	_	_
3-13	95-100	lives	abstract[6]	new[6]	_	_
3-14	101-102	.	_	_	_	_

#Text=This has inspired position-based services , which aim to provide personalized services to mobile users whose positions are changing .
4-1	103-107	This	abstract	giv	_	_
4-2	108-111	has	_	_	_	_
4-3	112-120	inspired	_	_	_	_
4-4	121-135	position-based	abstract[8]	new[8]	coref	4-11[9_8]
4-5	136-144	services	abstract[8]	new[8]	_	_
4-6	145-146	,	_	_	_	_
4-7	147-152	which	_	_	_	_
4-8	153-156	aim	_	_	_	_
4-9	157-159	to	_	_	_	_
4-10	160-167	provide	_	_	_	_
4-11	168-180	personalized	abstract[9]	giv[9]	coref	5-11[14_9]
4-12	181-189	services	abstract[9]	giv[9]	_	_
4-13	190-192	to	_	_	_	_
4-14	193-199	mobile	person[10]	new[10]	_	_
4-15	200-205	users	person[10]	new[10]	_	_
4-16	206-211	whose	abstract[11]	new[11]	_	_
4-17	212-221	positions	abstract[11]	new[11]	_	_
4-18	222-225	are	_	_	_	_
4-19	226-234	changing	_	_	_	_
4-20	235-236	.	_	_	_	_

#Text=Therefore , obtaining a precise position is a prerequisite for these services .
5-1	237-246	Therefore	_	_	_	_
5-2	247-248	,	_	_	_	_
5-3	249-258	obtaining	_	_	_	_
5-4	259-260	a	abstract[12]	giv[12]	coref	20-23[0_12]
5-5	261-268	precise	abstract[12]	giv[12]	_	_
5-6	269-277	position	abstract[12]	giv[12]	_	_
5-7	278-280	is	_	_	_	_
5-8	281-282	a	abstract[13]	new[13]	_	_
5-9	283-295	prerequisite	abstract[13]	new[13]	_	_
5-10	296-299	for	abstract[13]	new[13]	_	_
5-11	300-305	these	abstract[13]|abstract[14]	new[13]|giv[14]	_	_
5-12	306-314	services	abstract[13]|abstract[14]	new[13]|giv[14]	_	_
5-13	315-316	.	_	_	_	_

#Text=The most commonly used positioning method in the outdoor environment is the Global Navigation Satellite System ( GNSS ) .
6-1	317-320	The	abstract[16]	new[16]	coref	6-12[20_16]
6-2	321-325	most	abstract[16]	new[16]	_	_
6-3	326-334	commonly	abstract[16]	new[16]	_	_
6-4	335-339	used	abstract[16]	new[16]	_	_
6-5	340-351	positioning	abstract|abstract[16]	new|new[16]	coref	8-3[28_0]
6-6	352-358	method	abstract[16]	new[16]	_	_
6-7	359-361	in	abstract[16]	new[16]	_	_
6-8	362-365	the	abstract[16]|place[17]	new[16]|new[17]	_	_
6-9	366-373	outdoor	abstract[16]|place[17]	new[16]|new[17]	_	_
6-10	374-385	environment	abstract[16]|place[17]	new[16]|new[17]	_	_
6-11	386-388	is	_	_	_	_
6-12	389-392	the	abstract[20]	giv[20]	coref	24-29[139_20]
6-13	393-399	Global	abstract[20]	giv[20]	_	_
6-14	400-410	Navigation	abstract|abstract[20]	new|giv[20]	_	_
6-15	411-420	Satellite	organization|abstract[20]	new|giv[20]	_	_
6-16	421-427	System	abstract[20]	giv[20]	_	_
6-17	428-429	(	_	_	_	_
6-18	430-434	GNSS	place	new	coref	9-2
6-19	435-436	)	_	_	_	_
6-20	437-438	.	_	_	_	_

#Text=In most cases , however , people spend more than 70 % of their time indoors .
7-1	439-441	In	_	_	_	_
7-2	442-446	most	abstract[22]	new[22]	_	_
7-3	447-452	cases	abstract[22]	new[22]	_	_
7-4	453-454	,	_	_	_	_
7-5	455-462	however	_	_	_	_
7-6	463-464	,	_	_	_	_
7-7	465-471	people	person	giv	ana	7-14
7-8	472-477	spend	_	_	_	_
7-9	478-482	more	quantity[24]	new[24]	_	_
7-10	483-487	than	quantity[24]	new[24]	_	_
7-11	488-490	70	quantity[24]	new[24]	_	_
7-12	491-492	%	quantity[24]	new[24]	_	_
7-13	493-495	of	quantity[24]	new[24]	_	_
7-14	496-501	their	quantity[24]|person|time[26]	new[24]|giv|new[26]	coref|coref	10-4|10-4
7-15	502-506	time	quantity[24]|time[26]	new[24]|new[26]	_	_
7-16	507-514	indoors	_	_	_	_
7-17	515-516	.	_	_	_	_

#Text=Therefore , accurate indoor positioning has important practical significance .
8-1	517-526	Therefore	_	_	_	_
8-2	527-528	,	_	_	_	_
8-3	529-537	accurate	abstract[28]	giv[28]	coref	14-14[64_28]
8-4	538-544	indoor	place|abstract[28]	new|giv[28]	coref	9-23
8-5	545-556	positioning	abstract[28]	giv[28]	_	_
8-6	557-560	has	_	_	_	_
8-7	561-570	important	abstract[29]	new[29]	_	_
8-8	571-580	practical	abstract[29]	new[29]	_	_
8-9	581-593	significance	abstract[29]	new[29]	_	_
8-10	594-595	.	_	_	_	_

#Text=Although GNSS is a good choice for outdoor positioning , due to signal occlusion and attenuations , it is often useless in indoor environments .
9-1	596-604	Although	_	_	_	_
9-2	605-609	GNSS	abstract	giv	coref	9-4[31_0]
9-3	610-612	is	_	_	_	_
9-4	613-614	a	abstract[31]	giv[31]	_	_
9-5	615-619	good	abstract[31]	giv[31]	_	_
9-6	620-626	choice	abstract[31]	giv[31]	_	_
9-7	627-630	for	abstract[31]	giv[31]	_	_
9-8	631-638	outdoor	abstract[31]|abstract[32]	giv[31]|new[32]	_	_
9-9	639-650	positioning	abstract[31]|abstract[32]	giv[31]|new[32]	_	_
9-10	651-652	,	_	_	_	_
9-11	653-656	due	_	_	_	_
9-12	657-659	to	_	_	_	_
9-13	660-666	signal	_	_	_	_
9-14	667-676	occlusion	abstract	new	ana	9-18
9-15	677-680	and	_	_	_	_
9-16	681-693	attenuations	abstract	new	_	_
9-17	694-695	,	_	_	_	_
9-18	696-698	it	abstract	giv	_	_
9-19	699-701	is	_	_	_	_
9-20	702-707	often	_	_	_	_
9-21	708-715	useless	_	_	_	_
9-22	716-718	in	_	_	_	_
9-23	719-725	indoor	abstract|place[37]	giv|new[37]	coref|coref	10-7|10-7
9-24	726-738	environments	place[37]	new[37]	_	_
9-25	739-740	.	_	_	_	_

#Text=Thus , positioning people accurately in indoor scenes remains a challenge and it has stimulated a large number of indoor-positioning methods in recent years .
10-1	741-745	Thus	_	_	_	_
10-2	746-747	,	_	_	_	_
10-3	748-759	positioning	_	_	_	_
10-4	760-766	people	person	giv	_	_
10-5	767-777	accurately	_	_	_	_
10-6	778-780	in	_	_	_	_
10-7	781-787	indoor	place|abstract[40]	giv|new[40]	coref|coref	14-15|14-15
10-8	788-794	scenes	abstract[40]	new[40]	_	_
10-9	795-802	remains	_	_	_	_
10-10	803-804	a	abstract[41]	new[41]	ana	10-13[0_41]
10-11	805-814	challenge	abstract[41]	new[41]	_	_
10-12	815-818	and	_	_	_	_
10-13	819-821	it	abstract	giv	_	_
10-14	822-825	has	_	_	_	_
10-15	826-836	stimulated	_	_	_	_
10-16	837-838	a	abstract[43]	new[43]	_	_
10-17	839-844	large	abstract[43]	new[43]	_	_
10-18	845-851	number	abstract[43]	new[43]	_	_
10-19	852-854	of	abstract[43]	new[43]	_	_
10-20	855-873	indoor-positioning	abstract[43]|abstract[44]	new[43]|new[44]	coref	11-2[46_44]
10-21	874-881	methods	abstract[43]|abstract[44]	new[43]|new[44]	_	_
10-22	882-884	in	_	_	_	_
10-23	885-891	recent	time[45]	new[45]	coref	16-11[74_45]
10-24	892-897	years	time[45]	new[45]	_	_
10-25	898-899	.	_	_	_	_

#Text=Among these methods , fingerprint-based algorithms are widely used .
11-1	900-905	Among	_	_	_	_
11-2	906-911	these	abstract[46]	giv[46]	coref	13-2[55_46]
11-3	912-919	methods	abstract[46]	giv[46]	_	_
11-4	920-921	,	_	_	_	_
11-5	922-939	fingerprint-based	abstract[47]	new[47]	ana	12-1[0_47]
11-6	940-950	algorithms	abstract[47]	new[47]	_	_
11-7	951-954	are	_	_	_	_
11-8	955-961	widely	_	_	_	_
11-9	962-966	used	_	_	_	_
11-10	967-968	.	_	_	_	_

#Text=Their fingerprint databases include Wi-Fi , Bluetooth , and magnetic field strengths .
12-1	969-974	Their	abstract|object[50]	giv|new[50]	coref|coref	27-4[172_0]|27-4[172_0]
12-2	975-986	fingerprint	object|object[50]	new|new[50]	coref	13-12
12-3	987-996	databases	object[50]	new[50]	_	_
12-4	997-1004	include	_	_	_	_
12-5	1005-1010	Wi-Fi	object	new	_	_
12-6	1011-1012	,	_	_	_	_
12-7	1013-1022	Bluetooth	person	new	_	_
12-8	1023-1024	,	_	_	_	_
12-9	1025-1028	and	_	_	_	_
12-10	1029-1037	magnetic	abstract[54]	new[54]	_	_
12-11	1038-1043	field	abstract|abstract[54]	new|new[54]	_	_
12-12	1044-1053	strengths	abstract[54]	new[54]	_	_
12-13	1054-1055	.	_	_	_	_

#Text=Although these methods are easy to implement , construction of a fingerprint database is usually labor-intensive and time-consuming .
13-1	1056-1064	Although	_	_	_	_
13-2	1065-1070	these	abstract[55]	giv[55]	ana	14-7[0_55]
13-3	1071-1078	methods	abstract[55]	giv[55]	_	_
13-4	1079-1082	are	_	_	_	_
13-5	1083-1087	easy	_	_	_	_
13-6	1088-1090	to	_	_	_	_
13-7	1091-1100	implement	_	_	_	_
13-8	1101-1102	,	_	_	_	_
13-9	1103-1115	construction	event[56]	new[56]	ana	14-3[0_56]
13-10	1116-1118	of	event[56]	new[56]	_	_
13-11	1119-1120	a	event[56]|object[58]	new[56]|new[58]	coref	19-4[92_58]
13-12	1121-1132	fingerprint	event[56]|object|object[58]	new[56]|giv|new[58]	_	_
13-13	1133-1141	database	event[56]|object[58]	new[56]|new[58]	_	_
13-14	1142-1144	is	_	_	_	_
13-15	1145-1152	usually	_	_	_	_
13-16	1153-1168	labor-intensive	_	_	_	_
13-17	1169-1172	and	_	_	_	_
13-18	1173-1187	time-consuming	_	_	_	_
13-19	1188-1189	.	_	_	_	_

#Text=Moreover , it is difficult for their results to meet the needs of high-accuracy indoor positioning .
14-1	1190-1198	Moreover	_	_	_	_
14-2	1199-1200	,	_	_	_	_
14-3	1201-1203	it	event	giv	_	_
14-4	1204-1206	is	_	_	_	_
14-5	1207-1216	difficult	_	_	_	_
14-6	1217-1220	for	_	_	_	_
14-7	1221-1226	their	abstract|abstract[61]	giv|new[61]	coref|coref	16-1[73_0]|16-1[73_0]
14-8	1227-1234	results	abstract[61]	new[61]	_	_
14-9	1235-1237	to	_	_	_	_
14-10	1238-1242	meet	_	_	_	_
14-11	1243-1246	the	abstract[62]	new[62]	_	_
14-12	1247-1252	needs	abstract[62]	new[62]	_	_
14-13	1253-1255	of	abstract[62]	new[62]	_	_
14-14	1256-1269	high-accuracy	abstract[62]|abstract[64]	new[62]|giv[64]	ana	15-18[0_64]
14-15	1270-1276	indoor	abstract[62]|place|abstract[64]	new[62]|giv|giv[64]	coref	33-7
14-16	1277-1288	positioning	abstract[62]|abstract[64]	new[62]|giv[64]	_	_
14-17	1289-1290	.	_	_	_	_

#Text=Given that humans use their eyes to see where they are , mobile platforms can also do this with cameras .
15-1	1291-1296	Given	_	_	_	_
15-2	1297-1301	that	_	_	_	_
15-3	1302-1308	humans	person	new	ana	15-5
15-4	1309-1312	use	_	_	_	_
15-5	1313-1318	their	person|object[67]	giv|new[67]	ana|ana	15-10|15-10
15-6	1319-1323	eyes	object[67]	new[67]	_	_
15-7	1324-1326	to	_	_	_	_
15-8	1327-1330	see	_	_	_	_
15-9	1331-1336	where	_	_	_	_
15-10	1337-1341	they	person	giv	_	_
15-11	1342-1345	are	_	_	_	_
15-12	1346-1347	,	_	_	_	_
15-13	1348-1354	mobile	object[69]	new[69]	_	_
15-14	1355-1364	platforms	object[69]	new[69]	_	_
15-15	1365-1368	can	_	_	_	_
15-16	1369-1373	also	_	_	_	_
15-17	1374-1376	do	_	_	_	_
15-18	1377-1381	this	abstract	giv	coref	16-5
15-19	1382-1386	with	_	_	_	_
15-20	1387-1394	cameras	object	new	_	_
15-21	1395-1396	.	_	_	_	_

#Text=A number of visual positioning methods have been proposed in recent years .
16-1	1397-1398	A	abstract[73]	giv[73]	coref	17-1[76_73]
16-2	1399-1405	number	abstract[73]	giv[73]	_	_
16-3	1406-1408	of	abstract[73]	giv[73]	_	_
16-4	1409-1415	visual	abstract[73]	giv[73]	_	_
16-5	1416-1427	positioning	abstract|abstract[73]	giv|giv[73]	coref	17-2
16-6	1428-1435	methods	abstract[73]	giv[73]	_	_
16-7	1436-1440	have	_	_	_	_
16-8	1441-1445	been	_	_	_	_
16-9	1446-1454	proposed	_	_	_	_
16-10	1455-1457	in	_	_	_	_
16-11	1458-1464	recent	time[74]	giv[74]	_	_
16-12	1465-1470	years	time[74]	giv[74]	_	_
16-13	1471-1472	.	_	_	_	_

#Text=These positioning methods are divided into three categories : image retrieval based methods , visual landmarks-based methods , and learning-based methods .
17-1	1473-1478	These	abstract[76]	giv[76]	coref	17-13[80_76]
17-2	1479-1490	positioning	abstract|abstract[76]	giv|giv[76]	coref	18-7
17-3	1491-1498	methods	abstract[76]	giv[76]	_	_
17-4	1499-1502	are	_	_	_	_
17-5	1503-1510	divided	_	_	_	_
17-6	1511-1515	into	_	_	_	_
17-7	1516-1521	three	abstract[77]	new[77]	appos	17-10[79_77]
17-8	1522-1532	categories	abstract[77]	new[77]	_	_
17-9	1533-1534	:	_	_	_	_
17-10	1535-1540	image	abstract|abstract[79]	new|giv[79]	coref|coref|coref|coref	18-1|18-1[83_79]|18-1|18-1[83_79]
17-11	1541-1550	retrieval	abstract[79]	giv[79]	_	_
17-12	1551-1556	based	_	_	_	_
17-13	1557-1564	methods	abstract[80]	giv[80]	coref	17-20[81_80]
17-14	1565-1566	,	abstract[80]	giv[80]	_	_
17-15	1567-1573	visual	abstract[80]	giv[80]	_	_
17-16	1574-1589	landmarks-based	abstract[80]	giv[80]	_	_
17-17	1590-1597	methods	abstract[80]	giv[80]	_	_
17-18	1598-1599	,	_	_	_	_
17-19	1600-1603	and	_	_	_	_
17-20	1604-1618	learning-based	abstract[81]	giv[81]	coref	18-4[0_81]
17-21	1619-1626	methods	abstract[81]	giv[81]	_	_
17-22	1627-1628	.	_	_	_	_

#Text=Image retrieval based methods treat the positioning task as an image retrieval or recognition process .
18-1	1629-1634	Image	abstract|abstract[83]	giv|giv[83]	coref|coref|coref|coref	18-12[0_83]|19-14[94_0]|18-12[0_83]|19-14[94_0]
18-2	1635-1644	retrieval	abstract[83]	giv[83]	_	_
18-3	1645-1650	based	_	_	_	_
18-4	1651-1658	methods	abstract	giv	coref	20-1[97_0]
18-5	1659-1664	treat	_	_	_	_
18-6	1665-1668	the	abstract[86]	new[86]	_	_
18-7	1669-1680	positioning	abstract|abstract[86]	giv|new[86]	coref	23-2
18-8	1681-1685	task	abstract[86]	new[86]	_	_
18-9	1686-1688	as	_	_	_	_
18-10	1689-1691	an	_	_	_	_
18-11	1692-1697	image	_	_	_	_
18-12	1698-1707	retrieval	abstract|abstract[88]	giv|new[88]	ana|coref|ana|coref	19-1[0_88]|23-17[130_0]|19-1[0_88]|23-17[130_0]
18-13	1708-1710	or	abstract[88]	new[88]	_	_
18-14	1711-1722	recognition	abstract[88]|abstract|abstract[90]	new[88]|new|new[90]	coref|coref	21-6[112_90]|21-6[112_90]
18-15	1723-1730	process	abstract[88]|abstract[90]	new[88]|new[90]	_	_
18-16	1731-1732	.	_	_	_	_

#Text=They usually have a database that are augmented with geospatial information , and every image in the database is described through the same specific features .
19-1	1733-1737	They	abstract	giv	_	_
19-2	1738-1745	usually	_	_	_	_
19-3	1746-1750	have	_	_	_	_
19-4	1751-1752	a	object[92]	giv[92]	coref	19-17[95_92]
19-5	1753-1761	database	object[92]	giv[92]	_	_
19-6	1762-1766	that	_	_	_	_
19-7	1767-1770	are	_	_	_	_
19-8	1771-1780	augmented	_	_	_	_
19-9	1781-1785	with	_	_	_	_
19-10	1786-1796	geospatial	abstract[93]	giv[93]	coref	20-21[104_93]
19-11	1797-1808	information	abstract[93]	giv[93]	_	_
19-12	1809-1810	,	_	_	_	_
19-13	1811-1814	and	_	_	_	_
19-14	1815-1820	every	abstract[94]	giv[94]	coref	20-26[106_94]
19-15	1821-1826	image	abstract[94]	giv[94]	_	_
19-16	1827-1829	in	abstract[94]	giv[94]	_	_
19-17	1830-1833	the	abstract[94]|object[95]	giv[94]|giv[95]	coref	20-12[100_95]
19-18	1834-1842	database	abstract[94]|object[95]	giv[94]|giv[95]	_	_
19-19	1843-1845	is	_	_	_	_
19-20	1846-1855	described	_	_	_	_
19-21	1856-1863	through	_	_	_	_
19-22	1864-1867	the	abstract[96]	new[96]	coref	24-16[134_96]
19-23	1868-1872	same	abstract[96]	new[96]	_	_
19-24	1873-1881	specific	abstract[96]	new[96]	_	_
19-25	1882-1890	features	abstract[96]	new[96]	_	_
19-26	1891-1892	.	_	_	_	_

#Text=These methods perform a first step to retrieve candidate images from the database according to a similarity search , and the coarse position information of the query image is then obtained based on the geospatial information of these candidate images .
20-1	1893-1898	These	abstract[97]	giv[97]	coref	23-1[122_97]
20-2	1899-1906	methods	abstract[97]	giv[97]	_	_
20-3	1907-1914	perform	_	_	_	_
20-4	1915-1916	a	event[98]	new[98]	coref	21-2[110_98]
20-5	1917-1922	first	event[98]	new[98]	_	_
20-6	1923-1927	step	event[98]	new[98]	_	_
20-7	1928-1930	to	_	_	_	_
20-8	1931-1939	retrieve	_	_	_	_
20-9	1940-1949	candidate	object[99]	new[99]	coref	20-38[109_99]
20-10	1950-1956	images	object[99]	new[99]	_	_
20-11	1957-1961	from	_	_	_	_
20-12	1962-1965	the	object[100]	giv[100]	coref	24-11[133_100]
20-13	1966-1974	database	object[100]	giv[100]	_	_
20-14	1975-1984	according	_	_	_	_
20-15	1985-1987	to	_	_	_	_
20-16	1988-1989	a	event[102]	new[102]	coref	22-19[120_102]
20-17	1990-2000	similarity	abstract|event[102]	new|new[102]	coref	22-19
20-18	2001-2007	search	event[102]	new[102]	_	_
20-19	2008-2009	,	_	_	_	_
20-20	2010-2013	and	_	_	_	_
20-21	2014-2017	the	abstract[104]	giv[104]	_	_
20-22	2018-2024	coarse	abstract[104]	giv[104]	_	_
20-23	2025-2033	position	abstract|abstract[104]	giv|giv[104]	coref	30-11
20-24	2034-2045	information	abstract[104]	giv[104]	_	_
20-25	2046-2048	of	abstract[104]	giv[104]	_	_
20-26	2049-2052	the	abstract[104]|abstract[106]	giv[104]|giv[106]	coref	21-6[111_106]
20-27	2053-2058	query	abstract[104]|abstract|abstract[106]	giv[104]|new|giv[106]	coref	27-21
20-28	2059-2064	image	abstract[104]|abstract[106]	giv[104]|giv[106]	_	_
20-29	2065-2067	is	_	_	_	_
20-30	2068-2072	then	_	_	_	_
20-31	2073-2081	obtained	_	_	_	_
20-32	2082-2087	based	_	_	_	_
20-33	2088-2090	on	_	_	_	_
20-34	2091-2094	the	abstract[107]	new[107]	_	_
20-35	2095-2105	geospatial	abstract[107]	new[107]	_	_
20-36	2106-2117	information	abstract[107]	new[107]	_	_
20-37	2118-2120	of	abstract[107]	new[107]	_	_
20-38	2121-2126	these	abstract[107]|object[109]	new[107]|giv[109]	coref	24-8[132_109]
20-39	2127-2136	candidate	abstract[107]|person|object[109]	new[107]|new|giv[109]	coref	30-26
20-40	2137-2143	images	abstract[107]|object[109]	new[107]|giv[109]	_	_
20-41	2144-2145	.	_	_	_	_

#Text=So the first step , similar image retrieval process , is critical .
21-1	2146-2148	So	_	_	_	_
21-2	2149-2152	the	event[110]	giv[110]	_	_
21-3	2153-2158	first	event[110]	giv[110]	_	_
21-4	2159-2163	step	event[110]	giv[110]	_	_
21-5	2164-2165	,	_	_	_	_
21-6	2166-2173	similar	abstract[111]|abstract[112]	giv[111]|giv[112]	coref|coref|coref|coref	23-14[128_112]|23-17[0_111]|23-14[128_112]|23-17[0_111]
21-7	2174-2179	image	abstract[111]|abstract[112]	giv[111]|giv[112]	_	_
21-8	2180-2189	retrieval	abstract[112]	giv[112]	_	_
21-9	2190-2197	process	abstract[112]	giv[112]	_	_
21-10	2198-2199	,	_	_	_	_
21-11	2200-2202	is	_	_	_	_
21-12	2203-2211	critical	_	_	_	_
21-13	2212-2213	.	_	_	_	_

#Text=The brute-force approach , which is a distance comparison between feature descriptor vectors , is often used for similarity search .
22-1	2214-2217	The	abstract[113]	new[113]	_	_
22-2	2218-2229	brute-force	abstract[113]	new[113]	_	_
22-3	2230-2238	approach	abstract[113]	new[113]	_	_
22-4	2239-2240	,	_	_	_	_
22-5	2241-2246	which	_	_	_	_
22-6	2247-2249	is	_	_	_	_
22-7	2250-2251	a	abstract[115]	new[115]	coref	23-9[125_115]
22-8	2252-2260	distance	place|abstract[115]	new|new[115]	_	_
22-9	2261-2271	comparison	abstract[115]	new[115]	_	_
22-10	2272-2279	between	abstract[115]	new[115]	_	_
22-11	2280-2287	feature	abstract[115]|abstract|object[118]	new[115]|new|new[118]	coref|coref|coref|coref	23-6|26-29[168_118]|23-6|26-29[168_118]
22-12	2288-2298	descriptor	abstract[115]|person|object[118]	new[115]|new|new[118]	_	_
22-13	2299-2306	vectors	abstract[115]|object[118]	new[115]|new[118]	_	_
22-14	2307-2308	,	_	_	_	_
22-15	2309-2311	is	_	_	_	_
22-16	2312-2317	often	_	_	_	_
22-17	2318-2322	used	_	_	_	_
22-18	2323-2326	for	_	_	_	_
22-19	2327-2337	similarity	abstract|event[120]	giv|giv[120]	coref|coref|coref|coref	23-12[126_0]|23-14[0_120]|23-12[126_0]|23-14[0_120]
22-20	2338-2344	search	event[120]	giv[120]	_	_
22-21	2345-2346	.	_	_	_	_

#Text=Some positioning methods based on feature descriptors adopt brute-force comparison for the similarity search process of image retrieval .
23-1	2347-2351	Some	abstract[122]	giv[122]	coref	28-12[184_122]
23-2	2352-2363	positioning	abstract|abstract[122]	giv|giv[122]	coref	31-28[216_0]
23-3	2364-2371	methods	abstract[122]	giv[122]	_	_
23-4	2372-2377	based	_	_	_	_
23-5	2378-2380	on	_	_	_	_
23-6	2381-2388	feature	abstract|abstract[124]	giv|new[124]	coref|coref|coref|coref	24-49|26-32[0_124]|24-49|26-32[0_124]
23-7	2389-2400	descriptors	abstract[124]	new[124]	_	_
23-8	2401-2406	adopt	_	_	_	_
23-9	2407-2418	brute-force	abstract[125]	giv[125]	_	_
23-10	2419-2429	comparison	abstract[125]	giv[125]	_	_
23-11	2430-2433	for	abstract[125]	giv[125]	_	_
23-12	2434-2437	the	abstract[125]|abstract[126]	giv[125]|giv[126]	coref	26-10[0_126]
23-13	2438-2448	similarity	abstract[125]|abstract[126]	giv[125]|giv[126]	_	_
23-14	2449-2455	search	abstract[125]|abstract[126]|event|abstract[128]	giv[125]|giv[126]|giv|giv[128]	coref|coref	24-36|24-36
23-15	2456-2463	process	abstract[125]|abstract[126]|abstract[128]	giv[125]|giv[126]|giv[128]	_	_
23-16	2464-2466	of	abstract[125]|abstract[126]|abstract[128]	giv[125]|giv[126]|giv[128]	_	_
23-17	2467-2472	image	abstract[125]|abstract[126]|abstract[128]|abstract|abstract[130]	giv[125]|giv[126]|giv[128]|giv|giv[130]	ana|coref|ana|coref	24-3[0_130]|27-21[178_0]|24-3[0_130]|27-21[178_0]
23-18	2473-2482	retrieval	abstract[125]|abstract[126]|abstract[128]|abstract[130]	giv[125]|giv[126]|giv[128]|giv[130]	_	_
23-19	2483-2484	.	_	_	_	_

#Text=However , it is computationally intensive when the images of a database are described with high-dimensional features , limiting its scope of applications . Azzi et al. use a global feature-based system to reduce the search space and find candidate images in the database , then the local feature scale-invariant feature transform ( SIFT ) is adopted for points matching in pose estimation .
24-1	2485-2492	However	_	_	_	_
24-2	2493-2494	,	_	_	_	_
24-3	2495-2497	it	abstract	giv	ana	24-20
24-4	2498-2500	is	_	_	_	_
24-5	2501-2516	computationally	_	_	_	_
24-6	2517-2526	intensive	_	_	_	_
24-7	2527-2531	when	_	_	_	_
24-8	2532-2535	the	object[132]	giv[132]	coref	24-40[142_132]
24-9	2536-2542	images	object[132]	giv[132]	_	_
24-10	2543-2545	of	object[132]	giv[132]	_	_
24-11	2546-2547	a	object[132]|object[133]	giv[132]|giv[133]	coref	24-43[143_133]
24-12	2548-2556	database	object[132]|object[133]	giv[132]|giv[133]	_	_
24-13	2557-2560	are	_	_	_	_
24-14	2561-2570	described	_	_	_	_
24-15	2571-2575	with	_	_	_	_
24-16	2576-2592	high-dimensional	abstract[134]	giv[134]	coref	28-15[186_134]
24-17	2593-2601	features	abstract[134]	giv[134]	_	_
24-18	2602-2603	,	_	_	_	_
24-19	2604-2612	limiting	_	_	_	_
24-20	2613-2616	its	abstract|abstract[136]	giv|new[136]	coref|coref	28-30[190_0]|28-30[190_0]
24-21	2617-2622	scope	abstract[136]	new[136]	_	_
24-22	2623-2625	of	abstract[136]	new[136]	_	_
24-23	2626-2638	applications	abstract[136]|abstract	new[136]|new	_	_
24-24	2639-2640	.	_	_	_	_
24-25	2641-2645	Azzi	person	new	_	_
24-26	2646-2648	et	_	_	_	_
24-27	2649-2652	al.	_	_	_	_
24-28	2653-2656	use	_	_	_	_
24-29	2657-2658	a	abstract[139]	giv[139]	_	_
24-30	2659-2665	global	abstract[139]	giv[139]	_	_
24-31	2666-2679	feature-based	abstract[139]	giv[139]	_	_
24-32	2680-2686	system	abstract[139]	giv[139]	_	_
24-33	2687-2689	to	_	_	_	_
24-34	2690-2696	reduce	_	_	_	_
24-35	2697-2700	the	abstract[141]	new[141]	_	_
24-36	2701-2707	search	event|abstract[141]	giv|new[141]	coref	25-11[154_0]
24-37	2708-2713	space	abstract[141]	new[141]	_	_
24-38	2714-2717	and	_	_	_	_
24-39	2718-2722	find	_	_	_	_
24-40	2723-2732	candidate	object[142]	giv[142]	coref	27-24[180_142]
24-41	2733-2739	images	object[142]	giv[142]	_	_
24-42	2740-2742	in	_	_	_	_
24-43	2743-2746	the	object[143]	giv[143]	coref	27-24[0_143]
24-44	2747-2755	database	object[143]	giv[143]	_	_
24-45	2756-2757	,	_	_	_	_
24-46	2758-2762	then	_	_	_	_
24-47	2763-2766	the	abstract[146]	new[146]	_	_
24-48	2767-2772	local	abstract[146]	new[146]	_	_
24-49	2773-2780	feature	abstract|abstract[146]	giv|new[146]	coref	24-51
24-50	2781-2796	scale-invariant	abstract[146]	new[146]	_	_
24-51	2797-2804	feature	abstract|abstract[146]	giv|new[146]	coref	26-29
24-52	2805-2814	transform	abstract[146]	new[146]	_	_
24-53	2815-2816	(	_	_	_	_
24-54	2817-2821	SIFT	_	_	_	_
24-55	2822-2823	)	_	_	_	_
24-56	2824-2826	is	_	_	_	_
24-57	2827-2834	adopted	_	_	_	_
24-58	2835-2838	for	_	_	_	_
24-59	2839-2845	points	abstract	new	_	_
24-60	2846-2854	matching	_	_	_	_
24-61	2855-2857	in	_	_	_	_
24-62	2858-2862	pose	abstract|abstract[149]	new|new[149]	coref|coref	32-16[222_0]|32-16[222_0]
24-63	2863-2873	estimation	abstract[149]	new[149]	_	_
24-64	2874-2875	.	_	_	_	_

#Text=Some researchers try to trade accuracy for rapidity by using approximate nearest neighbor search , such as quantization and vocabulary tree .
25-1	2876-2880	Some	person[150]	new[150]	_	_
25-2	2881-2892	researchers	person[150]	new[150]	_	_
25-3	2893-2896	try	_	_	_	_
25-4	2897-2899	to	_	_	_	_
25-5	2900-2905	trade	_	_	_	_
25-6	2906-2914	accuracy	abstract	new	coref	37-9[250_0]
25-7	2915-2918	for	_	_	_	_
25-8	2919-2927	rapidity	abstract	new	_	_
25-9	2928-2930	by	_	_	_	_
25-10	2931-2936	using	_	_	_	_
25-11	2937-2948	approximate	event[154]	giv[154]	coref	26-10[162_154]
25-12	2949-2956	nearest	event[154]	giv[154]	_	_
25-13	2957-2965	neighbor	person|event[154]	new|giv[154]	_	_
25-14	2966-2972	search	event[154]	giv[154]	_	_
25-15	2973-2974	,	event[154]	giv[154]	_	_
25-16	2975-2979	such	event[154]	giv[154]	_	_
25-17	2980-2982	as	event[154]	giv[154]	_	_
25-18	2983-2995	quantization	event[154]|abstract|plant[157]	giv[154]|new|new[157]	_	_
25-19	2996-2999	and	event[154]|plant[157]	giv[154]|new[157]	_	_
25-20	3000-3010	vocabulary	event[154]|abstract|plant[157]	giv[154]|new|new[157]	_	_
25-21	3011-3015	tree	event[154]|plant[157]	giv[154]|new[157]	_	_
25-22	3016-3017	.	_	_	_	_

#Text=Another common way to save time and memory of similarity search is principal component analysis ( PCA ) , which has been used to reduce the size of feature vectors and descriptors .
26-1	3018-3025	Another	abstract[158]	new[158]	coref	26-13[164_158]
26-2	3026-3032	common	abstract[158]	new[158]	_	_
26-3	3033-3036	way	abstract[158]	new[158]	_	_
26-4	3037-3039	to	_	_	_	_
26-5	3040-3044	save	_	_	_	_
26-6	3045-3049	time	time	new	coref	37-14[252_0]
26-7	3050-3053	and	_	_	_	_
26-8	3054-3060	memory	abstract[160]	new[160]	_	_
26-9	3061-3063	of	abstract[160]	new[160]	_	_
26-10	3064-3074	similarity	abstract[160]|abstract|event[162]	new[160]|giv|giv[162]	coref|coref	27-18[176_0]|27-18[176_0]
26-11	3075-3081	search	abstract[160]|event[162]	new[160]|giv[162]	_	_
26-12	3082-3084	is	_	_	_	_
26-13	3085-3094	principal	abstract[164]	giv[164]	appos	26-17[0_164]
26-14	3095-3104	component	abstract|abstract[164]	new|giv[164]	_	_
26-15	3105-3113	analysis	abstract[164]	giv[164]	_	_
26-16	3114-3115	(	_	_	_	_
26-17	3116-3119	PCA	abstract	giv	_	_
26-18	3120-3121	)	_	_	_	_
26-19	3122-3123	,	_	_	_	_
26-20	3124-3129	which	_	_	_	_
26-21	3130-3133	has	_	_	_	_
26-22	3134-3138	been	_	_	_	_
26-23	3139-3143	used	_	_	_	_
26-24	3144-3146	to	_	_	_	_
26-25	3147-3153	reduce	_	_	_	_
26-26	3154-3157	the	abstract[166]	new[166]	_	_
26-27	3158-3162	size	abstract[166]	new[166]	_	_
26-28	3163-3165	of	abstract[166]	new[166]	_	_
26-29	3166-3173	feature	abstract[166]|abstract|object[168]	new[166]|giv|giv[168]	coref|coref	34-15|34-15
26-30	3174-3181	vectors	abstract[166]|object[168]	new[166]|giv[168]	_	_
26-31	3182-3185	and	abstract[166]|object[168]	new[166]|giv[168]	_	_
26-32	3186-3197	descriptors	abstract[166]|object[168]|abstract	new[166]|giv[168]|giv	coref	29-10[197_0]
26-33	3198-3199	.	_	_	_	_

#Text=Some works use correlation algorithms , such as sum of absolute difference ( SAD ) , for computing similarity between query image and database images .
27-1	3200-3204	Some	abstract[170]	new[170]	_	_
27-2	3205-3210	works	abstract[170]	new[170]	_	_
27-3	3211-3214	use	_	_	_	_
27-4	3215-3226	correlation	abstract|abstract[172]	new|giv[172]	coref|coref	28-5[182_172]|28-5[182_172]
27-5	3227-3237	algorithms	abstract[172]	giv[172]	_	_
27-6	3238-3239	,	abstract[172]	giv[172]	_	_
27-7	3240-3244	such	abstract[172]	giv[172]	_	_
27-8	3245-3247	as	abstract[172]	giv[172]	_	_
27-9	3248-3251	sum	abstract[172]|abstract[173]	giv[172]|new[173]	_	_
27-10	3252-3254	of	abstract[172]|abstract[173]	giv[172]|new[173]	_	_
27-11	3255-3263	absolute	abstract[172]|abstract[173]|abstract[174]	giv[172]|new[173]|new[174]	_	_
27-12	3264-3274	difference	abstract[172]|abstract[173]|abstract[174]	giv[172]|new[173]|new[174]	_	_
27-13	3275-3276	(	abstract[172]	giv[172]	_	_
27-14	3277-3280	SAD	abstract[172]	giv[172]	_	_
27-15	3281-3282	)	abstract[172]	giv[172]	_	_
27-16	3283-3284	,	abstract[172]	giv[172]	_	_
27-17	3285-3288	for	abstract[172]	giv[172]	_	_
27-18	3289-3298	computing	abstract[172]|abstract|abstract[176]	giv[172]|new|giv[176]	_	_
27-19	3299-3309	similarity	abstract[172]|abstract[176]	giv[172]|giv[176]	_	_
27-20	3310-3317	between	abstract[172]|abstract[176]	giv[172]|giv[176]	_	_
27-21	3318-3323	query	abstract[172]|abstract[176]|abstract|abstract[178]	giv[172]|giv[176]|giv|giv[178]	coref|coref|coref|coref	28-26[0_178]|30-15|28-26[0_178]|30-15
27-22	3324-3329	image	abstract[172]|abstract[176]|abstract[178]	giv[172]|giv[176]|giv[178]	_	_
27-23	3330-3333	and	abstract[172]|abstract[176]	giv[172]|giv[176]	_	_
27-24	3334-3342	database	abstract[172]|abstract[176]|object|object[180]	giv[172]|giv[176]|giv|giv[180]	coref|coref|coref|coref	30-2[200_180]|34-6[232_0]|30-2[200_180]|34-6[232_0]
27-25	3343-3349	images	abstract[172]|abstract[176]|object[180]	giv[172]|giv[176]|giv[180]	_	_
27-26	3350-3351	.	_	_	_	_

#Text=In recent studies , deep learning-based algorithms are an alternative to aforementioned methods . Razavian et al. use features extracted from a network as an image representation for image retrieval in a diverse set of datasets . Yandex et al.
28-1	3352-3354	In	_	_	_	_
28-2	3355-3361	recent	abstract[181]	new[181]	_	_
28-3	3362-3369	studies	abstract[181]	new[181]	_	_
28-4	3370-3371	,	_	_	_	_
28-5	3372-3376	deep	abstract[182]	giv[182]	coref	28-9[183_182]
28-6	3377-3391	learning-based	abstract[182]	giv[182]	_	_
28-7	3392-3402	algorithms	abstract[182]	giv[182]	_	_
28-8	3403-3406	are	_	_	_	_
28-9	3407-3409	an	abstract[183]	giv[183]	_	_
28-10	3410-3421	alternative	abstract[183]	giv[183]	_	_
28-11	3422-3424	to	abstract[183]	giv[183]	_	_
28-12	3425-3439	aforementioned	abstract[183]|abstract[184]	giv[183]|giv[184]	coref	32-1[218_184]
28-13	3440-3447	methods	abstract[183]|abstract[184]	giv[183]|giv[184]	_	_
28-14	3448-3449	.	abstract[183]|abstract[184]	giv[183]|giv[184]	_	_
28-15	3450-3458	Razavian	abstract[183]|abstract[184]|abstract[186]	giv[183]|giv[184]|giv[186]	coref	29-6[195_186]
28-16	3459-3461	et	abstract[183]|abstract[184]|abstract[186]	giv[183]|giv[184]|giv[186]	_	_
28-17	3462-3465	al.	abstract[183]|abstract[184]|abstract[186]	giv[183]|giv[184]|giv[186]	_	_
28-18	3466-3469	use	abstract[183]|abstract[184]|abstract|abstract[186]	giv[183]|giv[184]|new|giv[186]	_	_
28-19	3470-3478	features	abstract[183]|abstract[184]|abstract[186]	giv[183]|giv[184]|giv[186]	_	_
28-20	3479-3488	extracted	_	_	_	_
28-21	3489-3493	from	_	_	_	_
28-22	3494-3495	a	abstract[187]	new[187]	_	_
28-23	3496-3503	network	abstract[187]	new[187]	_	_
28-24	3504-3506	as	_	_	_	_
28-25	3507-3509	an	_	_	_	_
28-26	3510-3515	image	abstract	giv	coref	28-29
28-27	3516-3530	representation	_	_	_	_
28-28	3531-3534	for	_	_	_	_
28-29	3535-3540	image	abstract	giv	coref	29-13
28-30	3541-3550	retrieval	abstract[190]	giv[190]	coref	29-13[199_190]
28-31	3551-3553	in	abstract[190]	giv[190]	_	_
28-32	3554-3555	a	abstract[190]|object[191]	giv[190]|new[191]	_	_
28-33	3556-3563	diverse	abstract[190]|object[191]	giv[190]|new[191]	_	_
28-34	3564-3567	set	abstract[190]|object[191]	giv[190]|new[191]	_	_
28-35	3568-3570	of	abstract[190]|object[191]	giv[190]|new[191]	_	_
28-36	3571-3579	datasets	abstract[190]|object[191]|abstract	giv[190]|new[191]|new	_	_
28-37	3580-3581	.	_	_	_	_
28-38	3582-3588	Yandex	person	new	_	_
28-39	3589-3591	et	_	_	_	_
28-40	3592-3595	al.	_	_	_	_

#Text=propose a method that aggregates local deep features to product descriptors for image retrieval .
29-1	3596-3603	propose	_	_	_	_
29-2	3604-3605	a	abstract[194]	new[194]	coref	39-7[270_194]
29-3	3606-3612	method	abstract[194]	new[194]	_	_
29-4	3613-3617	that	_	_	_	_
29-5	3618-3628	aggregates	_	_	_	_
29-6	3629-3634	local	abstract[195]	giv[195]	coref	37-19[253_195]
29-7	3635-3639	deep	abstract[195]	giv[195]	_	_
29-8	3640-3648	features	abstract[195]	giv[195]	_	_
29-9	3649-3651	to	_	_	_	_
29-10	3652-3659	product	abstract|abstract[197]	new|giv[197]	coref|coref	34-15[234_197]|34-15[234_197]
29-11	3660-3671	descriptors	abstract[197]	giv[197]	_	_
29-12	3672-3675	for	abstract[197]	giv[197]	_	_
29-13	3676-3681	image	abstract[197]|abstract|abstract[199]	giv[197]|giv|giv[199]	coref|coref	30-14[204_0]|30-14[204_0]
29-14	3682-3691	retrieval	abstract[197]|abstract[199]	giv[197]|giv[199]	_	_
29-15	3692-3693	.	_	_	_	_

#Text=After a set of candidate images are retrieved , the position information of the query image is calculated according to the geospatial information of these candidate images through a weighting scheme or linear combination .
30-1	3694-3699	After	_	_	_	_
30-2	3700-3701	a	object[200]	giv[200]	coref	30-25[207_200]
30-3	3702-3705	set	object[200]	giv[200]	_	_
30-4	3706-3708	of	object[200]	giv[200]	_	_
30-5	3709-3718	candidate	object[200]	giv[200]	_	_
30-6	3719-3725	images	object[200]	giv[200]	_	_
30-7	3726-3729	are	_	_	_	_
30-8	3730-3739	retrieved	_	_	_	_
30-9	3740-3741	,	_	_	_	_
30-10	3742-3745	the	abstract[202]	new[202]	_	_
30-11	3746-3754	position	abstract|abstract[202]	giv|new[202]	coref	31-5
30-12	3755-3766	information	abstract[202]	new[202]	_	_
30-13	3767-3769	of	abstract[202]	new[202]	_	_
30-14	3770-3773	the	abstract[202]|abstract[204]	new[202]|giv[204]	coref	32-18[224_204]
30-15	3774-3779	query	abstract[202]|abstract|abstract[204]	new[202]|giv|giv[204]	coref	32-19
30-16	3780-3785	image	abstract[202]|abstract[204]	new[202]|giv[204]	_	_
30-17	3786-3788	is	_	_	_	_
30-18	3789-3799	calculated	_	_	_	_
30-19	3800-3809	according	_	_	_	_
30-20	3810-3812	to	_	_	_	_
30-21	3813-3816	the	abstract[205]	new[205]	_	_
30-22	3817-3827	geospatial	abstract[205]	new[205]	_	_
30-23	3828-3839	information	abstract[205]	new[205]	_	_
30-24	3840-3842	of	abstract[205]	new[205]	_	_
30-25	3843-3848	these	abstract[205]|object[207]	new[205]|giv[207]	coref	34-18[235_207]
30-26	3849-3858	candidate	abstract[205]|person|object[207]	new[205]|giv|giv[207]	_	_
30-27	3859-3865	images	abstract[205]|object[207]	new[205]|giv[207]	_	_
30-28	3866-3873	through	abstract[205]|object[207]	new[205]|giv[207]	_	_
30-29	3874-3875	a	abstract[205]|object[207]|abstract[208]	new[205]|giv[207]|new[208]	_	_
30-30	3876-3885	weighting	abstract[205]|object[207]|abstract[208]	new[205]|giv[207]|new[208]	_	_
30-31	3886-3892	scheme	abstract[205]|object[207]|abstract[208]	new[205]|giv[207]|new[208]	_	_
30-32	3893-3895	or	abstract[205]|object[207]	new[205]|giv[207]	_	_
30-33	3896-3902	linear	abstract[205]|object[207]|abstract[209]	new[205]|giv[207]|new[209]	_	_
30-34	3903-3914	combination	abstract[205]|object[207]|abstract[209]	new[205]|giv[207]|new[209]	_	_
30-35	3915-3916	.	_	_	_	_

#Text=However , because this position result is not calculated by strict geometric relations , it is rough in most cases and difficult to meet the requirement of high-accuracy positioning .
31-1	3917-3924	However	_	_	_	_
31-2	3925-3926	,	_	_	_	_
31-3	3927-3934	because	_	_	_	_
31-4	3935-3939	this	abstract[211]	new[211]	ana	31-15[0_211]
31-5	3940-3948	position	abstract|abstract[211]	giv|new[211]	_	_
31-6	3949-3955	result	abstract[211]	new[211]	_	_
31-7	3956-3958	is	_	_	_	_
31-8	3959-3962	not	_	_	_	_
31-9	3963-3973	calculated	_	_	_	_
31-10	3974-3976	by	_	_	_	_
31-11	3977-3983	strict	abstract[212]	new[212]	_	_
31-12	3984-3993	geometric	abstract[212]	new[212]	_	_
31-13	3994-4003	relations	abstract[212]	new[212]	_	_
31-14	4004-4005	,	_	_	_	_
31-15	4006-4008	it	abstract	giv	_	_
31-16	4009-4011	is	_	_	_	_
31-17	4012-4017	rough	_	_	_	_
31-18	4018-4020	in	_	_	_	_
31-19	4021-4025	most	abstract[214]	new[214]	_	_
31-20	4026-4031	cases	abstract[214]	new[214]	_	_
31-21	4032-4035	and	_	_	_	_
31-22	4036-4045	difficult	_	_	_	_
31-23	4046-4048	to	_	_	_	_
31-24	4049-4053	meet	_	_	_	_
31-25	4054-4057	the	abstract[215]	new[215]	_	_
31-26	4058-4069	requirement	abstract[215]	new[215]	_	_
31-27	4070-4072	of	abstract[215]	new[215]	_	_
31-28	4073-4086	high-accuracy	abstract[215]|abstract[216]	new[215]|giv[216]	coref	32-3[0_216]
31-29	4087-4098	positioning	abstract[215]|abstract[216]	new[215]|giv[216]	_	_
31-30	4099-4100	.	_	_	_	_

#Text=Visual landmarks-based positioning methods aim to provide a six degrees of freedom ( DoF ) pose of the query image .
32-1	4101-4107	Visual	abstract[218]	giv[218]	coref	37-5[249_218]
32-2	4108-4123	landmarks-based	abstract[218]	giv[218]	_	_
32-3	4124-4135	positioning	abstract|abstract[218]	giv|giv[218]	coref	39-15
32-4	4136-4143	methods	abstract[218]	giv[218]	_	_
32-5	4144-4147	aim	_	_	_	_
32-6	4148-4150	to	_	_	_	_
32-7	4151-4158	provide	_	_	_	_
32-8	4159-4160	a	abstract[219]	new[219]	_	_
32-9	4161-4164	six	abstract[219]	new[219]	_	_
32-10	4165-4172	degrees	abstract[219]	new[219]	_	_
32-11	4173-4175	of	abstract[219]	new[219]	_	_
32-12	4176-4183	freedom	abstract[219]|abstract	new[219]|new	appos	32-14
32-13	4184-4185	(	_	_	_	_
32-14	4186-4189	DoF	abstract	giv	_	_
32-15	4190-4191	)	_	_	_	_
32-16	4192-4196	pose	abstract[222]	giv[222]	_	_
32-17	4197-4199	of	abstract[222]	giv[222]	_	_
32-18	4200-4203	the	abstract[222]|abstract[224]	giv[222]|giv[224]	coref	36-5[244_224]
32-19	4204-4209	query	abstract[222]|abstract|abstract[224]	giv[222]|giv|giv[224]	coref	36-5
32-20	4210-4215	image	abstract[222]|abstract[224]	giv[222]|giv[224]	_	_
32-21	4216-4217	.	_	_	_	_

#Text=Generally , visual landmarks in the indoor environments includes natural landmarks and artificial landmarks .
33-1	4218-4227	Generally	_	_	_	_
33-2	4228-4229	,	_	_	_	_
33-3	4230-4236	visual	object[225]	new[225]	coref	34-1[230_225]
33-4	4237-4246	landmarks	object[225]	new[225]	_	_
33-5	4247-4249	in	object[225]	new[225]	_	_
33-6	4250-4253	the	object[225]|place[227]	new[225]|new[227]	coref	41-8[279_227]
33-7	4254-4260	indoor	object[225]|abstract|place[227]	new[225]|giv|new[227]	coref	37-32
33-8	4261-4273	environments	object[225]|place[227]	new[225]|new[227]	_	_
33-9	4274-4282	includes	_	_	_	_
33-10	4283-4290	natural	abstract[228]	new[228]	_	_
33-11	4291-4300	landmarks	abstract[228]	new[228]	_	_
33-12	4301-4304	and	_	_	_	_
33-13	4305-4315	artificial	object[229]	new[229]	coref	38-14[262_229]
33-14	4316-4325	landmarks	object[229]	new[229]	_	_
33-15	4326-4327	.	_	_	_	_

#Text=The natural landmarks refer to the geo-tagged 3D database , which is represented by feature descriptors or images with poses .
34-1	4328-4331	The	object[230]	giv[230]	coref	38-4[260_230]
34-2	4332-4339	natural	object[230]	giv[230]	_	_
34-3	4340-4349	landmarks	object[230]	giv[230]	_	_
34-4	4350-4355	refer	_	_	_	_
34-5	4356-4358	to	_	_	_	_
34-6	4359-4362	the	object[232]	giv[232]	coref	35-1[237_232]
34-7	4363-4373	geo-tagged	object[232]	giv[232]	_	_
34-8	4374-4376	3D	abstract|object[232]	new|giv[232]	coref	37-26
34-9	4377-4385	database	object[232]	giv[232]	_	_
34-10	4386-4387	,	_	_	_	_
34-11	4388-4393	which	_	_	_	_
34-12	4394-4396	is	_	_	_	_
34-13	4397-4408	represented	_	_	_	_
34-14	4409-4411	by	_	_	_	_
34-15	4412-4419	feature	abstract|abstract[234]	giv|giv[234]	_	_
34-16	4420-4431	descriptors	abstract[234]	giv[234]	_	_
34-17	4432-4434	or	abstract[234]	giv[234]	_	_
34-18	4435-4441	images	abstract[234]|object[235]	giv[234]|giv[235]	_	_
34-19	4442-4446	with	abstract[234]|object[235]	giv[234]|giv[235]	_	_
34-20	4447-4452	poses	abstract[234]|object[235]|abstract	giv[234]|giv[235]|new	_	_
34-21	4453-4454	.	_	_	_	_

#Text=This database could have been built thanks to the mapping module of simultaneous localization and mapping ( SLAM ) .
35-1	4455-4459	This	object[237]	giv[237]	coref	37-25[257_237]
35-2	4460-4468	database	object[237]	giv[237]	_	_
35-3	4469-4474	could	_	_	_	_
35-4	4475-4479	have	_	_	_	_
35-5	4480-4484	been	_	_	_	_
35-6	4485-4490	built	_	_	_	_
35-7	4491-4497	thanks	_	_	_	_
35-8	4498-4500	to	_	_	_	_
35-9	4501-4504	the	abstract[239]	new[239]	coref	36-12[246_239]
35-10	4505-4512	mapping	object|abstract[239]	new|new[239]	_	_
35-11	4513-4519	module	abstract[239]	new[239]	_	_
35-12	4520-4522	of	abstract[239]	new[239]	_	_
35-13	4523-4535	simultaneous	abstract[239]|abstract[240]	new[239]|new[240]	_	_
35-14	4536-4548	localization	abstract[239]|abstract[240]	new[239]|new[240]	_	_
35-15	4549-4552	and	abstract[239]	new[239]	_	_
35-16	4553-4560	mapping	abstract[239]|abstract	new[239]|new	_	_
35-17	4561-4562	(	_	_	_	_
35-18	4563-4567	SLAM	_	_	_	_
35-19	4568-4569	)	_	_	_	_
35-20	4570-4571	.	_	_	_	_

#Text=Then the pose of query image is estimated by means of re-localization module and feature correspondence .
36-1	4572-4576	Then	_	_	_	_
36-2	4577-4580	the	abstract[242]	new[242]	_	_
36-3	4581-4585	pose	abstract[242]	new[242]	_	_
36-4	4586-4588	of	abstract[242]	new[242]	_	_
36-5	4589-4594	query	abstract[242]|abstract|abstract[244]	new[242]|giv|giv[244]	coref|coref|coref|coref	37-22|37-22[255_244]|37-22|37-22[255_244]
36-6	4595-4600	image	abstract[242]|abstract[244]	new[242]|giv[244]	_	_
36-7	4601-4603	is	_	_	_	_
36-8	4604-4613	estimated	_	_	_	_
36-9	4614-4616	by	_	_	_	_
36-10	4617-4622	means	_	_	_	_
36-11	4623-4625	of	_	_	_	_
36-12	4626-4641	re-localization	abstract|abstract[246]	new|giv[246]	_	_
36-13	4642-4648	module	abstract[246]	giv[246]	_	_
36-14	4649-4652	and	_	_	_	_
36-15	4653-4660	feature	_	_	_	_
36-16	4661-4675	correspondence	abstract	new	ana	37-12
36-17	4676-4677	.	_	_	_	_

#Text=Although the results of these methods are of good accuracy , it takes a long time to match the features of query image with geo-tagged 3D database , especially when the indoor scenes are large .
37-1	4678-4686	Although	_	_	_	_
37-2	4687-4690	the	abstract[248]	new[248]	_	_
37-3	4691-4698	results	abstract[248]	new[248]	_	_
37-4	4699-4701	of	abstract[248]	new[248]	_	_
37-5	4702-4707	these	abstract[248]|abstract[249]	new[248]|giv[249]	coref	38-11[0_249]
37-6	4708-4715	methods	abstract[248]|abstract[249]	new[248]|giv[249]	_	_
37-7	4716-4719	are	_	_	_	_
37-8	4720-4722	of	_	_	_	_
37-9	4723-4727	good	abstract[250]	giv[250]	_	_
37-10	4728-4736	accuracy	abstract[250]	giv[250]	_	_
37-11	4737-4738	,	_	_	_	_
37-12	4739-4741	it	abstract	giv	_	_
37-13	4742-4747	takes	_	_	_	_
37-14	4748-4749	a	time[252]	giv[252]	_	_
37-15	4750-4754	long	time[252]	giv[252]	_	_
37-16	4755-4759	time	time[252]	giv[252]	_	_
37-17	4760-4762	to	_	_	_	_
37-18	4763-4768	match	_	_	_	_
37-19	4769-4772	the	abstract[253]	giv[253]	_	_
37-20	4773-4781	features	abstract[253]	giv[253]	_	_
37-21	4782-4784	of	abstract[253]	giv[253]	_	_
37-22	4785-4790	query	abstract[253]|abstract|abstract[255]	giv[253]|giv|giv[255]	_	_
37-23	4791-4796	image	abstract[253]|abstract[255]	giv[253]|giv[255]	_	_
37-24	4797-4801	with	abstract[253]|abstract[255]	giv[253]|giv[255]	_	_
37-25	4802-4812	geo-tagged	abstract[253]|abstract[255]|object[257]	giv[253]|giv[255]|giv[257]	_	_
37-26	4813-4815	3D	abstract[253]|abstract[255]|abstract|object[257]	giv[253]|giv[255]|giv|giv[257]	_	_
37-27	4816-4824	database	abstract[253]|abstract[255]|object[257]	giv[253]|giv[255]|giv[257]	_	_
37-28	4825-4826	,	_	_	_	_
37-29	4827-4837	especially	_	_	_	_
37-30	4838-4842	when	_	_	_	_
37-31	4843-4846	the	abstract[259]	new[259]	_	_
37-32	4847-4853	indoor	place|abstract[259]	giv|new[259]	_	_
37-33	4854-4860	scenes	abstract[259]	new[259]	_	_
37-34	4861-4864	are	_	_	_	_
37-35	4865-4870	large	_	_	_	_
37-36	4871-4872	.	_	_	_	_

#Text=In addition to natural landmarks , there are also positioning methods based on artificial landmarks , e. g. , Degol et al. proposed a fiducial marker and detection algorithm .
38-1	4873-4875	In	_	_	_	_
38-2	4876-4884	addition	_	_	_	_
38-3	4885-4887	to	_	_	_	_
38-4	4888-4895	natural	object[260]	giv[260]	_	_
38-5	4896-4905	landmarks	object[260]	giv[260]	_	_
38-6	4906-4907	,	_	_	_	_
38-7	4908-4913	there	_	_	_	_
38-8	4914-4917	are	_	_	_	_
38-9	4918-4922	also	_	_	_	_
38-10	4923-4934	positioning	_	_	_	_
38-11	4935-4942	methods	abstract	giv	coref	41-2[277_0]
38-12	4943-4948	based	_	_	_	_
38-13	4949-4951	on	_	_	_	_
38-14	4952-4962	artificial	object[262]	giv[262]	_	_
38-15	4963-4972	landmarks	object[262]	giv[262]	_	_
38-16	4973-4974	,	_	_	_	_
38-17	4975-4977	e.	plant[263]	new[263]	_	_
38-18	4978-4980	g.	plant[263]	new[263]	_	_
38-19	4981-4982	,	_	_	_	_
38-20	4983-4988	Degol	person	new	_	_
38-21	4989-4991	et	_	_	_	_
38-22	4992-4995	al.	_	_	_	_
38-23	4996-5004	proposed	_	_	_	_
38-24	5005-5006	a	abstract[265]	new[265]	coref	40-5[274_265]
38-25	5007-5015	fiducial	abstract[265]	new[265]	_	_
38-26	5016-5022	marker	abstract[265]	new[265]	_	_
38-27	5023-5026	and	_	_	_	_
38-28	5027-5036	detection	abstract|abstract[267]	new|new[267]	_	_
38-29	5037-5046	algorithm	abstract[267]	new[267]	_	_
38-30	5047-5048	.	_	_	_	_

#Text=In reference , the authors proposed a method to simultaneously solve the problems of positioning from a set of squared planar markers .
39-1	5049-5051	In	_	_	_	_
39-2	5052-5061	reference	abstract	new	_	_
39-3	5062-5063	,	_	_	_	_
39-4	5064-5067	the	person[269]	new[269]	_	_
39-5	5068-5075	authors	person[269]	new[269]	_	_
39-6	5076-5084	proposed	_	_	_	_
39-7	5085-5086	a	abstract[270]	giv[270]	_	_
39-8	5087-5093	method	abstract[270]	giv[270]	_	_
39-9	5094-5096	to	_	_	_	_
39-10	5097-5111	simultaneously	_	_	_	_
39-11	5112-5117	solve	_	_	_	_
39-12	5118-5121	the	abstract[271]	new[271]	_	_
39-13	5122-5130	problems	abstract[271]	new[271]	_	_
39-14	5131-5133	of	abstract[271]	new[271]	_	_
39-15	5134-5145	positioning	abstract[271]|abstract	new[271]|giv	_	_
39-16	5146-5150	from	_	_	_	_
39-17	5151-5152	a	object[273]	new[273]	coref	41-6[0_273]
39-18	5153-5156	set	object[273]	new[273]	_	_
39-19	5157-5159	of	object[273]	new[273]	_	_
39-20	5160-5167	squared	object[273]	new[273]	_	_
39-21	5168-5174	planar	object[273]	new[273]	_	_
39-22	5175-5182	markers	object[273]	new[273]	_	_
39-23	5183-5184	.	_	_	_	_

#Text=However , positioning from a planar marker suffers from the ambiguity problem .
40-1	5185-5192	However	_	_	_	_
40-2	5193-5194	,	_	_	_	_
40-3	5195-5206	positioning	_	_	_	_
40-4	5207-5211	from	_	_	_	_
40-5	5212-5213	a	abstract[274]	giv[274]	_	_
40-6	5214-5220	planar	abstract[274]	giv[274]	_	_
40-7	5221-5227	marker	abstract[274]	giv[274]	_	_
40-8	5228-5235	suffers	_	_	_	_
40-9	5236-5240	from	_	_	_	_
40-10	5241-5244	the	abstract[276]	new[276]	_	_
40-11	5245-5254	ambiguity	abstract|abstract[276]	new|new[276]	_	_
40-12	5255-5262	problem	abstract[276]	new[276]	_	_
40-13	5263-5264	.	_	_	_	_

#Text=Since these methods require posting markers in the environments , they are not suitable for places such as shopping malls that maintain a clean appearance .
41-1	5265-5270	Since	_	_	_	_
41-2	5271-5276	these	abstract[277]	giv[277]	ana	41-11[0_277]
41-3	5277-5284	methods	abstract[277]	giv[277]	_	_
41-4	5285-5292	require	_	_	_	_
41-5	5293-5300	posting	_	_	_	_
41-6	5301-5308	markers	object	giv	_	_
41-7	5309-5311	in	_	_	_	_
41-8	5312-5315	the	place[279]	giv[279]	_	_
41-9	5316-5328	environments	place[279]	giv[279]	_	_
41-10	5329-5330	,	_	_	_	_
41-11	5331-5335	they	abstract	giv	_	_
41-12	5336-5339	are	_	_	_	_
41-13	5340-5343	not	_	_	_	_
41-14	5344-5352	suitable	_	_	_	_
41-15	5353-5356	for	_	_	_	_
41-16	5357-5363	places	place[281]	new[281]	_	_
41-17	5364-5368	such	place[281]	new[281]	_	_
41-18	5369-5371	as	place[281]	new[281]	_	_
41-19	5372-5380	shopping	place[281]|place[282]	new[281]|new[282]	_	_
41-20	5381-5386	malls	place[281]|place[282]	new[281]|new[282]	_	_
41-21	5387-5391	that	_	_	_	_
41-22	5392-5400	maintain	_	_	_	_
41-23	5401-5402	a	abstract[283]	new[283]	_	_
41-24	5403-5408	clean	abstract[283]	new[283]	_	_
41-25	5409-5419	appearance	abstract[283]	new[283]	_	_
41-26	5420-5421	.	_	_	_	_
