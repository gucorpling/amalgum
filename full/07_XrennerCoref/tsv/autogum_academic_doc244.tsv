#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=2 .
1-1	0-1	2	quantity	new	_	_
1-2	2-3	.	_	_	_	_

#Text=Gradient Descent Learning of Stochastic Neural Networks
2-1	4-12	Gradient	substance	new	_	_
2-2	13-20	Descent	abstract|abstract[4]	new|new[4]	coref|coref	8-5[0_4]|8-5[0_4]
2-3	21-29	Learning	abstract[4]	new[4]	_	_
2-4	30-32	of	abstract[4]	new[4]	_	_
2-5	33-43	Stochastic	abstract[4]|abstract[6]	new[4]|new[6]	coref	4-1[8_6]
2-6	44-50	Neural	abstract[4]|abstract|abstract[6]	new[4]|new|new[6]	_	_
2-7	51-59	Networks	abstract[4]|abstract[6]	new[4]|new[6]	_	_

#Text=2.1 .
3-1	60-63	2.1	abstract	new	_	_
3-2	64-65	.	_	_	_	_

#Text=Stochastic Neural Networks
4-1	66-76	Stochastic	abstract[8]	giv[8]	_	_
4-2	77-83	Neural	abstract[8]	giv[8]	_	_
4-3	84-92	Networks	abstract[8]	giv[8]	_	_

#Text=Since the natural gradient is derived from stochastic neural network models , let us start from the brief description of the two popular stochastic models .
5-1	93-98	Since	_	_	_	_
5-2	99-102	the	abstract[9]	new[9]	coref	20-1[126_9]
5-3	103-110	natural	abstract[9]	new[9]	_	_
5-4	111-119	gradient	abstract[9]	new[9]	_	_
5-5	120-122	is	_	_	_	_
5-6	123-130	derived	_	_	_	_
5-7	131-135	from	_	_	_	_
5-8	136-146	stochastic	abstract[11]	new[11]	coref	5-21[14_11]
5-9	147-153	neural	abstract[11]	new[11]	_	_
5-10	154-161	network	abstract|abstract[11]	new|new[11]	coref	14-6
5-11	162-168	models	abstract[11]	new[11]	_	_
5-12	169-170	,	_	_	_	_
5-13	171-174	let	_	_	_	_
5-14	175-177	us	person	acc	ana	13-2
5-15	178-183	start	_	_	_	_
5-16	184-188	from	_	_	_	_
5-17	189-192	the	abstract[13]	new[13]	_	_
5-18	193-198	brief	abstract[13]	new[13]	_	_
5-19	199-210	description	abstract[13]	new[13]	_	_
5-20	211-213	of	abstract[13]	new[13]	_	_
5-21	214-217	the	abstract[13]|abstract[14]	new[13]|giv[14]	coref	19-16[119_14]
5-22	218-221	two	abstract[13]|abstract[14]	new[13]|giv[14]	_	_
5-23	222-229	popular	abstract[13]|abstract[14]	new[13]|giv[14]	_	_
5-24	230-240	stochastic	abstract[13]|abstract[14]	new[13]|giv[14]	_	_
5-25	241-247	models	abstract[13]|abstract[14]	new[13]|giv[14]	_	_
5-26	248-249	.	_	_	_	_

#Text=Although typical neural networks , including deep networks , are defined as deterministic input-output mapping functions of input , output , and parameter , the observed data for training the networks always have inevitable noises and thus the input-output relation can be described in the stochastic manner .
6-1	250-258	Although	_	_	_	_
6-2	259-266	typical	place[15]	new[15]	coref	11-7[50_15]
6-3	267-273	neural	place[15]	new[15]	_	_
6-4	274-282	networks	place[15]	new[15]	_	_
6-5	283-284	,	place[15]	new[15]	_	_
6-6	285-294	including	place[15]	new[15]	_	_
6-7	295-299	deep	place[15]|place[16]	new[15]|new[16]	coref	6-30[22_16]
6-8	300-308	networks	place[15]|place[16]	new[15]|new[16]	_	_
6-9	309-310	,	_	_	_	_
6-10	311-314	are	_	_	_	_
6-11	315-322	defined	_	_	_	_
6-12	323-325	as	_	_	_	_
6-13	326-339	deterministic	_	_	_	_
6-14	340-352	input-output	_	_	_	_
6-15	353-360	mapping	object	new	_	_
6-16	361-370	functions	_	_	_	_
6-17	371-373	of	_	_	_	_
6-18	374-379	input	abstract	new	_	_
6-19	380-381	,	_	_	_	_
6-20	382-388	output	abstract	new	coref	7-5[27_0]
6-21	389-390	,	_	_	_	_
6-22	391-394	and	_	_	_	_
6-23	395-404	parameter	abstract	new	coref	8-13
6-24	405-406	,	_	_	_	_
6-25	407-410	the	abstract[21]	new[21]	_	_
6-26	411-419	observed	abstract[21]	new[21]	_	_
6-27	420-424	data	abstract[21]	new[21]	_	_
6-28	425-428	for	_	_	_	_
6-29	429-437	training	_	_	_	_
6-30	438-441	the	place[22]	giv[22]	_	_
6-31	442-450	networks	place[22]	giv[22]	_	_
6-32	451-457	always	_	_	_	_
6-33	458-462	have	_	_	_	_
6-34	463-473	inevitable	abstract[23]	new[23]	_	_
6-35	474-480	noises	abstract[23]	new[23]	_	_
6-36	481-484	and	_	_	_	_
6-37	485-489	thus	_	_	_	_
6-38	490-493	the	abstract[24]	new[24]	_	_
6-39	494-506	input-output	abstract[24]	new[24]	_	_
6-40	507-515	relation	abstract[24]	new[24]	_	_
6-41	516-519	can	_	_	_	_
6-42	520-522	be	_	_	_	_
6-43	523-532	described	_	_	_	_
6-44	533-535	in	_	_	_	_
6-45	536-539	the	abstract[25]	new[25]	_	_
6-46	540-550	stochastic	abstract[25]	new[25]	_	_
6-47	551-557	manner	abstract[25]	new[25]	_	_
6-48	558-559	.	_	_	_	_

#Text=In other words , the observed output can be regarded as a random vector that is dependent on the deterministic function and some additional stochastic process that is described by the conditional probability .
7-1	560-562	In	_	_	_	_
7-2	563-568	other	abstract[26]	new[26]	_	_
7-3	569-574	words	abstract[26]	new[26]	_	_
7-4	575-576	,	_	_	_	_
7-5	577-580	the	abstract[27]	giv[27]	coref	12-6[53_27]
7-6	581-589	observed	abstract[27]	giv[27]	_	_
7-7	590-596	output	abstract[27]	giv[27]	_	_
7-8	597-600	can	_	_	_	_
7-9	601-603	be	_	_	_	_
7-10	604-612	regarded	_	_	_	_
7-11	613-615	as	_	_	_	_
7-12	616-617	a	_	_	_	_
7-13	618-624	random	_	_	_	_
7-14	625-631	vector	_	_	_	_
7-15	632-636	that	_	_	_	_
7-16	637-639	is	_	_	_	_
7-17	640-649	dependent	_	_	_	_
7-18	650-652	on	_	_	_	_
7-19	653-656	the	abstract[28]	new[28]	coref	8-16[36_28]
7-20	657-670	deterministic	abstract[28]	new[28]	_	_
7-21	671-679	function	abstract[28]	new[28]	_	_
7-22	680-683	and	_	_	_	_
7-23	684-688	some	abstract[29]	new[29]	_	_
7-24	689-699	additional	abstract[29]	new[29]	_	_
7-25	700-710	stochastic	abstract[29]	new[29]	_	_
7-26	711-718	process	abstract[29]	new[29]	_	_
7-27	719-723	that	_	_	_	_
7-28	724-726	is	_	_	_	_
7-29	727-736	described	_	_	_	_
7-30	737-739	by	_	_	_	_
7-31	740-743	the	abstract[30]	new[30]	coref	11-22[0_30]
7-32	744-755	conditional	abstract[30]	new[30]	_	_
7-33	756-767	probability	abstract[30]	new[30]	_	_
7-34	768-769	.	_	_	_	_

#Text=Then the goal of learning is to find an optimal value of parameter that minimizes the loss function defined as negative log likelihood of given input-output sample .
8-1	770-774	Then	_	_	_	_
8-2	775-778	the	event[31]	new[31]	_	_
8-3	779-783	goal	event[31]	new[31]	_	_
8-4	784-786	of	event[31]	new[31]	_	_
8-5	787-795	learning	event[31]|abstract	new[31]|giv	coref	19-42[125_0]
8-6	796-798	is	_	_	_	_
8-7	799-801	to	_	_	_	_
8-8	802-806	find	_	_	_	_
8-9	807-809	an	abstract[33]	new[33]	_	_
8-10	810-817	optimal	abstract[33]	new[33]	_	_
8-11	818-823	value	abstract[33]	new[33]	_	_
8-12	824-826	of	abstract[33]	new[33]	_	_
8-13	827-836	parameter	abstract[33]|abstract	new[33]|giv	coref	9-13[41_0]
8-14	837-841	that	_	_	_	_
8-15	842-851	minimizes	abstract	new	coref|none	29-33[206_0]|8-15[0_206]
8-16	852-855	the	abstract[36]	giv[36]	coref	9-1[40_36]
8-17	856-860	loss	abstract|abstract[36]	new|giv[36]	coref	9-2
8-18	861-869	function	abstract[36]	giv[36]	_	_
8-19	870-877	defined	_	_	_	_
8-20	878-880	as	_	_	_	_
8-21	881-889	negative	_	_	_	_
8-22	890-893	log	object	new	coref	17-14
8-23	894-904	likelihood	_	_	_	_
8-24	905-907	of	_	_	_	_
8-25	908-913	given	abstract[38]	new[38]	_	_
8-26	914-926	input-output	abstract[38]	new[38]	_	_
8-27	927-933	sample	abstract[38]	new[38]	_	_
8-28	934-935	.	_	_	_	_

#Text=The loss function can then be written as ( 1 ) and the optimal parameter is described as ( 2 )
9-1	936-939	The	abstract[40]	giv[40]	coref	10-20[47_40]
9-2	940-944	loss	abstract|abstract[40]	giv|giv[40]	coref	13-29
9-3	945-953	function	abstract[40]	giv[40]	_	_
9-4	954-957	can	_	_	_	_
9-5	958-962	then	_	_	_	_
9-6	963-965	be	_	_	_	_
9-7	966-973	written	_	_	_	_
9-8	974-976	as	_	_	_	_
9-9	977-978	(	_	_	_	_
9-10	979-980	1	_	_	_	_
9-11	981-982	)	_	_	_	_
9-12	983-986	and	_	_	_	_
9-13	987-990	the	abstract[41]	giv[41]	coref	10-14[0_41]
9-14	991-998	optimal	abstract[41]	giv[41]	_	_
9-15	999-1008	parameter	abstract[41]	giv[41]	_	_
9-16	1009-1011	is	_	_	_	_
9-17	1012-1021	described	_	_	_	_
9-18	1022-1024	as	_	_	_	_
9-19	1025-1026	(	_	_	_	_
9-20	1027-1028	2	_	_	_	_
9-21	1029-1030	)	_	_	_	_

#Text=Note that the last term in equation ( 1 ) is independent of parameter and can be ignored in the objective function for optimization .
10-1	1031-1035	Note	_	_	_	_
10-2	1036-1040	that	_	_	_	_
10-3	1041-1044	the	abstract[42]	new[42]	_	_
10-4	1045-1049	last	abstract[42]	new[42]	_	_
10-5	1050-1054	term	abstract[42]	new[42]	_	_
10-6	1055-1057	in	abstract[42]	new[42]	_	_
10-7	1058-1066	equation	abstract[42]|abstract	new[42]|new	coref	17-17[105_0]
10-8	1067-1068	(	_	_	_	_
10-9	1069-1070	1	quantity	new	_	_
10-10	1071-1072	)	_	_	_	_
10-11	1073-1075	is	_	_	_	_
10-12	1076-1087	independent	_	_	_	_
10-13	1088-1090	of	_	_	_	_
10-14	1091-1100	parameter	abstract	giv	coref	28-27
10-15	1101-1104	and	_	_	_	_
10-16	1105-1108	can	_	_	_	_
10-17	1109-1111	be	_	_	_	_
10-18	1112-1119	ignored	_	_	_	_
10-19	1120-1122	in	_	_	_	_
10-20	1123-1126	the	abstract[47]	giv[47]	coref	12-14[57_47]
10-21	1127-1136	objective	abstract|abstract[47]	new|giv[47]	coref	29-26[204_0]
10-22	1137-1145	function	abstract[47]	giv[47]	_	_
10-23	1146-1149	for	abstract[47]	giv[47]	_	_
10-24	1150-1162	optimization	abstract[47]|abstract	giv[47]|new	_	_
10-25	1163-1164	.	_	_	_	_

#Text=Based on the general definition , the conventional neural networks can be regarded as a special case with a specific conditional probability distribution , .
11-1	1165-1170	Based	_	_	_	_
11-2	1171-1173	on	_	_	_	_
11-3	1174-1177	the	abstract[49]	new[49]	_	_
11-4	1178-1185	general	abstract[49]	new[49]	_	_
11-5	1186-1196	definition	abstract[49]	new[49]	_	_
11-6	1197-1198	,	_	_	_	_
11-7	1199-1202	the	place[50]	giv[50]	coref	12-17[0_50]
11-8	1203-1215	conventional	place[50]	giv[50]	_	_
11-9	1216-1222	neural	place[50]	giv[50]	_	_
11-10	1223-1231	networks	place[50]	giv[50]	_	_
11-11	1232-1235	can	_	_	_	_
11-12	1236-1238	be	_	_	_	_
11-13	1239-1247	regarded	_	_	_	_
11-14	1248-1250	as	_	_	_	_
11-15	1251-1252	a	_	_	_	_
11-16	1253-1260	special	_	_	_	_
11-17	1261-1265	case	_	_	_	_
11-18	1266-1270	with	_	_	_	_
11-19	1271-1272	a	abstract[52]	new[52]	coref	13-11[63_52]
11-20	1273-1281	specific	abstract[52]	new[52]	_	_
11-21	1282-1293	conditional	abstract[52]	new[52]	_	_
11-22	1294-1305	probability	abstract|abstract[52]	giv|new[52]	coref	13-17
11-23	1306-1318	distribution	abstract[52]	new[52]	_	_
11-24	1319-1320	,	_	_	_	_
11-25	1321-1322	.	_	_	_	_

#Text=For example , consider that the output is observed with additive noise to the deterministic neural networks function such as ( 3 ) where is a random noise vector .
12-1	1323-1326	For	_	_	_	_
12-2	1327-1334	example	_	_	_	_
12-3	1335-1336	,	_	_	_	_
12-4	1337-1345	consider	_	_	_	_
12-5	1346-1350	that	_	_	_	_
12-6	1351-1354	the	abstract[53]	giv[53]	coref	15-9[83_53]
12-7	1355-1361	output	abstract[53]	giv[53]	_	_
12-8	1362-1364	is	_	_	_	_
12-9	1365-1373	observed	_	_	_	_
12-10	1374-1378	with	_	_	_	_
12-11	1379-1387	additive	person|object[55]	new|new[55]	coref|coref|coref|coref	12-28[0_55]|21-23[141_0]|12-28[0_55]|21-23[141_0]
12-12	1388-1393	noise	object[55]	new[55]	_	_
12-13	1394-1396	to	object[55]	new[55]	_	_
12-14	1397-1400	the	object[55]|abstract[57]	new[55]|giv[57]	coref	13-28[70_57]
12-15	1401-1414	deterministic	object[55]|abstract[57]	new[55]|giv[57]	_	_
12-16	1415-1421	neural	object[55]|abstract[57]	new[55]|giv[57]	_	_
12-17	1422-1430	networks	object[55]|place|abstract[57]	new[55]|giv|giv[57]	coref	18-21[111_0]
12-18	1431-1439	function	object[55]|abstract[57]	new[55]|giv[57]	_	_
12-19	1440-1444	such	_	_	_	_
12-20	1445-1447	as	_	_	_	_
12-21	1448-1449	(	_	_	_	_
12-22	1450-1451	3	quantity	new	_	_
12-23	1452-1453	)	_	_	_	_
12-24	1454-1459	where	_	_	_	_
12-25	1460-1462	is	_	_	_	_
12-26	1463-1464	a	abstract[60]	new[60]	coref	13-5[62_60]
12-27	1465-1471	random	abstract[60]	new[60]	_	_
12-28	1472-1477	noise	object|abstract[60]	giv|new[60]	coref	14-9[75_0]
12-29	1478-1484	vector	abstract[60]	new[60]	_	_
12-30	1485-1486	.	_	_	_	_

#Text=When we assume that the random vector is subject to the standard Gaussian distribution , its probability density is defined as the normal distribution function , and its loss function becomes the well-known squared error function , which can be written as ( 4 )
13-1	1487-1491	When	_	_	_	_
13-2	1492-1494	we	person	giv	ana	18-10
13-3	1495-1501	assume	_	_	_	_
13-4	1502-1506	that	_	_	_	_
13-5	1507-1510	the	abstract[62]	giv[62]	_	_
13-6	1511-1517	random	abstract[62]	giv[62]	_	_
13-7	1518-1524	vector	abstract[62]	giv[62]	_	_
13-8	1525-1527	is	_	_	_	_
13-9	1528-1535	subject	_	_	_	_
13-10	1536-1538	to	_	_	_	_
13-11	1539-1542	the	abstract[63]	giv[63]	ana	13-16[0_63]
13-12	1543-1551	standard	abstract[63]	giv[63]	_	_
13-13	1552-1560	Gaussian	abstract[63]	giv[63]	_	_
13-14	1561-1573	distribution	abstract[63]	giv[63]	_	_
13-15	1574-1575	,	_	_	_	_
13-16	1576-1579	its	abstract|abstract[66]	giv|new[66]	coref|coref|coref|coref	13-24|20-15[0_66]|13-24|20-15[0_66]
13-17	1580-1591	probability	abstract|abstract[66]	giv|new[66]	coref	15-18
13-18	1592-1599	density	abstract[66]	new[66]	_	_
13-19	1600-1602	is	_	_	_	_
13-20	1603-1610	defined	_	_	_	_
13-21	1611-1613	as	_	_	_	_
13-22	1614-1617	the	_	_	_	_
13-23	1618-1624	normal	_	_	_	_
13-24	1625-1637	distribution	abstract	giv	ana	13-28
13-25	1638-1646	function	_	_	_	_
13-26	1647-1648	,	_	_	_	_
13-27	1649-1652	and	_	_	_	_
13-28	1653-1656	its	abstract|abstract[70]	giv|giv[70]	coref|coref|coref|coref	13-32[72_70]|15-16[86_0]|13-32[72_70]|15-16[86_0]
13-29	1657-1661	loss	abstract|abstract[70]	giv|giv[70]	coref	17-3
13-30	1662-1670	function	abstract[70]	giv[70]	_	_
13-31	1671-1678	becomes	_	_	_	_
13-32	1679-1682	the	abstract[72]	giv[72]	coref	14-22[79_72]
13-33	1683-1693	well-known	abstract[72]	giv[72]	_	_
13-34	1694-1701	squared	abstract[72]	giv[72]	_	_
13-35	1702-1707	error	abstract|abstract[72]	new|giv[72]	coref	14-23
13-36	1708-1716	function	abstract[72]	giv[72]	_	_
13-37	1717-1718	,	_	_	_	_
13-38	1719-1724	which	_	_	_	_
13-39	1725-1728	can	_	_	_	_
13-40	1729-1731	be	_	_	_	_
13-41	1732-1739	written	_	_	_	_
13-42	1740-1742	as	_	_	_	_
13-43	1743-1744	(	_	_	_	_
13-44	1745-1746	4	_	_	_	_
13-45	1747-1748	)	_	_	_	_

#Text=Therefore , the stochastic neural network model with additive Gaussian noise is equivalent to the typical neural network model trained with squared error function , which is widely used for regression task .
14-1	1749-1758	Therefore	_	_	_	_
14-2	1759-1760	,	_	_	_	_
14-3	1761-1764	the	abstract[74]	new[74]	_	_
14-4	1765-1775	stochastic	abstract[74]	new[74]	_	_
14-5	1776-1782	neural	abstract[74]	new[74]	_	_
14-6	1783-1790	network	abstract|abstract[74]	giv|new[74]	coref	14-18
14-7	1791-1796	model	abstract[74]	new[74]	_	_
14-8	1797-1801	with	abstract[74]	new[74]	_	_
14-9	1802-1810	additive	abstract[74]|object[75]	new[74]|giv[75]	coref	21-26[0_75]
14-10	1811-1819	Gaussian	abstract[74]|object[75]	new[74]|giv[75]	_	_
14-11	1820-1825	noise	abstract[74]|object[75]	new[74]|giv[75]	_	_
14-12	1826-1828	is	_	_	_	_
14-13	1829-1839	equivalent	_	_	_	_
14-14	1840-1842	to	_	_	_	_
14-15	1843-1846	the	abstract[77]	new[77]	coref	15-25[87_77]
14-16	1847-1854	typical	abstract[77]	new[77]	_	_
14-17	1855-1861	neural	abstract[77]	new[77]	_	_
14-18	1862-1869	network	abstract|abstract[77]	giv|new[77]	coref	19-17
14-19	1870-1875	model	abstract[77]	new[77]	_	_
14-20	1876-1883	trained	_	_	_	_
14-21	1884-1888	with	_	_	_	_
14-22	1889-1896	squared	abstract[79]	giv[79]	coref	17-1[101_79]
14-23	1897-1902	error	abstract|abstract[79]	giv|giv[79]	_	_
14-24	1903-1911	function	abstract[79]	giv[79]	_	_
14-25	1912-1913	,	_	_	_	_
14-26	1914-1919	which	_	_	_	_
14-27	1920-1922	is	_	_	_	_
14-28	1923-1929	widely	_	_	_	_
14-29	1930-1934	used	_	_	_	_
14-30	1935-1938	for	_	_	_	_
14-31	1939-1949	regression	abstract|abstract[81]	new|new[81]	coref|coref|coref|coref	18-16|18-34[114_81]|18-16|18-34[114_81]
14-32	1950-1954	task	abstract[81]	new[81]	_	_
14-33	1955-1956	.	_	_	_	_

#Text=On the other hand , in case that the output is a binary vector , the corresponding probability distribution can be defined by using a logistic model , such as ( 5 ) where and are the i -th component of L -dimensional vector and , respectively .
15-1	1957-1959	On	_	_	_	_
15-2	1960-1963	the	_	_	_	_
15-3	1964-1969	other	_	_	_	_
15-4	1970-1974	hand	_	_	_	_
15-5	1975-1976	,	_	_	_	_
15-6	1977-1979	in	_	_	_	_
15-7	1980-1984	case	abstract[82]	new[82]	_	_
15-8	1985-1989	that	abstract[82]	new[82]	_	_
15-9	1990-1993	the	abstract[82]|abstract[83]	new[82]|giv[83]	coref	15-12[84_83]
15-10	1994-2000	output	abstract[82]|abstract[83]	new[82]|giv[83]	_	_
15-11	2001-2003	is	abstract[82]	new[82]	_	_
15-12	2004-2005	a	abstract[82]|abstract[84]	new[82]|giv[84]	coref	15-42[90_84]
15-13	2006-2012	binary	abstract[82]|abstract[84]	new[82]|giv[84]	_	_
15-14	2013-2019	vector	abstract[82]|abstract[84]	new[82]|giv[84]	_	_
15-15	2020-2021	,	_	_	_	_
15-16	2022-2025	the	abstract[86]	giv[86]	_	_
15-17	2026-2039	corresponding	abstract[86]	giv[86]	_	_
15-18	2040-2051	probability	abstract|abstract[86]	giv|giv[86]	coref	20-14
15-19	2052-2064	distribution	abstract[86]	giv[86]	_	_
15-20	2065-2068	can	_	_	_	_
15-21	2069-2071	be	_	_	_	_
15-22	2072-2079	defined	_	_	_	_
15-23	2080-2082	by	_	_	_	_
15-24	2083-2088	using	_	_	_	_
15-25	2089-2090	a	abstract[87]	giv[87]	ana	15-37[88_87]
15-26	2091-2099	logistic	abstract[87]	giv[87]	_	_
15-27	2100-2105	model	abstract[87]	giv[87]	_	_
15-28	2106-2107	,	_	_	_	_
15-29	2108-2112	such	_	_	_	_
15-30	2113-2115	as	_	_	_	_
15-31	2116-2117	(	_	_	_	_
15-32	2118-2119	5	_	_	_	_
15-33	2120-2121	)	_	_	_	_
15-34	2122-2127	where	_	_	_	_
15-35	2128-2131	and	_	_	_	_
15-36	2132-2135	are	_	_	_	_
15-37	2136-2139	the	abstract[88]	giv[88]	coref	16-14[96_88]
15-38	2140-2141	i	abstract[88]	giv[88]	_	_
15-39	2142-2145	-th	_	_	_	_
15-40	2146-2155	component	abstract[89]	new[89]	_	_
15-41	2156-2158	of	abstract[89]	new[89]	_	_
15-42	2159-2160	L	abstract[89]|abstract[90]	new[89]|giv[90]	coref	16-6[93_90]
15-43	2161-2173	-dimensional	abstract[89]|abstract[90]	new[89]|giv[90]	_	_
15-44	2174-2180	vector	abstract[89]|abstract[90]	new[89]|giv[90]	_	_
15-45	2181-2184	and	_	_	_	_
15-46	2185-2186	,	_	_	_	_
15-47	2187-2199	respectively	_	_	_	_
15-48	2200-2201	.	_	_	_	_

#Text=Since the typical problems with binary target output vector is pattern classification , the logistic model is appropriate for L -class classification tasks .
16-1	2202-2207	Since	_	_	_	_
16-2	2208-2211	the	abstract[91]	new[91]	coref	16-11[95_91]
16-3	2212-2219	typical	abstract[91]	new[91]	_	_
16-4	2220-2228	problems	abstract[91]	new[91]	_	_
16-5	2229-2233	with	abstract[91]	new[91]	_	_
16-6	2234-2240	binary	abstract[91]|abstract[93]	new[91]|giv[93]	coref	28-26[197_93]
16-7	2241-2247	target	abstract[91]|abstract|abstract[93]	new[91]|new|giv[93]	_	_
16-8	2248-2254	output	abstract[91]|abstract[93]	new[91]|giv[93]	_	_
16-9	2255-2261	vector	abstract[91]|abstract[93]	new[91]|giv[93]	_	_
16-10	2262-2264	is	_	_	_	_
16-11	2265-2272	pattern	abstract|abstract[95]	new|giv[95]	coref|coref	19-35[122_95]|19-35[122_95]
16-12	2273-2287	classification	abstract[95]	giv[95]	_	_
16-13	2288-2289	,	_	_	_	_
16-14	2290-2293	the	abstract[96]	giv[96]	coref	17-6[102_96]
16-15	2294-2302	logistic	abstract[96]	giv[96]	_	_
16-16	2303-2308	model	abstract[96]	giv[96]	_	_
16-17	2309-2311	is	_	_	_	_
16-18	2312-2323	appropriate	_	_	_	_
16-19	2324-2327	for	_	_	_	_
16-20	2328-2329	L	person|abstract[99]	new|new[99]	_	_
16-21	2330-2336	-class	abstract[99]	new[99]	_	_
16-22	2337-2351	classification	abstract|abstract[99]	new|new[99]	coref	18-34
16-23	2352-2357	tasks	abstract[99]	new[99]	_	_
16-24	2358-2359	.	_	_	_	_

#Text=The corresponding loss function of the logistic model is obtained by taking negative log likelihood of Equation ( 5 ) , which can be written as , ( 6 )
17-1	2360-2363	The	abstract[101]	giv[101]	_	_
17-2	2364-2377	corresponding	abstract[101]	giv[101]	_	_
17-3	2378-2382	loss	abstract|abstract[101]	giv|giv[101]	coref	24-12
17-4	2383-2391	function	abstract[101]	giv[101]	_	_
17-5	2392-2394	of	abstract[101]	giv[101]	_	_
17-6	2395-2398	the	abstract[101]|abstract[102]	giv[101]|giv[102]	coref	18-14[110_102]
17-7	2399-2407	logistic	abstract[101]|abstract[102]	giv[101]|giv[102]	_	_
17-8	2408-2413	model	abstract[101]|abstract[102]	giv[101]|giv[102]	_	_
17-9	2414-2416	is	_	_	_	_
17-10	2417-2425	obtained	_	_	_	_
17-11	2426-2428	by	_	_	_	_
17-12	2429-2435	taking	_	_	_	_
17-13	2436-2444	negative	abstract[104]	new[104]	_	_
17-14	2445-2448	log	object|abstract[104]	giv|new[104]	_	_
17-15	2449-2459	likelihood	abstract[104]	new[104]	_	_
17-16	2460-2462	of	abstract[104]	new[104]	_	_
17-17	2463-2471	Equation	abstract[104]|abstract[105]	new[104]|giv[105]	ana	18-3[0_105]
17-18	2472-2473	(	abstract[104]|abstract[105]	new[104]|giv[105]	_	_
17-19	2474-2475	5	abstract[104]|abstract[105]	new[104]|giv[105]	_	_
17-20	2476-2477	)	abstract[104]|abstract[105]	new[104]|giv[105]	_	_
17-21	2478-2479	,	_	_	_	_
17-22	2480-2485	which	_	_	_	_
17-23	2486-2489	can	_	_	_	_
17-24	2490-2492	be	_	_	_	_
17-25	2493-2500	written	_	_	_	_
17-26	2501-2503	as	_	_	_	_
17-27	2504-2505	,	_	_	_	_
17-28	2506-2507	(	_	_	_	_
17-29	2508-2509	6	_	_	_	_
17-30	2510-2511	)	_	_	_	_

#Text=Noting that this is the well-known cross-entropy error , we can say that the logistic regression model is equivalent to the typical neural networks with cross-entropy error , which is widely used for classification task .
18-1	2512-2518	Noting	_	_	_	_
18-2	2519-2523	that	_	_	_	_
18-3	2524-2528	this	abstract	giv	coref	18-5[107_0]
18-4	2529-2531	is	_	_	_	_
18-5	2532-2535	the	abstract[107]	giv[107]	coref	18-26[112_107]
18-6	2536-2546	well-known	abstract[107]	giv[107]	_	_
18-7	2547-2560	cross-entropy	abstract[107]	giv[107]	_	_
18-8	2561-2566	error	abstract[107]	giv[107]	_	_
18-9	2567-2568	,	_	_	_	_
18-10	2569-2571	we	person	giv	ana	19-10
18-11	2572-2575	can	_	_	_	_
18-12	2576-2579	say	_	_	_	_
18-13	2580-2584	that	_	_	_	_
18-14	2585-2588	the	abstract[110]	giv[110]	coref	19-5[115_110]
18-15	2589-2597	logistic	abstract[110]	giv[110]	_	_
18-16	2598-2608	regression	abstract|abstract[110]	giv|giv[110]	coref	21-31
18-17	2609-2614	model	abstract[110]	giv[110]	_	_
18-18	2615-2617	is	_	_	_	_
18-19	2618-2628	equivalent	_	_	_	_
18-20	2629-2631	to	_	_	_	_
18-21	2632-2635	the	place[111]	giv[111]	coref	20-18[132_111]
18-22	2636-2643	typical	place[111]	giv[111]	_	_
18-23	2644-2650	neural	place[111]	giv[111]	_	_
18-24	2651-2659	networks	place[111]	giv[111]	_	_
18-25	2660-2664	with	place[111]	giv[111]	_	_
18-26	2665-2678	cross-entropy	place[111]|abstract[112]	giv[111]|giv[112]	coref	29-1[198_112]
18-27	2679-2684	error	place[111]|abstract[112]	giv[111]|giv[112]	_	_
18-28	2685-2686	,	_	_	_	_
18-29	2687-2692	which	_	_	_	_
18-30	2693-2695	is	_	_	_	_
18-31	2696-2702	widely	_	_	_	_
18-32	2703-2707	used	_	_	_	_
18-33	2708-2711	for	_	_	_	_
18-34	2712-2726	classification	abstract|abstract[114]	giv|giv[114]	coref|coref	19-23[120_114]|19-23[120_114]
18-35	2727-2731	task	abstract[114]	giv[114]	_	_
18-36	2732-2733	.	_	_	_	_

#Text=Likewise , by defining a proper stochastic model , we can derive various types of neural network models , which can explain the given task more adequately and get a new insight to solve many unresolved problems in the field of neural network learning .
19-1	2734-2742	Likewise	_	_	_	_
19-2	2743-2744	,	_	_	_	_
19-3	2745-2747	by	_	_	_	_
19-4	2748-2756	defining	_	_	_	_
19-5	2757-2758	a	abstract[115]	giv[115]	coref	21-25[143_115]
19-6	2759-2765	proper	abstract[115]	giv[115]	_	_
19-7	2766-2776	stochastic	abstract[115]	giv[115]	_	_
19-8	2777-2782	model	abstract[115]	giv[115]	_	_
19-9	2783-2784	,	_	_	_	_
19-10	2785-2787	we	person	giv	ana	21-5
19-11	2788-2791	can	_	_	_	_
19-12	2792-2798	derive	_	_	_	_
19-13	2799-2806	various	abstract[117]	new[117]	_	_
19-14	2807-2812	types	abstract[117]	new[117]	_	_
19-15	2813-2815	of	abstract[117]	new[117]	_	_
19-16	2816-2822	neural	abstract[117]|abstract[119]	new[117]|giv[119]	coref	21-16[140_119]
19-17	2823-2830	network	abstract[117]|abstract|abstract[119]	new[117]|giv|giv[119]	coref	19-43
19-18	2831-2837	models	abstract[117]|abstract[119]	new[117]|giv[119]	_	_
19-19	2838-2839	,	_	_	_	_
19-20	2840-2845	which	_	_	_	_
19-21	2846-2849	can	_	_	_	_
19-22	2850-2857	explain	_	_	_	_
19-23	2858-2861	the	abstract[120]	giv[120]	_	_
19-24	2862-2867	given	abstract[120]	giv[120]	_	_
19-25	2868-2872	task	abstract[120]	giv[120]	_	_
19-26	2873-2877	more	_	_	_	_
19-27	2878-2888	adequately	_	_	_	_
19-28	2889-2892	and	_	_	_	_
19-29	2893-2896	get	_	_	_	_
19-30	2897-2898	a	abstract[121]	new[121]	_	_
19-31	2899-2902	new	abstract[121]	new[121]	_	_
19-32	2903-2910	insight	abstract[121]	new[121]	_	_
19-33	2911-2913	to	_	_	_	_
19-34	2914-2919	solve	_	_	_	_
19-35	2920-2924	many	abstract[122]	giv[122]	_	_
19-36	2925-2935	unresolved	abstract[122]	giv[122]	_	_
19-37	2936-2944	problems	abstract[122]	giv[122]	_	_
19-38	2945-2947	in	_	_	_	_
19-39	2948-2951	the	abstract[123]	new[123]	_	_
19-40	2952-2957	field	abstract[123]	new[123]	_	_
19-41	2958-2960	of	abstract[123]	new[123]	_	_
19-42	2961-2967	neural	abstract[123]|abstract[125]	new[123]|giv[125]	coref	21-13[0_125]
19-43	2968-2975	network	abstract[123]|abstract|abstract[125]	new[123]|giv|giv[125]	coref	21-20
19-44	2976-2984	learning	abstract[123]|abstract[125]	new[123]|giv[125]	_	_
19-45	2985-2986	.	_	_	_	_

#Text=Natural gradient is also derived from a new metric for the space of probability density function of stochastic neural networks .
20-1	2987-2994	Natural	abstract[126]	giv[126]	coref	21-12[0_126]
20-2	2995-3003	gradient	abstract[126]	giv[126]	_	_
20-3	3004-3006	is	_	_	_	_
20-4	3007-3011	also	_	_	_	_
20-5	3012-3019	derived	_	_	_	_
20-6	3020-3024	from	_	_	_	_
20-7	3025-3026	a	abstract[127]	new[127]	_	_
20-8	3027-3030	new	abstract[127]	new[127]	_	_
20-9	3031-3037	metric	abstract[127]	new[127]	_	_
20-10	3038-3041	for	abstract[127]	new[127]	_	_
20-11	3042-3045	the	abstract[127]|abstract[128]	new[127]|new[128]	_	_
20-12	3046-3051	space	abstract[127]|abstract[128]	new[127]|new[128]	_	_
20-13	3052-3054	of	abstract[127]|abstract[128]	new[127]|new[128]	_	_
20-14	3055-3066	probability	abstract[127]|abstract[128]|abstract|abstract[131]	new[127]|new[128]|giv|new[131]	coref|coref	24-10[154_131]|24-10[154_131]
20-15	3067-3074	density	abstract[127]|abstract[128]|abstract|abstract[131]	new[127]|new[128]|giv|new[131]	_	_
20-16	3075-3083	function	abstract[127]|abstract[128]|abstract[131]	new[127]|new[128]|new[131]	_	_
20-17	3084-3086	of	abstract[127]|abstract[128]|abstract[131]	new[127]|new[128]|new[131]	_	_
20-18	3087-3097	stochastic	abstract[127]|abstract[128]|abstract[131]|place[132]	new[127]|new[128]|new[131]|giv[132]	coref	24-6[152_132]
20-19	3098-3104	neural	abstract[127]|abstract[128]|abstract[131]|place[132]	new[127]|new[128]|new[131]|giv[132]	_	_
20-20	3105-3113	networks	abstract[127]|abstract[128]|abstract[131]|place[132]	new[127]|new[128]|new[131]|giv[132]	_	_
20-21	3114-3115	.	_	_	_	_

#Text=In this paper , we present explicit algorithms of adaptive natural gradient learning method for two representative stochastic neural network models : The additive Gaussian noise model and the logistic regression model .
21-1	3116-3118	In	_	_	_	_
21-2	3119-3123	this	object[133]	new[133]	_	_
21-3	3124-3129	paper	object[133]	new[133]	_	_
21-4	3130-3131	,	_	_	_	_
21-5	3132-3134	we	person	giv	ana	27-16
21-6	3135-3142	present	_	_	_	_
21-7	3143-3151	explicit	abstract[135]	new[135]	_	_
21-8	3152-3162	algorithms	abstract[135]	new[135]	_	_
21-9	3163-3165	of	abstract[135]	new[135]	_	_
21-10	3166-3174	adaptive	abstract[135]|abstract[138]	new[135]|new[138]	coref	24-23[159_138]
21-11	3175-3182	natural	abstract[135]|abstract[138]	new[135]|new[138]	_	_
21-12	3183-3191	gradient	abstract[135]|abstract|abstract[138]	new[135]|giv|new[138]	coref	23-1
21-13	3192-3200	learning	abstract[135]|abstract|abstract[138]	new[135]|giv|new[138]	coref	23-1[150_0]
21-14	3201-3207	method	abstract[135]|abstract[138]	new[135]|new[138]	_	_
21-15	3208-3211	for	abstract[135]	new[135]	_	_
21-16	3212-3215	two	abstract[135]|abstract[140]	new[135]|giv[140]	_	_
21-17	3216-3230	representative	abstract[135]|abstract[140]	new[135]|giv[140]	_	_
21-18	3231-3241	stochastic	abstract[135]|abstract[140]	new[135]|giv[140]	_	_
21-19	3242-3248	neural	abstract[135]|abstract[140]	new[135]|giv[140]	_	_
21-20	3249-3256	network	abstract[135]|abstract|abstract[140]	new[135]|giv|giv[140]	coref	28-10[193_0]
21-21	3257-3263	models	abstract[135]|abstract[140]	new[135]|giv[140]	_	_
21-22	3264-3265	:	_	_	_	_
21-23	3266-3269	The	person[141]	giv[141]	appos	21-25[144_141]
21-24	3270-3278	additive	person[141]	giv[141]	_	_
21-25	3279-3287	Gaussian	abstract[143]|person[144]	giv[143]|giv[144]	coref|coref	21-29[146_143]|21-29[146_143]
21-26	3288-3293	noise	object|abstract[143]|person[144]	giv|giv[143]|giv[144]	_	_
21-27	3294-3299	model	abstract[143]|person[144]	giv[143]|giv[144]	_	_
21-28	3300-3303	and	person[144]	giv[144]	_	_
21-29	3304-3307	the	person[144]|abstract[146]	giv[144]|giv[146]	coref	24-2[151_146]
21-30	3308-3316	logistic	person[144]|abstract[146]	giv[144]|giv[146]	_	_
21-31	3317-3327	regression	person[144]|abstract|abstract[146]	giv[144]|giv|giv[146]	_	_
21-32	3328-3333	model	person[144]|abstract[146]	giv[144]|giv[146]	_	_
21-33	3334-3335	.	_	_	_	_

#Text=2.2 .
22-1	3336-3339	2.2	abstract	new	_	_
22-2	3340-3341	.	_	_	_	_

#Text=Gradient Descent Learning
23-1	3342-3350	Gradient	substance|abstract[149]|abstract[150]	giv|new[149]|giv[150]	coref|coref|coref|coref|coref|coref|coref|coref|coref	24-23|24-24[0_149]|25-12[0_150]|24-23|24-24[0_149]|25-12[0_150]|24-23|24-24[0_149]|25-12[0_150]
23-2	3351-3358	Descent	abstract[149]|abstract[150]	new[149]|giv[150]	_	_
23-3	3359-3367	Learning	abstract[150]	giv[150]	_	_

#Text=Once a specific model of stochastic neural networks and its corresponding loss function are determined , the weight parameters are optimized by gradient descent method .
24-1	3368-3372	Once	_	_	_	_
24-2	3373-3374	a	abstract[151]	giv[151]	_	_
24-3	3375-3383	specific	abstract[151]	giv[151]	_	_
24-4	3384-3389	model	abstract[151]	giv[151]	_	_
24-5	3390-3392	of	abstract[151]	giv[151]	_	_
24-6	3393-3403	stochastic	abstract[151]|place[152]	giv[151]|giv[152]	_	_
24-7	3404-3410	neural	abstract[151]|place[152]	giv[151]|giv[152]	_	_
24-8	3411-3419	networks	abstract[151]|place[152]	giv[151]|giv[152]	_	_
24-9	3420-3423	and	abstract[151]|place[152]	giv[151]|giv[152]	_	_
24-10	3424-3427	its	abstract[151]|place[152]|abstract[154]	giv[151]|giv[152]|giv[154]	coref	29-17[203_154]
24-11	3428-3441	corresponding	abstract[151]|place[152]|abstract[154]	giv[151]|giv[152]|giv[154]	_	_
24-12	3442-3446	loss	abstract[151]|place[152]|abstract|abstract[154]	giv[151]|giv[152]|giv|giv[154]	coref	29-18
24-13	3447-3455	function	abstract[151]|place[152]|abstract[154]	giv[151]|giv[152]|giv[154]	_	_
24-14	3456-3459	are	_	_	_	_
24-15	3460-3470	determined	_	_	_	_
24-16	3471-3472	,	_	_	_	_
24-17	3473-3476	the	abstract[156]	new[156]	coref	35-17[0_156]
24-18	3477-3483	weight	abstract|abstract[156]	new|new[156]	coref	28-26
24-19	3484-3494	parameters	abstract[156]	new[156]	_	_
24-20	3495-3498	are	_	_	_	_
24-21	3499-3508	optimized	_	_	_	_
24-22	3509-3511	by	_	_	_	_
24-23	3512-3520	gradient	abstract|abstract[159]	giv|giv[159]	coref|coref|coref|coref	25-10|25-11[166_159]|25-10|25-11[166_159]
24-24	3521-3528	descent	abstract|abstract[159]	giv|giv[159]	coref	25-11
24-25	3529-3535	method	abstract[159]	giv[159]	_	_
24-26	3536-3537	.	_	_	_	_

#Text=The well-known error-backpropagation algorithm is the standard type of gradient descent learning method .
25-1	3538-3541	The	abstract[161]	new[161]	coref	25-6[162_161]
25-2	3542-3552	well-known	abstract[161]	new[161]	_	_
25-3	3553-3574	error-backpropagation	abstract|abstract[161]	new|new[161]	_	_
25-4	3575-3584	algorithm	abstract[161]	new[161]	_	_
25-5	3585-3587	is	_	_	_	_
25-6	3588-3591	the	abstract[162]	giv[162]	_	_
25-7	3592-3600	standard	abstract[162]	giv[162]	_	_
25-8	3601-3605	type	abstract[162]	giv[162]	_	_
25-9	3606-3608	of	abstract[162]	giv[162]	_	_
25-10	3609-3617	gradient	abstract[162]|abstract	giv[162]|giv	coref	26-9
25-11	3618-3625	descent	abstract[162]|abstract|abstract[166]	giv[162]|giv|giv[166]	coref|coref|coref|coref	26-10|26-7[170_166]|26-10|26-7[170_166]
25-12	3626-3634	learning	abstract[162]|abstract|abstract[166]	giv[162]|giv|giv[166]	coref	26-22
25-13	3635-3641	method	abstract[162]|abstract[166]	giv[162]|giv[166]	_	_
25-14	3642-3643	.	_	_	_	_

#Text=There have been numerous variations of the standard gradient descent method , including second-order methods , momentum method , and adaptive learning rate methods .
26-1	3644-3649	There	_	_	_	_
26-2	3650-3654	have	_	_	_	_
26-3	3655-3659	been	_	_	_	_
26-4	3660-3668	numerous	abstract[167]	new[167]	_	_
26-5	3669-3679	variations	abstract[167]	new[167]	_	_
26-6	3680-3682	of	abstract[167]	new[167]	_	_
26-7	3683-3686	the	abstract[167]|abstract[170]	new[167]|giv[170]	coref	27-2[178_170]
26-8	3687-3695	standard	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-9	3696-3704	gradient	abstract[167]|abstract|abstract[170]	new[167]|giv|giv[170]	coref	27-4
26-10	3705-3712	descent	abstract[167]|abstract|abstract[170]	new[167]|giv|giv[170]	coref	27-13
26-11	3713-3719	method	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-12	3720-3721	,	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-13	3722-3731	including	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-14	3732-3744	second-order	abstract[167]|abstract[170]|abstract[171]	new[167]|giv[170]|new[171]	_	_
26-15	3745-3752	methods	abstract[167]|abstract[170]|abstract[171]	new[167]|giv[170]|new[171]	_	_
26-16	3753-3754	,	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-17	3755-3763	momentum	abstract[167]|abstract[170]|abstract	new[167]|giv[170]|new	_	_
26-18	3764-3770	method	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-19	3771-3772	,	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-20	3773-3776	and	abstract[167]|abstract[170]	new[167]|giv[170]	_	_
26-21	3777-3785	adaptive	abstract[167]|abstract[170]|abstract[175]	new[167]|giv[170]|new[175]	_	_
26-22	3786-3794	learning	abstract[167]|abstract[170]|abstract|abstract[175]	new[167]|giv[170]|giv|new[175]	coref	27-5
26-23	3795-3799	rate	abstract[167]|abstract[170]|abstract|abstract[175]	new[167]|giv[170]|new|new[175]	_	_
26-24	3800-3807	methods	abstract[167]|abstract[170]|abstract[175]	new[167]|giv[170]|new[175]	_	_
26-25	3808-3809	.	_	_	_	_

#Text=Since the natural gradient learning method is also based on the gradient descent method , we describe the basic formula of gradient descent learning and its online version that is called stochastic gradient descent method .
27-1	3810-3815	Since	_	_	_	_
27-2	3816-3819	the	abstract[178]	giv[178]	coref	27-11[181_178]
27-3	3820-3827	natural	abstract[178]	giv[178]	_	_
27-4	3828-3836	gradient	abstract|abstract[178]	giv|giv[178]	coref	27-12
27-5	3837-3845	learning	abstract|abstract[178]	giv|giv[178]	coref	27-23[186_0]
27-6	3846-3852	method	abstract[178]	giv[178]	_	_
27-7	3853-3855	is	_	_	_	_
27-8	3856-3860	also	_	_	_	_
27-9	3861-3866	based	_	_	_	_
27-10	3867-3869	on	_	_	_	_
27-11	3870-3873	the	abstract[181]	giv[181]	coref	27-32[190_181]
27-12	3874-3882	gradient	abstract|abstract[181]	giv|giv[181]	coref	27-22
27-13	3883-3890	descent	abstract|abstract[181]	giv|giv[181]	coref	27-23
27-14	3891-3897	method	abstract[181]	giv[181]	_	_
27-15	3898-3899	,	_	_	_	_
27-16	3900-3902	we	person	giv	_	_
27-17	3903-3911	describe	_	_	_	_
27-18	3912-3915	the	abstract[183]	new[183]	_	_
27-19	3916-3921	basic	abstract[183]	new[183]	_	_
27-20	3922-3929	formula	abstract[183]	new[183]	_	_
27-21	3930-3932	of	abstract[183]	new[183]	_	_
27-22	3933-3941	gradient	abstract[183]|abstract	new[183]|giv	coref	27-33
27-23	3942-3949	descent	abstract[183]|abstract|abstract[186]	new[183]|giv|giv[186]	coref|coref|coref|coref	27-34|29-29[0_186]|27-34|29-29[0_186]
27-24	3950-3958	learning	abstract[183]|abstract[186]	new[183]|giv[186]	_	_
27-25	3959-3962	and	abstract[183]	new[183]	_	_
27-26	3963-3966	its	abstract[183]|abstract[187]	new[183]|new[187]	_	_
27-27	3967-3973	online	abstract[183]|abstract[187]	new[183]|new[187]	_	_
27-28	3974-3981	version	abstract[183]|abstract[187]	new[183]|new[187]	_	_
27-29	3982-3986	that	_	_	_	_
27-30	3987-3989	is	_	_	_	_
27-31	3990-3996	called	_	_	_	_
27-32	3997-4007	stochastic	abstract[190]	giv[190]	_	_
27-33	4008-4016	gradient	abstract|abstract[190]	giv|giv[190]	coref	30-23[212_0]
27-34	4017-4024	descent	abstract|abstract[190]	giv|giv[190]	coref	32-10
27-35	4025-4031	method	abstract[190]	giv[190]	_	_
27-36	4032-4033	.	_	_	_	_

#Text=When a set of training data is given , a neural network is trained in order to find an input-output mapping that is specified with weight parameter vector .
28-1	4034-4038	When	_	_	_	_
28-2	4039-4040	a	abstract[192]	new[192]	_	_
28-3	4041-4044	set	abstract[192]	new[192]	_	_
28-4	4045-4047	of	abstract[192]	new[192]	_	_
28-5	4048-4056	training	abstract|abstract[192]	new|new[192]	_	_
28-6	4057-4061	data	abstract[192]	new[192]	_	_
28-7	4062-4064	is	_	_	_	_
28-8	4065-4070	given	_	_	_	_
28-9	4071-4072	,	_	_	_	_
28-10	4073-4074	a	abstract[193]	giv[193]	coref	29-4[199_193]
28-11	4075-4081	neural	abstract[193]	giv[193]	_	_
28-12	4082-4089	network	abstract[193]	giv[193]	_	_
28-13	4090-4092	is	_	_	_	_
28-14	4093-4100	trained	_	_	_	_
28-15	4101-4103	in	_	_	_	_
28-16	4104-4109	order	_	_	_	_
28-17	4110-4112	to	_	_	_	_
28-18	4113-4117	find	_	_	_	_
28-19	4118-4120	an	abstract[194]	new[194]	_	_
28-20	4121-4133	input-output	abstract[194]	new[194]	_	_
28-21	4134-4141	mapping	abstract[194]	new[194]	_	_
28-22	4142-4146	that	_	_	_	_
28-23	4147-4149	is	_	_	_	_
28-24	4150-4159	specified	_	_	_	_
28-25	4160-4164	with	_	_	_	_
28-26	4165-4171	weight	abstract|abstract[197]	giv|giv[197]	coref|coref	30-7|30-7
28-27	4172-4181	parameter	abstract|abstract[197]	giv|giv[197]	coref	30-6[209_0]
28-28	4182-4188	vector	abstract[197]	giv[197]	_	_
28-29	4189-4190	.	_	_	_	_

#Text=The error of neural network for the whole data set can then be defined by using a loss function such as ( 7 ) and the goal of learning is to get the optimal minimizing .
29-1	4191-4194	The	abstract[198]	giv[198]	_	_
29-2	4195-4200	error	abstract[198]	giv[198]	_	_
29-3	4201-4203	of	abstract[198]	giv[198]	_	_
29-4	4204-4210	neural	abstract[198]|abstract[199]	giv[198]|giv[199]	_	_
29-5	4211-4218	network	abstract[198]|abstract[199]	giv[198]|giv[199]	_	_
29-6	4219-4222	for	abstract[198]|abstract[199]	giv[198]|giv[199]	_	_
29-7	4223-4226	the	abstract[198]|abstract[199]|abstract[201]	giv[198]|giv[199]|new[201]	coref	31-17[220_201]
29-8	4227-4232	whole	abstract[198]|abstract[199]|abstract[201]	giv[198]|giv[199]|new[201]	_	_
29-9	4233-4237	data	abstract[198]|abstract[199]|abstract|abstract[201]	giv[198]|giv[199]|new|new[201]	coref	34-8
29-10	4238-4241	set	abstract[198]|abstract[199]|abstract[201]	giv[198]|giv[199]|new[201]	_	_
29-11	4242-4245	can	_	_	_	_
29-12	4246-4250	then	_	_	_	_
29-13	4251-4253	be	_	_	_	_
29-14	4254-4261	defined	_	_	_	_
29-15	4262-4264	by	_	_	_	_
29-16	4265-4270	using	_	_	_	_
29-17	4271-4272	a	abstract[203]	giv[203]	coref	35-28[246_203]
29-18	4273-4277	loss	abstract|abstract[203]	giv|giv[203]	coref	35-28
29-19	4278-4286	function	abstract[203]	giv[203]	_	_
29-20	4287-4291	such	abstract[203]	giv[203]	_	_
29-21	4292-4294	as	abstract[203]	giv[203]	_	_
29-22	4295-4296	(	abstract[203]	giv[203]	_	_
29-23	4297-4298	7	abstract[203]	giv[203]	_	_
29-24	4299-4300	)	abstract[203]	giv[203]	_	_
29-25	4301-4304	and	_	_	_	_
29-26	4305-4308	the	abstract[204]	giv[204]	coref	30-3[207_204]
29-27	4309-4313	goal	abstract[204]	giv[204]	_	_
29-28	4314-4316	of	abstract[204]	giv[204]	_	_
29-29	4317-4325	learning	abstract[204]|abstract	giv[204]|giv	coref	31-7
29-30	4326-4328	is	_	_	_	_
29-31	4329-4331	to	_	_	_	_
29-32	4332-4335	get	_	_	_	_
29-33	4336-4339	the	abstract[206]	new[206]	_	_
29-34	4340-4347	optimal	abstract[206]	new[206]	_	_
29-35	4348-4358	minimizing	abstract[206]	new[206]	_	_
29-36	4359-4360	.	_	_	_	_

#Text=To achieve the goal , the weight parameter is updated starting from the current position , according to the opposite direction of the gradient of , which can be written as ( 8 )
30-1	4361-4363	To	_	_	_	_
30-2	4364-4371	achieve	_	_	_	_
30-3	4372-4375	the	abstract[207]	giv[207]	_	_
30-4	4376-4380	goal	abstract[207]	giv[207]	_	_
30-5	4381-4382	,	_	_	_	_
30-6	4383-4386	the	abstract[209]	giv[209]	_	_
30-7	4387-4393	weight	abstract|abstract[209]	giv|giv[209]	_	_
30-8	4394-4403	parameter	abstract[209]	giv[209]	_	_
30-9	4404-4406	is	_	_	_	_
30-10	4407-4414	updated	_	_	_	_
30-11	4415-4423	starting	_	_	_	_
30-12	4424-4428	from	_	_	_	_
30-13	4429-4432	the	place[210]	new[210]	coref	32-14[226_210]
30-14	4433-4440	current	place[210]	new[210]	_	_
30-15	4441-4449	position	place[210]	new[210]	_	_
30-16	4450-4451	,	_	_	_	_
30-17	4452-4461	according	_	_	_	_
30-18	4462-4464	to	_	_	_	_
30-19	4465-4468	the	abstract[211]	new[211]	_	_
30-20	4469-4477	opposite	abstract[211]	new[211]	_	_
30-21	4478-4487	direction	abstract[211]	new[211]	_	_
30-22	4488-4490	of	abstract[211]	new[211]	_	_
30-23	4491-4494	the	abstract[211]|abstract[212]	new[211]|giv[212]	coref	35-7[239_212]
30-24	4495-4503	gradient	abstract[211]|abstract[212]	new[211]|giv[212]	_	_
30-25	4504-4506	of	abstract[211]|abstract[212]	new[211]|giv[212]	_	_
30-26	4507-4508	,	_	_	_	_
30-27	4509-4514	which	_	_	_	_
30-28	4515-4518	can	_	_	_	_
30-29	4519-4521	be	_	_	_	_
30-30	4522-4529	written	_	_	_	_
30-31	4530-4532	as	_	_	_	_
30-32	4533-4534	(	_	_	_	_
30-33	4535-4536	8	_	_	_	_
30-34	4537-4538	)	_	_	_	_

#Text=This update rule is called batch learning mode , meaning that an update is done for the whole batch set .
31-1	4539-4543	This	abstract[214]	new[214]	_	_
31-2	4544-4550	update	event|abstract[214]	new|new[214]	coref	31-12[218_0]
31-3	4551-4555	rule	abstract[214]	new[214]	_	_
31-4	4556-4558	is	_	_	_	_
31-5	4559-4565	called	_	_	_	_
31-6	4566-4571	batch	quantity|abstract[217]	new|new[217]	coref|coref|coref|coref	31-19|32-5[0_217]|31-19|32-5[0_217]
31-7	4572-4580	learning	abstract|abstract[217]	giv|new[217]	coref	32-3[223_0]
31-8	4581-4585	mode	abstract[217]	new[217]	_	_
31-9	4586-4587	,	_	_	_	_
31-10	4588-4595	meaning	_	_	_	_
31-11	4596-4600	that	_	_	_	_
31-12	4601-4603	an	event[218]	giv[218]	coref	34-19[236_218]
31-13	4604-4610	update	event[218]	giv[218]	_	_
31-14	4611-4613	is	_	_	_	_
31-15	4614-4618	done	_	_	_	_
31-16	4619-4622	for	_	_	_	_
31-17	4623-4626	the	abstract[220]	giv[220]	_	_
31-18	4627-4632	whole	abstract[220]	giv[220]	_	_
31-19	4633-4638	batch	quantity|abstract[220]	giv|giv[220]	coref	32-4
31-20	4639-4642	set	abstract[220]	giv[220]	_	_
31-21	4643-4644	.	_	_	_	_

#Text=Theoretically , the batch mode learning gives the steepest descent direction of at the current position of , but it has two practical drawbacks .
32-1	4645-4658	Theoretically	_	_	_	_
32-2	4659-4660	,	_	_	_	_
32-3	4661-4664	the	abstract[223]	giv[223]	_	_
32-4	4665-4670	batch	quantity|abstract[223]	giv|giv[223]	_	_
32-5	4671-4675	mode	abstract|abstract[223]	giv|giv[223]	_	_
32-6	4676-4684	learning	abstract[223]	giv[223]	_	_
32-7	4685-4690	gives	_	_	_	_
32-8	4691-4694	the	abstract[225]	new[225]	_	_
32-9	4695-4703	steepest	abstract[225]	new[225]	_	_
32-10	4704-4711	descent	abstract|abstract[225]	giv|new[225]	_	_
32-11	4712-4721	direction	abstract[225]	new[225]	_	_
32-12	4722-4724	of	abstract[225]	new[225]	_	_
32-13	4725-4727	at	abstract[225]	new[225]	_	_
32-14	4728-4731	the	abstract[225]|abstract[226]	new[225]|giv[226]	ana	32-20[0_226]
32-15	4732-4739	current	abstract[225]|abstract[226]	new[225]|giv[226]	_	_
32-16	4740-4748	position	abstract[225]|abstract[226]	new[225]|giv[226]	_	_
32-17	4749-4751	of	_	_	_	_
32-18	4752-4753	,	_	_	_	_
32-19	4754-4757	but	_	_	_	_
32-20	4758-4760	it	place	giv	ana	33-3
32-21	4761-4764	has	_	_	_	_
32-22	4765-4768	two	abstract[228]	new[228]	_	_
32-23	4769-4778	practical	abstract[228]	new[228]	_	_
32-24	4779-4788	drawbacks	abstract[228]	new[228]	_	_
32-25	4789-4790	.	_	_	_	_

#Text=First , it is too stable to be easily trapped in undesirable local minima .
33-1	4791-4796	First	_	_	_	_
33-2	4797-4798	,	_	_	_	_
33-3	4799-4801	it	abstract	giv	_	_
33-4	4802-4804	is	_	_	_	_
33-5	4805-4808	too	_	_	_	_
33-6	4809-4815	stable	_	_	_	_
33-7	4816-4818	to	_	_	_	_
33-8	4819-4821	be	_	_	_	_
33-9	4822-4828	easily	_	_	_	_
33-10	4829-4836	trapped	_	_	_	_
33-11	4837-4839	in	_	_	_	_
33-12	4840-4851	undesirable	abstract[230]	new[230]	_	_
33-13	4852-4857	local	abstract[230]	new[230]	_	_
33-14	4858-4864	minima	abstract[230]	new[230]	_	_
33-15	4865-4866	.	_	_	_	_

#Text=In addition , when the number of data is large , it needs large amounts of computation for just a single update , making the learning process slow .
34-1	4867-4869	In	_	_	_	_
34-2	4870-4878	addition	_	_	_	_
34-3	4879-4880	,	_	_	_	_
34-4	4881-4885	when	_	_	_	_
34-5	4886-4889	the	quantity[231]	new[231]	_	_
34-6	4890-4896	number	quantity[231]	new[231]	_	_
34-7	4897-4899	of	quantity[231]	new[231]	_	_
34-8	4900-4904	data	quantity[231]|abstract	new[231]|giv	ana	34-12
34-9	4905-4907	is	_	_	_	_
34-10	4908-4913	large	_	_	_	_
34-11	4914-4915	,	_	_	_	_
34-12	4916-4918	it	abstract	giv	coref	35-22
34-13	4919-4924	needs	_	_	_	_
34-14	4925-4930	large	quantity[234]	new[234]	_	_
34-15	4931-4938	amounts	quantity[234]	new[234]	_	_
34-16	4939-4941	of	quantity[234]	new[234]	_	_
34-17	4942-4953	computation	quantity[234]|abstract[235]	new[234]|new[235]	_	_
34-18	4954-4957	for	quantity[234]|abstract[235]	new[234]|new[235]	_	_
34-19	4958-4962	just	quantity[234]|abstract[235]|event[236]	new[234]|new[235]|giv[236]	_	_
34-20	4963-4964	a	quantity[234]|abstract[235]|event[236]	new[234]|new[235]|giv[236]	_	_
34-21	4965-4971	single	quantity[234]|abstract[235]|event[236]	new[234]|new[235]|giv[236]	_	_
34-22	4972-4978	update	quantity[234]|abstract[235]|event[236]	new[234]|new[235]|giv[236]	_	_
34-23	4979-4980	,	_	_	_	_
34-24	4981-4987	making	_	_	_	_
34-25	4988-4991	the	abstract[237]	new[237]	_	_
34-26	4992-5000	learning	abstract[237]	new[237]	_	_
34-27	5001-5008	process	abstract[237]	new[237]	_	_
34-28	5009-5013	slow	_	_	_	_
34-29	5014-5015	.	_	_	_	_

#Text=To overcome this practical inefficiency , online stochastic gradient decent learning is proposed , in which parameters are updated for each data sample by using gradient of loss function defined with a single data pair , such as ( 9 )
35-1	5016-5018	To	_	_	_	_
35-2	5019-5027	overcome	_	_	_	_
35-3	5028-5032	this	abstract[238]	new[238]	_	_
35-4	5033-5042	practical	abstract[238]	new[238]	_	_
35-5	5043-5055	inefficiency	abstract[238]	new[238]	_	_
35-6	5056-5057	,	_	_	_	_
35-7	5058-5064	online	abstract[239]	giv[239]	appos	35-10[240_239]
35-8	5065-5075	stochastic	abstract[239]	giv[239]	_	_
35-9	5076-5084	gradient	abstract[239]	giv[239]	_	_
35-10	5085-5091	decent	abstract[240]	giv[240]	_	_
35-11	5092-5100	learning	abstract[240]	giv[240]	_	_
35-12	5101-5103	is	_	_	_	_
35-13	5104-5112	proposed	_	_	_	_
35-14	5113-5114	,	_	_	_	_
35-15	5115-5117	in	_	_	_	_
35-16	5118-5123	which	_	_	_	_
35-17	5124-5134	parameters	abstract	giv	_	_
35-18	5135-5138	are	_	_	_	_
35-19	5139-5146	updated	_	_	_	_
35-20	5147-5150	for	_	_	_	_
35-21	5151-5155	each	abstract[243]	new[243]	_	_
35-22	5156-5160	data	abstract|abstract[243]	giv|new[243]	coref	35-34
35-23	5161-5167	sample	abstract[243]	new[243]	_	_
35-24	5168-5170	by	_	_	_	_
35-25	5171-5176	using	_	_	_	_
35-26	5177-5185	gradient	abstract[244]	new[244]	_	_
35-27	5186-5188	of	abstract[244]	new[244]	_	_
35-28	5189-5193	loss	abstract[244]|abstract|abstract[246]	new[244]|giv|giv[246]	_	_
35-29	5194-5202	function	abstract[244]|abstract[246]	new[244]|giv[246]	_	_
35-30	5203-5210	defined	_	_	_	_
35-31	5211-5215	with	_	_	_	_
35-32	5216-5217	a	plant[248]	new[248]	_	_
35-33	5218-5224	single	plant[248]	new[248]	_	_
35-34	5225-5229	data	abstract|plant[248]	giv|new[248]	_	_
35-35	5230-5234	pair	plant[248]	new[248]	_	_
35-36	5235-5236	,	plant[248]	new[248]	_	_
35-37	5237-5241	such	plant[248]	new[248]	_	_
35-38	5242-5244	as	plant[248]	new[248]	_	_
35-39	5245-5246	(	plant[248]	new[248]	_	_
35-40	5247-5248	9	plant[248]	new[248]	_	_
35-41	5249-5250	)	plant[248]	new[248]	_	_
