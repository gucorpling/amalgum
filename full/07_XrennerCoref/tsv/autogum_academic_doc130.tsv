#FORMAT=WebAnno TSV 3.2
#T_SP=webanno.custom.Referent|entity|infstat
#T_RL=webanno.custom.Coref|type|BT_webanno.custom.Referent


#Text=5 .
1-1	0-1	5	quantity	new	_	_
1-2	2-3	.	_	_	_	_

#Text=Ethical Concerns from Algorithmic Decision-Making in AVs
2-1	4-11	Ethical	abstract[2]	new[2]	_	_
2-2	12-20	Concerns	abstract[2]	new[2]	_	_
2-3	21-25	from	abstract[2]	new[2]	_	_
2-4	26-37	Algorithmic	abstract[2]|abstract|abstract[4]	new[2]|new|new[4]	coref|coref	3-8[8_4]|3-8[8_4]
2-5	38-53	Decision-Making	abstract[2]|abstract[4]	new[2]|new[4]	_	_
2-6	54-56	in	abstract[2]|abstract[4]	new[2]|new[4]	_	_
2-7	57-60	AVs	abstract[2]|abstract[4]|abstract	new[2]|new[4]|new	coref	3-11

#Text=This Section explores ethical issues associated with algorithmic decision-making in AVs , their implications for AV safety risks and discrimination and the steps taken to tackle these issues .
3-1	61-65	This	organization[6]	new[6]	coref	4-1[17_6]
3-2	66-73	Section	organization[6]	new[6]	_	_
3-3	74-82	explores	_	_	_	_
3-4	83-90	ethical	abstract[7]	new[7]	appos	3-13[10_7]
3-5	91-97	issues	abstract[7]	new[7]	_	_
3-6	98-108	associated	_	_	_	_
3-7	109-113	with	_	_	_	_
3-8	114-125	algorithmic	abstract[8]	giv[8]	coref	5-11[32_8]
3-9	126-141	decision-making	abstract[8]	giv[8]	_	_
3-10	142-144	in	abstract[8]	giv[8]	_	_
3-11	145-148	AVs	abstract[8]|abstract	giv[8]|giv	coref	4-9[20_0]
3-12	149-150	,	_	_	_	_
3-13	151-156	their	abstract[10]	giv[10]	coref	3-27[16_10]
3-14	157-169	implications	abstract[10]	giv[10]	_	_
3-15	170-173	for	abstract[10]	giv[10]	_	_
3-16	174-176	AV	abstract[10]|abstract|abstract[13]	giv[10]|new|new[13]	coref|coref|coref|coref	4-19[24_13]|5-11|4-19[24_13]|5-11
3-17	177-183	safety	abstract[10]|abstract|abstract[13]	giv[10]|new|new[13]	coref	4-20
3-18	184-189	risks	abstract[10]|abstract[13]	giv[10]|new[13]	_	_
3-19	190-193	and	abstract[10]	giv[10]	_	_
3-20	194-208	discrimination	abstract[10]|abstract	giv[10]|new	coref	4-15
3-21	209-212	and	abstract[10]	giv[10]	_	_
3-22	213-216	the	abstract[10]|abstract[15]	giv[10]|new[15]	_	_
3-23	217-222	steps	abstract[10]|abstract[15]	giv[10]|new[15]	_	_
3-24	223-228	taken	_	_	_	_
3-25	229-231	to	_	_	_	_
3-26	232-238	tackle	_	_	_	_
3-27	239-244	these	abstract[16]	giv[16]	coref	5-17[34_16]
3-28	245-251	issues	abstract[16]	giv[16]	_	_
3-29	252-253	.	_	_	_	_

#Text=Section 5.1 discusses the sources of bias in AVs ’ algorithms that can yield discrimination by disproportionately allocating more safety risks to some groups of individuals .
4-1	254-261	Section	organization[17]	giv[17]	coref	5-3[27_17]
4-2	262-265	5.1	organization[17]	giv[17]	_	_
4-3	266-275	discusses	_	_	_	_
4-4	276-279	the	abstract[18]	new[18]	_	_
4-5	280-287	sources	abstract[18]	new[18]	_	_
4-6	288-290	of	abstract[18]	new[18]	_	_
4-7	291-295	bias	abstract[18]|abstract[19]	new[18]|new[19]	coref	8-1[0_19]
4-8	296-298	in	abstract[18]|abstract[19]	new[18]|new[19]	_	_
4-9	299-302	AVs	abstract[18]|abstract[19]|abstract[20]|abstract[21]	new[18]|new[19]|giv[20]|new[21]	coref|coref|coref|coref	5-11[31_21]|11-6[0_20]|5-11[31_21]|11-6[0_20]
4-10	303-304	’	abstract[18]|abstract[19]|abstract[20]|abstract[21]	new[18]|new[19]|giv[20]|new[21]	_	_
4-11	305-315	algorithms	abstract[18]|abstract[19]|abstract[21]	new[18]|new[19]|new[21]	_	_
4-12	316-320	that	_	_	_	_
4-13	321-324	can	_	_	_	_
4-14	325-330	yield	_	_	_	_
4-15	331-345	discrimination	abstract	giv	coref	5-23
4-16	346-348	by	_	_	_	_
4-17	349-367	disproportionately	_	_	_	_
4-18	368-378	allocating	_	_	_	_
4-19	379-383	more	abstract[24]	giv[24]	coref	6-23[47_24]
4-20	384-390	safety	abstract|abstract[24]	giv|giv[24]	coref	5-20[36_0]
4-21	391-396	risks	abstract[24]	giv[24]	_	_
4-22	397-399	to	_	_	_	_
4-23	400-404	some	person[25]	new[25]	_	_
4-24	405-411	groups	person[25]	new[25]	_	_
4-25	412-414	of	person[25]	new[25]	_	_
4-26	415-426	individuals	person[25]|person	new[25]|new	coref	9-21[53_0]
4-27	427-428	.	_	_	_	_

#Text=Next , Section 5.2 explores approaches to incorporate ethics into AV algorithms ’ decision-making and highlight their implications for AV safety and discrimination .
5-1	429-433	Next	_	_	_	_
5-2	434-435	,	_	_	_	_
5-3	436-443	Section	organization[27]	giv[27]	coref	6-3[38_27]
5-4	444-447	5.2	organization[27]	giv[27]	_	_
5-5	448-456	explores	_	_	_	_
5-6	457-467	approaches	abstract	new	_	_
5-7	468-470	to	_	_	_	_
5-8	471-482	incorporate	_	_	_	_
5-9	483-489	ethics	abstract	new	_	_
5-10	490-494	into	_	_	_	_
5-11	495-497	AV	abstract|abstract[31]|abstract[32]	giv|giv[31]|giv[32]	ana|coref|ana|coref|ana|coref	5-17[0_31]|5-20|5-17[0_31]|5-20|5-17[0_31]|5-20
5-12	498-508	algorithms	abstract[31]|abstract[32]	giv[31]|giv[32]	_	_
5-13	509-510	’	abstract[31]|abstract[32]	giv[31]|giv[32]	_	_
5-14	511-526	decision-making	abstract[32]	giv[32]	_	_
5-15	527-530	and	_	_	_	_
5-16	531-540	highlight	_	_	_	_
5-17	541-546	their	abstract|abstract[34]	giv|giv[34]	coref|coref|coref|coref	6-13[43_0]|18-18[138_34]|6-13[43_0]|18-18[138_34]
5-18	547-559	implications	abstract[34]	giv[34]	_	_
5-19	560-563	for	abstract[34]	giv[34]	_	_
5-20	564-566	AV	abstract[34]|abstract|abstract[36]	giv[34]|giv|giv[36]	coref|coref|coref|coref	6-10|6-24[0_36]|6-10|6-24[0_36]
5-21	567-573	safety	abstract[34]|abstract[36]	giv[34]|giv[36]	_	_
5-22	574-577	and	abstract[34]	giv[34]	_	_
5-23	578-592	discrimination	abstract[34]|abstract	giv[34]|giv	coref	6-27
5-24	593-594	.	_	_	_	_

#Text=Lastly , Section 5.3 examines how the incentives of AV stakeholders shape AV algorithms ’ design and resulting decisions that can introduce new safety risks and discrimination .
6-1	595-601	Lastly	_	_	_	_
6-2	602-603	,	_	_	_	_
6-3	604-611	Section	abstract[38]	giv[38]	coref	18-1[133_38]
6-4	612-615	5.3	abstract[38]	giv[38]	_	_
6-5	616-624	examines	_	_	_	_
6-6	625-628	how	_	_	_	_
6-7	629-632	the	abstract[39]	new[39]	coref	18-33[144_39]
6-8	633-643	incentives	abstract[39]	new[39]	_	_
6-9	644-646	of	abstract[39]	new[39]	_	_
6-10	647-649	AV	abstract[39]|organization|person[41]	new[39]|giv|new[41]	coref|coref|coref|coref	6-13|17-45[132_41]|6-13|17-45[132_41]
6-11	650-662	stakeholders	abstract[39]|person[41]	new[39]|new[41]	_	_
6-12	663-668	shape	_	_	_	_
6-13	669-671	AV	abstract|abstract[43]|abstract[44]	giv|giv[43]|new[44]	coref|coref|coref|coref|coref|coref	13-5[83_0]|16-24[107_43]|13-5[83_0]|16-24[107_43]|13-5[83_0]|16-24[107_43]
6-14	672-682	algorithms	abstract[43]|abstract[44]	giv[43]|new[44]	_	_
6-15	683-684	’	abstract[43]|abstract[44]	giv[43]|new[44]	_	_
6-16	685-691	design	abstract[44]	new[44]	_	_
6-17	692-695	and	_	_	_	_
6-18	696-705	resulting	abstract[45]	new[45]	coref	25-7[0_45]
6-19	706-715	decisions	abstract[45]	new[45]	_	_
6-20	716-720	that	_	_	_	_
6-21	721-724	can	_	_	_	_
6-22	725-734	introduce	_	_	_	_
6-23	735-738	new	abstract[47]	giv[47]	coref	11-35[78_47]
6-24	739-745	safety	abstract|abstract[47]	giv|giv[47]	coref	11-35
6-25	746-751	risks	abstract[47]	giv[47]	_	_
6-26	752-755	and	abstract[47]	giv[47]	_	_
6-27	756-770	discrimination	abstract[47]|abstract	giv[47]|giv	coref	10-6
6-28	771-772	.	_	_	_	_

#Text=5.1 .
7-1	773-776	5.1	abstract	new	coref	16-45[112_0]
7-2	777-778	.	_	_	_	_

#Text=Bias
8-1	779-783	Bias	abstract	giv	coref	11-1

#Text=A system is considered biased when it contains “ intended ” or “ unintended ” characteristics that unfairly discriminate against certain individuals or groups of individuals in society .
9-1	784-785	A	abstract[51]	new[51]	ana	9-7[0_51]
9-2	786-792	system	abstract[51]	new[51]	_	_
9-3	793-795	is	_	_	_	_
9-4	796-806	considered	_	_	_	_
9-5	807-813	biased	_	_	_	_
9-6	814-818	when	_	_	_	_
9-7	819-821	it	abstract	giv	coref	22-23[194_0]
9-8	822-830	contains	_	_	_	_
9-9	831-832	“	_	_	_	_
9-10	833-841	intended	_	_	_	_
9-11	842-843	”	_	_	_	_
9-12	844-846	or	_	_	_	_
9-13	847-848	“	_	_	_	_
9-14	849-859	unintended	_	_	_	_
9-15	860-861	”	_	_	_	_
9-16	862-877	characteristics	_	_	_	_
9-17	878-882	that	_	_	_	_
9-18	883-891	unfairly	_	_	_	_
9-19	892-904	discriminate	_	_	_	_
9-20	905-912	against	_	_	_	_
9-21	913-920	certain	person[53]	giv[53]	coref	16-59[0_53]
9-22	921-932	individuals	person[53]	giv[53]	_	_
9-23	933-935	or	person[53]	giv[53]	_	_
9-24	936-942	groups	person[53]|person[54]	giv[53]|new[54]	coref	10-30[64_54]
9-25	943-945	of	person[53]|person[54]	giv[53]|new[54]	_	_
9-26	946-957	individuals	person[53]|person[54]	giv[53]|new[54]	_	_
9-27	958-960	in	person[53]|person[54]	giv[53]|new[54]	_	_
9-28	961-968	society	person[53]|person[54]|abstract	giv[53]|new[54]|new	_	_
9-29	969-970	.	_	_	_	_

#Text=In American anti-discrimination law , discrimination exists when there is disparate treatment , which is the “ discriminatory intent or the formal application of different rules to people of different groups ” , and/or disparate impact , which is the result that “ differ for different groups ” .
10-1	971-973	In	_	_	_	_
10-2	974-982	American	abstract[57]	new[57]	_	_
10-3	983-1002	anti-discrimination	abstract|abstract[57]	new|new[57]	_	_
10-4	1003-1006	law	abstract[57]	new[57]	_	_
10-5	1007-1008	,	_	_	_	_
10-6	1009-1023	discrimination	abstract	giv	coref	20-59
10-7	1024-1030	exists	_	_	_	_
10-8	1031-1035	when	_	_	_	_
10-9	1036-1041	there	_	_	_	_
10-10	1042-1044	is	_	_	_	_
10-11	1045-1054	disparate	abstract[59]	new[59]	_	_
10-12	1055-1064	treatment	abstract[59]	new[59]	_	_
10-13	1065-1066	,	_	_	_	_
10-14	1067-1072	which	_	_	_	_
10-15	1073-1075	is	_	_	_	_
10-16	1076-1079	the	abstract[60]	new[60]	coref	26-53[238_60]
10-17	1080-1081	“	abstract[60]	new[60]	_	_
10-18	1082-1096	discriminatory	abstract[60]	new[60]	_	_
10-19	1097-1103	intent	abstract[60]	new[60]	_	_
10-20	1104-1106	or	_	_	_	_
10-21	1107-1110	the	abstract[61]	new[61]	_	_
10-22	1111-1117	formal	abstract[61]	new[61]	_	_
10-23	1118-1129	application	abstract[61]	new[61]	_	_
10-24	1130-1132	of	abstract[61]	new[61]	_	_
10-25	1133-1142	different	abstract[61]|abstract[62]	new[61]|new[62]	_	_
10-26	1143-1148	rules	abstract[61]|abstract[62]	new[61]|new[62]	_	_
10-27	1149-1151	to	abstract[61]|abstract[62]	new[61]|new[62]	_	_
10-28	1152-1158	people	abstract[61]|abstract[62]|person[63]	new[61]|new[62]|new[63]	coref	28-9[0_63]
10-29	1159-1161	of	abstract[61]|abstract[62]|person[63]	new[61]|new[62]|new[63]	_	_
10-30	1162-1171	different	abstract[61]|abstract[62]|person[63]|person[64]	new[61]|new[62]|new[63]|giv[64]	coref	10-46[67_64]
10-31	1172-1178	groups	abstract[61]|abstract[62]|person[63]|person[64]	new[61]|new[62]|new[63]|giv[64]	_	_
10-32	1179-1180	”	_	_	_	_
10-33	1181-1182	,	_	_	_	_
10-34	1183-1189	and/or	abstract[65]	new[65]	_	_
10-35	1190-1199	disparate	abstract[65]	new[65]	_	_
10-36	1200-1206	impact	abstract[65]	new[65]	_	_
10-37	1207-1208	,	_	_	_	_
10-38	1209-1214	which	_	_	_	_
10-39	1215-1217	is	_	_	_	_
10-40	1218-1221	the	abstract[66]	new[66]	_	_
10-41	1222-1228	result	abstract[66]	new[66]	_	_
10-42	1229-1233	that	_	_	_	_
10-43	1234-1235	“	_	_	_	_
10-44	1236-1242	differ	_	_	_	_
10-45	1243-1246	for	_	_	_	_
10-46	1247-1256	different	person[67]	giv[67]	coref	14-8[93_67]
10-47	1257-1263	groups	person[67]	giv[67]	_	_
10-48	1264-1265	”	_	_	_	_
10-49	1266-1267	.	_	_	_	_

#Text=Bias can be introduced into AVs during the human designers ’ construction of the datasets , models , and the parameters of the algorithm , which potentially leads to unfair or discriminatory allocations of safety risks .
11-1	1268-1272	Bias	person	giv	coref	12-3[79_0]
11-2	1273-1276	can	_	_	_	_
11-3	1277-1279	be	_	_	_	_
11-4	1280-1290	introduced	_	_	_	_
11-5	1291-1295	into	_	_	_	_
11-6	1296-1299	AVs	abstract	giv	coref	16-25[106_0]
11-7	1300-1306	during	_	_	_	_
11-8	1307-1310	the	place[71]	new[71]	_	_
11-9	1311-1316	human	person[70]|place[71]	new[70]|new[71]	coref	17-12[121_70]
11-10	1317-1326	designers	person[70]|place[71]	new[70]|new[71]	_	_
11-11	1327-1328	’	person[70]|place[71]	new[70]|new[71]	_	_
11-12	1329-1341	construction	place[71]	new[71]	_	_
11-13	1342-1344	of	place[71]	new[71]	_	_
11-14	1345-1348	the	place[71]|object[72]	new[71]|new[72]	_	_
11-15	1349-1357	datasets	place[71]|object[72]	new[71]|new[72]	_	_
11-16	1358-1359	,	place[71]	new[71]	_	_
11-17	1360-1366	models	place[71]|abstract	new[71]|new	_	_
11-18	1367-1368	,	place[71]	new[71]	_	_
11-19	1369-1372	and	place[71]	new[71]	_	_
11-20	1373-1376	the	place[71]|abstract[74]	new[71]|new[74]	_	_
11-21	1377-1387	parameters	place[71]|abstract[74]	new[71]|new[74]	_	_
11-22	1388-1390	of	place[71]|abstract[74]	new[71]|new[74]	_	_
11-23	1391-1394	the	place[71]|abstract[74]|abstract[75]	new[71]|new[74]|new[75]	coref	15-3[97_75]
11-24	1395-1404	algorithm	place[71]|abstract[74]|abstract[75]	new[71]|new[74]|new[75]	_	_
11-25	1405-1406	,	_	_	_	_
11-26	1407-1412	which	_	_	_	_
11-27	1413-1424	potentially	_	_	_	_
11-28	1425-1430	leads	_	_	_	_
11-29	1431-1433	to	_	_	_	_
11-30	1434-1440	unfair	abstract[76]	new[76]	_	_
11-31	1441-1443	or	abstract[76]	new[76]	_	_
11-32	1444-1458	discriminatory	abstract[76]	new[76]	_	_
11-33	1459-1470	allocations	abstract[76]	new[76]	_	_
11-34	1471-1473	of	abstract[76]	new[76]	_	_
11-35	1474-1480	safety	abstract[76]|abstract|abstract[78]	new[76]|giv|giv[78]	coref|coref|coref|coref	16-39[110_0]|16-52[115_78]|16-39[110_0]|16-52[115_78]
11-36	1481-1486	risks	abstract[76]|abstract[78]	new[76]|giv[78]	_	_
11-37	1487-1488	.	_	_	_	_

#Text=Firstly , statistical bias exists when the input data are not statistically representative of the overall population .
12-1	1489-1496	Firstly	_	_	_	_
12-2	1497-1498	,	_	_	_	_
12-3	1499-1510	statistical	abstract[79]	giv[79]	coref	16-7[103_79]
12-4	1511-1515	bias	abstract[79]	giv[79]	_	_
12-5	1516-1522	exists	_	_	_	_
12-6	1523-1527	when	_	_	_	_
12-7	1528-1531	the	abstract[81]	new[81]	coref	13-8[0_81]
12-8	1532-1537	input	abstract|abstract[81]	new|new[81]	coref	15-18
12-9	1538-1542	data	abstract[81]	new[81]	_	_
12-10	1543-1546	are	_	_	_	_
12-11	1547-1550	not	_	_	_	_
12-12	1551-1564	statistically	_	_	_	_
12-13	1565-1579	representative	_	_	_	_
12-14	1580-1582	of	_	_	_	_
12-15	1583-1586	the	abstract[82]	new[82]	_	_
12-16	1587-1594	overall	abstract[82]	new[82]	_	_
12-17	1595-1605	population	abstract[82]	new[82]	_	_
12-18	1606-1607	.	_	_	_	_

#Text=For instance , training an AV using data from only one country could result in the AV learning localised patterns and not accurately modelling driving behaviours that apply in other countries or contexts .
13-1	1608-1611	For	_	_	_	_
13-2	1612-1620	instance	_	_	_	_
13-3	1621-1622	,	_	_	_	_
13-4	1623-1631	training	_	_	_	_
13-5	1632-1634	an	abstract[83]	giv[83]	coref	13-16[86_83]
13-6	1635-1637	AV	abstract[83]	giv[83]	_	_
13-7	1638-1643	using	_	_	_	_
13-8	1644-1648	data	abstract	giv	_	_
13-9	1649-1653	from	_	_	_	_
13-10	1654-1658	only	place[85]	new[85]	_	_
13-11	1659-1662	one	place[85]	new[85]	_	_
13-12	1663-1670	country	place[85]	new[85]	_	_
13-13	1671-1676	could	_	_	_	_
13-14	1677-1683	result	_	_	_	_
13-15	1684-1686	in	_	_	_	_
13-16	1687-1690	the	abstract[86]	giv[86]	coref	17-15[0_86]
13-17	1691-1693	AV	abstract[86]	giv[86]	_	_
13-18	1694-1702	learning	_	_	_	_
13-19	1703-1712	localised	abstract[87]	new[87]	_	_
13-20	1713-1721	patterns	abstract[87]	new[87]	_	_
13-21	1722-1725	and	_	_	_	_
13-22	1726-1729	not	_	_	_	_
13-23	1730-1740	accurately	_	_	_	_
13-24	1741-1750	modelling	_	_	_	_
13-25	1751-1758	driving	_	_	_	_
13-26	1759-1769	behaviours	abstract	new	_	_
13-27	1770-1774	that	_	_	_	_
13-28	1775-1780	apply	_	_	_	_
13-29	1781-1783	in	_	_	_	_
13-30	1784-1789	other	place[89]	new[89]	_	_
13-31	1790-1799	countries	place[89]	new[89]	_	_
13-32	1800-1802	or	_	_	_	_
13-33	1803-1811	contexts	abstract	new	_	_
13-34	1812-1813	.	_	_	_	_

#Text=Thus , the under- or overrepresentation of certain groups in the data can lead to inaccurate classifications and biased outcomes .
14-1	1814-1818	Thus	_	_	_	_
14-2	1819-1820	,	_	_	_	_
14-3	1821-1824	the	abstract[91]	new[91]	_	_
14-4	1825-1831	under-	abstract[91]	new[91]	_	_
14-5	1832-1834	or	_	_	_	_
14-6	1835-1853	overrepresentation	abstract[92]	new[92]	_	_
14-7	1854-1856	of	abstract[92]	new[92]	_	_
14-8	1857-1864	certain	abstract[92]|person[93]	new[92]|giv[93]	coref	20-32[162_93]
14-9	1865-1871	groups	abstract[92]|person[93]	new[92]|giv[93]	_	_
14-10	1872-1874	in	abstract[92]|person[93]	new[92]|giv[93]	_	_
14-11	1875-1878	the	abstract[92]|person[93]|abstract[94]	new[92]|giv[93]|new[94]	coref	20-45[165_94]
14-12	1879-1883	data	abstract[92]|person[93]|abstract[94]	new[92]|giv[93]|new[94]	_	_
14-13	1884-1887	can	_	_	_	_
14-14	1888-1892	lead	_	_	_	_
14-15	1893-1895	to	_	_	_	_
14-16	1896-1906	inaccurate	abstract[95]	new[95]	_	_
14-17	1907-1922	classifications	abstract[95]	new[95]	_	_
14-18	1923-1926	and	_	_	_	_
14-19	1927-1933	biased	abstract[96]	new[96]	coref	19-10[151_96]
14-20	1934-1942	outcomes	abstract[96]	new[96]	_	_
14-21	1943-1944	.	_	_	_	_

#Text=Secondly , the algorithm can be biased relative to legal and moral standards if it utilises sensitive input variables .
15-1	1945-1953	Secondly	_	_	_	_
15-2	1954-1955	,	_	_	_	_
15-3	1956-1959	the	abstract[97]	giv[97]	ana	15-15[0_97]
15-4	1960-1969	algorithm	abstract[97]	giv[97]	_	_
15-5	1970-1973	can	_	_	_	_
15-6	1974-1976	be	_	_	_	_
15-7	1977-1983	biased	_	_	_	_
15-8	1984-1992	relative	_	_	_	_
15-9	1993-1995	to	_	_	_	_
15-10	1996-2001	legal	abstract[98]	new[98]	coref	21-20[185_98]
15-11	2002-2005	and	abstract[98]	new[98]	_	_
15-12	2006-2011	moral	abstract[98]	new[98]	_	_
15-13	2012-2021	standards	abstract[98]	new[98]	_	_
15-14	2022-2024	if	_	_	_	_
15-15	2025-2027	it	abstract	giv	coref	16-31[108_0]
15-16	2028-2036	utilises	_	_	_	_
15-17	2037-2046	sensitive	abstract[101]	new[101]	coref	21-11[181_101]
15-18	2047-2052	input	abstract|abstract[101]	giv|new[101]	coref	22-17
15-19	2053-2062	variables	abstract[101]	new[101]	_	_
15-20	2063-2064	.	_	_	_	_

#Text=Individual-specific characteristics , such as a person ’s age and gender that are used as decision-making criteria can be penalised or privileged by the AVs ’ algorithms to meet the algorithm ’s pre-defined preferences , such as prioritising the safety of children or minimising the total quantity of harm , causing more safety risks to be allocated to individuals that share the penalised characteristics .
16-1	2065-2084	Individual-specific	abstract[102]	new[102]	coref	16-62[117_102]
16-2	2085-2100	characteristics	abstract[102]	new[102]	_	_
16-3	2101-2102	,	abstract[102]	new[102]	_	_
16-4	2103-2107	such	abstract[102]	new[102]	_	_
16-5	2108-2110	as	abstract[102]	new[102]	_	_
16-6	2111-2112	a	abstract[102]|abstract[104]	new[102]|new[104]	_	_
16-7	2113-2119	person	abstract[102]|person[103]|abstract[104]	new[102]|giv[103]|new[104]	coref	17-4[0_103]
16-8	2120-2122	’s	abstract[102]|person[103]|abstract[104]	new[102]|giv[103]|new[104]	_	_
16-9	2123-2126	age	abstract[102]|abstract[104]	new[102]|new[104]	_	_
16-10	2127-2130	and	abstract[102]	new[102]	_	_
16-11	2131-2137	gender	abstract[102]|abstract	new[102]|new	_	_
16-12	2138-2142	that	_	_	_	_
16-13	2143-2146	are	_	_	_	_
16-14	2147-2151	used	_	_	_	_
16-15	2152-2154	as	_	_	_	_
16-16	2155-2170	decision-making	_	_	_	_
16-17	2171-2179	criteria	_	_	_	_
16-18	2180-2183	can	_	_	_	_
16-19	2184-2186	be	_	_	_	_
16-20	2187-2196	penalised	_	_	_	_
16-21	2197-2199	or	_	_	_	_
16-22	2200-2210	privileged	_	_	_	_
16-23	2211-2213	by	_	_	_	_
16-24	2214-2217	the	abstract[107]	giv[107]	coref	18-44[148_107]
16-25	2218-2221	AVs	abstract[106]|abstract[107]	giv[106]|giv[107]	coref	18-11[0_106]
16-26	2222-2223	’	abstract[106]|abstract[107]	giv[106]|giv[107]	_	_
16-27	2224-2234	algorithms	abstract[107]	giv[107]	_	_
16-28	2235-2237	to	_	_	_	_
16-29	2238-2242	meet	_	_	_	_
16-30	2243-2246	the	abstract[109]	new[109]	coref	18-7[135_109]
16-31	2247-2256	algorithm	abstract[108]|abstract[109]	giv[108]|new[109]	coref	17-12[0_108]
16-32	2257-2259	’s	abstract[108]|abstract[109]	giv[108]|new[109]	_	_
16-33	2260-2271	pre-defined	abstract[109]	new[109]	_	_
16-34	2272-2283	preferences	abstract[109]	new[109]	_	_
16-35	2284-2285	,	_	_	_	_
16-36	2286-2290	such	_	_	_	_
16-37	2291-2293	as	_	_	_	_
16-38	2294-2306	prioritising	_	_	_	_
16-39	2307-2310	the	abstract[110]	giv[110]	coref	16-53[0_110]
16-40	2311-2317	safety	abstract[110]	giv[110]	_	_
16-41	2318-2320	of	abstract[110]	giv[110]	_	_
16-42	2321-2329	children	abstract[110]|person	giv[110]|new	_	_
16-43	2330-2332	or	_	_	_	_
16-44	2333-2343	minimising	_	_	_	_
16-45	2344-2347	the	abstract[112]	giv[112]	_	_
16-46	2348-2353	total	abstract[112]	giv[112]	_	_
16-47	2354-2362	quantity	abstract[112]	giv[112]	_	_
16-48	2363-2365	of	abstract[112]	giv[112]	_	_
16-49	2366-2370	harm	abstract[112]|abstract	giv[112]|new	_	_
16-50	2371-2372	,	_	_	_	_
16-51	2373-2380	causing	_	_	_	_
16-52	2381-2385	more	abstract[115]	giv[115]	coref	18-21[141_115]
16-53	2386-2392	safety	abstract|abstract[115]	giv|giv[115]	_	_
16-54	2393-2398	risks	abstract[115]	giv[115]	_	_
16-55	2399-2401	to	_	_	_	_
16-56	2402-2404	be	_	_	_	_
16-57	2405-2414	allocated	_	_	_	_
16-58	2415-2417	to	_	_	_	_
16-59	2418-2429	individuals	person	giv	coref	20-49[166_0]
16-60	2430-2434	that	_	_	_	_
16-61	2435-2440	share	_	_	_	_
16-62	2441-2444	the	abstract[117]	giv[117]	_	_
16-63	2445-2454	penalised	abstract[117]	giv[117]	_	_
16-64	2455-2470	characteristics	abstract[117]	giv[117]	_	_
16-65	2471-2472	.	_	_	_	_

#Text=These forms of bias can be introduced unintentionally or intentionally by algorithm designers and AV manufacturers to maximise profits , such as prioritising the safety of AV passengers to maximise profits , and this is exacerbated by the lack of legal frameworks to hold these stakeholders accountable .
17-1	2473-2478	These	abstract[118]	new[118]	_	_
17-2	2479-2484	forms	abstract[118]	new[118]	_	_
17-3	2485-2487	of	abstract[118]	new[118]	_	_
17-4	2488-2492	bias	abstract[118]|abstract	new[118]|giv	coref	19-2
17-5	2493-2496	can	_	_	_	_
17-6	2497-2499	be	_	_	_	_
17-7	2500-2510	introduced	_	_	_	_
17-8	2511-2526	unintentionally	_	_	_	_
17-9	2527-2529	or	_	_	_	_
17-10	2530-2543	intentionally	_	_	_	_
17-11	2544-2546	by	_	_	_	_
17-12	2547-2556	algorithm	abstract|person[121]	giv|giv[121]	coref|coref|coref|coref	24-18[212_0]|24-25[213_121]|24-18[212_0]|24-25[213_121]
17-13	2557-2566	designers	person[121]	giv[121]	_	_
17-14	2567-2570	and	person[121]	giv[121]	_	_
17-15	2571-2573	AV	person[121]|object|organization[123]	giv[121]|giv|new[123]	coref|coref	17-27|17-27
17-16	2574-2587	manufacturers	person[121]|organization[123]	giv[121]|new[123]	_	_
17-17	2588-2590	to	_	_	_	_
17-18	2591-2599	maximise	_	_	_	_
17-19	2600-2607	profits	abstract	new	coref	17-31
17-20	2608-2609	,	_	_	_	_
17-21	2610-2614	such	_	_	_	_
17-22	2615-2617	as	_	_	_	_
17-23	2618-2630	prioritising	_	_	_	_
17-24	2631-2634	the	abstract[125]	new[125]	ana	17-34[0_125]
17-25	2635-2641	safety	abstract[125]	new[125]	_	_
17-26	2642-2644	of	abstract[125]	new[125]	_	_
17-27	2645-2647	AV	abstract[125]|organization|person[127]	new[125]|giv|new[127]	coref|coref	18-21|18-21
17-28	2648-2658	passengers	abstract[125]|person[127]	new[125]|new[127]	_	_
17-29	2659-2661	to	_	_	_	_
17-30	2662-2670	maximise	_	_	_	_
17-31	2671-2678	profits	abstract	giv	_	_
17-32	2679-2680	,	_	_	_	_
17-33	2681-2684	and	_	_	_	_
17-34	2685-2689	this	abstract	giv	coref	18-22
17-35	2690-2692	is	_	_	_	_
17-36	2693-2704	exacerbated	_	_	_	_
17-37	2705-2707	by	_	_	_	_
17-38	2708-2711	the	abstract[130]	new[130]	_	_
17-39	2712-2716	lack	abstract[130]	new[130]	_	_
17-40	2717-2719	of	abstract[130]	new[130]	_	_
17-41	2720-2725	legal	abstract[130]|abstract[131]	new[130]|new[131]	_	_
17-42	2726-2736	frameworks	abstract[130]|abstract[131]	new[130]|new[131]	_	_
17-43	2737-2739	to	_	_	_	_
17-44	2740-2744	hold	_	_	_	_
17-45	2745-2750	these	person[132]	giv[132]	_	_
17-46	2751-2763	stakeholders	person[132]	giv[132]	_	_
17-47	2764-2775	accountable	_	_	_	_
17-48	2776-2777	.	_	_	_	_

#Text=Section 5.2 explores various types of ethical preferences to which AVs may be programmed to follow and their implications of AV safety risks in greater detail , and Section 5.3 explores how perverse incentives influence the choice of preferences that are programmed into AVs ’ algorithms .
18-1	2778-2785	Section	organization[133]	giv[133]	coref	18-29[143_133]
18-2	2786-2789	5.2	organization[133]	giv[133]	_	_
18-3	2790-2798	explores	_	_	_	_
18-4	2799-2806	various	abstract[134]	new[134]	_	_
18-5	2807-2812	types	abstract[134]	new[134]	_	_
18-6	2813-2815	of	abstract[134]	new[134]	_	_
18-7	2816-2823	ethical	abstract[134]|abstract[135]	new[134]|giv[135]	ana	18-18[0_135]
18-8	2824-2835	preferences	abstract[134]|abstract[135]	new[134]|giv[135]	_	_
18-9	2836-2838	to	_	_	_	_
18-10	2839-2844	which	_	_	_	_
18-11	2845-2848	AVs	abstract	giv	coref	18-44[147_0]
18-12	2849-2852	may	_	_	_	_
18-13	2853-2855	be	_	_	_	_
18-14	2856-2866	programmed	_	_	_	_
18-15	2867-2869	to	_	_	_	_
18-16	2870-2876	follow	_	_	_	_
18-17	2877-2880	and	_	_	_	_
18-18	2881-2886	their	abstract|abstract[138]	giv|giv[138]	coref|coref	18-39|18-39
18-19	2887-2899	implications	abstract[138]	giv[138]	_	_
18-20	2900-2902	of	abstract[138]	giv[138]	_	_
18-21	2903-2905	AV	abstract[138]|abstract|abstract[141]	giv[138]|giv|giv[141]	coref|coref	28-13[255_141]|28-13[255_141]
18-22	2906-2912	safety	abstract[138]|abstract|abstract[141]	giv[138]|giv|giv[141]	coref	22-53
18-23	2913-2918	risks	abstract[138]|abstract[141]	giv[138]|giv[141]	_	_
18-24	2919-2921	in	abstract[138]|abstract[141]	giv[138]|giv[141]	_	_
18-25	2922-2929	greater	abstract[138]|abstract[141]|abstract[142]	giv[138]|giv[141]|new[142]	_	_
18-26	2930-2936	detail	abstract[138]|abstract[141]|abstract[142]	giv[138]|giv[141]|new[142]	_	_
18-27	2937-2938	,	_	_	_	_
18-28	2939-2942	and	_	_	_	_
18-29	2943-2950	Section	abstract[143]	giv[143]	_	_
18-30	2951-2954	5.3	abstract[143]	giv[143]	_	_
18-31	2955-2963	explores	_	_	_	_
18-32	2964-2967	how	_	_	_	_
18-33	2968-2976	perverse	abstract[144]	giv[144]	_	_
18-34	2977-2987	incentives	abstract[144]	giv[144]	_	_
18-35	2988-2997	influence	_	_	_	_
18-36	2998-3001	the	abstract[145]	new[145]	_	_
18-37	3002-3008	choice	abstract[145]	new[145]	_	_
18-38	3009-3011	of	abstract[145]	new[145]	_	_
18-39	3012-3023	preferences	abstract[145]|abstract	new[145]|giv	coref	27-14[244_0]
18-40	3024-3028	that	_	_	_	_
18-41	3029-3032	are	_	_	_	_
18-42	3033-3043	programmed	_	_	_	_
18-43	3044-3048	into	_	_	_	_
18-44	3049-3052	AVs	abstract[147]|abstract[148]	giv[147]|giv[148]	coref|coref|coref|coref	19-4[0_148]|19-13[0_147]|19-4[0_148]|19-13[0_147]
18-45	3053-3054	’	abstract[147]|abstract[148]	giv[147]|giv[148]	_	_
18-46	3055-3065	algorithms	abstract[148]	giv[148]	_	_
18-47	3066-3067	.	_	_	_	_

#Text=Lessening bias in algorithms is therefore crucial to mitigate discriminatory outcomes from AVs .
19-1	3068-3077	Lessening	_	_	_	_
19-2	3078-3082	bias	abstract	giv	coref	20-18
19-3	3083-3085	in	_	_	_	_
19-4	3086-3096	algorithms	abstract	giv	coref	20-73
19-5	3097-3099	is	_	_	_	_
19-6	3100-3109	therefore	_	_	_	_
19-7	3110-3117	crucial	_	_	_	_
19-8	3118-3120	to	_	_	_	_
19-9	3121-3129	mitigate	_	_	_	_
19-10	3130-3144	discriminatory	abstract[151]	giv[151]	coref	27-8[242_151]
19-11	3145-3153	outcomes	abstract[151]	giv[151]	_	_
19-12	3154-3158	from	_	_	_	_
19-13	3159-3162	AVs	abstract	giv	coref	27-12[243_0]
19-14	3163-3164	.	_	_	_	_

#Text=In autonomous systems in general , scholars have recommended ways to detect and offset the effects of bias , such as modifying algorithmic outputs to balance the effects of bias between protected and unprotected groups , introducing minimally intrusive modification to remove bias from the data , incorporating individuals from potentially discriminated groups , testing techniques to measure discrimination and identify groups of users significantly affected by bias in software and creating algorithms that certify the absence of data bias .
20-1	3165-3167	In	_	_	_	_
20-2	3168-3178	autonomous	abstract[153]	new[153]	_	_
20-3	3179-3186	systems	abstract[153]	new[153]	_	_
20-4	3187-3189	in	abstract[153]	new[153]	_	_
20-5	3190-3197	general	abstract[153]|abstract	new[153]|new	_	_
20-6	3198-3199	,	_	_	_	_
20-7	3200-3208	scholars	person	new	coref	22-3
20-8	3209-3213	have	_	_	_	_
20-9	3214-3225	recommended	_	_	_	_
20-10	3226-3230	ways	abstract	new	_	_
20-11	3231-3233	to	_	_	_	_
20-12	3234-3240	detect	_	_	_	_
20-13	3241-3244	and	_	_	_	_
20-14	3245-3251	offset	_	_	_	_
20-15	3252-3255	the	abstract[157]	new[157]	_	_
20-16	3256-3263	effects	abstract[157]	new[157]	_	_
20-17	3264-3266	of	abstract[157]	new[157]	_	_
20-18	3267-3271	bias	abstract[157]|abstract	new[157]|giv	coref	20-30[161_0]
20-19	3272-3273	,	_	_	_	_
20-20	3274-3278	such	_	_	_	_
20-21	3279-3281	as	_	_	_	_
20-22	3282-3291	modifying	_	_	_	_
20-23	3292-3303	algorithmic	abstract[159]	new[159]	coref	22-45[201_159]
20-24	3304-3311	outputs	abstract[159]	new[159]	_	_
20-25	3312-3314	to	_	_	_	_
20-26	3315-3322	balance	_	_	_	_
20-27	3323-3326	the	abstract[160]	new[160]	coref	23-12[208_160]
20-28	3327-3334	effects	abstract[160]	new[160]	_	_
20-29	3335-3337	of	abstract[160]	new[160]	_	_
20-30	3338-3342	bias	abstract[160]|abstract[161]	new[160]|giv[161]	coref	20-43[0_161]
20-31	3343-3350	between	abstract[160]|abstract[161]	new[160]|giv[161]	_	_
20-32	3351-3360	protected	abstract[160]|abstract[161]|person[162]	new[160]|giv[161]|giv[162]	coref	20-51[167_162]
20-33	3361-3364	and	abstract[160]|abstract[161]|person[162]	new[160]|giv[161]|giv[162]	_	_
20-34	3365-3376	unprotected	abstract[160]|abstract[161]|person[162]	new[160]|giv[161]|giv[162]	_	_
20-35	3377-3383	groups	abstract[160]|abstract[161]|person[162]	new[160]|giv[161]|giv[162]	_	_
20-36	3384-3385	,	_	_	_	_
20-37	3386-3397	introducing	_	_	_	_
20-38	3398-3407	minimally	abstract[163]	new[163]	_	_
20-39	3408-3417	intrusive	abstract[163]	new[163]	_	_
20-40	3418-3430	modification	abstract[163]	new[163]	_	_
20-41	3431-3433	to	_	_	_	_
20-42	3434-3440	remove	_	_	_	_
20-43	3441-3445	bias	abstract	giv	coref	20-68[172_0]
20-44	3446-3450	from	_	_	_	_
20-45	3451-3454	the	abstract[165]	giv[165]	coref	20-79[0_165]
20-46	3455-3459	data	abstract[165]	giv[165]	_	_
20-47	3460-3461	,	_	_	_	_
20-48	3462-3475	incorporating	_	_	_	_
20-49	3476-3487	individuals	person[166]	giv[166]	_	_
20-50	3488-3492	from	person[166]	giv[166]	_	_
20-51	3493-3504	potentially	person[166]|person[167]	giv[166]|giv[167]	_	_
20-52	3505-3518	discriminated	person[166]|person[167]	giv[166]|giv[167]	_	_
20-53	3519-3525	groups	person[166]|person[167]	giv[166]|giv[167]	_	_
20-54	3526-3527	,	_	_	_	_
20-55	3528-3535	testing	_	_	_	_
20-56	3536-3546	techniques	abstract	new	_	_
20-57	3547-3549	to	_	_	_	_
20-58	3550-3557	measure	_	_	_	_
20-59	3558-3572	discrimination	abstract	giv	coref	28-18[256_0]
20-60	3573-3576	and	_	_	_	_
20-61	3577-3585	identify	_	_	_	_
20-62	3586-3592	groups	person[170]	new[170]	_	_
20-63	3593-3595	of	person[170]	new[170]	_	_
20-64	3596-3601	users	person[170]|person	new[170]|new	_	_
20-65	3602-3615	significantly	_	_	_	_
20-66	3616-3624	affected	_	_	_	_
20-67	3625-3627	by	_	_	_	_
20-68	3628-3632	bias	abstract[172]	giv[172]	coref	20-79[177_172]
20-69	3633-3635	in	abstract[172]	giv[172]	_	_
20-70	3636-3644	software	abstract[172]|abstract	giv[172]|new	_	_
20-71	3645-3648	and	_	_	_	_
20-72	3649-3657	creating	_	_	_	_
20-73	3658-3668	algorithms	abstract	giv	coref	22-14
20-74	3669-3673	that	_	_	_	_
20-75	3674-3681	certify	_	_	_	_
20-76	3682-3685	the	abstract[175]	new[175]	_	_
20-77	3686-3693	absence	abstract[175]	new[175]	_	_
20-78	3694-3696	of	abstract[175]	new[175]	_	_
20-79	3697-3701	data	abstract[175]|abstract|abstract[177]	new[175]|giv|giv[177]	coref|coref|coref|coref	21-3[0_177]|21-6[179_0]|21-3[0_177]|21-6[179_0]
20-80	3702-3706	bias	abstract[175]|abstract[177]	new[175]|giv[177]	_	_
20-81	3707-3708	.	_	_	_	_

#Text=Apart from bias originating from the data and selection of variables and criterion , Danks and London recommend clarifying ethical standards such as fairness to evaluate bias .
21-1	3709-3714	Apart	_	_	_	_
21-2	3715-3719	from	_	_	_	_
21-3	3720-3724	bias	abstract	giv	coref	21-27
21-4	3725-3736	originating	_	_	_	_
21-5	3737-3741	from	_	_	_	_
21-6	3742-3745	the	abstract[179]	giv[179]	coref	25-11[221_179]
21-7	3746-3750	data	abstract[179]	giv[179]	_	_
21-8	3751-3754	and	abstract[179]	giv[179]	_	_
21-9	3755-3764	selection	abstract[179]|abstract[180]	giv[179]|new[180]	_	_
21-10	3765-3767	of	abstract[179]|abstract[180]	giv[179]|new[180]	_	_
21-11	3768-3777	variables	abstract[179]|abstract[180]|abstract[181]	giv[179]|new[180]|giv[181]	coref	22-15[193_181]
21-12	3778-3781	and	abstract[179]|abstract[180]|abstract[181]	giv[179]|new[180]|giv[181]	_	_
21-13	3782-3791	criterion	abstract[179]|abstract[180]|abstract[181]|abstract	giv[179]|new[180]|giv[181]|new	_	_
21-14	3792-3793	,	abstract[179]|abstract[180]|abstract[181]	giv[179]|new[180]|giv[181]	_	_
21-15	3794-3799	Danks	abstract[179]|abstract[180]|abstract[181]|person	giv[179]|new[180]|giv[181]|new	_	_
21-16	3800-3803	and	abstract[179]|abstract[180]|abstract[181]	giv[179]|new[180]|giv[181]	_	_
21-17	3804-3810	London	abstract[179]|abstract[180]|abstract[181]|person	giv[179]|new[180]|giv[181]|new	_	_
21-18	3811-3820	recommend	_	_	_	_
21-19	3821-3831	clarifying	_	_	_	_
21-20	3832-3839	ethical	abstract[185]	giv[185]	_	_
21-21	3840-3849	standards	abstract[185]	giv[185]	_	_
21-22	3850-3854	such	abstract[185]	giv[185]	_	_
21-23	3855-3857	as	abstract[185]	giv[185]	_	_
21-24	3858-3866	fairness	abstract[185]|abstract	giv[185]|new	_	_
21-25	3867-3869	to	_	_	_	_
21-26	3870-3878	evaluate	_	_	_	_
21-27	3879-3883	bias	abstract	giv	coref	23-8[205_0]
21-28	3884-3885	.	_	_	_	_

#Text=Furthermore , scholars recommend increasing transparency to identify biases , such as designing algorithms whose original input variables can be traced throughout the system ( i. e. , traceability ) and auditing algorithms to enhance their interpretability so that biases can be detected and the system ’s outputs can be verified against safety requirements .
22-1	3886-3897	Furthermore	_	_	_	_
22-2	3898-3899	,	_	_	_	_
22-3	3900-3908	scholars	person	giv	_	_
22-4	3909-3918	recommend	_	_	_	_
22-5	3919-3929	increasing	abstract[189]	new[189]	_	_
22-6	3930-3942	transparency	abstract[189]	new[189]	_	_
22-7	3943-3945	to	_	_	_	_
22-8	3946-3954	identify	_	_	_	_
22-9	3955-3961	biases	abstract	new	coref	22-40
22-10	3962-3963	,	_	_	_	_
22-11	3964-3968	such	_	_	_	_
22-12	3969-3971	as	_	_	_	_
22-13	3972-3981	designing	_	_	_	_
22-14	3982-3992	algorithms	abstract	giv	coref	22-33
22-15	3993-3998	whose	abstract[193]	giv[193]	ana	23-12[0_193]
22-16	3999-4007	original	abstract[193]	giv[193]	_	_
22-17	4008-4013	input	abstract|abstract[193]	giv|giv[193]	_	_
22-18	4014-4023	variables	abstract[193]	giv[193]	_	_
22-19	4024-4027	can	_	_	_	_
22-20	4028-4030	be	_	_	_	_
22-21	4031-4037	traced	_	_	_	_
22-22	4038-4048	throughout	_	_	_	_
22-23	4049-4052	the	abstract[194]	giv[194]	coref	22-46[200_194]
22-24	4053-4059	system	abstract[194]	giv[194]	_	_
22-25	4060-4061	(	_	_	_	_
22-26	4062-4064	i.	_	_	_	_
22-27	4065-4067	e.	_	_	_	_
22-28	4068-4069	,	_	_	_	_
22-29	4070-4082	traceability	abstract	new	_	_
22-30	4083-4084	)	_	_	_	_
22-31	4085-4088	and	_	_	_	_
22-32	4089-4097	auditing	_	_	_	_
22-33	4098-4108	algorithms	abstract	giv	ana	22-36
22-34	4109-4111	to	_	_	_	_
22-35	4112-4119	enhance	_	_	_	_
22-36	4120-4125	their	abstract|abstract[198]	giv|new[198]	coref|coref	23-10[206_0]|23-10[206_0]
22-37	4126-4142	interpretability	abstract[198]	new[198]	_	_
22-38	4143-4145	so	_	_	_	_
22-39	4146-4150	that	_	_	_	_
22-40	4151-4157	biases	abstract	giv	_	_
22-41	4158-4161	can	_	_	_	_
22-42	4162-4164	be	_	_	_	_
22-43	4165-4173	detected	_	_	_	_
22-44	4174-4177	and	_	_	_	_
22-45	4178-4181	the	abstract[201]	giv[201]	_	_
22-46	4182-4188	system	abstract[200]|abstract[201]	giv[200]|giv[201]	_	_
22-47	4189-4191	’s	abstract[200]|abstract[201]	giv[200]|giv[201]	_	_
22-48	4192-4199	outputs	abstract[201]	giv[201]	_	_
22-49	4200-4203	can	_	_	_	_
22-50	4204-4206	be	_	_	_	_
22-51	4207-4215	verified	_	_	_	_
22-52	4216-4223	against	_	_	_	_
22-53	4224-4230	safety	abstract|abstract[203]	giv|new[203]	coref|coref	28-14|28-14
22-54	4231-4243	requirements	abstract[203]	new[203]	_	_
22-55	4244-4245	.	_	_	_	_

#Text=However , there are challenges in identifying bias in algorithms and their discriminatory effects .
23-1	4246-4253	However	_	_	_	_
23-2	4254-4255	,	_	_	_	_
23-3	4256-4261	there	_	_	_	_
23-4	4262-4265	are	_	_	_	_
23-5	4266-4276	challenges	abstract	new	_	_
23-6	4277-4279	in	_	_	_	_
23-7	4280-4291	identifying	_	_	_	_
23-8	4292-4296	bias	abstract[205]	giv[205]	coref	24-35[0_205]
23-9	4297-4299	in	abstract[205]	giv[205]	_	_
23-10	4300-4310	algorithms	abstract[205]|abstract[206]	giv[205]|giv[206]	coref	24-3[209_206]
23-11	4311-4314	and	abstract[205]|abstract[206]	giv[205]|giv[206]	_	_
23-12	4315-4320	their	abstract[205]|abstract[206]|abstract|abstract[208]	giv[205]|giv[206]|giv|giv[208]	coref|coref	25-24[224_208]|25-24[224_208]
23-13	4321-4335	discriminatory	abstract[205]|abstract[206]|abstract[208]	giv[205]|giv[206]|giv[208]	_	_
23-14	4336-4343	effects	abstract[205]|abstract[206]|abstract[208]	giv[205]|giv[206]|giv[208]	_	_
23-15	4344-4345	.	_	_	_	_

#Text=Firstly , many algorithms are designed to be highly complex for greater accuracy , but this renders the algorithm opaque and difficult to interpret even by the designers themselves , concealing the sources of bias .
24-1	4346-4353	Firstly	_	_	_	_
24-2	4354-4355	,	_	_	_	_
24-3	4356-4360	many	abstract[209]	giv[209]	ana	24-29[0_209]
24-4	4361-4371	algorithms	abstract[209]	giv[209]	_	_
24-5	4372-4375	are	_	_	_	_
24-6	4376-4384	designed	_	_	_	_
24-7	4385-4387	to	_	_	_	_
24-8	4388-4390	be	_	_	_	_
24-9	4391-4397	highly	_	_	_	_
24-10	4398-4405	complex	_	_	_	_
24-11	4406-4409	for	_	_	_	_
24-12	4410-4417	greater	abstract[210]	new[210]	ana	24-16[0_210]
24-13	4418-4426	accuracy	abstract[210]	new[210]	_	_
24-14	4427-4428	,	_	_	_	_
24-15	4429-4432	but	_	_	_	_
24-16	4433-4437	this	abstract	giv	_	_
24-17	4438-4445	renders	_	_	_	_
24-18	4446-4449	the	abstract[212]	giv[212]	coref	26-41[235_212]
24-19	4450-4459	algorithm	abstract[212]	giv[212]	_	_
24-20	4460-4466	opaque	abstract[212]	giv[212]	_	_
24-21	4467-4470	and	abstract[212]	giv[212]	_	_
24-22	4471-4480	difficult	abstract[212]	giv[212]	_	_
24-23	4481-4483	to	abstract[212]	giv[212]	_	_
24-24	4484-4493	interpret	abstract[212]	giv[212]	_	_
24-25	4494-4498	even	person[213]	giv[213]	_	_
24-26	4499-4501	by	person[213]	giv[213]	_	_
24-27	4502-4505	the	person[213]	giv[213]	_	_
24-28	4506-4515	designers	person[213]	giv[213]	_	_
24-29	4516-4526	themselves	person[213]|abstract	giv[213]|giv	coref	25-4[218_0]
24-30	4527-4528	,	_	_	_	_
24-31	4529-4539	concealing	_	_	_	_
24-32	4540-4543	the	abstract[215]	new[215]	_	_
24-33	4544-4551	sources	abstract[215]	new[215]	_	_
24-34	4552-4554	of	abstract[215]	new[215]	_	_
24-35	4555-4559	bias	abstract[215]|abstract	new[215]|giv	coref	26-30[232_0]
24-36	4560-4561	.	_	_	_	_

#Text=Secondly , as ML algorithms make decisions mainly based on the training data that changes over time , it is difficult to predict potentially discriminatory effects in advance .
25-1	4562-4570	Secondly	_	_	_	_
25-2	4571-4572	,	_	_	_	_
25-3	4573-4575	as	_	_	_	_
25-4	4576-4578	ML	quantity|abstract[218]	new|giv[218]	coref|coref	26-18[229_218]|26-18[229_218]
25-5	4579-4589	algorithms	abstract[218]	giv[218]	_	_
25-6	4590-4594	make	_	_	_	_
25-7	4595-4604	decisions	abstract	giv	coref	26-10[227_0]
25-8	4605-4611	mainly	_	_	_	_
25-9	4612-4617	based	_	_	_	_
25-10	4618-4620	on	_	_	_	_
25-11	4621-4624	the	abstract[221]	giv[221]	coref	26-45[236_221]
25-12	4625-4633	training	abstract|abstract[221]	new|giv[221]	_	_
25-13	4634-4638	data	abstract[221]	giv[221]	_	_
25-14	4639-4643	that	_	_	_	_
25-15	4644-4651	changes	_	_	_	_
25-16	4652-4656	over	_	_	_	_
25-17	4657-4661	time	_	_	_	_
25-18	4662-4663	,	_	_	_	_
25-19	4664-4666	it	abstract	new	cata	25-19[0_223]
25-20	4667-4669	is	_	_	_	_
25-21	4670-4679	difficult	_	_	_	_
25-22	4680-4682	to	abstract[223]	new[223]	_	_
25-23	4683-4690	predict	abstract[223]	new[223]	_	_
25-24	4691-4702	potentially	abstract[223]|abstract[224]	new[223]|giv[224]	_	_
25-25	4703-4717	discriminatory	abstract[223]|abstract[224]	new[223]|giv[224]	_	_
25-26	4718-4725	effects	abstract[223]|abstract[224]	new[223]|giv[224]	_	_
25-27	4726-4728	in	_	_	_	_
25-28	4729-4736	advance	abstract	new	_	_
25-29	4737-4738	.	_	_	_	_

#Text=Humans are also excessively trusting and insufficiently critical of algorithmic decisions due to the popular perception of algorithms as objective and fair , a problem referred to as “ automation bias ” and the seemingly “ objective ” correlations that the algorithm learns from the data makes it difficult to legally establish discriminatory intent in algorithms .
26-1	4739-4745	Humans	person	new	_	_
26-2	4746-4749	are	_	_	_	_
26-3	4750-4754	also	_	_	_	_
26-4	4755-4766	excessively	_	_	_	_
26-5	4767-4775	trusting	_	_	_	_
26-6	4776-4779	and	_	_	_	_
26-7	4780-4794	insufficiently	_	_	_	_
26-8	4795-4803	critical	_	_	_	_
26-9	4804-4806	of	_	_	_	_
26-10	4807-4818	algorithmic	abstract[227]	giv[227]	coref	27-33[250_227]
26-11	4819-4828	decisions	abstract[227]	giv[227]	_	_
26-12	4829-4832	due	_	_	_	_
26-13	4833-4835	to	_	_	_	_
26-14	4836-4839	the	abstract[228]	new[228]	_	_
26-15	4840-4847	popular	abstract[228]	new[228]	_	_
26-16	4848-4858	perception	abstract[228]	new[228]	_	_
26-17	4859-4861	of	abstract[228]	new[228]	_	_
26-18	4862-4872	algorithms	abstract[228]|abstract[229]	new[228]|giv[229]	coref	26-56[0_229]
26-19	4873-4875	as	abstract[228]|abstract[229]	new[228]|giv[229]	_	_
26-20	4876-4885	objective	abstract[228]|abstract[229]	new[228]|giv[229]	_	_
26-21	4886-4889	and	abstract[228]|abstract[229]	new[228]|giv[229]	_	_
26-22	4890-4894	fair	abstract[228]|abstract[229]	new[228]|giv[229]	_	_
26-23	4895-4896	,	_	_	_	_
26-24	4897-4898	a	abstract[230]	new[230]	_	_
26-25	4899-4906	problem	abstract[230]	new[230]	_	_
26-26	4907-4915	referred	_	_	_	_
26-27	4916-4918	to	_	_	_	_
26-28	4919-4921	as	_	_	_	_
26-29	4922-4923	“	_	_	_	_
26-30	4924-4934	automation	abstract|abstract[232]	new|giv[232]	_	_
26-31	4935-4939	bias	abstract[232]	giv[232]	_	_
26-32	4940-4941	”	_	_	_	_
26-33	4942-4945	and	_	_	_	_
26-34	4946-4949	the	abstract[233]	new[233]	_	_
26-35	4950-4959	seemingly	abstract[233]	new[233]	_	_
26-36	4960-4961	“	abstract[233]	new[233]	_	_
26-37	4962-4971	objective	abstract[233]	new[233]	_	_
26-38	4972-4973	”	abstract[233]	new[233]	_	_
26-39	4974-4986	correlations	abstract[233]|abstract	new[233]|new	_	_
26-40	4987-4991	that	_	_	_	_
26-41	4992-4995	the	abstract[235]	giv[235]	_	_
26-42	4996-5005	algorithm	abstract[235]	giv[235]	_	_
26-43	5006-5012	learns	_	_	_	_
26-44	5013-5017	from	_	_	_	_
26-45	5018-5021	the	abstract[236]	giv[236]	ana	26-48[0_236]
26-46	5022-5026	data	abstract[236]	giv[236]	_	_
26-47	5027-5032	makes	_	_	_	_
26-48	5033-5035	it	abstract	giv	_	_
26-49	5036-5045	difficult	_	_	_	_
26-50	5046-5048	to	_	_	_	_
26-51	5049-5056	legally	_	_	_	_
26-52	5057-5066	establish	_	_	_	_
26-53	5067-5081	discriminatory	abstract[238]	giv[238]	_	_
26-54	5082-5088	intent	abstract[238]	giv[238]	_	_
26-55	5089-5091	in	_	_	_	_
26-56	5092-5102	algorithms	abstract	giv	_	_
26-57	5103-5104	.	_	_	_	_

#Text=An emerging issue is the aggregation of individually biased outcomes when AVs with similar preferences are deployed on a large-scale , as doing so would centralise and replicate algorithmic preferences along with their individually biased risk allocation decisions .
27-1	5105-5107	An	abstract[240]	new[240]	coref	27-5[241_240]
27-2	5108-5116	emerging	abstract[240]	new[240]	_	_
27-3	5117-5122	issue	abstract[240]	new[240]	_	_
27-4	5123-5125	is	_	_	_	_
27-5	5126-5129	the	abstract[241]	giv[241]	_	_
27-6	5130-5141	aggregation	abstract[241]	giv[241]	_	_
27-7	5142-5144	of	abstract[241]	giv[241]	_	_
27-8	5145-5157	individually	abstract[241]|abstract[242]	giv[241]|giv[242]	coref	28-34[259_242]
27-9	5158-5164	biased	abstract[241]|abstract[242]	giv[241]|giv[242]	_	_
27-10	5165-5173	outcomes	abstract[241]|abstract[242]	giv[241]|giv[242]	_	_
27-11	5174-5178	when	_	_	_	_
27-12	5179-5182	AVs	abstract[243]	giv[243]	ana	28-1[0_243]
27-13	5183-5187	with	abstract[243]	giv[243]	_	_
27-14	5188-5195	similar	abstract[243]|abstract[244]	giv[243]|giv[244]	coref	27-29[246_244]
27-15	5196-5207	preferences	abstract[243]|abstract[244]	giv[243]|giv[244]	_	_
27-16	5208-5211	are	_	_	_	_
27-17	5212-5220	deployed	_	_	_	_
27-18	5221-5223	on	_	_	_	_
27-19	5224-5225	a	object[245]	new[245]	_	_
27-20	5226-5237	large-scale	object[245]	new[245]	_	_
27-21	5238-5239	,	_	_	_	_
27-22	5240-5242	as	_	_	_	_
27-23	5243-5248	doing	_	_	_	_
27-24	5249-5251	so	_	_	_	_
27-25	5252-5257	would	_	_	_	_
27-26	5258-5268	centralise	_	_	_	_
27-27	5269-5272	and	_	_	_	_
27-28	5273-5282	replicate	_	_	_	_
27-29	5283-5294	algorithmic	abstract[246]	giv[246]	ana	27-33[0_246]
27-30	5295-5306	preferences	abstract[246]	giv[246]	_	_
27-31	5307-5312	along	_	_	_	_
27-32	5313-5317	with	_	_	_	_
27-33	5318-5323	their	abstract|abstract[250]	giv|giv[250]	_	_
27-34	5324-5336	individually	abstract[250]	giv[250]	_	_
27-35	5337-5343	biased	abstract[250]	giv[250]	_	_
27-36	5344-5348	risk	abstract|abstract[250]	new|giv[250]	_	_
27-37	5349-5359	allocation	abstract|abstract[250]	new|giv[250]	_	_
27-38	5360-5369	decisions	abstract[250]	giv[250]	_	_
27-39	5370-5371	.	_	_	_	_

#Text=This could lead to the same groups of people being consistently allocated more safety risks and perpetuate systemic discrimination , which is more difficult to detect as it results from the accumulation of similar driving outcomes .
28-1	5372-5376	This	abstract	giv	ana	28-28
28-2	5377-5382	could	_	_	_	_
28-3	5383-5387	lead	_	_	_	_
28-4	5388-5390	to	_	_	_	_
28-5	5391-5394	the	person[252]	new[252]	_	_
28-6	5395-5399	same	person[252]	new[252]	_	_
28-7	5400-5406	groups	person[252]	new[252]	_	_
28-8	5407-5409	of	person[252]	new[252]	_	_
28-9	5410-5416	people	person[252]|person	new[252]|giv	_	_
28-10	5417-5422	being	_	_	_	_
28-11	5423-5435	consistently	_	_	_	_
28-12	5436-5445	allocated	_	_	_	_
28-13	5446-5450	more	abstract[255]	giv[255]	_	_
28-14	5451-5457	safety	abstract|abstract[255]	giv|giv[255]	_	_
28-15	5458-5463	risks	abstract[255]	giv[255]	_	_
28-16	5464-5467	and	_	_	_	_
28-17	5468-5478	perpetuate	_	_	_	_
28-18	5479-5487	systemic	abstract[256]	giv[256]	_	_
28-19	5488-5502	discrimination	abstract[256]	giv[256]	_	_
28-20	5503-5504	,	_	_	_	_
28-21	5505-5510	which	_	_	_	_
28-22	5511-5513	is	_	_	_	_
28-23	5514-5518	more	_	_	_	_
28-24	5519-5528	difficult	_	_	_	_
28-25	5529-5531	to	_	_	_	_
28-26	5532-5538	detect	_	_	_	_
28-27	5539-5541	as	_	_	_	_
28-28	5542-5544	it	abstract	giv	_	_
28-29	5545-5552	results	_	_	_	_
28-30	5553-5557	from	_	_	_	_
28-31	5558-5561	the	abstract[258]	new[258]	_	_
28-32	5562-5574	accumulation	abstract[258]	new[258]	_	_
28-33	5575-5577	of	abstract[258]	new[258]	_	_
28-34	5578-5585	similar	abstract[258]|abstract[259]	new[258]|giv[259]	_	_
28-35	5586-5593	driving	abstract[258]|abstract[259]	new[258]|giv[259]	_	_
28-36	5594-5602	outcomes	abstract[258]|abstract[259]	new[258]|giv[259]	_	_
28-37	5603-5604	.	_	_	_	_
