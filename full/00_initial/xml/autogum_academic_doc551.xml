<text id="autogum_academic_doc551" title="(Hyper)Graph Embedding and Classification via Simplicial Complexes" shortTile="hypergraph-embedding" author="Alessio Martino, Alessandro Giuliani, Antonello Rizzi" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/1999-4893/12/11/223/htm" speakerList="none" speakerCount="0">
<head> 3. Results</head><head> 3.1. On Benchmark Data</head>
<p>In order to show the effectiveness of the proposed embedding procedure, both of the classification strategies (-SVM and -SVM) have been considered. The genetic algorithm has been configured as follows: 100 individuals per 100 generations with a strict early-stop criterion if the average fitness function over rd of the total number of generations is less than or equal to , the elitism is set to 10% of the population, the selection follows the roulette wheel heuristic, the crossover operator generates new offsprings in a scattered fashion and the mutation acts in a flip-the-bit fashion for boolean genes and adds to real-valued genes a random number extracted from a zero-mean Gaussian distribution whose variance shrinks as generations go by. The upper and lower bounds for SVMs hyperparameters are  by definition, ,  and  has entries in range . </p>

<p>Two classification systems have been used as competitors:
<list>
<item><b>The Weighted Jaccard Kernel</b>. Originally proposed in Ref., the Weighted Jaccard Kernel (WJK) is an hypergraph kernel working on the top of the simplicial complexes from the underlying graphs. As a proper kernel function, WJK performs an implicit embedding procedure towards a possibly infinite-dimensional Hilbert space. In synthesis, the WJK between two simplicial complexes, say  and , is evaluated as follows: after considering the ’simplices-of-node-labels’ rather than the ’simplices-of-nodes’ as described in Section 2.2.1, the set of unique simplices belonging to either  or  is considered. Then,  and  are transformed in two vectors, say  and , by counting the occurrences of simplices in the unique set within the two simplicial complexes. Finally, . The kernel matrix obtained by evaluating the pairwise weighted Jaccard similarity between any two pairs of simplicial complexes in the available dataset is finally fed to a -SVM.</item>
<item><b>GRALG</b>. Originally proposed in Ref. and later used in Refs. for image classification, GRALG is a Granular Computing-based classification system for graphs. Despite the fact that it considers network motifs rather than simplices, it is still based on the same embedding procedure by means of symbolic histograms. In synthesis, GRALG extracts network motifs from the training data and runs a clustering procedure on such subgraphs by using a graph edit distance as the core (dis)similarity measure. The medoids (MinSODs) of these clusters form the alphabet on top of which the embedding space is built. Two genetic algorithms take care of tuning the alphabet synthesis and the feature selection procedure, respectively. GRALG, however, suffers from an heavy computational burden which may become unfeasible for large datasets. In order to overcome this problem, the random walk-based variant proposed in Ref. has been used.</item>
</list>
</p>

<p>Thirty datasets freely available from Ref. have been considered for testing, all of which well suit the classification problem at hand being labelled on nodes with categorical attributes. Each dataset has been split into a training set () and test set () in a stratified manner in order to preserve ground-truth labels distribution across the two splits. Validation data have been taken from the training set via 5-fold cross-validation. For the proposed embedding procedure and WJK, the Clique complex has been used since the underlying 1-skeleton is already available from the considered datasets. For GRALG, the maximum motifs size has been set to 5 and, following Ref., a subsampling rate of  has been performed on the training set. Alongside GRALG and WJK, the accuracy of the dummy classifier is also included: the latter serves as a baseline solution and quantifies the performance obtained by a purely random decision rule. Indeed, the dummy classifier outputs a given label, say  with a probability related to the relative frequency of  amongst the training patterns and, by definition, does not consider the information carried out by the pattern descriptions (input domain) in training data. </p>
</text>
