<text id="autogum_academic_doc542" title="Augmenting Deep Learning Performance in an Evidential Multiple Classifier System" shortTile="augmenting-deep-learning" author="Jennifer Vandoni, Sylvie  Le Hégarat-Mascle, Emanuel Aldea" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/1424-8220/19/21/4664/htm" speakerList="none" speakerCount="0">
<head> 1. Introduction</head>
<p>
Even though deep learning solutions tend to outperform the other supervised learning techniques when trained on large amounts of data, applying them effectively in presence of few labeled data is nowadays an open issue. Most of the existing works are devoted to finding the best network for applications for which huge datasets exist, but few attention is given to specific real-setting problems where training data are hard to obtain and therefore out-of-the-box networks may be impossible to be trained. Nonetheless, in recent years, many regularization techniques have been proposed to tackle the problem of overfitting, from data augmentation to early stopping and dropout, besides the traditional weight decay. These techniques used together could help in applying deep learning techniques in the presence of small datasets. In addition to these techniques, fusion with another strong classifier may be considered. </p>

<p>Simultaneously, a criticism that is often made of deep learning methods is the fact that they act like “black-boxes”, making it hard for their users to interpret the obtained results. This limitation is highly relevant when learning from small amounts of data, where a measure of model uncertainty would be particularly important. To this extent Bayesian Neural Networks (BNNs, Bayesian NNs) offer a probabilistic interpretation of deep learning models by inferring distributions over the models’ weights, allowing to measure model uncertainty, but they are usually practically limited. Recently, an ensemble-based method relying on the use of dropout at inference time has been proposed in (Monte Carlo dropout), allowing to obtain several realizations sampled from the same network with randomly dropped-out units at test time, from which a confidence measure on the prediction can be derived. </p>

<p>Following this line of work, we intend to investigate the use of deep learning techniques in presence of small training datasets for specific applications (in our case high-density crowd pedestrian detection). A solution proposed for instance by in the case of hyperspectral data is to reduce the number of weight parameters required to train the model by considering some constraints related to the physical interpretation of the weights. In this work, the type of the data (grayscale images) is not suitable for such prior constraints, we propose the use of an ensemble method that is justified according to two different reasons. Firstly, it acts as another regularization technique to mitigate the risk of overfitting; secondly, it allows us to measure the model confidence about each prediction. To this extent, we propose to work in the context of the Belief Function Theory (BFT) to better leverage the classifier’s unique properties. The evidential framework is indeed able to naturally model the concept of <hi rend="italic">imprecision</hi> in addition to the uncertainty value provided by the classifiers. </p>

<p>We thus propose an evidential Multiple Classifier System (MCS), which is in turn composed by two ensembles of classifiers. The first one, called CNN-ensemble, is an ensemble of convolutional neural networks (CNNs) derived using the Monte Carlo dropout technique. The second one, called SVM-ensemble, is an ensemble of Support Vector Machine (SVM) classifiers trained with different descriptors in an active learning (AL) Query-by-Committee fashion previously proposed in. Specifically, starting from a single sensor input, i.e., an image lattice, we derive two different ensembles based on complementary classifiers. These two ensembles are then considered as different information sources, like virtual sensors. </p>

<p>We apply the proposed Evidential MCS to the difficult application of high-density crowd pedestrian detection for multiple reasons. Indeed, although in the last years, many efforts have been devoted to improve the performance of pedestrian detection, baseline methods cannot be always applied in crowded contexts because of scarce labeled data, and intrinsic differences with respect to the sparse case which may be cause of imprecision in the final detection results. </p>

<p>Pedestrian detection by itself is noticeably one of the most challenging categories of object detection. There exists indeed a large variability in the local and global pedestrians’ appearance, due to the variety of possible body shapes, or different styles and types of clothes and accessories which may alter the silhouettes of the individuals. Besides, in real-world scenarios several people can occupy the same region, partially occluding each other, and this phenomenon becomes more prevalent as the crowd density increases. </p>
</text>
