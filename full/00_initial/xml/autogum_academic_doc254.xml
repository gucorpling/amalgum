<text id="autogum_academic_doc254" title="Improving Basic Natural Language Processing Tools for the Ainu Language" shortTile="improving-basic-natural" author="Karol Nowakowski, Michal Ptaszynski, Fumito Masui, Yoshio Momouchi" type="academic" dateCollected="2019-11-03" sourceURL="https://www.mdpi.com/2078-2489/10/11/329/htm" speakerList="none" speakerCount="0">
<head> 8. Results and Discussion</head><head> 8.1. Transcription Normalization</head>
<p>
Table 11 shows the results of transcription normalization experiments. Transcription normalization based on Kirikae’s lexicon achieved the highest scores for the Y9–13 dataset, which is not surprising, since the dictionary is based on <hi rend="italic">yukar</hi> epics. In the case of JK samples, however, performance with the combined dictionary (JK+KK) was as good as with the JK dictionary only. Furthermore, the combined dictionary achieved the best overall results. In all test configurations the results for texts with original word segmentation retained were slightly better. Relatively low values of recall for normalization in JK samples, observed across all combinations of dictionaries and input text versions, can be explained by a high occurrence of forms transcribed according to non-standard rules modified by Bugaeva et al. in the modernized version of the dictionary, but not included in the list of universal transcription change rules applied in this research, such as ‘ra’→‘r’ (e.g., <hi rend="italic">arapa</hi>→<hi rend="italic">arpa</hi>), ‘ri’→‘r’ (e.g., <hi rend="italic">pirika</hi>→<hi rend="italic">pirka</hi>), ‘ru’→‘r’ (e.g., <hi rend="italic">kuru</hi>→<hi rend="italic">kur</hi>), ‘ro’→‘r’ (e.g., <hi rend="italic">koro</hi>→<hi rend="italic">kor</hi>) or ‘ei’→‘e’ (e.g., <hi rend="italic">reihei</hi>→<hi rend="italic">rehe</hi>). This is due to the fact that these rules are so far only observed in the dictionary of Jinbō and Kanazawa and more importantly, initial tests performed during the development of the algorithm showed that including them in the algorithm can cause errors when processing yukars and other texts. </p>

<head> 8.2. Tokenization</head>
<p>
The results of tokenization experiments are shown in Table 12. Table 13 shows a fragment from Y9–13 (M-SR) before and after segmentation. Similarly to transcription normalization, the tokenization algorithm also performed the best for <hi rend="italic">yukar</hi> stories (Y9–13) when coupled with the <hi rend="italic">Ainu shin-yōshū jiten</hi> (KK). Analogically, for JK samples, the JK dictionary was the best. It shows a weak point of the presented segmentation algorithm: while adding new forms to the lexicon improves its versatility (ability to process texts from different domains), it also increases the number of possible mistakes the tokenizer can make with texts for which the original lexicon had been (nearly) optimal. The combined dictionary performed better than the other two dictionaries on test data unrelated to the training data (Shib. and Muk.), and also achieved the best overall results (F-score). On the other hand, overall recall was higher with the KK dictionary. To some extent this might be explained by the differences in word segmentation between the two dictionaries applied in this research: many expressions (e.g., <hi rend="italic">oro wa</hi>, ’from’ or <hi rend="italic">pet turasi</hi>, ’to go upstream’) written as two separate segments by Kirikae (both in the lexicon part of the <hi rend="italic">Ainu shin-yōshū jiten</hi>, as well as in his modernized transcriptions of the <hi rend="italic">yukar</hi> stories, which we use as the gold standard data), are transcribed as a single unit (<hi rend="italic">orowa</hi>, <hi rend="italic">petturasi</hi>) by Bugaeva et al. Once these forms are added to the lexicon, the word segmentation algorithm, which prefers long tokens over shorter ones, stops applying segmentation to the tokens <hi rend="italic">orowa</hi> and <hi rend="italic">petturasi</hi> (and that causes recall to drop). This phenomenon occurs in the opposite direction as well: The only two types of tokenization errors made in the JK samples (O/M) when the combined dictionary was used, but not with the JK dictionary, were both of this type—the expressions transcribed by Bugaeva et al. as <hi rend="italic">somo ki</hi> (’do not’) and <hi rend="italic">te ta</hi> (’here’) are listed as <hi rend="italic">somoki</hi> and <hi rend="italic">teta</hi> in the <hi rend="italic">Ainu shin-yōshū jiten</hi>. Scores achieved by the tokenizer on texts with original word boundaries retained (Y9–13 (O/M) and JK samples (O/M)) were higher than with spaces removed. This means that the original word segmentation, even if it causes some errors (as with the word <hi rend="italic">tuyka</hi>—see Section 6.2), still supports tokenization rather than hindering it. </p>
</text>
