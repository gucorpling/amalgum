<?xml version="1.0" ?><text author="Jennifer Vandoni, Sylvie  Le Hégarat-Mascle, Emanuel Aldea" dateCollected="2019-11-03" id="autogum_academic_doc542" shortTile="augmenting-deep-learning" sourceURL="https://www.mdpi.com/1424-8220/19/21/4664/htm" speakerCount="0" speakerList="none" title="Augmenting Deep Learning Performance in an Evidential Multiple Classifier System" type="academic">
<head>
1
.
Introduction
</head>
<p>
Even
though
deep
learning
solutions
tend
to
outperform
the
other
supervised
learning
techniques
when
trained
on
large
amounts
of
data
,
applying
them
effectively
in
presence
of
few
labeled
data
is
nowadays
an
open
issue
.
Most
of
the
existing
works
are
devoted
to
finding
the
best
network
for
applications
for
which
huge
datasets
exist
,
but
few
attention
is
given
to
specific
real-setting
problems
where
training
data
are
hard
to
obtain
and
therefore
out-of-the-box
networks
may
be
impossible
to
be
trained
.
Nonetheless
,
in
recent
years
,
many
regularization
techniques
have
been
proposed
to
tackle
the
problem
of
overfitting
,
from
data
augmentation
to
early
stopping
and
dropout
,
besides
the
traditional
weight
decay
.
These
techniques
used
together
could
help
in
applying
deep
learning
techniques
in
the
presence
of
small
datasets
.
In
addition
to
these
techniques
,
fusion
with
another
strong
classifier
may
be
considered
.
</p>
<p>
Simultaneously
,
a
criticism
that
is
often
made
of
deep
learning
methods
is
the
fact
that
they
act
like
“
black-boxes
”
,
making
it
hard
for
their
users
to
interpret
the
obtained
results
.
This
limitation
is
highly
relevant
when
learning
from
small
amounts
of
data
,
where
a
measure
of
model
uncertainty
would
be
particularly
important
.
To
this
extent
Bayesian
Neural
Networks
(
BNNs
,
Bayesian
NNs
)
offer
a
probabilistic
interpretation
of
deep
learning
models
by
inferring
distributions
over
the
models
’
weights
,
allowing
to
measure
model
uncertainty
,
but
they
are
usually
practically
limited
.
Recently
,
an
ensemble-based
method
relying
on
the
use
of
dropout
at
inference
time
has
been
proposed
in
(
Monte
Carlo
dropout
)
,
allowing
to
obtain
several
realizations
sampled
from
the
same
network
with
randomly
dropped-out
units
at
test
time
,
from
which
a
confidence
measure
on
the
prediction
can
be
derived
.
</p>
<p>
Following
this
line
of
work
,
we
intend
to
investigate
the
use
of
deep
learning
techniques
in
presence
of
small
training
datasets
for
specific
applications
(
in
our
case
high-density
crowd
pedestrian
detection
)
.
A
solution
proposed
for
instance
by
in
the
case
of
hyperspectral
data
is
to
reduce
the
number
of
weight
parameters
required
to
train
the
model
by
considering
some
constraints
related
to
the
physical
interpretation
of
the
weights
.
In
this
work
,
the
type
of
the
data
(
grayscale
images
)
is
not
suitable
for
such
prior
constraints
,
we
propose
the
use
of
an
ensemble
method
that
is
justified
according
to
two
different
reasons
.
Firstly
,
it
acts
as
another
regularization
technique
to
mitigate
the
risk
of
overfitting
;
secondly
,
it
allows
us
to
measure
the
model
confidence
about
each
prediction
.
To
this
extent
,
we
propose
to
work
in
the
context
of
the
Belief
Function
Theory
(
BFT
)
to
better
leverage
the
classifier
’s
unique
properties
.
The
evidential
framework
is
indeed
able
to
naturally
model
the
concept
of
<hi rend="italic">
imprecision
</hi>
in
addition
to
the
uncertainty
value
provided
by
the
classifiers
.
</p>
<p>
We
thus
propose
an
evidential
Multiple
Classifier
System
(
MCS
)
,
which
is
in
turn
composed
by
two
ensembles
of
classifiers
.
The
first
one
,
called
CNN-ensemble
,
is
an
ensemble
of
convolutional
neural
networks
(
CNNs
)
derived
using
the
Monte
Carlo
dropout
technique
.
The
second
one
,
called
SVM-ensemble
,
is
an
ensemble
of
Support
Vector
Machine
(
SVM
)
classifiers
trained
with
different
descriptors
in
an
active
learning
(
AL
)
Query-by-Committee
fashion
previously
proposed
in
.
Specifically
,
starting
from
a
single
sensor
input
,
i.
e.
,
an
image
lattice
,
we
derive
two
different
ensembles
based
on
complementary
classifiers
.
These
two
ensembles
are
then
considered
as
different
information
sources
,
like
virtual
sensors
.
</p>
<p>
We
apply
the
proposed
Evidential
MCS
to
the
difficult
application
of
high-density
crowd
pedestrian
detection
for
multiple
reasons
.
Indeed
,
although
in
the
last
years
,
many
efforts
have
been
devoted
to
improve
the
performance
of
pedestrian
detection
,
baseline
methods
cannot
be
always
applied
in
crowded
contexts
because
of
scarce
labeled
data
,
and
intrinsic
differences
with
respect
to
the
sparse
case
which
may
be
cause
of
imprecision
in
the
final
detection
results
.
</p>
<p>
Pedestrian
detection
by
itself
is
noticeably
one
of
the
most
challenging
categories
of
object
detection
.
There
exists
indeed
a
large
variability
in
the
local
and
global
pedestrians
’
appearance
,
due
to
the
variety
of
possible
body
shapes
,
or
different
styles
and
types
of
clothes
and
accessories
which
may
alter
the
silhouettes
of
the
individuals
.
Besides
,
in
real-world
scenarios
several
people
can
occupy
the
same
region
,
partially
occluding
each
other
,
and
this
phenomenon
becomes
more
prevalent
as
the
crowd
density
increases
.
</p>
</text>