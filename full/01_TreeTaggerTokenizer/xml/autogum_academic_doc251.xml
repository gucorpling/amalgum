<?xml version="1.0" ?><text author="Yong Huh" dateCollected="2019-11-03" id="autogum_academic_doc251" shortTile="hierarchical-semantic" sourceURL="https://www.mdpi.com/2220-9964/8/11/479/htm" speakerCount="0" speakerList="none" title="Hierarchical Semantic Correspondence Analysis on Feature Classes between Two Geospatial Datasets Using a Graph Embedding Method" type="academic">
<head>
2
.
Laplacian
Graph
Embedding
</head>
<head>
2.1
.
One-dimensional
Embedding
</head>
<p>
In
this
paper
,
we
assume
an
undirected
and
connected
graph
.
The
graph
is
represented
by
sets
of
vertices
and
edges
.
Given
a
weighted
graph
,
edge
weights
are
represented
as
a
weight
matrix
.
One-dimensional
graph
embedding
finds
a
configuration
of
embedded
vertices
in
one-dimensional
space
,
such
that
the
vertices
’
proximities
from
the
edge
weights
are
preserved
as
the
embedded
vertices
’
distances
.
Assuming
each
entry
of
a
column
vector
as
coordinates
of
the
embedded
vertices
,
this
problem
can
be
solved
through
minimization
of
the
following
objective
function
.
(
1
)
</p>
<p>
This
function
could
be
minimized
when
vertices
<hi rend="italic">
i
</hi>
and
<hi rend="italic">
j
</hi>
with
large
are
embedded
at
close
coordinates
,
whereas
vertices
with
small
are
embedded
into
distant
coordinates
.
In
this
study
,
this
mathematical
property
is
applied
as
follows
:
feature
classes
(
e.
g.
,
land-use
category
and
record
)
with
a
greater
degree
of
object
sharing
have
close
coordinates
in
their
embedding
space
and
feature
classes
with
a
lesser
degree
of
object
sharing
have
distant
coordinates
.
Equation
(
1
)
can
be
expressed
in
a
matrix
operation
form
with
a
Laplacian
matrix
,
and
can
be
represented
as
Equation
(
2
)
.
(
2
)
where
,
the
Laplacian
matrix
is
defined
as
Equation
(
3
)
with
a
vertex
degree
matrix
whose
diagonal
entries
are
obtained
as
and
the
remaining
entries
are
0
.
(
3
)
</p>
<p>
Now
,
the
problem
can
be
changed
to
find
a
vector
that
minimizes
,
and
can
be
represented
as
Equation
(
4
)
.
(
4
)
</p>
<p>
Since
the
value
of
is
vulnerable
to
the
scaling
of
a
vector
,
a
constraint
is
imposed
to
remove
any
such
arbitrary
scaling
effect
.
The
diagonal
matrix
provides
weights
on
the
vertices
,
so
that
the
higher
is
,
the
more
important
is
that
vertex
.
Equation
(
4
)
with
the
constraint
can
be
solved
by
the
Lagrange
multiplier
method
as
in
Equations
(
5
)
–
(
7
)
.
(
5
)
(
6
)
(
7
)
</p>
<p>
Thus
,
the
solution
of
one-dimensional
embedding
,
,
is
obtained
by
solving
the
eigenproblem
.
However
,
according
to
the
rank
of
matrix
,
there
could
be
more
than
one
eigenvector
.
In
the
field
of
graph
spectral
theory
,
the
eigenvector
corresponding
to
the
smallest
eigenvalue
larger
than
0
is
the
proven
solution
,
which
is
called
a
Fiedler
vector
.
Thus
,
the
coordinates
of
vertices
in
one-dimensional
embedding
are
obtained
as
components
of
the
Fiedler
vector
as
represented
by
Equation
(
7
)
.
</p>
<head>
2.2
.
k-dimensional
Embedding
</head>
<p>
Now
,
consider
k-dimensional
graph
embedding
.
These
embedded
coordinates
are
represented
as
an
matrix
,
so
that
the
<hi rend="italic">
i
</hi>
th
row
of
,
,
contains
the
k-dimensional
coordinates
of
vertex
.
Now
,
an
objective
function
is
defined
as
Equation
(
8
)
with
the
constraint
,
.
(
8
)
</p>
<p>
Sameh
and
Wisniewski
proved
that
the
solution
to
this
trace
minimization
problem
is
obtained
by
the
k-eigenvectors
of
that
correspond
to
its
smallest
eigenvalues
other
than
0
.
Thus
,
the
solution
of
Equation
(
8
)
is
obtained
by
a
matrix
,
where
represents
an
eigenvector
corresponding
to
eigenvalue
under
the
condition
.
</p>
<p>
However
,
the
constraint
normalizes
the
scales
of
the
coordinates
in
each
dimension
.
Thus
,
it
is
necessary
to
rescale
them
according
to
each
dimension
’s
relative
importance
.
Sameh
and
Wisniewski
also
proved
that
the
minimum
value
of
in
Equation
(
8
)
equals
the
sum
of
the
corresponding
eigenvalues
,
as
shown
by
Equation
(
9
)
.
(
9
)
</p>
<p>
Accordingly
,
we
can
assume
the
eigenvalue
as
the
amount
of
either
the
penalty
or
the
cost
caused
by
the
<hi rend="italic">
i
</hi>
th
dimensional
space
in
the
embedding
problem
.
So
,
when
,
it
is
appropriate
to
apply
more
weight
to
than
in
measuring
the
proximity
for
a
clustering
analysis
.
Based
on
these
mathematical
properties
,
we
determined
the
embedded
coordinates
as
Equation
(
10
)
,
because
the
increase
in
distance
is
proportional
to
that
of
the
root-squared
coordinate
difference
.
(
10
)
</p>
</text>