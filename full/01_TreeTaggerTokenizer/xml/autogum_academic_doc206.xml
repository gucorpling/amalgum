<?xml version="1.0" ?><text author="Xilin Yu, Zhenning Mei, Chen Chen, Wei Chen" dateCollected="2019-11-03" id="autogum_academic_doc206" shortTile="ranking-power-spectra" sourceURL="https://www.mdpi.com/1099-4300/21/11/1057/htm" speakerCount="0" speakerList="none" title="Ranking Power Spectra: A Proof of Concept" type="academic">
<head>
1
.
Introduction
</head>
<p>
Many
real-world
signals
including
physiological
signals
are
irregular
in
some
aspects
.
They
are
neither
purely
periodic
nor
can
they
be
expressed
by
an
analytic
formula
.
The
inherent
irregularities
imply
the
uncertainty
during
the
evolution
of
the
underlying
process
from
which
the
signals
are
observed
.
The
uncertainty
enables
information
transfer
but
also
limits
the
predictability
of
those
signals
.
The
unpredictability
of
signal
in
time
domain
makes
researchers
have
to
toil
in
frequency
domain
.
Fourier
transform
(
FT
)
bridges
signals
in
original
space
(
time
domain
)
with
their
representations
in
dual
space
(
frequency
domain
)
by
decomposing
a
signal
satisfying
some
weak
constraints
into
infinitely
many
periodic
components
,
which
has
numerous
applications
in
signal
processing
.
</p>
<p>
For
most
of
the
real-world
applications
,
finite
samples
drawn
(
usually
evenly
spaced
)
from
a
continuous
random
process
cannot
give
us
full
information
about
the
process
’
evolution
but
only
a
discrete
depiction
.
FT
was
adapted
into
discrete
Fourier
transform
(
DFT
)
for
such
scenarios
.
Moreover
,
line
spectrum
wherein
the
total
energy
of
the
signal
distributes
on
only
few
frequency
components
is
rarely
encountered
among
physiological
signals
due
to
the
inherent
irregularities
therein
.
</p>
<p>
To
characterize
the
irregularity
of
digital
signals
in
frequency
domain
,
spectral
entropy
is
introduced
analogous
to
the
Shannon
entropy
in
information
theory
.
The
estimations
on
the
frequency
grid
are
firstly
divided
by
the
total
power
,
and
then
,
a
list
of
proxies
in
the
form
of
probabilities
whose
sum
is
1
is
obtained
.
Then
,
the
Shannon
entropy
formula
,
which
is
the
negative
sum
of
probability-weighted
log
probabilities
,
map
those
proxies
into
a
quantity
representing
the
irregularity
of
energy
distribution
on
frequency
domain
.
Under
this
perspective
,
a
flat
spectrum
has
maximal
spectral
entropy
,
and
the
spectrum
of
a
single
frequency
signal
has
minimal
spectral
entropy
,
which
is
zero
.
Spectral
entropy
has
been
applied
in
diverse
areas
,
including
endpoint
detection
in
speech
segmentation
and
spectrum
sensing
in
cognitive
radio
.
Moreover
,
it
has
also
served
as
the
base
of
a
famous
inductive
bias
,
maximum
entropy
,
which
is
widely
adopted
for
spectrum
estimation
of
some
kinds
of
physiological
signals
like
electroencephalogram
(
EEG
)
.
</p>
<p>
Although
spectral
entropy
is
well-defined
and
can
be
computed
efficiently
by
Fast
Fourier
Transform
(
FFT
)
,
it
is
difficult
to
relate
spectral
entropy
with
other
interpretable
properties
of
interest
of
original
signal
,
especially
when
taking
no
account
of
the
overwhelming
endorsement
from
its
counterpart
(
information
entropy
)
,
which
is
the
foundational
concept
in
information
theory
which
quantifies
the
uncertainty
.
Furthermore
,
it
is
apparent
that
the
spectral
entropy
ignored
the
order
information
since
the
power
estimations
are
arranged
on
the
frequency
grid
with
intrinsic
partial
order
structure
.
Any
permutations
of
these
values
on
the
grid
yields
a
same
spectral
entropy
,
but
obviously
,
the
representations
of
those
signals
in
time
domain
can
look
very
different
.
</p>
<p>
The
motivation
to
incorporate
the
order
information
carried
by
the
power
spectrum
is
guided
by
the
following
belief
.
The
normal
operations
of
any
system
(
biological/electromechanical
,
etc.
)
are
impossible
without
the
proper
processing
of
information
through
some
physical/chemical
process
.
It
could
be
the
signaling
between
different
modules
within
the
system
or
the
communications
between
the
system
as
a
whole
and
the
external
environment
.
Information
transfers
happening
in
those
scenarios
are
accomplished
with
the
help
of
carrier
signals
of
particular
forms
with
nontrivial
structures
in
their
spectra
.
Moreover
,
only
limited
frequency
precision
of
the
control
and
recognition
of
those
signals
is
practical
for
real
systems
.
Therefore
,
it
is
unreasonable
for
well-designed
artificial
systems
or
natural
systems
that
have
gone
through
long-term
evolution
to
arrange
the
internal
signals
responsible
for
different
functions
close
with
each
other
in
frequency
domain
within
a
certain
time
window
.
Otherwise
,
the
efficient
transfer
of
information
could
be
degraded
,
and
frequency
divided
multiplex
in
modern
communication
systems
can
be
considered
as
a
living
example
of
this
belief
.
</p>
<p>
Therefore
,
if
we
use
power
estimations
on
the
frequency
grid
as
proxies
of
the
intensities
of
activities
corresponding
to
those
frequencies
,
it
seems
reasonable
to
infer
that
the
energy
distributed
on
neighboring
rather
than
remote
frequency
grids
is
more
likely
caused
by
the
very
same
function
.
The
alpha
band
activities
(
8
–
13
Hz
)
which
can
be
interrupted
by
visual
perception
tasks
in
human
’s
EEG
is
one
of
the
examples
.
To
sum
up
,
we
want
to
develop
a
metric
to
characterize
the
aforementioned
structural
irregularities
of
the
power
spectra
,
that
is
,
how
the
frequency
components
of
different
intensities
in
a
spectrum
close
to
each
other
instead
of
what
is
captured
in
spectral
entropy
,
which
is
how
the
intensities
of
frequency
components
are
distributed
no
matter
their
locations
in
frequency
domain
.
It
was
supposed
to
assign
a
larger
value
to
a
signal
wherein
the
frequency
components
having
similar
intensities
are
distributed
far
apart
from
,
rather
than
close
to
,
each
other
.
In
addition
,
the
similarities
of
intensities
can
be
reflected
(
partially
and
heuristically
)
by
the
relative
order
of
power
estimates
on
discrete
frequency
grid
.
That
is
why
the
order
information
in
the
spectrum
can
shed
new
light
on
the
structure
aspects
of
signal
and
how
the
order
information
is
incorporated
into
our
analysis
.
</p>
</text>