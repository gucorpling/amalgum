<?xml version="1.0" ?><text author="Marco Leo, Pierluigi CarcagnÃ¬, Cosimo Distante, Pier  Luigi Mazzeo, Paolo Spagnolo, Annalisa Levante, Serena Petrocchi, Flavia Lecciso" dateCollected="2019-11-03" id="autogum_academic_doc248" shortTile="computational-analysis" sourceURL="https://www.mdpi.com/2076-3417/9/21/4542/htm" speakerCount="0" speakerList="none" title="Computational Analysis of Deep Visual Data for Quantifying Facial Expression Production" type="academic">
<head>
5
.
Experimental
Results
</head>
<p>
This
section
reports
experimental
outcomes
gathered
by
processing
acquired
videos
by
the
algorithmic
pipeline
described
in
Section
3
.
In
particular
,
a
modeling
window
s
and
an
observation
window
s
were
used
.
The
observation
window
depends
on
the
experimental
setting
.
The
interval
between
two
consecutive
requests
has
been
set
to
4
s
by
the
clinicians
.
This
means
that
the
caregiver
has
to
wait
4
s
before
moving
to
the
following
request
for
facial
expression
.
The
modeling
window
was
consequently
set
to
half
of
the
observation
window
since
lower
values
were
experimentally
proved
to
be
not
sufficient
to
model
the
neutral
expression
whereas
higher
values
could
include
the
offset
of
the
previous
facial
expression
.
The
experimental
proofs
were
carried
out
in
different
phases
.
In
the
first
phase
,
videos
related
to
the
TD
children
were
processed
and
quantitative
comparison
with
the
annotations
provided
by
professionals
was
then
performed
.
In
the
second
phase
,
the
videos
related
to
the
ASD
children
were
processed
and
outputs
were
subsequently
compared
with
human
annotations
.
As
a
final
experimental
phase
,
outcomes
extracted
on
TD
and
ASD
groups
were
put
together
to
draw
some
conclusions
from
the
different
distribution
of
related
numerical
values
.
</p>
<head>
5.1
.
Assessment
on
TD
Children
</head>
<p>
In
the
first
experimental
phase
,
production
scores
on
the
group
of
TD
children
were
computed
and
their
graphic
representations
are
reported
in
<figure>
Figure
2
</figure>
,
<figure>
Figure
3
</figure>
,
<figure>
Figure
4
</figure>
and
<figure>
Figure
5
</figure>
.
Please
be
aware
that
the
highest
scores
were
kept
at
a
value
of
1500
in
order
to
increase
graph
readability
.
</p>
<p>
It
is
worth
to
point
out
that
scores
come
from
negative
logarithmic
functions
of
likelihood
values
(
see
Equation
(
4
)
)
.
When
the
likelihood
values
become
very
close
to
zero
(
in
the
case
of
a
modification
of
action
unit
during
a
proper
facial
expression
production
)
related
logarithmic
functions
tend
to
very
high
values
.
The
outcomes
greater
than
1500
are
equivalent
to
probability
values
so
small
that
can
be
considered
as
0
(
and
their
logarithms
kept
as
a
large
constant
)
for
the
considered
application
purposes
.
In
addition
,
the
figures
have
a
different
scale
on
the
axes
since
the
gathered
scores
have
a
more
uniform
distribution
when
related
to
the
upper
face
part
than
when
related
to
the
lower
face
part
.
This
is
not
surprising
since
the
use
of
the
upper
face
part
in
emotion
production
is
more
difficult
and
then
this
can
lead
to
man
different
levels
of
ability
.
For
the
lower
face
part
,
when
children
start
reacting
to
the
request
usually
their
production
level
goes
in
saturation
to
the
maximum
allowed
score
.
As
a
consequence
,
the
<hi rend="italic">
x
</hi>
-axis
has
a
larger
scale
to
point
out
that
.
</p>
<p>
In
figures
,
black
circles
refer
to
cases
in
which
the
team
of
psychologists
labeled
facial
expressions
as
compliant
with
the
supplied
request
(
i.
e.
,
expressions
correctly
performed
by
the
child
)
,
whereas
red
circles
refer
to
cases
in
which
the
professionals
labeled
the
related
facial
expressions
as
not
compliant
with
the
supplied
requests
(
i.
e.
,
expressions
not
performed
by
the
child
)
.
At
first
glance
,
it
is
quite
clear
that
highest
scores
were
properly
associated
with
occurrences
that
professionals
annotated
as
expression
performed
whereas
lowest
scores
were
associated
with
occurrences
that
professionals
annotated
as
expressions
not
performed
.
Going
into
details
,
it
is
of
interest
to
observe
that
,
in
correspondence
of
some
requests
of
the
happy
face
that
psychologists
annotated
as
performed
,
the
automatic
system
gave
low
outcomes
(
either
for
lower
or
upper
face
part
)
.
This
is
the
case
,
for
example
,
of
the
two
black
spots
that
are
close
to
the
origin
of
the
reference
system
in
<figure>
Figure
2
</figure>
.
</p>
<p>
This
evident
misalignment
between
manual
annotations
and
automatic
scores
depended
on
a
wrong
positioning
of
facial
landmarks
due
to
occlusions
of
the
mouth
(
and
deformation
of
cheeks
and
consequently
of
eye
regions
)
caused
by
the
hands
of
the
child
touching
his
face
.
Concerning
sad
expression
there
were
,
once
again
,
some
misalignment
occurred
in
case
of
mouth
occlusion
(
resulting
in
low
scores
for
lower
face
part
in
<figure>
Figure
3
</figure>
)
but
,
in
addition
,
there
were
also
some
occurrences
(
manually
annotated
as
performed
)
that
experienced
low
scores
only
for
lower
face
part
(
with
very
high
scores
for
upper
face
part
instead
)
.
These
happened
since
,
in
correspondence
to
the
requests
of
sad
expression
,
some
children
occluded
the
mouth
but
without
affecting
the
upper
face
part
.
</p>
<p>
Similar
conclusions
can
be
drawn
for
some
spots
corresponding
to
requests
of
fear
expressions
(
<figure>
Figure
4
</figure>
)
whereas
this
problem
was
never
encountered
during
requests
of
anger
expression
(
<figure>
Figure
5
</figure>
)
.
</p>
</text>