<?xml version="1.0" ?><text author="Yang Hu, Cees de Laat, Zhiming Zhao" dateCollected="2019-11-03" id="autogum_academic_doc228" shortTile="optimizing-service" sourceURL="https://www.mdpi.com/2076-3417/9/21/4663/htm" speakerCount="0" speakerList="none" title="Optimizing Service Placement for Microservice Architecture in Clouds" type="academic">
<head>
1
.
Introduction
</head>
<p>
Microservice
architecture
is
a
new
trend
rising
fast
for
application
development
,
as
it
enhances
flexibility
to
incorporate
different
technologies
,
it
reduces
complexity
by
using
lightweight
and
modular
services
,
and
it
improves
overall
scalability
and
resilience
of
the
system
.
In
the
definition
(
Microservices
:
<ref target="https://martinfowler.com/tags/microservices.html">
https://martinfowler.com/tags/microservices.html
</ref>
)
,
the
microservice
architectural
style
is
an
approach
to
develop
a
single
application
as
a
suite
of
small
services
,
each
running
in
its
own
process
and
communicating
with
lightweight
mechanisms
(
e.
g.
,
HTTP
resource
API
)
.
The
application
then
is
composed
of
a
number
of
services
(
service-based
application
)
that
work
cohesively
to
provide
complex
functionalities
.
Due
to
the
advantages
of
microservices
architecture
,
many
developers
intend
to
transform
traditional
monolithic
applications
into
service-based
applications
.
For
instance
,
an
online
shopping
application
could
be
basically
divided
into
product
service
,
cart
service
,
and
order
service
,
which
can
greatly
improve
the
productivity
,
agility
,
and
resilience
of
the
application
.
However
,
it
also
brings
challenges
.
When
deploying
a
service-based
application
in
clouds
,
the
scheduler
has
to
carefully
schedule
each
service
,
which
may
have
diverse
resource
demands
,
on
distributed
compute
clusters
.
Furthermore
,
the
network
communication
between
different
services
needs
to
be
handled
well
,
as
the
communication
conditions
significantly
influence
the
quality
of
service
(
e.
g.
,
the
response
time
of
a
service
)
.
Ensuring
the
desired
performance
of
service-based
applications
,
especially
the
network
performance
between
the
involved
services
,
becomes
increasingly
important
.
</p>
<p>
In
general
,
service-based
applications
involve
numerous
distributed
and
complex
services
which
usually
require
more
computing
resources
beyond
single
machine
capability
.
Therefore
,
a
cluster
of
networked
machines
or
cloud
computing
platforms
(
e.
g.
,
Amazon
EC2
(
Amazon
EC2
:
<ref target="https://aws.amazon.com">
https://aws.amazon.com
</ref>
)
,
Microsoft
Azure
(
Microsoft
Azure
:
<ref target="https://azure.microsoft.com">
https://azure.microsoft.com
</ref>
)
,
or
Google
Cloud
Platform
(
Google
Cloud
Platform
:
<ref target="https://cloud.google.com">
https://cloud.google.com
</ref>
)
)
are
generally
leveraged
to
run
service-based
applications
.
More
importantly
,
containers
are
emerging
as
the
disruptive
technology
for
effectively
encapsulating
runtime
contexts
of
software
components
and
services
,
which
significantly
improves
portability
and
efficiency
of
deploying
applications
in
clouds
.
When
deploying
a
service-based
application
in
clouds
,
several
essential
aspects
have
to
be
taken
into
account
.
First
,
services
involved
in
the
application
often
have
diverse
resource
demands
,
such
as
CPU
,
memory
and
disk
.
The
underlying
machine
has
to
ensure
sufficient
resources
to
run
each
service
and
at
the
same
time
provide
cohesive
functionalities
.
Efficient
resource
allocation
to
each
service
is
difficult
,
while
it
becomes
more
challenging
when
the
cluster
consists
of
heterogeneous
machines
.
Second
,
services
involved
in
the
application
often
have
traffic
demands
among
them
due
to
data
communication
,
which
require
meticulous
treatment
.
Poor
handling
of
the
traffic
demands
can
result
in
severe
performance
degradation
,
as
the
response
time
of
a
service
is
directly
affected
by
its
traffic
situation
.
Considering
the
traffic
demands
,
an
intuitive
solution
is
to
place
the
services
that
have
large
traffic
demands
among
them
on
the
same
machine
,
which
can
achieve
intra-machine
communication
and
reduce
inter-machine
traffic
.
However
,
such
services
cannot
all
be
co-located
on
one
machine
due
to
limited
resource
capacities
.
Hence
,
placement
of
service-based
applications
is
quite
complicated
in
clouds
.
In
order
to
achieve
a
desired
performance
of
a
service-based
application
,
cluster
schedulers
have
to
carefully
place
each
service
of
the
application
with
respect
to
the
resource
demands
and
traffic
demands
.
</p>
<p>
Recent
cluster
scheduling
methods
mainly
focus
on
the
cluster
resource
efficiency
or
job
completion
time
of
batch
workloads
.
For
instance
,
Tetris
,
a
multi-resource
cluster
scheduler
,
adapts
heuristics
for
the
multi-dimensional
bin
packing
problem
to
efficiently
pack
tasks
on
multi-resource
cluster
.
Firmament
,
a
centralized
cluster
scheduler
,
can
make
high-quality
placement
decisions
on
large-scale
clusters
via
a
min-cost
max-flow
optimization
.
Unfortunately
,
these
solutions
would
face
difficulties
for
handling
service-based
applications
,
as
they
ignore
the
traffic
demands
when
making
placement
decisions
.
Some
other
research
works
concentrate
on
composite
Software
as
a
service
(
SaaS
)
placement
problem
,
which
try
to
minimize
the
total
execution
time
for
composite
SaaS
.
However
,
they
only
focus
on
a
set
of
predefined
service
components
for
the
application
placement
.
For
traffic-aware
scheduling
,
relevant
research
solutions
are
proposed
to
handle
virtual
machine
(
VM
)
placement
problem
,
which
aims
to
optimize
network
resource
usage
over
the
cluster
.
However
,
these
solutions
rely
on
a
certain
network
topology
,
while
most
of
existing
cluster
schedulers
are
agnostic
to
network
topology
.
In
particular
,
it
is
hard
to
get
the
network
topology
information
when
deploying
a
service-based
application
on
a
virtual
infrastructure
.
</p>
</text>